{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, lag, excluded_name=[]):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):        \n",
    "        folder = os.path.join(directory, foldername)\n",
    "        # print(folder)\n",
    "        if str(lag) in folder:\n",
    "            # print(os.listdir(folder))\n",
    "            for name in os.listdir(folder):\n",
    "                if name in excluded_name:\n",
    "                    # print(name)\n",
    "                    continue\n",
    "                filename = os.path.join(folder, name)\n",
    "                # print(filename)\n",
    "                for files in os.listdir(filename):\n",
    "                    rel_path = os.path.join(filename, files)\n",
    "                    # print(rel_path)\n",
    "                    temp_label = folder\n",
    "                    if \"autism\" in temp_label:\n",
    "                        label = 'autism'\n",
    "                    else:\n",
    "                        label = 'normal'\n",
    "\n",
    "                    temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                    rwb = np.load(rel_path)\n",
    "                    rwb.astype(np.float64).reshape(-1,1)\n",
    "                                    \n",
    "                    temp_data.loc[0, \"data\"] = rwb\n",
    "                    temp_data['label'] = label\n",
    "                    data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"autism\": 1, \"normal\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_value(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "    missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "    clean_data = data.drop(index=data.index[missing_indices])\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "# rwb = np.load(\"datasets/features/rwb/segment_1 seconds/autism_256/bader/Bader_segment_100.csv_bispectrum.npy\")\n",
    "# rwb.astype(np.float64).reshape(-1,1)\n",
    "# temp_data.loc[0, \"data\"] = rwb\n",
    "# temp_data['label'] = \"autism\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data, des_path):\n",
    "    if not os.path.exists(des_path):\n",
    "        os.makedirs(des_path)\n",
    "    data.save(des_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, train_split: float):\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "        data['data'],\n",
    "        data[['label', 'label_map']],\n",
    "        train_size=train_split,\n",
    "        stratify=data['label_map']\n",
    "    )\n",
    "\n",
    "    train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "    test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "    train_df[\"data\"] = train_x\n",
    "    train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "    test_df[\"data\"] = test_x\n",
    "    test_df[['label', 'label_map']] = test_y\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    # loading extracted feature & label\n",
    "    # x = get_dataset(path, lag, excluded_name)\n",
    "\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = data[\"label_map\"].values\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list))\n",
    "\n",
    "    # train_size = int(train_split * len(labels_list))  \n",
    "    # test_size = len(labels_list) - train_size  \n",
    "\n",
    "    # train_dataset = dataset.take(train_size)\n",
    "    # test_dataset = dataset.skip(train_size)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "#         data['data'],\n",
    "#         data[['label', 'label_map']],\n",
    "#         train_size=0.8,\n",
    "#         stratify=data['label_map']\n",
    "#     )\n",
    "\n",
    "# train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "# test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "# train_df[\"data\"] = train_x\n",
    "# train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "# test_df[\"data\"] = test_x\n",
    "# test_df[['label', 'label_map']] = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = [\"zyad\"]\n",
    "# data = get_dataset(data_dir, 256, excluded_name=excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = data[\"data\"].values\n",
    "# series_list = np.vstack(temp)\n",
    "# series_list = series_list.reshape(-1, 96, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/features/rwb/segment_1 seconds\"\n",
    "\n",
    "train_dir = \"datasets/tf_batch/rwb/segment_1 seconds/train\"\n",
    "test_dir = \"datasets/tf_batch/rwb/segment_1 seconds/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = [\"zyad\"]\n",
    "# data = get_dataset(data_dir, 256, excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_list = np.vstack(data[\"data\"].values)\n",
    "# labels_list = data[\"label_map\"].values    \n",
    "# missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "# clean_data = data.drop(index=data.index[missing_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1552, 3)\n",
      "(1552, 3)\n",
      "train:  (1241, 3)\n",
      "test:  (311, 3)\n"
     ]
    }
   ],
   "source": [
    "excluded = [\"zyad\"]\n",
    "train_split = 0.8\n",
    "LAG = [256]\n",
    "\n",
    "for lag in LAG:\n",
    "    data = get_dataset(data_dir, lag, excluded)\n",
    "    print(data.shape)\n",
    "    data = remove_missing_value(data)\n",
    "    print(data.shape)\n",
    "    train_data, test_data = get_train_test(data, train_split)\n",
    "    print(\"train: \", train_data.shape)\n",
    "    print(\"test: \", test_data.shape)\n",
    "    train_batch = get_batch(train_data)\n",
    "    test_batch = get_batch(test_data)\n",
    "    tf.data.Dataset.save(train_batch, f\"{train_dir}_{lag}\")\n",
    "    tf.data.Dataset.save(test_batch, f\"{test_dir}_{lag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_list = np.vstack(data[\"data\"].values)\n",
    "# labels_list = data[\"label_map\"].values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15434, 96)\n",
      "(15434,)\n"
     ]
    }
   ],
   "source": [
    "# print(series_list.shape)\n",
    "# print(labels_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = remove_missing_value(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "# clean_series_list = np.delete(series_list, missing_indices, axis=0)\n",
    "# clean_labels_list = np.delete(labels_list, missing_indices, axis=0)\n",
    "\n",
    "# print(clean_series_list.shape)\n",
    "# print(clean_labels_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11694, 3)\n",
      "(2924, 3)\n"
     ]
    }
   ],
   "source": [
    "# train_data, test_data = get_train_test(data, train_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = get_batch(train_data)\n",
    "# test_batch = get_batch(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset.save(train_batch, f\"{train_dir}_{LAG}\")\n",
    "# tf.data.Dataset.save(test_batch, f\"{test_dir}_{LAG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = tf.data.Dataset.load(\"datasets/tf_batch/rwb/segment_1 seconds/train_256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_batches = []\n",
    "# label_batches = []\n",
    "# for feature_batch, label_batch in train_batch:\n",
    "#     feature_batches.append(feature_batch.numpy())\n",
    "#     label_batches.append(label_batch.numpy())\n",
    "\n",
    "# arrrrr = []\n",
    "\n",
    "# for batch in train_batch:\n",
    "#     arrrrr.append(batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_array = np.concatenate(feature_batches)\n",
    "# label_array = np.concatenate(label_batches)\n",
    "\n",
    "# print(feature_array)\n",
    "# print(label_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load(\"datasets/features/rwb/segment_1 seconds/normal_256/amer/Amer_segment_1.csv_bispectrum.npy\")\n",
    "\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(96,)))\n",
    "    model.add(layers.Reshape((96, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [256]\n",
    "# folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000\n",
    "log_dirs = [f\"train_logs/logs7/RWB_ANN_512_256_Adagrad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 5.2132 - acc: 0.5940\n",
      "Epoch 1: val_acc improved from -inf to 0.66054, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 5.1022 - acc: 0.5952 - val_loss: 0.9426 - val_acc: 0.6605\n",
      "Epoch 2/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 2.9690 - acc: 0.6026\n",
      "Epoch 2: val_acc improved from 0.66054 to 0.69089, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 2.9413 - acc: 0.6045 - val_loss: 0.7446 - val_acc: 0.6909\n",
      "Epoch 3/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 2.1978 - acc: 0.6071\n",
      "Epoch 3: val_acc did not improve from 0.69089\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 2.1967 - acc: 0.6076 - val_loss: 0.6641 - val_acc: 0.6879\n",
      "Epoch 4/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 1.8128 - acc: 0.6091\n",
      "Epoch 4: val_acc did not improve from 0.69089\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 1.8065 - acc: 0.6103 - val_loss: 0.6352 - val_acc: 0.6853\n",
      "Epoch 5/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 1.5176 - acc: 0.6096\n",
      "Epoch 5: val_acc did not improve from 0.69089\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.5125 - acc: 0.6106 - val_loss: 0.6183 - val_acc: 0.6866\n",
      "Epoch 6/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 1.3578 - acc: 0.6185\n",
      "Epoch 6: val_acc did not improve from 0.69089\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 1.3565 - acc: 0.6196 - val_loss: 0.6145 - val_acc: 0.6866\n",
      "Epoch 7/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 1.2083 - acc: 0.6258\n",
      "Epoch 7: val_acc did not improve from 0.69089\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 1.2044 - acc: 0.6264 - val_loss: 0.6099 - val_acc: 0.6836\n",
      "Epoch 8/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 1.1241 - acc: 0.6285\n",
      "Epoch 8: val_acc did not improve from 0.69089\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 1.1233 - acc: 0.6292 - val_loss: 0.6104 - val_acc: 0.6858\n",
      "Epoch 9/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 1.0547 - acc: 0.6302\n",
      "Epoch 9: val_acc did not improve from 0.69089\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 1.0487 - acc: 0.6308 - val_loss: 0.6107 - val_acc: 0.6905\n",
      "Epoch 10/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.9879 - acc: 0.6235\n",
      "Epoch 10: val_acc improved from 0.69089 to 0.69517, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.9889 - acc: 0.6241 - val_loss: 0.6109 - val_acc: 0.6952\n",
      "Epoch 11/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.9310 - acc: 0.6360\n",
      "Epoch 11: val_acc did not improve from 0.69517\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.9321 - acc: 0.6357 - val_loss: 0.6132 - val_acc: 0.6930\n",
      "Epoch 12/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.9101 - acc: 0.6320\n",
      "Epoch 12: val_acc did not improve from 0.69517\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.9097 - acc: 0.6317 - val_loss: 0.6133 - val_acc: 0.6939\n",
      "Epoch 13/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.8802 - acc: 0.6378\n",
      "Epoch 13: val_acc improved from 0.69517 to 0.69902, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.8816 - acc: 0.6368 - val_loss: 0.6133 - val_acc: 0.6990\n",
      "Epoch 14/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.8532 - acc: 0.6362\n",
      "Epoch 14: val_acc improved from 0.69902 to 0.69987, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8523 - acc: 0.6371 - val_loss: 0.6149 - val_acc: 0.6999\n",
      "Epoch 15/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.8170 - acc: 0.6408\n",
      "Epoch 15: val_acc improved from 0.69987 to 0.70073, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8195 - acc: 0.6405 - val_loss: 0.6149 - val_acc: 0.7007\n",
      "Epoch 16/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.8036 - acc: 0.6391\n",
      "Epoch 16: val_acc improved from 0.70073 to 0.70158, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8026 - acc: 0.6393 - val_loss: 0.6152 - val_acc: 0.7016\n",
      "Epoch 17/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.7750 - acc: 0.6523\n",
      "Epoch 17: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.7739 - acc: 0.6522 - val_loss: 0.6149 - val_acc: 0.7016\n",
      "Epoch 18/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.7760 - acc: 0.6390\n",
      "Epoch 18: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7752 - acc: 0.6396 - val_loss: 0.6168 - val_acc: 0.7016\n",
      "Epoch 19/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7481 - acc: 0.6487\n",
      "Epoch 19: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.7481 - acc: 0.6487 - val_loss: 0.6149 - val_acc: 0.6986\n",
      "Epoch 20/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.7438 - acc: 0.6487\n",
      "Epoch 20: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7434 - acc: 0.6484 - val_loss: 0.6157 - val_acc: 0.6986\n",
      "Epoch 21/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.7406 - acc: 0.6439\n",
      "Epoch 21: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7398 - acc: 0.6445 - val_loss: 0.6141 - val_acc: 0.6986\n",
      "Epoch 22/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.7346 - acc: 0.6520\n",
      "Epoch 22: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.7344 - acc: 0.6529 - val_loss: 0.6147 - val_acc: 0.6973\n",
      "Epoch 23/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.7272 - acc: 0.6453\n",
      "Epoch 23: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.7240 - acc: 0.6469 - val_loss: 0.6152 - val_acc: 0.6973\n",
      "Epoch 24/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.7066 - acc: 0.6576\n",
      "Epoch 24: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7088 - acc: 0.6562 - val_loss: 0.6153 - val_acc: 0.6965\n",
      "Epoch 25/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.7018 - acc: 0.6561\n",
      "Epoch 25: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.7004 - acc: 0.6564 - val_loss: 0.6153 - val_acc: 0.6956\n",
      "Epoch 26/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6932 - acc: 0.6643\n",
      "Epoch 26: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6912 - acc: 0.6650 - val_loss: 0.6149 - val_acc: 0.6943\n",
      "Epoch 27/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6918 - acc: 0.6660\n",
      "Epoch 27: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6943 - acc: 0.6660 - val_loss: 0.6155 - val_acc: 0.6943\n",
      "Epoch 28/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6898 - acc: 0.6641\n",
      "Epoch 28: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6883 - acc: 0.6650 - val_loss: 0.6143 - val_acc: 0.6943\n",
      "Epoch 29/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6833 - acc: 0.6581\n",
      "Epoch 29: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6819 - acc: 0.6593 - val_loss: 0.6144 - val_acc: 0.6930\n",
      "Epoch 30/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6757 - acc: 0.6637\n",
      "Epoch 30: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6762 - acc: 0.6627 - val_loss: 0.6144 - val_acc: 0.6939\n",
      "Epoch 31/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6676 - acc: 0.6660\n",
      "Epoch 31: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6666 - acc: 0.6660 - val_loss: 0.6145 - val_acc: 0.6947\n",
      "Epoch 32/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6697 - acc: 0.6716\n",
      "Epoch 32: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6689 - acc: 0.6719 - val_loss: 0.6145 - val_acc: 0.6930\n",
      "Epoch 33/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6605 - acc: 0.6716\n",
      "Epoch 33: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6617 - acc: 0.6709 - val_loss: 0.6144 - val_acc: 0.6943\n",
      "Epoch 34/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6668 - acc: 0.6671\n",
      "Epoch 34: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6647 - acc: 0.6691 - val_loss: 0.6137 - val_acc: 0.6913\n",
      "Epoch 35/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6682 - acc: 0.6661\n",
      "Epoch 35: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6680 - acc: 0.6665 - val_loss: 0.6141 - val_acc: 0.6913\n",
      "Epoch 36/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6524 - acc: 0.6739\n",
      "Epoch 36: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6524 - acc: 0.6739 - val_loss: 0.6139 - val_acc: 0.6926\n",
      "Epoch 37/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.6585 - acc: 0.6687\n",
      "Epoch 37: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6577 - acc: 0.6692 - val_loss: 0.6138 - val_acc: 0.6913\n",
      "Epoch 38/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6551 - acc: 0.6661\n",
      "Epoch 38: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6540 - acc: 0.6661 - val_loss: 0.6140 - val_acc: 0.6917\n",
      "Epoch 39/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.6419 - acc: 0.6795\n",
      "Epoch 39: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6426 - acc: 0.6794 - val_loss: 0.6136 - val_acc: 0.6922\n",
      "Epoch 40/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6500 - acc: 0.6758\n",
      "Epoch 40: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6506 - acc: 0.6761 - val_loss: 0.6132 - val_acc: 0.6922\n",
      "Epoch 41/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6416 - acc: 0.6788\n",
      "Epoch 41: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6422 - acc: 0.6778 - val_loss: 0.6134 - val_acc: 0.6922\n",
      "Epoch 42/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6527 - acc: 0.6715\n",
      "Epoch 42: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6516 - acc: 0.6713 - val_loss: 0.6128 - val_acc: 0.6913\n",
      "Epoch 43/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6426 - acc: 0.6772\n",
      "Epoch 43: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6421 - acc: 0.6778 - val_loss: 0.6124 - val_acc: 0.6913\n",
      "Epoch 44/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.6835\n",
      "Epoch 44: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6362 - acc: 0.6838 - val_loss: 0.6118 - val_acc: 0.6913\n",
      "Epoch 45/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6435 - acc: 0.6783\n",
      "Epoch 45: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6422 - acc: 0.6797 - val_loss: 0.6126 - val_acc: 0.6913\n",
      "Epoch 46/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6348 - acc: 0.6790\n",
      "Epoch 46: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6351 - acc: 0.6802 - val_loss: 0.6124 - val_acc: 0.6909\n",
      "Epoch 47/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6359 - acc: 0.6783\n",
      "Epoch 47: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6361 - acc: 0.6789 - val_loss: 0.6121 - val_acc: 0.6900\n",
      "Epoch 48/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.6273 - acc: 0.6874\n",
      "Epoch 48: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6250 - acc: 0.6886 - val_loss: 0.6116 - val_acc: 0.6909\n",
      "Epoch 49/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.6326 - acc: 0.6828\n",
      "Epoch 49: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6313 - acc: 0.6833 - val_loss: 0.6118 - val_acc: 0.6913\n",
      "Epoch 50/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.6363 - acc: 0.6783\n",
      "Epoch 50: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.6348 - acc: 0.6809 - val_loss: 0.6117 - val_acc: 0.6913\n",
      "Epoch 51/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6300 - acc: 0.6787\n",
      "Epoch 51: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6276 - acc: 0.6795 - val_loss: 0.6119 - val_acc: 0.6909\n",
      "Epoch 52/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6207 - acc: 0.6845\n",
      "Epoch 52: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6205 - acc: 0.6846 - val_loss: 0.6113 - val_acc: 0.6909\n",
      "Epoch 53/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6206 - acc: 0.6867\n",
      "Epoch 53: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6200 - acc: 0.6872 - val_loss: 0.6119 - val_acc: 0.6917\n",
      "Epoch 54/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6226 - acc: 0.6901\n",
      "Epoch 54: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6226 - acc: 0.6901 - val_loss: 0.6116 - val_acc: 0.6913\n",
      "Epoch 55/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6257 - acc: 0.6887\n",
      "Epoch 55: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6243 - acc: 0.6893 - val_loss: 0.6118 - val_acc: 0.6913\n",
      "Epoch 56/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6208 - acc: 0.6870\n",
      "Epoch 56: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6193 - acc: 0.6883 - val_loss: 0.6110 - val_acc: 0.6922\n",
      "Epoch 57/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6267 - acc: 0.6853\n",
      "Epoch 57: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6265 - acc: 0.6867 - val_loss: 0.6103 - val_acc: 0.6930\n",
      "Epoch 58/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6263 - acc: 0.6856\n",
      "Epoch 58: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6242 - acc: 0.6871 - val_loss: 0.6097 - val_acc: 0.6926\n",
      "Epoch 59/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6126 - acc: 0.6897\n",
      "Epoch 59: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6126 - acc: 0.6897 - val_loss: 0.6092 - val_acc: 0.6939\n",
      "Epoch 60/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6094 - acc: 0.6924\n",
      "Epoch 60: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6090 - acc: 0.6927 - val_loss: 0.6088 - val_acc: 0.6935\n",
      "Epoch 61/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.6141 - acc: 0.6948\n",
      "Epoch 61: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6131 - acc: 0.6961 - val_loss: 0.6088 - val_acc: 0.6930\n",
      "Epoch 62/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6066 - acc: 0.6920\n",
      "Epoch 62: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6055 - acc: 0.6940 - val_loss: 0.6079 - val_acc: 0.6930\n",
      "Epoch 63/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6161 - acc: 0.6899\n",
      "Epoch 63: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6154 - acc: 0.6904 - val_loss: 0.6081 - val_acc: 0.6935\n",
      "Epoch 64/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.6884\n",
      "Epoch 64: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6188 - acc: 0.6890 - val_loss: 0.6079 - val_acc: 0.6943\n",
      "Epoch 65/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6054 - acc: 0.6955\n",
      "Epoch 65: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6052 - acc: 0.6954 - val_loss: 0.6072 - val_acc: 0.6939\n",
      "Epoch 66/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.6135 - acc: 0.6923\n",
      "Epoch 66: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6112 - acc: 0.6936 - val_loss: 0.6067 - val_acc: 0.6939\n",
      "Epoch 67/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6038 - acc: 0.6981\n",
      "Epoch 67: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6029 - acc: 0.6991 - val_loss: 0.6065 - val_acc: 0.6943\n",
      "Epoch 68/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5991 - acc: 0.6978\n",
      "Epoch 68: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5990 - acc: 0.6983 - val_loss: 0.6061 - val_acc: 0.6947\n",
      "Epoch 69/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6027 - acc: 0.6991\n",
      "Epoch 69: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6035 - acc: 0.6989 - val_loss: 0.6058 - val_acc: 0.6943\n",
      "Epoch 70/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6063 - acc: 0.6955\n",
      "Epoch 70: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6060 - acc: 0.6958 - val_loss: 0.6055 - val_acc: 0.6935\n",
      "Epoch 71/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6054 - acc: 0.6940\n",
      "Epoch 71: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6048 - acc: 0.6956 - val_loss: 0.6054 - val_acc: 0.6943\n",
      "Epoch 72/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6070 - acc: 0.6954\n",
      "Epoch 72: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6057 - acc: 0.6964 - val_loss: 0.6048 - val_acc: 0.6960\n",
      "Epoch 73/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6050 - acc: 0.6919\n",
      "Epoch 73: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6043 - acc: 0.6932 - val_loss: 0.6044 - val_acc: 0.6952\n",
      "Epoch 74/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6048 - acc: 0.7005\n",
      "Epoch 74: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6050 - acc: 0.7002 - val_loss: 0.6043 - val_acc: 0.6939\n",
      "Epoch 75/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.7010\n",
      "Epoch 75: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5966 - acc: 0.7013 - val_loss: 0.6042 - val_acc: 0.6947\n",
      "Epoch 76/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6018 - acc: 0.6966\n",
      "Epoch 76: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6003 - acc: 0.6976 - val_loss: 0.6039 - val_acc: 0.6947\n",
      "Epoch 77/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5940 - acc: 0.7015\n",
      "Epoch 77: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5921 - acc: 0.7035 - val_loss: 0.6035 - val_acc: 0.6960\n",
      "Epoch 78/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5963 - acc: 0.6998\n",
      "Epoch 78: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5974 - acc: 0.6994 - val_loss: 0.6032 - val_acc: 0.6965\n",
      "Epoch 79/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5994 - acc: 0.7027\n",
      "Epoch 79: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5986 - acc: 0.7028 - val_loss: 0.6032 - val_acc: 0.6973\n",
      "Epoch 80/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.6980\n",
      "Epoch 80: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6003 - acc: 0.6982 - val_loss: 0.6027 - val_acc: 0.6969\n",
      "Epoch 81/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5986 - acc: 0.6997\n",
      "Epoch 81: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5983 - acc: 0.6998 - val_loss: 0.6022 - val_acc: 0.6973\n",
      "Epoch 82/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.6977\n",
      "Epoch 82: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5993 - acc: 0.6988 - val_loss: 0.6023 - val_acc: 0.6990\n",
      "Epoch 83/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5939 - acc: 0.7026\n",
      "Epoch 83: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5945 - acc: 0.7028 - val_loss: 0.6021 - val_acc: 0.6990\n",
      "Epoch 84/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6002 - acc: 0.6965\n",
      "Epoch 84: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6001 - acc: 0.6976 - val_loss: 0.6015 - val_acc: 0.6990\n",
      "Epoch 85/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5954 - acc: 0.7043\n",
      "Epoch 85: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5945 - acc: 0.7051 - val_loss: 0.6013 - val_acc: 0.6994\n",
      "Epoch 86/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5944 - acc: 0.7013\n",
      "Epoch 86: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5941 - acc: 0.7037 - val_loss: 0.6006 - val_acc: 0.6994\n",
      "Epoch 87/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5955 - acc: 0.6980\n",
      "Epoch 87: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5939 - acc: 0.6998 - val_loss: 0.6002 - val_acc: 0.6994\n",
      "Epoch 88/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5874 - acc: 0.7021\n",
      "Epoch 88: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5882 - acc: 0.7024 - val_loss: 0.5999 - val_acc: 0.6999\n",
      "Epoch 89/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5879 - acc: 0.7080\n",
      "Epoch 89: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5878 - acc: 0.7097 - val_loss: 0.5990 - val_acc: 0.7003\n",
      "Epoch 90/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.6978\n",
      "Epoch 90: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5951 - acc: 0.6994 - val_loss: 0.5987 - val_acc: 0.7003\n",
      "Epoch 91/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5982 - acc: 0.7042\n",
      "Epoch 91: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5979 - acc: 0.7041 - val_loss: 0.5989 - val_acc: 0.7003\n",
      "Epoch 92/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5925 - acc: 0.7016\n",
      "Epoch 92: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5921 - acc: 0.7019 - val_loss: 0.5988 - val_acc: 0.7007\n",
      "Epoch 93/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5956 - acc: 0.7038\n",
      "Epoch 93: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5964 - acc: 0.7044 - val_loss: 0.5985 - val_acc: 0.6999\n",
      "Epoch 94/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5909 - acc: 0.7018\n",
      "Epoch 94: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5917 - acc: 0.7023 - val_loss: 0.5981 - val_acc: 0.7003\n",
      "Epoch 95/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5945 - acc: 0.7073\n",
      "Epoch 95: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5940 - acc: 0.7075 - val_loss: 0.5978 - val_acc: 0.7003\n",
      "Epoch 96/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5878 - acc: 0.7025\n",
      "Epoch 96: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5864 - acc: 0.7039 - val_loss: 0.5975 - val_acc: 0.7007\n",
      "Epoch 97/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5867 - acc: 0.7011\n",
      "Epoch 97: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5861 - acc: 0.7030 - val_loss: 0.5975 - val_acc: 0.7007\n",
      "Epoch 98/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5952 - acc: 0.6992\n",
      "Epoch 98: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5949 - acc: 0.7006 - val_loss: 0.5973 - val_acc: 0.7007\n",
      "Epoch 99/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5867 - acc: 0.7022\n",
      "Epoch 99: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5851 - acc: 0.7025 - val_loss: 0.5968 - val_acc: 0.7007\n",
      "Epoch 100/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5876 - acc: 0.7048\n",
      "Epoch 100: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5877 - acc: 0.7051 - val_loss: 0.5962 - val_acc: 0.7007\n",
      "Epoch 101/2000\n",
      "266/293 [==========================>...] - ETA: 0s - loss: 0.5894 - acc: 0.7011\n",
      "Epoch 101: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5886 - acc: 0.7036 - val_loss: 0.5963 - val_acc: 0.7007\n",
      "Epoch 102/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5856 - acc: 0.7064\n",
      "Epoch 102: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5849 - acc: 0.7071 - val_loss: 0.5956 - val_acc: 0.7007\n",
      "Epoch 103/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5881 - acc: 0.7034\n",
      "Epoch 103: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5868 - acc: 0.7043 - val_loss: 0.5957 - val_acc: 0.7003\n",
      "Epoch 104/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5876 - acc: 0.7034\n",
      "Epoch 104: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5872 - acc: 0.7042 - val_loss: 0.5954 - val_acc: 0.7016\n",
      "Epoch 105/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.7088\n",
      "Epoch 105: val_acc improved from 0.70158 to 0.70286, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5811 - acc: 0.7100 - val_loss: 0.5955 - val_acc: 0.7029\n",
      "Epoch 106/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5842 - acc: 0.7063\n",
      "Epoch 106: val_acc improved from 0.70286 to 0.70329, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5838 - acc: 0.7073 - val_loss: 0.5953 - val_acc: 0.7033\n",
      "Epoch 107/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5828 - acc: 0.7074\n",
      "Epoch 107: val_acc did not improve from 0.70329\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5827 - acc: 0.7084 - val_loss: 0.5948 - val_acc: 0.7029\n",
      "Epoch 108/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5879 - acc: 0.7003\n",
      "Epoch 108: val_acc did not improve from 0.70329\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.5870 - acc: 0.7026 - val_loss: 0.5942 - val_acc: 0.7029\n",
      "Epoch 109/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5813 - acc: 0.7099\n",
      "Epoch 109: val_acc did not improve from 0.70329\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5814 - acc: 0.7103 - val_loss: 0.5939 - val_acc: 0.7033\n",
      "Epoch 110/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5843 - acc: 0.7054\n",
      "Epoch 110: val_acc did not improve from 0.70329\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5840 - acc: 0.7077 - val_loss: 0.5933 - val_acc: 0.7033\n",
      "Epoch 111/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.7117\n",
      "Epoch 111: val_acc did not improve from 0.70329\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5794 - acc: 0.7118 - val_loss: 0.5927 - val_acc: 0.7033\n",
      "Epoch 112/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.7070\n",
      "Epoch 112: val_acc improved from 0.70329 to 0.70372, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5784 - acc: 0.7081 - val_loss: 0.5925 - val_acc: 0.7037\n",
      "Epoch 113/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5817 - acc: 0.7093\n",
      "Epoch 113: val_acc did not improve from 0.70372\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5805 - acc: 0.7110 - val_loss: 0.5924 - val_acc: 0.7037\n",
      "Epoch 114/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5863 - acc: 0.7107\n",
      "Epoch 114: val_acc did not improve from 0.70372\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5863 - acc: 0.7107 - val_loss: 0.5925 - val_acc: 0.7033\n",
      "Epoch 115/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5808 - acc: 0.7056\n",
      "Epoch 115: val_acc did not improve from 0.70372\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.5793 - acc: 0.7074 - val_loss: 0.5922 - val_acc: 0.7037\n",
      "Epoch 116/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5834 - acc: 0.7065\n",
      "Epoch 116: val_acc did not improve from 0.70372\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.5830 - acc: 0.7075 - val_loss: 0.5919 - val_acc: 0.7037\n",
      "Epoch 117/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5735 - acc: 0.7163\n",
      "Epoch 117: val_acc improved from 0.70372 to 0.70415, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5733 - acc: 0.7164 - val_loss: 0.5916 - val_acc: 0.7041\n",
      "Epoch 118/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5860 - acc: 0.7055\n",
      "Epoch 118: val_acc did not improve from 0.70415\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5844 - acc: 0.7072 - val_loss: 0.5916 - val_acc: 0.7041\n",
      "Epoch 119/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5764 - acc: 0.7106\n",
      "Epoch 119: val_acc improved from 0.70415 to 0.70543, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5756 - acc: 0.7112 - val_loss: 0.5913 - val_acc: 0.7054\n",
      "Epoch 120/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5762 - acc: 0.7089\n",
      "Epoch 120: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5751 - acc: 0.7098 - val_loss: 0.5910 - val_acc: 0.7054\n",
      "Epoch 121/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5758 - acc: 0.7092\n",
      "Epoch 121: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5755 - acc: 0.7100 - val_loss: 0.5907 - val_acc: 0.7054\n",
      "Epoch 122/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5798 - acc: 0.7082\n",
      "Epoch 122: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5802 - acc: 0.7091 - val_loss: 0.5905 - val_acc: 0.7054\n",
      "Epoch 123/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5785 - acc: 0.7084\n",
      "Epoch 123: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5766 - acc: 0.7101 - val_loss: 0.5904 - val_acc: 0.7050\n",
      "Epoch 124/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5704 - acc: 0.7088\n",
      "Epoch 124: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5702 - acc: 0.7095 - val_loss: 0.5899 - val_acc: 0.7054\n",
      "Epoch 125/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5736 - acc: 0.7172\n",
      "Epoch 125: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.5748 - acc: 0.7163 - val_loss: 0.5896 - val_acc: 0.7050\n",
      "Epoch 126/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.7065\n",
      "Epoch 126: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5758 - acc: 0.7074 - val_loss: 0.5897 - val_acc: 0.7041\n",
      "Epoch 127/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5733 - acc: 0.7105\n",
      "Epoch 127: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.5737 - acc: 0.7106 - val_loss: 0.5892 - val_acc: 0.7041\n",
      "Epoch 128/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5822 - acc: 0.7107\n",
      "Epoch 128: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 2ms/step - loss: 0.5814 - acc: 0.7115 - val_loss: 0.5890 - val_acc: 0.7046\n",
      "Epoch 129/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.7104\n",
      "Epoch 129: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5819 - acc: 0.7104 - val_loss: 0.5889 - val_acc: 0.7041\n",
      "Epoch 130/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5712 - acc: 0.7119\n",
      "Epoch 130: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5725 - acc: 0.7125 - val_loss: 0.5885 - val_acc: 0.7041\n",
      "Epoch 131/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5765 - acc: 0.7105\n",
      "Epoch 131: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5746 - acc: 0.7116 - val_loss: 0.5882 - val_acc: 0.7041\n",
      "Epoch 132/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5809 - acc: 0.7091\n",
      "Epoch 132: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5804 - acc: 0.7095 - val_loss: 0.5883 - val_acc: 0.7041\n",
      "Epoch 133/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.7133\n",
      "Epoch 133: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5796 - acc: 0.7135 - val_loss: 0.5882 - val_acc: 0.7046\n",
      "Epoch 134/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5712 - acc: 0.7133\n",
      "Epoch 134: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5703 - acc: 0.7146 - val_loss: 0.5877 - val_acc: 0.7046\n",
      "Epoch 135/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5719 - acc: 0.7107\n",
      "Epoch 135: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5719 - acc: 0.7122 - val_loss: 0.5875 - val_acc: 0.7046\n",
      "Epoch 136/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5797 - acc: 0.7105\n",
      "Epoch 136: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5798 - acc: 0.7117 - val_loss: 0.5877 - val_acc: 0.7046\n",
      "Epoch 137/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5796 - acc: 0.7074\n",
      "Epoch 137: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5785 - acc: 0.7081 - val_loss: 0.5873 - val_acc: 0.7050\n",
      "Epoch 138/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.7136\n",
      "Epoch 138: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5729 - acc: 0.7147 - val_loss: 0.5865 - val_acc: 0.7041\n",
      "Epoch 139/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5802 - acc: 0.7131\n",
      "Epoch 139: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5799 - acc: 0.7130 - val_loss: 0.5869 - val_acc: 0.7046\n",
      "Epoch 140/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5787 - acc: 0.7098\n",
      "Epoch 140: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5783 - acc: 0.7105 - val_loss: 0.5867 - val_acc: 0.7046\n",
      "Epoch 141/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.7165\n",
      "Epoch 141: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5719 - acc: 0.7161 - val_loss: 0.5862 - val_acc: 0.7046\n",
      "Epoch 142/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.7125\n",
      "Epoch 142: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5729 - acc: 0.7128 - val_loss: 0.5859 - val_acc: 0.7054\n",
      "Epoch 143/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5722 - acc: 0.7124\n",
      "Epoch 143: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5716 - acc: 0.7122 - val_loss: 0.5855 - val_acc: 0.7054\n",
      "Epoch 144/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5766 - acc: 0.7150\n",
      "Epoch 144: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5751 - acc: 0.7159 - val_loss: 0.5851 - val_acc: 0.7050\n",
      "Epoch 145/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5791 - acc: 0.7113\n",
      "Epoch 145: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5778 - acc: 0.7127 - val_loss: 0.5851 - val_acc: 0.7050\n",
      "Epoch 146/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5745 - acc: 0.7112\n",
      "Epoch 146: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5737 - acc: 0.7127 - val_loss: 0.5849 - val_acc: 0.7054\n",
      "Epoch 147/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.7150\n",
      "Epoch 147: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5700 - acc: 0.7157 - val_loss: 0.5847 - val_acc: 0.7054\n",
      "Epoch 148/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.7157\n",
      "Epoch 148: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5695 - acc: 0.7158 - val_loss: 0.5843 - val_acc: 0.7050\n",
      "Epoch 149/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.7169\n",
      "Epoch 149: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5716 - acc: 0.7173 - val_loss: 0.5839 - val_acc: 0.7046\n",
      "Epoch 150/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5693 - acc: 0.7162\n",
      "Epoch 150: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5693 - acc: 0.7172 - val_loss: 0.5835 - val_acc: 0.7054\n",
      "Epoch 151/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5769 - acc: 0.7114\n",
      "Epoch 151: val_acc did not improve from 0.70543\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5752 - acc: 0.7128 - val_loss: 0.5835 - val_acc: 0.7050\n",
      "Epoch 152/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5711 - acc: 0.7180\n",
      "Epoch 152: val_acc improved from 0.70543 to 0.70628, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5710 - acc: 0.7199 - val_loss: 0.5835 - val_acc: 0.7063\n",
      "Epoch 153/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.7131\n",
      "Epoch 153: val_acc improved from 0.70628 to 0.70757, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5712 - acc: 0.7142 - val_loss: 0.5835 - val_acc: 0.7076\n",
      "Epoch 154/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5697 - acc: 0.7132\n",
      "Epoch 154: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5692 - acc: 0.7141 - val_loss: 0.5834 - val_acc: 0.7071\n",
      "Epoch 155/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5710 - acc: 0.7167\n",
      "Epoch 155: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5713 - acc: 0.7173 - val_loss: 0.5832 - val_acc: 0.7076\n",
      "Epoch 156/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.7177\n",
      "Epoch 156: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5636 - acc: 0.7177 - val_loss: 0.5826 - val_acc: 0.7071\n",
      "Epoch 157/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5733 - acc: 0.7150\n",
      "Epoch 157: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5725 - acc: 0.7164 - val_loss: 0.5823 - val_acc: 0.7071\n",
      "Epoch 158/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.7164\n",
      "Epoch 158: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5707 - acc: 0.7163 - val_loss: 0.5820 - val_acc: 0.7071\n",
      "Epoch 159/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5606 - acc: 0.7181\n",
      "Epoch 159: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5610 - acc: 0.7198 - val_loss: 0.5819 - val_acc: 0.7071\n",
      "Epoch 160/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5690 - acc: 0.7166\n",
      "Epoch 160: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5683 - acc: 0.7172 - val_loss: 0.5817 - val_acc: 0.7076\n",
      "Epoch 161/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5653 - acc: 0.7196\n",
      "Epoch 161: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5659 - acc: 0.7198 - val_loss: 0.5813 - val_acc: 0.7076\n",
      "Epoch 162/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5784 - acc: 0.7146\n",
      "Epoch 162: val_acc improved from 0.70757 to 0.70842, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5769 - acc: 0.7152 - val_loss: 0.5815 - val_acc: 0.7084\n",
      "Epoch 163/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5699 - acc: 0.7150\n",
      "Epoch 163: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5702 - acc: 0.7147 - val_loss: 0.5815 - val_acc: 0.7080\n",
      "Epoch 164/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5633 - acc: 0.7193\n",
      "Epoch 164: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5633 - acc: 0.7193 - val_loss: 0.5811 - val_acc: 0.7076\n",
      "Epoch 165/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5638 - acc: 0.7180\n",
      "Epoch 165: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5639 - acc: 0.7194 - val_loss: 0.5812 - val_acc: 0.7084\n",
      "Epoch 166/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.7178\n",
      "Epoch 166: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5652 - acc: 0.7185 - val_loss: 0.5810 - val_acc: 0.7080\n",
      "Epoch 167/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5689 - acc: 0.7134\n",
      "Epoch 167: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5697 - acc: 0.7143 - val_loss: 0.5808 - val_acc: 0.7084\n",
      "Epoch 168/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.7199\n",
      "Epoch 168: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5674 - acc: 0.7199 - val_loss: 0.5806 - val_acc: 0.7084\n",
      "Epoch 169/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5707 - acc: 0.7135\n",
      "Epoch 169: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5708 - acc: 0.7142 - val_loss: 0.5805 - val_acc: 0.7084\n",
      "Epoch 170/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.7142\n",
      "Epoch 170: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5649 - acc: 0.7148 - val_loss: 0.5802 - val_acc: 0.7084\n",
      "Epoch 171/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5705 - acc: 0.7163\n",
      "Epoch 171: val_acc did not improve from 0.70842\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5700 - acc: 0.7172 - val_loss: 0.5801 - val_acc: 0.7084\n",
      "Epoch 172/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.7137\n",
      "Epoch 172: val_acc improved from 0.70842 to 0.70928, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5650 - acc: 0.7148 - val_loss: 0.5799 - val_acc: 0.7093\n",
      "Epoch 173/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.7151\n",
      "Epoch 173: val_acc did not improve from 0.70928\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5686 - acc: 0.7166 - val_loss: 0.5798 - val_acc: 0.7088\n",
      "Epoch 174/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5634 - acc: 0.7158\n",
      "Epoch 174: val_acc did not improve from 0.70928\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5633 - acc: 0.7172 - val_loss: 0.5796 - val_acc: 0.7093\n",
      "Epoch 175/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5683 - acc: 0.7209\n",
      "Epoch 175: val_acc did not improve from 0.70928\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5673 - acc: 0.7216 - val_loss: 0.5797 - val_acc: 0.7093\n",
      "Epoch 176/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5652 - acc: 0.7167\n",
      "Epoch 176: val_acc improved from 0.70928 to 0.70970, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5645 - acc: 0.7169 - val_loss: 0.5798 - val_acc: 0.7097\n",
      "Epoch 177/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.7163\n",
      "Epoch 177: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5614 - acc: 0.7168 - val_loss: 0.5796 - val_acc: 0.7093\n",
      "Epoch 178/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.7166\n",
      "Epoch 178: val_acc improved from 0.70970 to 0.71013, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5681 - acc: 0.7173 - val_loss: 0.5793 - val_acc: 0.7101\n",
      "Epoch 179/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5676 - acc: 0.7146\n",
      "Epoch 179: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5667 - acc: 0.7156 - val_loss: 0.5788 - val_acc: 0.7101\n",
      "Epoch 180/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5648 - acc: 0.7198\n",
      "Epoch 180: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5639 - acc: 0.7201 - val_loss: 0.5786 - val_acc: 0.7101\n",
      "Epoch 181/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5678 - acc: 0.7144\n",
      "Epoch 181: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5679 - acc: 0.7145 - val_loss: 0.5787 - val_acc: 0.7097\n",
      "Epoch 182/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5613 - acc: 0.7190\n",
      "Epoch 182: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5618 - acc: 0.7196 - val_loss: 0.5786 - val_acc: 0.7097\n",
      "Epoch 183/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.7211\n",
      "Epoch 183: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5604 - acc: 0.7213 - val_loss: 0.5785 - val_acc: 0.7097\n",
      "Epoch 184/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5619 - acc: 0.7199\n",
      "Epoch 184: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5615 - acc: 0.7200 - val_loss: 0.5784 - val_acc: 0.7097\n",
      "Epoch 185/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.7206\n",
      "Epoch 185: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5578 - acc: 0.7214 - val_loss: 0.5779 - val_acc: 0.7097\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.7175\n",
      "Epoch 186: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5662 - acc: 0.7176 - val_loss: 0.5776 - val_acc: 0.7101\n",
      "Epoch 187/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.7204\n",
      "Epoch 187: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5656 - acc: 0.7212 - val_loss: 0.5774 - val_acc: 0.7097\n",
      "Epoch 188/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5622 - acc: 0.7207\n",
      "Epoch 188: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5617 - acc: 0.7214 - val_loss: 0.5772 - val_acc: 0.7097\n",
      "Epoch 189/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5616 - acc: 0.7218\n",
      "Epoch 189: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5627 - acc: 0.7211 - val_loss: 0.5769 - val_acc: 0.7088\n",
      "Epoch 190/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5704 - acc: 0.7169\n",
      "Epoch 190: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5683 - acc: 0.7184 - val_loss: 0.5770 - val_acc: 0.7093\n",
      "Epoch 191/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5633 - acc: 0.7170\n",
      "Epoch 191: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5627 - acc: 0.7177 - val_loss: 0.5767 - val_acc: 0.7088\n",
      "Epoch 192/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5656 - acc: 0.7167\n",
      "Epoch 192: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5651 - acc: 0.7173 - val_loss: 0.5765 - val_acc: 0.7088\n",
      "Epoch 193/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5651 - acc: 0.7191\n",
      "Epoch 193: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5650 - acc: 0.7206 - val_loss: 0.5765 - val_acc: 0.7097\n",
      "Epoch 194/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.7200\n",
      "Epoch 194: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5557 - acc: 0.7201 - val_loss: 0.5760 - val_acc: 0.7093\n",
      "Epoch 195/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.7215\n",
      "Epoch 195: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5590 - acc: 0.7220 - val_loss: 0.5760 - val_acc: 0.7093\n",
      "Epoch 196/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.7181\n",
      "Epoch 196: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5600 - acc: 0.7195 - val_loss: 0.5759 - val_acc: 0.7097\n",
      "Epoch 197/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5646 - acc: 0.7194\n",
      "Epoch 197: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5643 - acc: 0.7204 - val_loss: 0.5758 - val_acc: 0.7093\n",
      "Epoch 198/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5619 - acc: 0.7180\n",
      "Epoch 198: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5612 - acc: 0.7182 - val_loss: 0.5756 - val_acc: 0.7093\n",
      "Epoch 199/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.7224\n",
      "Epoch 199: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5600 - acc: 0.7234 - val_loss: 0.5753 - val_acc: 0.7093\n",
      "Epoch 200/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.7192\n",
      "Epoch 200: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5636 - acc: 0.7198 - val_loss: 0.5757 - val_acc: 0.7101\n",
      "Epoch 201/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.7196\n",
      "Epoch 201: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5549 - acc: 0.7197 - val_loss: 0.5754 - val_acc: 0.7097\n",
      "Epoch 202/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5587 - acc: 0.7188\n",
      "Epoch 202: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5583 - acc: 0.7198 - val_loss: 0.5755 - val_acc: 0.7101\n",
      "Epoch 203/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5599 - acc: 0.7216\n",
      "Epoch 203: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5597 - acc: 0.7216 - val_loss: 0.5750 - val_acc: 0.7097\n",
      "Epoch 204/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.7196\n",
      "Epoch 204: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5588 - acc: 0.7204 - val_loss: 0.5751 - val_acc: 0.7097\n",
      "Epoch 205/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.7222\n",
      "Epoch 205: val_acc improved from 0.71013 to 0.71099, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5625 - acc: 0.7226 - val_loss: 0.5752 - val_acc: 0.7110\n",
      "Epoch 206/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.7192\n",
      "Epoch 206: val_acc did not improve from 0.71099\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5641 - acc: 0.7198 - val_loss: 0.5751 - val_acc: 0.7106\n",
      "Epoch 207/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.7192\n",
      "Epoch 207: val_acc did not improve from 0.71099\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5617 - acc: 0.7195 - val_loss: 0.5747 - val_acc: 0.7106\n",
      "Epoch 208/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7234\n",
      "Epoch 208: val_acc improved from 0.71099 to 0.71142, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7243 - val_loss: 0.5748 - val_acc: 0.7114\n",
      "Epoch 209/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.7180\n",
      "Epoch 209: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5603 - acc: 0.7184 - val_loss: 0.5744 - val_acc: 0.7101\n",
      "Epoch 210/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5627 - acc: 0.7157\n",
      "Epoch 210: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5623 - acc: 0.7167 - val_loss: 0.5745 - val_acc: 0.7110\n",
      "Epoch 211/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7167\n",
      "Epoch 211: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5624 - acc: 0.7176 - val_loss: 0.5746 - val_acc: 0.7114\n",
      "Epoch 212/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.7235\n",
      "Epoch 212: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5535 - acc: 0.7235 - val_loss: 0.5744 - val_acc: 0.7114\n",
      "Epoch 213/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.7182\n",
      "Epoch 213: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5573 - acc: 0.7191 - val_loss: 0.5741 - val_acc: 0.7106\n",
      "Epoch 214/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5563 - acc: 0.7207\n",
      "Epoch 214: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5549 - acc: 0.7212 - val_loss: 0.5738 - val_acc: 0.7106\n",
      "Epoch 215/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5599 - acc: 0.7192\n",
      "Epoch 215: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5602 - acc: 0.7190 - val_loss: 0.5735 - val_acc: 0.7101\n",
      "Epoch 216/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.7219\n",
      "Epoch 216: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5554 - acc: 0.7225 - val_loss: 0.5734 - val_acc: 0.7101\n",
      "Epoch 217/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5607 - acc: 0.7218\n",
      "Epoch 217: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5613 - acc: 0.7223 - val_loss: 0.5734 - val_acc: 0.7101\n",
      "Epoch 218/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5634 - acc: 0.7175\n",
      "Epoch 218: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5634 - acc: 0.7175 - val_loss: 0.5732 - val_acc: 0.7101\n",
      "Epoch 219/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5592 - acc: 0.7196\n",
      "Epoch 219: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5586 - acc: 0.7206 - val_loss: 0.5729 - val_acc: 0.7101\n",
      "Epoch 220/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5523 - acc: 0.7183\n",
      "Epoch 220: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5517 - acc: 0.7204 - val_loss: 0.5729 - val_acc: 0.7101\n",
      "Epoch 221/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.7245\n",
      "Epoch 221: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5565 - acc: 0.7255 - val_loss: 0.5727 - val_acc: 0.7106\n",
      "Epoch 222/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5581 - acc: 0.7222\n",
      "Epoch 222: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5581 - acc: 0.7226 - val_loss: 0.5724 - val_acc: 0.7110\n",
      "Epoch 223/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.7230\n",
      "Epoch 223: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5533 - acc: 0.7237 - val_loss: 0.5721 - val_acc: 0.7110\n",
      "Epoch 224/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5565 - acc: 0.7201\n",
      "Epoch 224: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5579 - acc: 0.7207 - val_loss: 0.5718 - val_acc: 0.7114\n",
      "Epoch 225/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5567 - acc: 0.7222\n",
      "Epoch 225: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5568 - acc: 0.7232 - val_loss: 0.5716 - val_acc: 0.7110\n",
      "Epoch 226/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.7285\n",
      "Epoch 226: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5522 - acc: 0.7286 - val_loss: 0.5717 - val_acc: 0.7110\n",
      "Epoch 227/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5564 - acc: 0.7216\n",
      "Epoch 227: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5563 - acc: 0.7211 - val_loss: 0.5718 - val_acc: 0.7110\n",
      "Epoch 228/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5552 - acc: 0.7235\n",
      "Epoch 228: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5546 - acc: 0.7242 - val_loss: 0.5717 - val_acc: 0.7110\n",
      "Epoch 229/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5604 - acc: 0.7200\n",
      "Epoch 229: val_acc did not improve from 0.71142\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5605 - acc: 0.7201 - val_loss: 0.5717 - val_acc: 0.7106\n",
      "Epoch 230/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5596 - acc: 0.7207\n",
      "Epoch 230: val_acc improved from 0.71142 to 0.71184, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5592 - acc: 0.7212 - val_loss: 0.5716 - val_acc: 0.7118\n",
      "Epoch 231/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5516 - acc: 0.7244\n",
      "Epoch 231: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5524 - acc: 0.7250 - val_loss: 0.5712 - val_acc: 0.7114\n",
      "Epoch 232/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5513 - acc: 0.7261\n",
      "Epoch 232: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5500 - acc: 0.7271 - val_loss: 0.5708 - val_acc: 0.7118\n",
      "Epoch 233/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5490 - acc: 0.7265\n",
      "Epoch 233: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5493 - acc: 0.7270 - val_loss: 0.5708 - val_acc: 0.7114\n",
      "Epoch 234/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5557 - acc: 0.7210\n",
      "Epoch 234: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5561 - acc: 0.7214 - val_loss: 0.5707 - val_acc: 0.7114\n",
      "Epoch 235/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.7276\n",
      "Epoch 235: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5519 - acc: 0.7284 - val_loss: 0.5703 - val_acc: 0.7106\n",
      "Epoch 236/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5546 - acc: 0.7226\n",
      "Epoch 236: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5541 - acc: 0.7236 - val_loss: 0.5703 - val_acc: 0.7110\n",
      "Epoch 237/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5575 - acc: 0.7231\n",
      "Epoch 237: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5565 - acc: 0.7241 - val_loss: 0.5705 - val_acc: 0.7118\n",
      "Epoch 238/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.7246\n",
      "Epoch 238: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5532 - acc: 0.7256 - val_loss: 0.5704 - val_acc: 0.7110\n",
      "Epoch 239/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5601 - acc: 0.7176\n",
      "Epoch 239: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5579 - acc: 0.7189 - val_loss: 0.5701 - val_acc: 0.7114\n",
      "Epoch 240/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7204\n",
      "Epoch 240: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5536 - acc: 0.7209 - val_loss: 0.5700 - val_acc: 0.7114\n",
      "Epoch 241/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.7177\n",
      "Epoch 241: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5545 - acc: 0.7191 - val_loss: 0.5698 - val_acc: 0.7110\n",
      "Epoch 242/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7240\n",
      "Epoch 242: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5509 - acc: 0.7245 - val_loss: 0.5695 - val_acc: 0.7110\n",
      "Epoch 243/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.7247\n",
      "Epoch 243: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5530 - acc: 0.7250 - val_loss: 0.5694 - val_acc: 0.7110\n",
      "Epoch 244/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.7229\n",
      "Epoch 244: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5536 - acc: 0.7239 - val_loss: 0.5692 - val_acc: 0.7110\n",
      "Epoch 245/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5484 - acc: 0.7264\n",
      "Epoch 245: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5482 - acc: 0.7265 - val_loss: 0.5688 - val_acc: 0.7114\n",
      "Epoch 246/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7233\n",
      "Epoch 246: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5497 - acc: 0.7230 - val_loss: 0.5689 - val_acc: 0.7106\n",
      "Epoch 247/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7222\n",
      "Epoch 247: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5516 - acc: 0.7232 - val_loss: 0.5688 - val_acc: 0.7118\n",
      "Epoch 248/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5579 - acc: 0.7212\n",
      "Epoch 248: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5578 - acc: 0.7218 - val_loss: 0.5688 - val_acc: 0.7114\n",
      "Epoch 249/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5562 - acc: 0.7221\n",
      "Epoch 249: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5549 - acc: 0.7241 - val_loss: 0.5686 - val_acc: 0.7118\n",
      "Epoch 250/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.7221\n",
      "Epoch 250: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7223 - val_loss: 0.5685 - val_acc: 0.7118\n",
      "Epoch 251/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5484 - acc: 0.7228\n",
      "Epoch 251: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5484 - acc: 0.7228 - val_loss: 0.5682 - val_acc: 0.7114\n",
      "Epoch 252/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.7206\n",
      "Epoch 252: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5553 - acc: 0.7209 - val_loss: 0.5682 - val_acc: 0.7114\n",
      "Epoch 253/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5508 - acc: 0.7224\n",
      "Epoch 253: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5504 - acc: 0.7228 - val_loss: 0.5680 - val_acc: 0.7110\n",
      "Epoch 254/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5561 - acc: 0.7226\n",
      "Epoch 254: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5557 - acc: 0.7231 - val_loss: 0.5681 - val_acc: 0.7114\n",
      "Epoch 255/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5519 - acc: 0.7258\n",
      "Epoch 255: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5515 - acc: 0.7268 - val_loss: 0.5678 - val_acc: 0.7114\n",
      "Epoch 256/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.7226\n",
      "Epoch 256: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5527 - acc: 0.7237 - val_loss: 0.5679 - val_acc: 0.7114\n",
      "Epoch 257/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5577 - acc: 0.7214\n",
      "Epoch 257: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5569 - acc: 0.7223 - val_loss: 0.5676 - val_acc: 0.7114\n",
      "Epoch 258/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.7234\n",
      "Epoch 258: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5556 - acc: 0.7244 - val_loss: 0.5676 - val_acc: 0.7114\n",
      "Epoch 259/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.7246\n",
      "Epoch 259: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5542 - acc: 0.7255 - val_loss: 0.5676 - val_acc: 0.7114\n",
      "Epoch 260/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5539 - acc: 0.7201\n",
      "Epoch 260: val_acc improved from 0.71184 to 0.71227, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5529 - acc: 0.7222 - val_loss: 0.5677 - val_acc: 0.7123\n",
      "Epoch 261/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.7240\n",
      "Epoch 261: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5526 - acc: 0.7244 - val_loss: 0.5675 - val_acc: 0.7118\n",
      "Epoch 262/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7250\n",
      "Epoch 262: val_acc improved from 0.71227 to 0.71270, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5495 - acc: 0.7262 - val_loss: 0.5675 - val_acc: 0.7127\n",
      "Epoch 263/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5526 - acc: 0.7248\n",
      "Epoch 263: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7263 - val_loss: 0.5673 - val_acc: 0.7118\n",
      "Epoch 264/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5583 - acc: 0.7229\n",
      "Epoch 264: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5573 - acc: 0.7244 - val_loss: 0.5672 - val_acc: 0.7118\n",
      "Epoch 265/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5530 - acc: 0.7227\n",
      "Epoch 265: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7238 - val_loss: 0.5669 - val_acc: 0.7123\n",
      "Epoch 266/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.7252\n",
      "Epoch 266: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5468 - acc: 0.7254 - val_loss: 0.5667 - val_acc: 0.7127\n",
      "Epoch 267/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5581 - acc: 0.7259\n",
      "Epoch 267: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5577 - acc: 0.7269 - val_loss: 0.5670 - val_acc: 0.7127\n",
      "Epoch 268/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5478 - acc: 0.7252\n",
      "Epoch 268: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5483 - acc: 0.7255 - val_loss: 0.5664 - val_acc: 0.7127\n",
      "Epoch 269/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5499 - acc: 0.7253\n",
      "Epoch 269: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5497 - acc: 0.7259 - val_loss: 0.5663 - val_acc: 0.7123\n",
      "Epoch 270/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5535 - acc: 0.7247\n",
      "Epoch 270: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5521 - acc: 0.7263 - val_loss: 0.5663 - val_acc: 0.7127\n",
      "Epoch 271/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5507 - acc: 0.7237\n",
      "Epoch 271: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5498 - acc: 0.7252 - val_loss: 0.5662 - val_acc: 0.7127\n",
      "Epoch 272/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5480 - acc: 0.7275\n",
      "Epoch 272: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5478 - acc: 0.7282 - val_loss: 0.5659 - val_acc: 0.7123\n",
      "Epoch 273/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7301\n",
      "Epoch 273: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5469 - acc: 0.7306 - val_loss: 0.5659 - val_acc: 0.7127\n",
      "Epoch 274/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5520 - acc: 0.7237\n",
      "Epoch 274: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7241 - val_loss: 0.5654 - val_acc: 0.7123\n",
      "Epoch 275/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7243\n",
      "Epoch 275: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5477 - acc: 0.7247 - val_loss: 0.5654 - val_acc: 0.7127\n",
      "Epoch 276/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5461 - acc: 0.7271\n",
      "Epoch 276: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5454 - acc: 0.7277 - val_loss: 0.5653 - val_acc: 0.7123\n",
      "Epoch 277/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5479 - acc: 0.7263\n",
      "Epoch 277: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5478 - acc: 0.7268 - val_loss: 0.5651 - val_acc: 0.7127\n",
      "Epoch 278/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5494 - acc: 0.7273\n",
      "Epoch 278: val_acc improved from 0.71270 to 0.71313, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5501 - acc: 0.7266 - val_loss: 0.5650 - val_acc: 0.7131\n",
      "Epoch 279/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7246\n",
      "Epoch 279: val_acc improved from 0.71313 to 0.71355, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5496 - acc: 0.7251 - val_loss: 0.5650 - val_acc: 0.7136\n",
      "Epoch 280/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5450 - acc: 0.7273\n",
      "Epoch 280: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5454 - acc: 0.7274 - val_loss: 0.5647 - val_acc: 0.7136\n",
      "Epoch 281/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5505 - acc: 0.7243\n",
      "Epoch 281: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5492 - acc: 0.7254 - val_loss: 0.5646 - val_acc: 0.7136\n",
      "Epoch 282/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.7250\n",
      "Epoch 282: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5479 - acc: 0.7247 - val_loss: 0.5644 - val_acc: 0.7136\n",
      "Epoch 283/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5436 - acc: 0.7306\n",
      "Epoch 283: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5436 - acc: 0.7306 - val_loss: 0.5641 - val_acc: 0.7136\n",
      "Epoch 284/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5513 - acc: 0.7217\n",
      "Epoch 284: val_acc improved from 0.71355 to 0.71398, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5520 - acc: 0.7215 - val_loss: 0.5641 - val_acc: 0.7140\n",
      "Epoch 285/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7250\n",
      "Epoch 285: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5485 - acc: 0.7251 - val_loss: 0.5643 - val_acc: 0.7140\n",
      "Epoch 286/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.7280\n",
      "Epoch 286: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5471 - acc: 0.7285 - val_loss: 0.5642 - val_acc: 0.7140\n",
      "Epoch 287/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5464 - acc: 0.7314\n",
      "Epoch 287: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5471 - acc: 0.7313 - val_loss: 0.5640 - val_acc: 0.7140\n",
      "Epoch 288/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7273\n",
      "Epoch 288: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5455 - acc: 0.7281 - val_loss: 0.5637 - val_acc: 0.7136\n",
      "Epoch 289/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.7238\n",
      "Epoch 289: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7246 - val_loss: 0.5638 - val_acc: 0.7140\n",
      "Epoch 290/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7291\n",
      "Epoch 290: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5489 - acc: 0.7294 - val_loss: 0.5636 - val_acc: 0.7140\n",
      "Epoch 291/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5448 - acc: 0.7249\n",
      "Epoch 291: val_acc improved from 0.71398 to 0.71441, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5440 - acc: 0.7257 - val_loss: 0.5633 - val_acc: 0.7144\n",
      "Epoch 292/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5513 - acc: 0.7231\n",
      "Epoch 292: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5506 - acc: 0.7241 - val_loss: 0.5634 - val_acc: 0.7144\n",
      "Epoch 293/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7269\n",
      "Epoch 293: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5471 - acc: 0.7274 - val_loss: 0.5634 - val_acc: 0.7136\n",
      "Epoch 294/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5478 - acc: 0.7237\n",
      "Epoch 294: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5467 - acc: 0.7245 - val_loss: 0.5633 - val_acc: 0.7140\n",
      "Epoch 295/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7309\n",
      "Epoch 295: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5420 - acc: 0.7314 - val_loss: 0.5628 - val_acc: 0.7144\n",
      "Epoch 296/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.7290\n",
      "Epoch 296: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5476 - acc: 0.7291 - val_loss: 0.5627 - val_acc: 0.7144\n",
      "Epoch 297/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5435 - acc: 0.7272\n",
      "Epoch 297: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5429 - acc: 0.7281 - val_loss: 0.5626 - val_acc: 0.7144\n",
      "Epoch 298/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5445 - acc: 0.7279\n",
      "Epoch 298: val_acc improved from 0.71441 to 0.71484, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5445 - acc: 0.7283 - val_loss: 0.5626 - val_acc: 0.7148\n",
      "Epoch 299/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5451 - acc: 0.7288\n",
      "Epoch 299: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5458 - acc: 0.7288 - val_loss: 0.5622 - val_acc: 0.7144\n",
      "Epoch 300/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.7253\n",
      "Epoch 300: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5470 - acc: 0.7260 - val_loss: 0.5623 - val_acc: 0.7148\n",
      "Epoch 301/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5407 - acc: 0.7295\n",
      "Epoch 301: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5405 - acc: 0.7293 - val_loss: 0.5619 - val_acc: 0.7144\n",
      "Epoch 302/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5512 - acc: 0.7226\n",
      "Epoch 302: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5516 - acc: 0.7229 - val_loss: 0.5621 - val_acc: 0.7144\n",
      "Epoch 303/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5466 - acc: 0.7227\n",
      "Epoch 303: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5459 - acc: 0.7238 - val_loss: 0.5618 - val_acc: 0.7148\n",
      "Epoch 304/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.7256\n",
      "Epoch 304: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5456 - acc: 0.7260 - val_loss: 0.5617 - val_acc: 0.7148\n",
      "Epoch 305/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5464 - acc: 0.7305\n",
      "Epoch 305: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5455 - acc: 0.7312 - val_loss: 0.5616 - val_acc: 0.7148\n",
      "Epoch 306/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5509 - acc: 0.7249\n",
      "Epoch 306: val_acc improved from 0.71484 to 0.71526, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5485 - acc: 0.7272 - val_loss: 0.5618 - val_acc: 0.7153\n",
      "Epoch 307/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5486 - acc: 0.7266\n",
      "Epoch 307: val_acc did not improve from 0.71526\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5482 - acc: 0.7271 - val_loss: 0.5617 - val_acc: 0.7153\n",
      "Epoch 308/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.7243\n",
      "Epoch 308: val_acc did not improve from 0.71526\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5436 - acc: 0.7254 - val_loss: 0.5614 - val_acc: 0.7148\n",
      "Epoch 309/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7288\n",
      "Epoch 309: val_acc did not improve from 0.71526\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5463 - acc: 0.7298 - val_loss: 0.5616 - val_acc: 0.7153\n",
      "Epoch 310/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5453 - acc: 0.7243\n",
      "Epoch 310: val_acc improved from 0.71526 to 0.71612, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5451 - acc: 0.7252 - val_loss: 0.5616 - val_acc: 0.7161\n",
      "Epoch 311/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.7257\n",
      "Epoch 311: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5435 - acc: 0.7260 - val_loss: 0.5613 - val_acc: 0.7161\n",
      "Epoch 312/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5446 - acc: 0.7269\n",
      "Epoch 312: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5442 - acc: 0.7284 - val_loss: 0.5610 - val_acc: 0.7161\n",
      "Epoch 313/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.7273\n",
      "Epoch 313: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5434 - acc: 0.7285 - val_loss: 0.5608 - val_acc: 0.7161\n",
      "Epoch 314/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5416 - acc: 0.7309\n",
      "Epoch 314: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5412 - acc: 0.7317 - val_loss: 0.5606 - val_acc: 0.7161\n",
      "Epoch 315/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5443 - acc: 0.7249\n",
      "Epoch 315: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5444 - acc: 0.7260 - val_loss: 0.5604 - val_acc: 0.7157\n",
      "Epoch 316/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5460 - acc: 0.7304\n",
      "Epoch 316: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5457 - acc: 0.7309 - val_loss: 0.5604 - val_acc: 0.7161\n",
      "Epoch 317/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5453 - acc: 0.7284\n",
      "Epoch 317: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5455 - acc: 0.7296 - val_loss: 0.5602 - val_acc: 0.7161\n",
      "Epoch 318/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7249\n",
      "Epoch 318: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5497 - acc: 0.7254 - val_loss: 0.5605 - val_acc: 0.7161\n",
      "Epoch 319/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7267\n",
      "Epoch 319: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5496 - acc: 0.7272 - val_loss: 0.5604 - val_acc: 0.7161\n",
      "Epoch 320/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7346\n",
      "Epoch 320: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5399 - acc: 0.7347 - val_loss: 0.5601 - val_acc: 0.7161\n",
      "Epoch 321/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5414 - acc: 0.7286\n",
      "Epoch 321: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5410 - acc: 0.7291 - val_loss: 0.5600 - val_acc: 0.7161\n",
      "Epoch 322/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.7281\n",
      "Epoch 322: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5428 - acc: 0.7286 - val_loss: 0.5598 - val_acc: 0.7161\n",
      "Epoch 323/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.7276\n",
      "Epoch 323: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5416 - acc: 0.7284 - val_loss: 0.5597 - val_acc: 0.7161\n",
      "Epoch 324/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7297\n",
      "Epoch 324: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5406 - acc: 0.7301 - val_loss: 0.5596 - val_acc: 0.7161\n",
      "Epoch 325/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.7297\n",
      "Epoch 325: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5447 - acc: 0.7298 - val_loss: 0.5593 - val_acc: 0.7161\n",
      "Epoch 326/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5397 - acc: 0.7330\n",
      "Epoch 326: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5402 - acc: 0.7335 - val_loss: 0.5590 - val_acc: 0.7161\n",
      "Epoch 327/2000\n",
      "267/293 [==========================>...] - ETA: 0s - loss: 0.5476 - acc: 0.7279\n",
      "Epoch 327: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5463 - acc: 0.7296 - val_loss: 0.5588 - val_acc: 0.7161\n",
      "Epoch 328/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5426 - acc: 0.7309\n",
      "Epoch 328: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5421 - acc: 0.7316 - val_loss: 0.5588 - val_acc: 0.7161\n",
      "Epoch 329/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5447 - acc: 0.7285\n",
      "Epoch 329: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5446 - acc: 0.7291 - val_loss: 0.5588 - val_acc: 0.7153\n",
      "Epoch 330/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5406 - acc: 0.7307\n",
      "Epoch 330: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5406 - acc: 0.7317 - val_loss: 0.5587 - val_acc: 0.7157\n",
      "Epoch 331/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7255\n",
      "Epoch 331: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5417 - acc: 0.7263 - val_loss: 0.5585 - val_acc: 0.7161\n",
      "Epoch 332/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5402 - acc: 0.7281\n",
      "Epoch 332: val_acc did not improve from 0.71612\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5400 - acc: 0.7289 - val_loss: 0.5583 - val_acc: 0.7157\n",
      "Epoch 333/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5440 - acc: 0.7267\n",
      "Epoch 333: val_acc improved from 0.71612 to 0.71655, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5440 - acc: 0.7278 - val_loss: 0.5580 - val_acc: 0.7165\n",
      "Epoch 334/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5414 - acc: 0.7290\n",
      "Epoch 334: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5406 - acc: 0.7302 - val_loss: 0.5580 - val_acc: 0.7153\n",
      "Epoch 335/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5428 - acc: 0.7304\n",
      "Epoch 335: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5428 - acc: 0.7304 - val_loss: 0.5580 - val_acc: 0.7157\n",
      "Epoch 336/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5444 - acc: 0.7275\n",
      "Epoch 336: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5436 - acc: 0.7287 - val_loss: 0.5579 - val_acc: 0.7157\n",
      "Epoch 337/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5434 - acc: 0.7315\n",
      "Epoch 337: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5429 - acc: 0.7323 - val_loss: 0.5578 - val_acc: 0.7157\n",
      "Epoch 338/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.7302\n",
      "Epoch 338: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5368 - acc: 0.7312 - val_loss: 0.5576 - val_acc: 0.7153\n",
      "Epoch 339/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.7258\n",
      "Epoch 339: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5439 - acc: 0.7267 - val_loss: 0.5576 - val_acc: 0.7165\n",
      "Epoch 340/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.7303\n",
      "Epoch 340: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5374 - acc: 0.7307 - val_loss: 0.5574 - val_acc: 0.7165\n",
      "Epoch 341/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7287\n",
      "Epoch 341: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5426 - acc: 0.7287 - val_loss: 0.5573 - val_acc: 0.7161\n",
      "Epoch 342/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.7312\n",
      "Epoch 342: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5403 - acc: 0.7315 - val_loss: 0.5570 - val_acc: 0.7161\n",
      "Epoch 343/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7317\n",
      "Epoch 343: val_acc improved from 0.71655 to 0.71697, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5380 - acc: 0.7325 - val_loss: 0.5570 - val_acc: 0.7170\n",
      "Epoch 344/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7299\n",
      "Epoch 344: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5401 - acc: 0.7302 - val_loss: 0.5571 - val_acc: 0.7170\n",
      "Epoch 345/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5411 - acc: 0.7311\n",
      "Epoch 345: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5401 - acc: 0.7321 - val_loss: 0.5570 - val_acc: 0.7170\n",
      "Epoch 346/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.7306\n",
      "Epoch 346: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5408 - acc: 0.7311 - val_loss: 0.5569 - val_acc: 0.7165\n",
      "Epoch 347/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7306\n",
      "Epoch 347: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5412 - acc: 0.7312 - val_loss: 0.5568 - val_acc: 0.7165\n",
      "Epoch 348/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5421 - acc: 0.7263\n",
      "Epoch 348: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5421 - acc: 0.7271 - val_loss: 0.5567 - val_acc: 0.7161\n",
      "Epoch 349/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5445 - acc: 0.7295\n",
      "Epoch 349: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5437 - acc: 0.7306 - val_loss: 0.5567 - val_acc: 0.7161\n",
      "Epoch 350/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.7320\n",
      "Epoch 350: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5367 - acc: 0.7324 - val_loss: 0.5569 - val_acc: 0.7153\n",
      "Epoch 351/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7290\n",
      "Epoch 351: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5420 - acc: 0.7293 - val_loss: 0.5565 - val_acc: 0.7161\n",
      "Epoch 352/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7301\n",
      "Epoch 352: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5446 - acc: 0.7311 - val_loss: 0.5565 - val_acc: 0.7157\n",
      "Epoch 353/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5387 - acc: 0.7309\n",
      "Epoch 353: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5379 - acc: 0.7324 - val_loss: 0.5564 - val_acc: 0.7157\n",
      "Epoch 354/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.7321\n",
      "Epoch 354: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5391 - acc: 0.7328 - val_loss: 0.5562 - val_acc: 0.7153\n",
      "Epoch 355/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5409 - acc: 0.7314\n",
      "Epoch 355: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5406 - acc: 0.7319 - val_loss: 0.5563 - val_acc: 0.7157\n",
      "Epoch 356/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5430 - acc: 0.7295\n",
      "Epoch 356: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5423 - acc: 0.7299 - val_loss: 0.5561 - val_acc: 0.7161\n",
      "Epoch 357/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7316\n",
      "Epoch 357: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7323 - val_loss: 0.5560 - val_acc: 0.7157\n",
      "Epoch 358/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.7333\n",
      "Epoch 358: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7342 - val_loss: 0.5558 - val_acc: 0.7157\n",
      "Epoch 359/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5390 - acc: 0.7329\n",
      "Epoch 359: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5389 - acc: 0.7336 - val_loss: 0.5556 - val_acc: 0.7157\n",
      "Epoch 360/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.7296\n",
      "Epoch 360: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5430 - acc: 0.7299 - val_loss: 0.5556 - val_acc: 0.7157\n",
      "Epoch 361/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5369 - acc: 0.7289\n",
      "Epoch 361: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5369 - acc: 0.7289 - val_loss: 0.5555 - val_acc: 0.7157\n",
      "Epoch 362/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7337\n",
      "Epoch 362: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5374 - acc: 0.7344 - val_loss: 0.5554 - val_acc: 0.7161\n",
      "Epoch 363/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5373 - acc: 0.7287\n",
      "Epoch 363: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5374 - acc: 0.7298 - val_loss: 0.5552 - val_acc: 0.7161\n",
      "Epoch 364/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5353 - acc: 0.7328\n",
      "Epoch 364: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5357 - acc: 0.7335 - val_loss: 0.5551 - val_acc: 0.7161\n",
      "Epoch 365/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5426 - acc: 0.7272\n",
      "Epoch 365: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5423 - acc: 0.7278 - val_loss: 0.5551 - val_acc: 0.7161\n",
      "Epoch 366/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.7298\n",
      "Epoch 366: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5435 - acc: 0.7300 - val_loss: 0.5551 - val_acc: 0.7157\n",
      "Epoch 367/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5370 - acc: 0.7304\n",
      "Epoch 367: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5374 - acc: 0.7312 - val_loss: 0.5551 - val_acc: 0.7165\n",
      "Epoch 368/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5350 - acc: 0.7315\n",
      "Epoch 368: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5359 - acc: 0.7315 - val_loss: 0.5547 - val_acc: 0.7170\n",
      "Epoch 369/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5381 - acc: 0.7290\n",
      "Epoch 369: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5386 - acc: 0.7300 - val_loss: 0.5548 - val_acc: 0.7161\n",
      "Epoch 370/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5422 - acc: 0.7319\n",
      "Epoch 370: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5422 - acc: 0.7327 - val_loss: 0.5548 - val_acc: 0.7153\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 4.6851 - acc: 0.5908\n",
      "Epoch 1: val_acc improved from -inf to 0.69431, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 4.6136 - acc: 0.5904 - val_loss: 1.1461 - val_acc: 0.6943\n",
      "Epoch 2/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 2.6437 - acc: 0.6005\n",
      "Epoch 2: val_acc improved from 0.69431 to 0.70158, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 2.6327 - acc: 0.6010 - val_loss: 0.6814 - val_acc: 0.7016\n",
      "Epoch 3/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 1.9523 - acc: 0.6105\n",
      "Epoch 3: val_acc did not improve from 0.70158\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 1.9483 - acc: 0.6109 - val_loss: 0.6114 - val_acc: 0.7007\n",
      "Epoch 4/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 1.6380 - acc: 0.6154\n",
      "Epoch 4: val_acc improved from 0.70158 to 0.70244, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 1.6312 - acc: 0.6155 - val_loss: 0.5887 - val_acc: 0.7024\n",
      "Epoch 5/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 1.4256 - acc: 0.6175\n",
      "Epoch 5: val_acc improved from 0.70244 to 0.70500, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 1.4256 - acc: 0.6176 - val_loss: 0.5863 - val_acc: 0.7050\n",
      "Epoch 6/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 1.2675 - acc: 0.6234\n",
      "Epoch 6: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 1.2740 - acc: 0.6224 - val_loss: 0.5839 - val_acc: 0.7050\n",
      "Epoch 7/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 1.1692 - acc: 0.6210\n",
      "Epoch 7: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 1.1676 - acc: 0.6214 - val_loss: 0.5882 - val_acc: 0.6999\n",
      "Epoch 8/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 1.1000 - acc: 0.6187\n",
      "Epoch 8: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 1.0951 - acc: 0.6202 - val_loss: 0.5881 - val_acc: 0.7007\n",
      "Epoch 9/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 1.0356 - acc: 0.6262\n",
      "Epoch 9: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.0345 - acc: 0.6262 - val_loss: 0.5932 - val_acc: 0.6986\n",
      "Epoch 10/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.9862 - acc: 0.6200\n",
      "Epoch 10: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.9863 - acc: 0.6202 - val_loss: 0.5936 - val_acc: 0.6994\n",
      "Epoch 11/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.9265 - acc: 0.6304\n",
      "Epoch 11: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.9247 - acc: 0.6304 - val_loss: 0.5970 - val_acc: 0.6994\n",
      "Epoch 12/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.6348\n",
      "Epoch 12: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.8767 - acc: 0.6350 - val_loss: 0.5979 - val_acc: 0.7020\n",
      "Epoch 13/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.8769 - acc: 0.6296\n",
      "Epoch 13: val_acc did not improve from 0.70500\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.8762 - acc: 0.6299 - val_loss: 0.6010 - val_acc: 0.7041\n",
      "Epoch 14/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.8205 - acc: 0.6438\n",
      "Epoch 14: val_acc improved from 0.70500 to 0.70757, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.8205 - acc: 0.6438 - val_loss: 0.6017 - val_acc: 0.7076\n",
      "Epoch 15/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.8231 - acc: 0.6316\n",
      "Epoch 15: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.8200 - acc: 0.6332 - val_loss: 0.6028 - val_acc: 0.7041\n",
      "Epoch 16/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.7942 - acc: 0.6405\n",
      "Epoch 16: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.7953 - acc: 0.6399 - val_loss: 0.6029 - val_acc: 0.7067\n",
      "Epoch 17/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.7832 - acc: 0.6436\n",
      "Epoch 17: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.7821 - acc: 0.6440 - val_loss: 0.6041 - val_acc: 0.7046\n",
      "Epoch 18/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.6388\n",
      "Epoch 18: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.7662 - acc: 0.6383 - val_loss: 0.6057 - val_acc: 0.7050\n",
      "Epoch 19/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.6421\n",
      "Epoch 19: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.7578 - acc: 0.6421 - val_loss: 0.6066 - val_acc: 0.7059\n",
      "Epoch 20/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.7474 - acc: 0.6494\n",
      "Epoch 20: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.7458 - acc: 0.6500 - val_loss: 0.6080 - val_acc: 0.7067\n",
      "Epoch 21/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.7294 - acc: 0.6500\n",
      "Epoch 21: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.7284 - acc: 0.6506 - val_loss: 0.6076 - val_acc: 0.7041\n",
      "Epoch 22/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.7161 - acc: 0.6575\n",
      "Epoch 22: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.7141 - acc: 0.6585 - val_loss: 0.6088 - val_acc: 0.7041\n",
      "Epoch 23/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.7227 - acc: 0.6465\n",
      "Epoch 23: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.7224 - acc: 0.6465 - val_loss: 0.6096 - val_acc: 0.7050\n",
      "Epoch 24/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7070 - acc: 0.6533\n",
      "Epoch 24: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.7070 - acc: 0.6533 - val_loss: 0.6098 - val_acc: 0.7041\n",
      "Epoch 25/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7041 - acc: 0.6524\n",
      "Epoch 25: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.7041 - acc: 0.6524 - val_loss: 0.6103 - val_acc: 0.7041\n",
      "Epoch 26/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6840 - acc: 0.6587\n",
      "Epoch 26: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6840 - acc: 0.6587 - val_loss: 0.6094 - val_acc: 0.7054\n",
      "Epoch 27/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6970 - acc: 0.6568\n",
      "Epoch 27: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6958 - acc: 0.6571 - val_loss: 0.6098 - val_acc: 0.7046\n",
      "Epoch 28/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6832 - acc: 0.6649\n",
      "Epoch 28: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6828 - acc: 0.6652 - val_loss: 0.6104 - val_acc: 0.7046\n",
      "Epoch 29/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.6671\n",
      "Epoch 29: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6704 - acc: 0.6676 - val_loss: 0.6105 - val_acc: 0.7059\n",
      "Epoch 30/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.6671\n",
      "Epoch 30: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6761 - acc: 0.6669 - val_loss: 0.6111 - val_acc: 0.7050\n",
      "Epoch 31/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6645 - acc: 0.6731\n",
      "Epoch 31: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6640 - acc: 0.6733 - val_loss: 0.6107 - val_acc: 0.7041\n",
      "Epoch 32/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.6624\n",
      "Epoch 32: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6729 - acc: 0.6630 - val_loss: 0.6104 - val_acc: 0.7041\n",
      "Epoch 33/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6643 - acc: 0.6677\n",
      "Epoch 33: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6663 - acc: 0.6665 - val_loss: 0.6099 - val_acc: 0.7041\n",
      "Epoch 34/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6603 - acc: 0.6677\n",
      "Epoch 34: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6603 - acc: 0.6688 - val_loss: 0.6101 - val_acc: 0.7050\n",
      "Epoch 35/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6546 - acc: 0.6769\n",
      "Epoch 35: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6544 - acc: 0.6772 - val_loss: 0.6105 - val_acc: 0.7046\n",
      "Epoch 36/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6453 - acc: 0.6722\n",
      "Epoch 36: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6466 - acc: 0.6725 - val_loss: 0.6100 - val_acc: 0.7059\n",
      "Epoch 37/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.6703\n",
      "Epoch 37: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6489 - acc: 0.6707 - val_loss: 0.6102 - val_acc: 0.7041\n",
      "Epoch 38/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6543 - acc: 0.6791\n",
      "Epoch 38: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6543 - acc: 0.6793 - val_loss: 0.6112 - val_acc: 0.7059\n",
      "Epoch 39/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6524 - acc: 0.6691\n",
      "Epoch 39: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6505 - acc: 0.6697 - val_loss: 0.6106 - val_acc: 0.7050\n",
      "Epoch 40/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.6769\n",
      "Epoch 40: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6460 - acc: 0.6764 - val_loss: 0.6107 - val_acc: 0.7050\n",
      "Epoch 41/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6355 - acc: 0.6837\n",
      "Epoch 41: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6355 - acc: 0.6837 - val_loss: 0.6099 - val_acc: 0.7041\n",
      "Epoch 42/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6411 - acc: 0.6756\n",
      "Epoch 42: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6410 - acc: 0.6757 - val_loss: 0.6094 - val_acc: 0.7046\n",
      "Epoch 43/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6413 - acc: 0.6800\n",
      "Epoch 43: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6413 - acc: 0.6797 - val_loss: 0.6093 - val_acc: 0.7046\n",
      "Epoch 44/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6265 - acc: 0.6826\n",
      "Epoch 44: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6257 - acc: 0.6835 - val_loss: 0.6092 - val_acc: 0.7041\n",
      "Epoch 45/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6304 - acc: 0.6810\n",
      "Epoch 45: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6288 - acc: 0.6817 - val_loss: 0.6084 - val_acc: 0.7037\n",
      "Epoch 46/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6329 - acc: 0.6796\n",
      "Epoch 46: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6326 - acc: 0.6795 - val_loss: 0.6078 - val_acc: 0.7041\n",
      "Epoch 47/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6264 - acc: 0.6871\n",
      "Epoch 47: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6261 - acc: 0.6875 - val_loss: 0.6078 - val_acc: 0.7046\n",
      "Epoch 48/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6330 - acc: 0.6836\n",
      "Epoch 48: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6345 - acc: 0.6832 - val_loss: 0.6075 - val_acc: 0.7041\n",
      "Epoch 49/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6288 - acc: 0.6782\n",
      "Epoch 49: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6286 - acc: 0.6784 - val_loss: 0.6074 - val_acc: 0.7041\n",
      "Epoch 50/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.6885\n",
      "Epoch 50: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6215 - acc: 0.6888 - val_loss: 0.6073 - val_acc: 0.7059\n",
      "Epoch 51/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6216 - acc: 0.6802\n",
      "Epoch 51: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6212 - acc: 0.6809 - val_loss: 0.6067 - val_acc: 0.7059\n",
      "Epoch 52/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6190 - acc: 0.6936\n",
      "Epoch 52: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6191 - acc: 0.6937 - val_loss: 0.6067 - val_acc: 0.7050\n",
      "Epoch 53/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6262 - acc: 0.6806\n",
      "Epoch 53: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6257 - acc: 0.6803 - val_loss: 0.6063 - val_acc: 0.7054\n",
      "Epoch 54/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6241 - acc: 0.6873\n",
      "Epoch 54: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6234 - acc: 0.6874 - val_loss: 0.6062 - val_acc: 0.7046\n",
      "Epoch 55/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6124 - acc: 0.6896\n",
      "Epoch 55: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6118 - acc: 0.6885 - val_loss: 0.6051 - val_acc: 0.7050\n",
      "Epoch 56/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6178 - acc: 0.6923\n",
      "Epoch 56: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6181 - acc: 0.6919 - val_loss: 0.6045 - val_acc: 0.7050\n",
      "Epoch 57/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6074 - acc: 0.6893\n",
      "Epoch 57: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6080 - acc: 0.6904 - val_loss: 0.6041 - val_acc: 0.7050\n",
      "Epoch 58/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6101 - acc: 0.6932\n",
      "Epoch 58: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6095 - acc: 0.6930 - val_loss: 0.6044 - val_acc: 0.7059\n",
      "Epoch 59/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.6893\n",
      "Epoch 59: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6107 - acc: 0.6894 - val_loss: 0.6040 - val_acc: 0.7054\n",
      "Epoch 60/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.6960\n",
      "Epoch 60: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6117 - acc: 0.6959 - val_loss: 0.6031 - val_acc: 0.7054\n",
      "Epoch 61/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6113 - acc: 0.6925\n",
      "Epoch 61: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6107 - acc: 0.6929 - val_loss: 0.6026 - val_acc: 0.7059\n",
      "Epoch 62/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6121 - acc: 0.6922\n",
      "Epoch 62: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6116 - acc: 0.6918 - val_loss: 0.6020 - val_acc: 0.7054\n",
      "Epoch 63/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6084 - acc: 0.7002\n",
      "Epoch 63: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6094 - acc: 0.6993 - val_loss: 0.6012 - val_acc: 0.7054\n",
      "Epoch 64/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.6966\n",
      "Epoch 64: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6027 - acc: 0.6967 - val_loss: 0.6010 - val_acc: 0.7059\n",
      "Epoch 65/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.6983\n",
      "Epoch 65: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5998 - acc: 0.6987 - val_loss: 0.6003 - val_acc: 0.7054\n",
      "Epoch 66/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6070 - acc: 0.6978\n",
      "Epoch 66: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6069 - acc: 0.6980 - val_loss: 0.6003 - val_acc: 0.7054\n",
      "Epoch 67/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6063 - acc: 0.6942\n",
      "Epoch 67: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6063 - acc: 0.6947 - val_loss: 0.5998 - val_acc: 0.7054\n",
      "Epoch 68/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.6954\n",
      "Epoch 68: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6052 - acc: 0.6955 - val_loss: 0.6004 - val_acc: 0.7059\n",
      "Epoch 69/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6022 - acc: 0.7034\n",
      "Epoch 69: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6022 - acc: 0.7034 - val_loss: 0.6000 - val_acc: 0.7054\n",
      "Epoch 70/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.6967\n",
      "Epoch 70: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6026 - acc: 0.6966 - val_loss: 0.5998 - val_acc: 0.7059\n",
      "Epoch 71/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6014 - acc: 0.6983\n",
      "Epoch 71: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6005 - acc: 0.6992 - val_loss: 0.5991 - val_acc: 0.7059\n",
      "Epoch 72/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.6959\n",
      "Epoch 72: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6030 - acc: 0.6958 - val_loss: 0.5993 - val_acc: 0.7067\n",
      "Epoch 73/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5997 - acc: 0.6972\n",
      "Epoch 73: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5997 - acc: 0.6973 - val_loss: 0.5987 - val_acc: 0.7067\n",
      "Epoch 74/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.7010\n",
      "Epoch 74: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5970 - acc: 0.7012 - val_loss: 0.5983 - val_acc: 0.7067\n",
      "Epoch 75/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5965 - acc: 0.7027\n",
      "Epoch 75: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5956 - acc: 0.7035 - val_loss: 0.5973 - val_acc: 0.7063\n",
      "Epoch 76/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5973 - acc: 0.7010\n",
      "Epoch 76: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5971 - acc: 0.7011 - val_loss: 0.5972 - val_acc: 0.7067\n",
      "Epoch 77/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.7000\n",
      "Epoch 77: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5999 - acc: 0.7002 - val_loss: 0.5974 - val_acc: 0.7071\n",
      "Epoch 78/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5982 - acc: 0.6989\n",
      "Epoch 78: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5981 - acc: 0.6990 - val_loss: 0.5971 - val_acc: 0.7071\n",
      "Epoch 79/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5913 - acc: 0.7061\n",
      "Epoch 79: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5911 - acc: 0.7064 - val_loss: 0.5964 - val_acc: 0.7076\n",
      "Epoch 80/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5961 - acc: 0.7047\n",
      "Epoch 80: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5968 - acc: 0.7042 - val_loss: 0.5953 - val_acc: 0.7076\n",
      "Epoch 81/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5986 - acc: 0.6986\n",
      "Epoch 81: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5985 - acc: 0.6988 - val_loss: 0.5954 - val_acc: 0.7071\n",
      "Epoch 82/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5941 - acc: 0.7005\n",
      "Epoch 82: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5955 - acc: 0.7005 - val_loss: 0.5947 - val_acc: 0.7067\n",
      "Epoch 83/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5988 - acc: 0.7031\n",
      "Epoch 83: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5987 - acc: 0.7028 - val_loss: 0.5949 - val_acc: 0.7067\n",
      "Epoch 84/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6026 - acc: 0.7000\n",
      "Epoch 84: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6027 - acc: 0.6996 - val_loss: 0.5946 - val_acc: 0.7067\n",
      "Epoch 85/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5874 - acc: 0.7066\n",
      "Epoch 85: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5862 - acc: 0.7074 - val_loss: 0.5943 - val_acc: 0.7067\n",
      "Epoch 86/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5984 - acc: 0.7004\n",
      "Epoch 86: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5982 - acc: 0.7004 - val_loss: 0.5940 - val_acc: 0.7071\n",
      "Epoch 87/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5992 - acc: 0.6991\n",
      "Epoch 87: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5981 - acc: 0.6994 - val_loss: 0.5942 - val_acc: 0.7071\n",
      "Epoch 88/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5858 - acc: 0.7036\n",
      "Epoch 88: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5853 - acc: 0.7037 - val_loss: 0.5934 - val_acc: 0.7071\n",
      "Epoch 89/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5895 - acc: 0.7060\n",
      "Epoch 89: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5895 - acc: 0.7059 - val_loss: 0.5932 - val_acc: 0.7067\n",
      "Epoch 90/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5964 - acc: 0.6959\n",
      "Epoch 90: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5964 - acc: 0.6959 - val_loss: 0.5931 - val_acc: 0.7076\n",
      "Epoch 91/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5904 - acc: 0.7004\n",
      "Epoch 91: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5904 - acc: 0.7004 - val_loss: 0.5926 - val_acc: 0.7076\n",
      "Epoch 92/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5974 - acc: 0.7000\n",
      "Epoch 92: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5982 - acc: 0.6992 - val_loss: 0.5924 - val_acc: 0.7076\n",
      "Epoch 93/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5891 - acc: 0.7060\n",
      "Epoch 93: val_acc did not improve from 0.70757\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5893 - acc: 0.7054 - val_loss: 0.5918 - val_acc: 0.7076\n",
      "Epoch 94/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.7021\n",
      "Epoch 94: val_acc improved from 0.70757 to 0.70799, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5876 - acc: 0.7022 - val_loss: 0.5919 - val_acc: 0.7080\n",
      "Epoch 95/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.7051\n",
      "Epoch 95: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5861 - acc: 0.7054 - val_loss: 0.5914 - val_acc: 0.7071\n",
      "Epoch 96/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5828 - acc: 0.7079\n",
      "Epoch 96: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5826 - acc: 0.7080 - val_loss: 0.5913 - val_acc: 0.7071\n",
      "Epoch 97/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5918 - acc: 0.7022\n",
      "Epoch 97: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5917 - acc: 0.7020 - val_loss: 0.5908 - val_acc: 0.7076\n",
      "Epoch 98/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.7033\n",
      "Epoch 98: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5891 - acc: 0.7034 - val_loss: 0.5908 - val_acc: 0.7080\n",
      "Epoch 99/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5859 - acc: 0.7069\n",
      "Epoch 99: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5861 - acc: 0.7073 - val_loss: 0.5906 - val_acc: 0.7071\n",
      "Epoch 100/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.7022\n",
      "Epoch 100: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5902 - acc: 0.7021 - val_loss: 0.5903 - val_acc: 0.7071\n",
      "Epoch 101/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.7055\n",
      "Epoch 101: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5923 - acc: 0.7052 - val_loss: 0.5903 - val_acc: 0.7076\n",
      "Epoch 102/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.7022\n",
      "Epoch 102: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5886 - acc: 0.7023 - val_loss: 0.5902 - val_acc: 0.7071\n",
      "Epoch 103/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5862 - acc: 0.7033\n",
      "Epoch 103: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5860 - acc: 0.7034 - val_loss: 0.5900 - val_acc: 0.7071\n",
      "Epoch 104/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.7099\n",
      "Epoch 104: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5814 - acc: 0.7099 - val_loss: 0.5895 - val_acc: 0.7071\n",
      "Epoch 105/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5900 - acc: 0.7037\n",
      "Epoch 105: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5900 - acc: 0.7040 - val_loss: 0.5890 - val_acc: 0.7071\n",
      "Epoch 106/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.7085\n",
      "Epoch 106: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5795 - acc: 0.7084 - val_loss: 0.5887 - val_acc: 0.7071\n",
      "Epoch 107/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.7039\n",
      "Epoch 107: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5869 - acc: 0.7037 - val_loss: 0.5888 - val_acc: 0.7076\n",
      "Epoch 108/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5826 - acc: 0.7091\n",
      "Epoch 108: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5839 - acc: 0.7086 - val_loss: 0.5880 - val_acc: 0.7076\n",
      "Epoch 109/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5836 - acc: 0.7047\n",
      "Epoch 109: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5830 - acc: 0.7052 - val_loss: 0.5880 - val_acc: 0.7076\n",
      "Epoch 110/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5808 - acc: 0.7087\n",
      "Epoch 110: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5797 - acc: 0.7091 - val_loss: 0.5873 - val_acc: 0.7080\n",
      "Epoch 111/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.7071\n",
      "Epoch 111: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5860 - acc: 0.7073 - val_loss: 0.5869 - val_acc: 0.7080\n",
      "Epoch 112/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5840 - acc: 0.7077\n",
      "Epoch 112: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5840 - acc: 0.7077 - val_loss: 0.5864 - val_acc: 0.7080\n",
      "Epoch 113/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5852 - acc: 0.7100\n",
      "Epoch 113: val_acc did not improve from 0.70799\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5842 - acc: 0.7108 - val_loss: 0.5867 - val_acc: 0.7080\n",
      "Epoch 114/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5788 - acc: 0.7120\n",
      "Epoch 114: val_acc improved from 0.70799 to 0.70928, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5788 - acc: 0.7120 - val_loss: 0.5861 - val_acc: 0.7093\n",
      "Epoch 115/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5798 - acc: 0.7081\n",
      "Epoch 115: val_acc did not improve from 0.70928\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5805 - acc: 0.7081 - val_loss: 0.5858 - val_acc: 0.7093\n",
      "Epoch 116/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5800 - acc: 0.7115\n",
      "Epoch 116: val_acc improved from 0.70928 to 0.70970, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5799 - acc: 0.7115 - val_loss: 0.5856 - val_acc: 0.7097\n",
      "Epoch 117/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.7154\n",
      "Epoch 117: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5771 - acc: 0.7149 - val_loss: 0.5848 - val_acc: 0.7093\n",
      "Epoch 118/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.7033\n",
      "Epoch 118: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5849 - acc: 0.7040 - val_loss: 0.5846 - val_acc: 0.7084\n",
      "Epoch 119/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.7099\n",
      "Epoch 119: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5816 - acc: 0.7104 - val_loss: 0.5841 - val_acc: 0.7093\n",
      "Epoch 120/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.7097\n",
      "Epoch 120: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5780 - acc: 0.7100 - val_loss: 0.5841 - val_acc: 0.7080\n",
      "Epoch 121/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5811 - acc: 0.7072\n",
      "Epoch 121: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5811 - acc: 0.7069 - val_loss: 0.5840 - val_acc: 0.7088\n",
      "Epoch 122/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5760 - acc: 0.7117\n",
      "Epoch 122: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5768 - acc: 0.7117 - val_loss: 0.5837 - val_acc: 0.7088\n",
      "Epoch 123/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.7089\n",
      "Epoch 123: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5791 - acc: 0.7095 - val_loss: 0.5835 - val_acc: 0.7093\n",
      "Epoch 124/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5747 - acc: 0.7127\n",
      "Epoch 124: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5744 - acc: 0.7126 - val_loss: 0.5834 - val_acc: 0.7093\n",
      "Epoch 125/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5786 - acc: 0.7100\n",
      "Epoch 125: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5782 - acc: 0.7102 - val_loss: 0.5832 - val_acc: 0.7097\n",
      "Epoch 126/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5755 - acc: 0.7108\n",
      "Epoch 126: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5745 - acc: 0.7102 - val_loss: 0.5829 - val_acc: 0.7093\n",
      "Epoch 127/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5724 - acc: 0.7135\n",
      "Epoch 127: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5720 - acc: 0.7133 - val_loss: 0.5823 - val_acc: 0.7097\n",
      "Epoch 128/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5748 - acc: 0.7133\n",
      "Epoch 128: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5752 - acc: 0.7123 - val_loss: 0.5819 - val_acc: 0.7097\n",
      "Epoch 129/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5757 - acc: 0.7138\n",
      "Epoch 129: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5756 - acc: 0.7135 - val_loss: 0.5817 - val_acc: 0.7097\n",
      "Epoch 130/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.7106\n",
      "Epoch 130: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5783 - acc: 0.7110 - val_loss: 0.5814 - val_acc: 0.7097\n",
      "Epoch 131/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5773 - acc: 0.7059\n",
      "Epoch 131: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5759 - acc: 0.7066 - val_loss: 0.5813 - val_acc: 0.7097\n",
      "Epoch 132/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.7139\n",
      "Epoch 132: val_acc did not improve from 0.70970\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5680 - acc: 0.7145 - val_loss: 0.5810 - val_acc: 0.7097\n",
      "Epoch 133/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.7173\n",
      "Epoch 133: val_acc improved from 0.70970 to 0.71013, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5700 - acc: 0.7170 - val_loss: 0.5809 - val_acc: 0.7101\n",
      "Epoch 134/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5714 - acc: 0.7137\n",
      "Epoch 134: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5707 - acc: 0.7134 - val_loss: 0.5808 - val_acc: 0.7097\n",
      "Epoch 135/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5707 - acc: 0.7155\n",
      "Epoch 135: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5710 - acc: 0.7156 - val_loss: 0.5810 - val_acc: 0.7097\n",
      "Epoch 136/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.7150\n",
      "Epoch 136: val_acc did not improve from 0.71013\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5677 - acc: 0.7146 - val_loss: 0.5806 - val_acc: 0.7097\n",
      "Epoch 137/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.7143\n",
      "Epoch 137: val_acc improved from 0.71013 to 0.71056, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5755 - acc: 0.7147 - val_loss: 0.5801 - val_acc: 0.7106\n",
      "Epoch 138/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5716 - acc: 0.7157\n",
      "Epoch 138: val_acc did not improve from 0.71056\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5727 - acc: 0.7153 - val_loss: 0.5804 - val_acc: 0.7106\n",
      "Epoch 139/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.7150\n",
      "Epoch 139: val_acc improved from 0.71056 to 0.71184, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5698 - acc: 0.7147 - val_loss: 0.5800 - val_acc: 0.7118\n",
      "Epoch 140/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5712 - acc: 0.7134\n",
      "Epoch 140: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5728 - acc: 0.7125 - val_loss: 0.5797 - val_acc: 0.7114\n",
      "Epoch 141/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5750 - acc: 0.7104\n",
      "Epoch 141: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5744 - acc: 0.7107 - val_loss: 0.5793 - val_acc: 0.7110\n",
      "Epoch 142/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.7180\n",
      "Epoch 142: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5744 - acc: 0.7179 - val_loss: 0.5794 - val_acc: 0.7114\n",
      "Epoch 143/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.7200\n",
      "Epoch 143: val_acc did not improve from 0.71184\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5622 - acc: 0.7201 - val_loss: 0.5791 - val_acc: 0.7118\n",
      "Epoch 144/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7154\n",
      "Epoch 144: val_acc improved from 0.71184 to 0.71270, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5597 - acc: 0.7157 - val_loss: 0.5786 - val_acc: 0.7127\n",
      "Epoch 145/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7137\n",
      "Epoch 145: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5693 - acc: 0.7147 - val_loss: 0.5782 - val_acc: 0.7123\n",
      "Epoch 146/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.7170\n",
      "Epoch 146: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5645 - acc: 0.7176 - val_loss: 0.5777 - val_acc: 0.7127\n",
      "Epoch 147/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5633 - acc: 0.7200\n",
      "Epoch 147: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5633 - acc: 0.7188 - val_loss: 0.5774 - val_acc: 0.7123\n",
      "Epoch 148/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5659 - acc: 0.7173\n",
      "Epoch 148: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5662 - acc: 0.7167 - val_loss: 0.5765 - val_acc: 0.7123\n",
      "Epoch 149/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.7142\n",
      "Epoch 149: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5735 - acc: 0.7143 - val_loss: 0.5765 - val_acc: 0.7123\n",
      "Epoch 150/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.7124\n",
      "Epoch 150: val_acc improved from 0.71270 to 0.71313, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5732 - acc: 0.7126 - val_loss: 0.5764 - val_acc: 0.7131\n",
      "Epoch 151/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5689 - acc: 0.7118\n",
      "Epoch 151: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5686 - acc: 0.7122 - val_loss: 0.5761 - val_acc: 0.7131\n",
      "Epoch 152/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.7164\n",
      "Epoch 152: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5701 - acc: 0.7163 - val_loss: 0.5761 - val_acc: 0.7131\n",
      "Epoch 153/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5636 - acc: 0.7141\n",
      "Epoch 153: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5641 - acc: 0.7145 - val_loss: 0.5756 - val_acc: 0.7131\n",
      "Epoch 154/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5689 - acc: 0.7158\n",
      "Epoch 154: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5693 - acc: 0.7159 - val_loss: 0.5756 - val_acc: 0.7131\n",
      "Epoch 155/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5704 - acc: 0.7193\n",
      "Epoch 155: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5700 - acc: 0.7193 - val_loss: 0.5762 - val_acc: 0.7131\n",
      "Epoch 156/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.7172\n",
      "Epoch 156: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5650 - acc: 0.7174 - val_loss: 0.5755 - val_acc: 0.7131\n",
      "Epoch 157/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.7134\n",
      "Epoch 157: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5698 - acc: 0.7133 - val_loss: 0.5751 - val_acc: 0.7131\n",
      "Epoch 158/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5719 - acc: 0.7140\n",
      "Epoch 158: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5731 - acc: 0.7131 - val_loss: 0.5752 - val_acc: 0.7131\n",
      "Epoch 159/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5649 - acc: 0.7164\n",
      "Epoch 159: val_acc did not improve from 0.71313\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5651 - acc: 0.7167 - val_loss: 0.5747 - val_acc: 0.7131\n",
      "Epoch 160/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5663 - acc: 0.7196\n",
      "Epoch 160: val_acc improved from 0.71313 to 0.71355, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5669 - acc: 0.7194 - val_loss: 0.5743 - val_acc: 0.7136\n",
      "Epoch 161/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.7177\n",
      "Epoch 161: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5662 - acc: 0.7174 - val_loss: 0.5742 - val_acc: 0.7131\n",
      "Epoch 162/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5663 - acc: 0.7164\n",
      "Epoch 162: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5670 - acc: 0.7151 - val_loss: 0.5739 - val_acc: 0.7131\n",
      "Epoch 163/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.7188\n",
      "Epoch 163: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5671 - acc: 0.7181 - val_loss: 0.5741 - val_acc: 0.7131\n",
      "Epoch 164/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5657 - acc: 0.7182\n",
      "Epoch 164: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5654 - acc: 0.7177 - val_loss: 0.5736 - val_acc: 0.7131\n",
      "Epoch 165/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5695 - acc: 0.7148\n",
      "Epoch 165: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5688 - acc: 0.7149 - val_loss: 0.5735 - val_acc: 0.7136\n",
      "Epoch 166/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5712 - acc: 0.7157\n",
      "Epoch 166: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5712 - acc: 0.7157 - val_loss: 0.5735 - val_acc: 0.7131\n",
      "Epoch 167/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5577 - acc: 0.7212\n",
      "Epoch 167: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5588 - acc: 0.7214 - val_loss: 0.5727 - val_acc: 0.7136\n",
      "Epoch 168/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5720 - acc: 0.7153\n",
      "Epoch 168: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5716 - acc: 0.7152 - val_loss: 0.5727 - val_acc: 0.7136\n",
      "Epoch 169/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5704 - acc: 0.7166\n",
      "Epoch 169: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5706 - acc: 0.7164 - val_loss: 0.5730 - val_acc: 0.7136\n",
      "Epoch 170/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.7216\n",
      "Epoch 170: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5634 - acc: 0.7211 - val_loss: 0.5725 - val_acc: 0.7136\n",
      "Epoch 171/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5659 - acc: 0.7175\n",
      "Epoch 171: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5663 - acc: 0.7168 - val_loss: 0.5726 - val_acc: 0.7136\n",
      "Epoch 172/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.7180\n",
      "Epoch 172: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5653 - acc: 0.7184 - val_loss: 0.5722 - val_acc: 0.7136\n",
      "Epoch 173/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7178\n",
      "Epoch 173: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5582 - acc: 0.7182 - val_loss: 0.5717 - val_acc: 0.7136\n",
      "Epoch 174/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5666 - acc: 0.7180\n",
      "Epoch 174: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5655 - acc: 0.7183 - val_loss: 0.5719 - val_acc: 0.7136\n",
      "Epoch 175/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5578 - acc: 0.7214\n",
      "Epoch 175: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5580 - acc: 0.7209 - val_loss: 0.5712 - val_acc: 0.7136\n",
      "Epoch 176/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.7163\n",
      "Epoch 176: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5648 - acc: 0.7165 - val_loss: 0.5713 - val_acc: 0.7136\n",
      "Epoch 177/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5645 - acc: 0.7166\n",
      "Epoch 177: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5644 - acc: 0.7172 - val_loss: 0.5711 - val_acc: 0.7131\n",
      "Epoch 178/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5646 - acc: 0.7178\n",
      "Epoch 178: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5645 - acc: 0.7178 - val_loss: 0.5711 - val_acc: 0.7131\n",
      "Epoch 179/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7188\n",
      "Epoch 179: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5593 - acc: 0.7195 - val_loss: 0.5707 - val_acc: 0.7131\n",
      "Epoch 180/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5648 - acc: 0.7179\n",
      "Epoch 180: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5652 - acc: 0.7174 - val_loss: 0.5707 - val_acc: 0.7131\n",
      "Epoch 181/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5633 - acc: 0.7170\n",
      "Epoch 181: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5633 - acc: 0.7178 - val_loss: 0.5706 - val_acc: 0.7131\n",
      "Epoch 182/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5575 - acc: 0.7188\n",
      "Epoch 182: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5584 - acc: 0.7187 - val_loss: 0.5702 - val_acc: 0.7131\n",
      "Epoch 183/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.7186\n",
      "Epoch 183: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5636 - acc: 0.7184 - val_loss: 0.5700 - val_acc: 0.7131\n",
      "Epoch 184/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.7185\n",
      "Epoch 184: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5605 - acc: 0.7187 - val_loss: 0.5702 - val_acc: 0.7131\n",
      "Epoch 185/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.7183\n",
      "Epoch 185: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5590 - acc: 0.7179 - val_loss: 0.5699 - val_acc: 0.7136\n",
      "Epoch 186/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5586 - acc: 0.7216\n",
      "Epoch 186: val_acc improved from 0.71355 to 0.71398, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5609 - acc: 0.7198 - val_loss: 0.5700 - val_acc: 0.7140\n",
      "Epoch 187/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5585 - acc: 0.7212\n",
      "Epoch 187: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5578 - acc: 0.7215 - val_loss: 0.5698 - val_acc: 0.7136\n",
      "Epoch 188/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5545 - acc: 0.7213\n",
      "Epoch 188: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5542 - acc: 0.7211 - val_loss: 0.5690 - val_acc: 0.7140\n",
      "Epoch 189/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.7181\n",
      "Epoch 189: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5591 - acc: 0.7180 - val_loss: 0.5687 - val_acc: 0.7140\n",
      "Epoch 190/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5648 - acc: 0.7165\n",
      "Epoch 190: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5645 - acc: 0.7165 - val_loss: 0.5688 - val_acc: 0.7140\n",
      "Epoch 191/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5574 - acc: 0.7170\n",
      "Epoch 191: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5580 - acc: 0.7176 - val_loss: 0.5682 - val_acc: 0.7140\n",
      "Epoch 192/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5614 - acc: 0.7189\n",
      "Epoch 192: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5609 - acc: 0.7183 - val_loss: 0.5682 - val_acc: 0.7140\n",
      "Epoch 193/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5583 - acc: 0.7224\n",
      "Epoch 193: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5577 - acc: 0.7216 - val_loss: 0.5677 - val_acc: 0.7140\n",
      "Epoch 194/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5557 - acc: 0.7266\n",
      "Epoch 194: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5562 - acc: 0.7260 - val_loss: 0.5675 - val_acc: 0.7140\n",
      "Epoch 195/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5608 - acc: 0.7229\n",
      "Epoch 195: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5603 - acc: 0.7231 - val_loss: 0.5675 - val_acc: 0.7136\n",
      "Epoch 196/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7203\n",
      "Epoch 196: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5605 - acc: 0.7203 - val_loss: 0.5673 - val_acc: 0.7140\n",
      "Epoch 197/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.7228\n",
      "Epoch 197: val_acc did not improve from 0.71398\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5561 - acc: 0.7229 - val_loss: 0.5669 - val_acc: 0.7140\n",
      "Epoch 198/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5583 - acc: 0.7190\n",
      "Epoch 198: val_acc improved from 0.71398 to 0.71441, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5588 - acc: 0.7190 - val_loss: 0.5666 - val_acc: 0.7144\n",
      "Epoch 199/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.7185\n",
      "Epoch 199: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5592 - acc: 0.7187 - val_loss: 0.5663 - val_acc: 0.7140\n",
      "Epoch 200/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5612 - acc: 0.7210\n",
      "Epoch 200: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5608 - acc: 0.7211 - val_loss: 0.5662 - val_acc: 0.7140\n",
      "Epoch 201/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.7194\n",
      "Epoch 201: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5579 - acc: 0.7194 - val_loss: 0.5661 - val_acc: 0.7144\n",
      "Epoch 202/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.7219\n",
      "Epoch 202: val_acc improved from 0.71441 to 0.71484, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5580 - acc: 0.7216 - val_loss: 0.5661 - val_acc: 0.7148\n",
      "Epoch 203/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5535 - acc: 0.7241\n",
      "Epoch 203: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5537 - acc: 0.7241 - val_loss: 0.5658 - val_acc: 0.7148\n",
      "Epoch 204/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.7253\n",
      "Epoch 204: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5546 - acc: 0.7254 - val_loss: 0.5657 - val_acc: 0.7148\n",
      "Epoch 205/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.7254\n",
      "Epoch 205: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5549 - acc: 0.7255 - val_loss: 0.5654 - val_acc: 0.7148\n",
      "Epoch 206/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.7164\n",
      "Epoch 206: val_acc did not improve from 0.71484\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5582 - acc: 0.7164 - val_loss: 0.5653 - val_acc: 0.7148\n",
      "Epoch 207/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5561 - acc: 0.7199\n",
      "Epoch 207: val_acc improved from 0.71484 to 0.71526, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5562 - acc: 0.7193 - val_loss: 0.5647 - val_acc: 0.7153\n",
      "Epoch 208/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.7252\n",
      "Epoch 208: val_acc did not improve from 0.71526\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5547 - acc: 0.7252 - val_loss: 0.5649 - val_acc: 0.7153\n",
      "Epoch 209/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.7233\n",
      "Epoch 209: val_acc improved from 0.71526 to 0.71612, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5512 - acc: 0.7238 - val_loss: 0.5644 - val_acc: 0.7161\n",
      "Epoch 210/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.7236\n",
      "Epoch 210: val_acc improved from 0.71612 to 0.71655, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5557 - acc: 0.7238 - val_loss: 0.5645 - val_acc: 0.7165\n",
      "Epoch 211/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5493 - acc: 0.7257\n",
      "Epoch 211: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5494 - acc: 0.7255 - val_loss: 0.5642 - val_acc: 0.7161\n",
      "Epoch 212/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5530 - acc: 0.7224\n",
      "Epoch 212: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5536 - acc: 0.7209 - val_loss: 0.5642 - val_acc: 0.7161\n",
      "Epoch 213/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5532 - acc: 0.7263\n",
      "Epoch 213: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5543 - acc: 0.7262 - val_loss: 0.5641 - val_acc: 0.7165\n",
      "Epoch 214/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5590 - acc: 0.7199\n",
      "Epoch 214: val_acc did not improve from 0.71655\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5587 - acc: 0.7204 - val_loss: 0.5638 - val_acc: 0.7165\n",
      "Epoch 215/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5552 - acc: 0.7241\n",
      "Epoch 215: val_acc improved from 0.71655 to 0.71697, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5556 - acc: 0.7232 - val_loss: 0.5640 - val_acc: 0.7170\n",
      "Epoch 216/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5506 - acc: 0.7249\n",
      "Epoch 216: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5507 - acc: 0.7242 - val_loss: 0.5637 - val_acc: 0.7170\n",
      "Epoch 217/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5583 - acc: 0.7212\n",
      "Epoch 217: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5593 - acc: 0.7207 - val_loss: 0.5635 - val_acc: 0.7170\n",
      "Epoch 218/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.7250\n",
      "Epoch 218: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5499 - acc: 0.7250 - val_loss: 0.5629 - val_acc: 0.7170\n",
      "Epoch 219/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5542 - acc: 0.7189\n",
      "Epoch 219: val_acc did not improve from 0.71697\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5551 - acc: 0.7182 - val_loss: 0.5628 - val_acc: 0.7165\n",
      "Epoch 220/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5528 - acc: 0.7262\n",
      "Epoch 220: val_acc improved from 0.71697 to 0.71740, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5529 - acc: 0.7258 - val_loss: 0.5623 - val_acc: 0.7174\n",
      "Epoch 221/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7216\n",
      "Epoch 221: val_acc improved from 0.71740 to 0.71783, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5539 - acc: 0.7218 - val_loss: 0.5623 - val_acc: 0.7178\n",
      "Epoch 222/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.7237\n",
      "Epoch 222: val_acc did not improve from 0.71783\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5547 - acc: 0.7229 - val_loss: 0.5623 - val_acc: 0.7178\n",
      "Epoch 223/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.7190\n",
      "Epoch 223: val_acc did not improve from 0.71783\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5588 - acc: 0.7193 - val_loss: 0.5620 - val_acc: 0.7178\n",
      "Epoch 224/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5520 - acc: 0.7255\n",
      "Epoch 224: val_acc did not improve from 0.71783\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5528 - acc: 0.7253 - val_loss: 0.5618 - val_acc: 0.7178\n",
      "Epoch 225/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.7180\n",
      "Epoch 225: val_acc did not improve from 0.71783\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5583 - acc: 0.7174 - val_loss: 0.5618 - val_acc: 0.7178\n",
      "Epoch 226/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5513 - acc: 0.7246\n",
      "Epoch 226: val_acc improved from 0.71783 to 0.71954, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7246 - val_loss: 0.5617 - val_acc: 0.7195\n",
      "Epoch 227/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5581 - acc: 0.7224\n",
      "Epoch 227: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5582 - acc: 0.7216 - val_loss: 0.5611 - val_acc: 0.7183\n",
      "Epoch 228/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7269\n",
      "Epoch 228: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5511 - acc: 0.7275 - val_loss: 0.5607 - val_acc: 0.7191\n",
      "Epoch 229/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.7206\n",
      "Epoch 229: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5531 - acc: 0.7207 - val_loss: 0.5606 - val_acc: 0.7187\n",
      "Epoch 230/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5507 - acc: 0.7286\n",
      "Epoch 230: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5498 - acc: 0.7292 - val_loss: 0.5601 - val_acc: 0.7187\n",
      "Epoch 231/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7249\n",
      "Epoch 231: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7247 - val_loss: 0.5600 - val_acc: 0.7191\n",
      "Epoch 232/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5495 - acc: 0.7262\n",
      "Epoch 232: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5505 - acc: 0.7250 - val_loss: 0.5595 - val_acc: 0.7191\n",
      "Epoch 233/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5532 - acc: 0.7248\n",
      "Epoch 233: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5546 - acc: 0.7238 - val_loss: 0.5594 - val_acc: 0.7191\n",
      "Epoch 234/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.7242\n",
      "Epoch 234: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5498 - acc: 0.7243 - val_loss: 0.5594 - val_acc: 0.7191\n",
      "Epoch 235/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.7265\n",
      "Epoch 235: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5504 - acc: 0.7253 - val_loss: 0.5592 - val_acc: 0.7191\n",
      "Epoch 236/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7259\n",
      "Epoch 236: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5490 - acc: 0.7257 - val_loss: 0.5588 - val_acc: 0.7191\n",
      "Epoch 237/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7248\n",
      "Epoch 237: val_acc did not improve from 0.71954\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5530 - acc: 0.7249 - val_loss: 0.5589 - val_acc: 0.7195\n",
      "Epoch 238/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5533 - acc: 0.7226\n",
      "Epoch 238: val_acc improved from 0.71954 to 0.72039, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5534 - acc: 0.7226 - val_loss: 0.5588 - val_acc: 0.7204\n",
      "Epoch 239/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.7268\n",
      "Epoch 239: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5521 - acc: 0.7269 - val_loss: 0.5585 - val_acc: 0.7204\n",
      "Epoch 240/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.7226\n",
      "Epoch 240: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5508 - acc: 0.7225 - val_loss: 0.5586 - val_acc: 0.7204\n",
      "Epoch 241/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.7266\n",
      "Epoch 241: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5519 - acc: 0.7266 - val_loss: 0.5583 - val_acc: 0.7195\n",
      "Epoch 242/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5516 - acc: 0.7240\n",
      "Epoch 242: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7237 - val_loss: 0.5584 - val_acc: 0.7200\n",
      "Epoch 243/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.7250\n",
      "Epoch 243: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5537 - acc: 0.7251 - val_loss: 0.5584 - val_acc: 0.7200\n",
      "Epoch 244/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5539 - acc: 0.7229\n",
      "Epoch 244: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5535 - acc: 0.7236 - val_loss: 0.5586 - val_acc: 0.7204\n",
      "Epoch 245/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5459 - acc: 0.7253\n",
      "Epoch 245: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5469 - acc: 0.7250 - val_loss: 0.5585 - val_acc: 0.7204\n",
      "Epoch 246/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.7249\n",
      "Epoch 246: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5484 - acc: 0.7250 - val_loss: 0.5585 - val_acc: 0.7204\n",
      "Epoch 247/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7293\n",
      "Epoch 247: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5497 - acc: 0.7298 - val_loss: 0.5580 - val_acc: 0.7200\n",
      "Epoch 248/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5467 - acc: 0.7276\n",
      "Epoch 248: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5468 - acc: 0.7271 - val_loss: 0.5577 - val_acc: 0.7204\n",
      "Epoch 249/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5451 - acc: 0.7294\n",
      "Epoch 249: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5458 - acc: 0.7284 - val_loss: 0.5576 - val_acc: 0.7204\n",
      "Epoch 250/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7251\n",
      "Epoch 250: val_acc improved from 0.72039 to 0.72125, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5527 - acc: 0.7250 - val_loss: 0.5576 - val_acc: 0.7212\n",
      "Epoch 251/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5445 - acc: 0.7258\n",
      "Epoch 251: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5453 - acc: 0.7249 - val_loss: 0.5570 - val_acc: 0.7212\n",
      "Epoch 252/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5495 - acc: 0.7264\n",
      "Epoch 252: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5502 - acc: 0.7258 - val_loss: 0.5569 - val_acc: 0.7208\n",
      "Epoch 253/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7295\n",
      "Epoch 253: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5461 - acc: 0.7291 - val_loss: 0.5565 - val_acc: 0.7208\n",
      "Epoch 254/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5454 - acc: 0.7256\n",
      "Epoch 254: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5459 - acc: 0.7242 - val_loss: 0.5562 - val_acc: 0.7208\n",
      "Epoch 255/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5510 - acc: 0.7233\n",
      "Epoch 255: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5508 - acc: 0.7232 - val_loss: 0.5563 - val_acc: 0.7204\n",
      "Epoch 256/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5471 - acc: 0.7283\n",
      "Epoch 256: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5464 - acc: 0.7284 - val_loss: 0.5561 - val_acc: 0.7208\n",
      "Epoch 257/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5467 - acc: 0.7271\n",
      "Epoch 257: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5465 - acc: 0.7267 - val_loss: 0.5563 - val_acc: 0.7208\n",
      "Epoch 258/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5475 - acc: 0.7273\n",
      "Epoch 258: val_acc did not improve from 0.72125\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5475 - acc: 0.7276 - val_loss: 0.5557 - val_acc: 0.7208\n",
      "Epoch 259/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.7276\n",
      "Epoch 259: val_acc improved from 0.72125 to 0.72168, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5489 - acc: 0.7272 - val_loss: 0.5559 - val_acc: 0.7217\n",
      "Epoch 260/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.7243\n",
      "Epoch 260: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5513 - acc: 0.7246 - val_loss: 0.5560 - val_acc: 0.7212\n",
      "Epoch 261/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5450 - acc: 0.7331\n",
      "Epoch 261: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5455 - acc: 0.7323 - val_loss: 0.5553 - val_acc: 0.7217\n",
      "Epoch 262/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5464 - acc: 0.7265\n",
      "Epoch 262: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5468 - acc: 0.7254 - val_loss: 0.5550 - val_acc: 0.7217\n",
      "Epoch 263/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5462 - acc: 0.7240\n",
      "Epoch 263: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5465 - acc: 0.7237 - val_loss: 0.5546 - val_acc: 0.7212\n",
      "Epoch 264/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.7255\n",
      "Epoch 264: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5465 - acc: 0.7258 - val_loss: 0.5542 - val_acc: 0.7212\n",
      "Epoch 265/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5447 - acc: 0.7265\n",
      "Epoch 265: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5453 - acc: 0.7259 - val_loss: 0.5541 - val_acc: 0.7217\n",
      "Epoch 266/2000\n",
      "267/293 [==========================>...] - ETA: 0s - loss: 0.5455 - acc: 0.7252\n",
      "Epoch 266: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5462 - acc: 0.7252 - val_loss: 0.5542 - val_acc: 0.7208\n",
      "Epoch 267/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5455 - acc: 0.7264\n",
      "Epoch 267: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5458 - acc: 0.7266 - val_loss: 0.5543 - val_acc: 0.7217\n",
      "Epoch 268/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5456 - acc: 0.7287\n",
      "Epoch 268: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5464 - acc: 0.7276 - val_loss: 0.5541 - val_acc: 0.7217\n",
      "Epoch 269/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5458 - acc: 0.7280\n",
      "Epoch 269: val_acc did not improve from 0.72168\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5462 - acc: 0.7280 - val_loss: 0.5540 - val_acc: 0.7212\n",
      "Epoch 270/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5501 - acc: 0.7247\n",
      "Epoch 270: val_acc improved from 0.72168 to 0.72210, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5501 - acc: 0.7247 - val_loss: 0.5535 - val_acc: 0.7221\n",
      "Epoch 271/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5442 - acc: 0.7246\n",
      "Epoch 271: val_acc did not improve from 0.72210\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5445 - acc: 0.7242 - val_loss: 0.5536 - val_acc: 0.7221\n",
      "Epoch 272/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.7316\n",
      "Epoch 272: val_acc did not improve from 0.72210\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5480 - acc: 0.7315 - val_loss: 0.5534 - val_acc: 0.7221\n",
      "Epoch 273/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5431 - acc: 0.7254\n",
      "Epoch 273: val_acc did not improve from 0.72210\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5431 - acc: 0.7254 - val_loss: 0.5533 - val_acc: 0.7221\n",
      "Epoch 274/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7317\n",
      "Epoch 274: val_acc improved from 0.72210 to 0.72253, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5402 - acc: 0.7315 - val_loss: 0.5533 - val_acc: 0.7225\n",
      "Epoch 275/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5475 - acc: 0.7253\n",
      "Epoch 275: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5466 - acc: 0.7259 - val_loss: 0.5531 - val_acc: 0.7225\n",
      "Epoch 276/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5454 - acc: 0.7237\n",
      "Epoch 276: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5464 - acc: 0.7224 - val_loss: 0.5531 - val_acc: 0.7225\n",
      "Epoch 277/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.7321\n",
      "Epoch 277: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5434 - acc: 0.7322 - val_loss: 0.5530 - val_acc: 0.7225\n",
      "Epoch 278/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5537 - acc: 0.7250\n",
      "Epoch 278: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5545 - acc: 0.7246 - val_loss: 0.5531 - val_acc: 0.7225\n",
      "Epoch 279/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.7251\n",
      "Epoch 279: val_acc improved from 0.72253 to 0.72296, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5420 - acc: 0.7259 - val_loss: 0.5526 - val_acc: 0.7230\n",
      "Epoch 280/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5402 - acc: 0.7298\n",
      "Epoch 280: val_acc did not improve from 0.72296\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5411 - acc: 0.7296 - val_loss: 0.5523 - val_acc: 0.7230\n",
      "Epoch 281/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5462 - acc: 0.7255\n",
      "Epoch 281: val_acc did not improve from 0.72296\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5460 - acc: 0.7255 - val_loss: 0.5523 - val_acc: 0.7225\n",
      "Epoch 282/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5460 - acc: 0.7305\n",
      "Epoch 282: val_acc did not improve from 0.72296\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5457 - acc: 0.7304 - val_loss: 0.5527 - val_acc: 0.7230\n",
      "Epoch 283/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5415 - acc: 0.7293\n",
      "Epoch 283: val_acc did not improve from 0.72296\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5413 - acc: 0.7292 - val_loss: 0.5522 - val_acc: 0.7225\n",
      "Epoch 284/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5410 - acc: 0.7306\n",
      "Epoch 284: val_acc improved from 0.72296 to 0.72339, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5425 - acc: 0.7300 - val_loss: 0.5518 - val_acc: 0.7234\n",
      "Epoch 285/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5408 - acc: 0.7294\n",
      "Epoch 285: val_acc did not improve from 0.72339\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5411 - acc: 0.7287 - val_loss: 0.5518 - val_acc: 0.7221\n",
      "Epoch 286/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5471 - acc: 0.7271\n",
      "Epoch 286: val_acc did not improve from 0.72339\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5475 - acc: 0.7269 - val_loss: 0.5517 - val_acc: 0.7230\n",
      "Epoch 287/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5452 - acc: 0.7314\n",
      "Epoch 287: val_acc did not improve from 0.72339\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5463 - acc: 0.7299 - val_loss: 0.5517 - val_acc: 0.7230\n",
      "Epoch 288/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7270\n",
      "Epoch 288: val_acc improved from 0.72339 to 0.72381, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5440 - acc: 0.7270 - val_loss: 0.5512 - val_acc: 0.7238\n",
      "Epoch 289/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5396 - acc: 0.7278\n",
      "Epoch 289: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5407 - acc: 0.7266 - val_loss: 0.5514 - val_acc: 0.7238\n",
      "Epoch 290/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5451 - acc: 0.7297\n",
      "Epoch 290: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5463 - acc: 0.7290 - val_loss: 0.5514 - val_acc: 0.7238\n",
      "Epoch 291/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5452 - acc: 0.7235\n",
      "Epoch 291: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5454 - acc: 0.7232 - val_loss: 0.5514 - val_acc: 0.7234\n",
      "Epoch 292/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5412 - acc: 0.7282\n",
      "Epoch 292: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5412 - acc: 0.7282 - val_loss: 0.5512 - val_acc: 0.7230\n",
      "Epoch 293/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.7287\n",
      "Epoch 293: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5421 - acc: 0.7287 - val_loss: 0.5509 - val_acc: 0.7230\n",
      "Epoch 294/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7288\n",
      "Epoch 294: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5404 - acc: 0.7289 - val_loss: 0.5507 - val_acc: 0.7230\n",
      "Epoch 295/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.7303\n",
      "Epoch 295: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5458 - acc: 0.7301 - val_loss: 0.5504 - val_acc: 0.7234\n",
      "Epoch 296/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5425 - acc: 0.7330\n",
      "Epoch 296: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5424 - acc: 0.7330 - val_loss: 0.5504 - val_acc: 0.7230\n",
      "Epoch 297/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5434 - acc: 0.7279\n",
      "Epoch 297: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5433 - acc: 0.7283 - val_loss: 0.5506 - val_acc: 0.7238\n",
      "Epoch 298/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5412 - acc: 0.7287\n",
      "Epoch 298: val_acc improved from 0.72381 to 0.72424, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5411 - acc: 0.7276 - val_loss: 0.5506 - val_acc: 0.7242\n",
      "Epoch 299/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5437 - acc: 0.7299\n",
      "Epoch 299: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5438 - acc: 0.7297 - val_loss: 0.5508 - val_acc: 0.7238\n",
      "Epoch 300/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5434 - acc: 0.7287\n",
      "Epoch 300: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5435 - acc: 0.7283 - val_loss: 0.5508 - val_acc: 0.7238\n",
      "Epoch 301/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5427 - acc: 0.7287\n",
      "Epoch 301: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5424 - acc: 0.7288 - val_loss: 0.5501 - val_acc: 0.7234\n",
      "Epoch 302/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5442 - acc: 0.7280\n",
      "Epoch 302: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5443 - acc: 0.7276 - val_loss: 0.5499 - val_acc: 0.7230\n",
      "Epoch 303/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7327\n",
      "Epoch 303: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5397 - acc: 0.7325 - val_loss: 0.5496 - val_acc: 0.7242\n",
      "Epoch 304/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5435 - acc: 0.7275\n",
      "Epoch 304: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5433 - acc: 0.7274 - val_loss: 0.5494 - val_acc: 0.7238\n",
      "Epoch 305/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.7298\n",
      "Epoch 305: val_acc improved from 0.72424 to 0.72510, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5413 - acc: 0.7289 - val_loss: 0.5494 - val_acc: 0.7251\n",
      "Epoch 306/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5379 - acc: 0.7316\n",
      "Epoch 306: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5390 - acc: 0.7305 - val_loss: 0.5493 - val_acc: 0.7251\n",
      "Epoch 307/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5440 - acc: 0.7287\n",
      "Epoch 307: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5450 - acc: 0.7284 - val_loss: 0.5489 - val_acc: 0.7251\n",
      "Epoch 308/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7308\n",
      "Epoch 308: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5362 - acc: 0.7309 - val_loss: 0.5486 - val_acc: 0.7247\n",
      "Epoch 309/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5366 - acc: 0.7305\n",
      "Epoch 309: val_acc improved from 0.72510 to 0.72552, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5381 - acc: 0.7301 - val_loss: 0.5487 - val_acc: 0.7255\n",
      "Epoch 310/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5382 - acc: 0.7291\n",
      "Epoch 310: val_acc did not improve from 0.72552\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5381 - acc: 0.7291 - val_loss: 0.5483 - val_acc: 0.7247\n",
      "Epoch 311/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7293\n",
      "Epoch 311: val_acc did not improve from 0.72552\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5407 - acc: 0.7294 - val_loss: 0.5484 - val_acc: 0.7251\n",
      "Epoch 312/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5402 - acc: 0.7332\n",
      "Epoch 312: val_acc did not improve from 0.72552\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5400 - acc: 0.7328 - val_loss: 0.5481 - val_acc: 0.7251\n",
      "Epoch 313/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5408 - acc: 0.7295\n",
      "Epoch 313: val_acc improved from 0.72552 to 0.72595, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5408 - acc: 0.7294 - val_loss: 0.5478 - val_acc: 0.7260\n",
      "Epoch 314/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5424 - acc: 0.7279\n",
      "Epoch 314: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5430 - acc: 0.7281 - val_loss: 0.5480 - val_acc: 0.7255\n",
      "Epoch 315/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.7311\n",
      "Epoch 315: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5352 - acc: 0.7309 - val_loss: 0.5479 - val_acc: 0.7251\n",
      "Epoch 316/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5378 - acc: 0.7301\n",
      "Epoch 316: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5386 - acc: 0.7294 - val_loss: 0.5477 - val_acc: 0.7251\n",
      "Epoch 317/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5415 - acc: 0.7288\n",
      "Epoch 317: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5407 - acc: 0.7287 - val_loss: 0.5475 - val_acc: 0.7247\n",
      "Epoch 318/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5345 - acc: 0.7312\n",
      "Epoch 318: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5345 - acc: 0.7312 - val_loss: 0.5470 - val_acc: 0.7251\n",
      "Epoch 319/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7296\n",
      "Epoch 319: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5382 - acc: 0.7297 - val_loss: 0.5469 - val_acc: 0.7251\n",
      "Epoch 320/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7299\n",
      "Epoch 320: val_acc improved from 0.72595 to 0.72681, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5446 - acc: 0.7306 - val_loss: 0.5472 - val_acc: 0.7268\n",
      "Epoch 321/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.7335\n",
      "Epoch 321: val_acc did not improve from 0.72681\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5407 - acc: 0.7330 - val_loss: 0.5469 - val_acc: 0.7268\n",
      "Epoch 322/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7306\n",
      "Epoch 322: val_acc did not improve from 0.72681\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5397 - acc: 0.7305 - val_loss: 0.5470 - val_acc: 0.7264\n",
      "Epoch 323/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5393 - acc: 0.7299\n",
      "Epoch 323: val_acc did not improve from 0.72681\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5407 - acc: 0.7289 - val_loss: 0.5469 - val_acc: 0.7260\n",
      "Epoch 324/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.7305\n",
      "Epoch 324: val_acc improved from 0.72681 to 0.72723, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5389 - acc: 0.7314 - val_loss: 0.5469 - val_acc: 0.7272\n",
      "Epoch 325/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5396 - acc: 0.7287\n",
      "Epoch 325: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5392 - acc: 0.7287 - val_loss: 0.5468 - val_acc: 0.7272\n",
      "Epoch 326/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.7337\n",
      "Epoch 326: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5391 - acc: 0.7338 - val_loss: 0.5467 - val_acc: 0.7268\n",
      "Epoch 327/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5356 - acc: 0.7279\n",
      "Epoch 327: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7270 - val_loss: 0.5463 - val_acc: 0.7272\n",
      "Epoch 328/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7339\n",
      "Epoch 328: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5360 - acc: 0.7344 - val_loss: 0.5462 - val_acc: 0.7272\n",
      "Epoch 329/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.7311\n",
      "Epoch 329: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5415 - acc: 0.7309 - val_loss: 0.5461 - val_acc: 0.7268\n",
      "Epoch 330/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7292\n",
      "Epoch 330: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5389 - acc: 0.7292 - val_loss: 0.5457 - val_acc: 0.7272\n",
      "Epoch 331/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.7303\n",
      "Epoch 331: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5371 - acc: 0.7307 - val_loss: 0.5456 - val_acc: 0.7268\n",
      "Epoch 332/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5379 - acc: 0.7318\n",
      "Epoch 332: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5389 - acc: 0.7314 - val_loss: 0.5456 - val_acc: 0.7272\n",
      "Epoch 333/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5383 - acc: 0.7294\n",
      "Epoch 333: val_acc improved from 0.72723 to 0.72766, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5388 - acc: 0.7291 - val_loss: 0.5454 - val_acc: 0.7277\n",
      "Epoch 334/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5358 - acc: 0.7303\n",
      "Epoch 334: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5374 - acc: 0.7302 - val_loss: 0.5456 - val_acc: 0.7272\n",
      "Epoch 335/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7318\n",
      "Epoch 335: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5379 - acc: 0.7316 - val_loss: 0.5454 - val_acc: 0.7272\n",
      "Epoch 336/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5347 - acc: 0.7303\n",
      "Epoch 336: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5364 - acc: 0.7293 - val_loss: 0.5452 - val_acc: 0.7272\n",
      "Epoch 337/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5427 - acc: 0.7267\n",
      "Epoch 337: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5427 - acc: 0.7267 - val_loss: 0.5453 - val_acc: 0.7272\n",
      "Epoch 338/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7281\n",
      "Epoch 338: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5379 - acc: 0.7287 - val_loss: 0.5452 - val_acc: 0.7268\n",
      "Epoch 339/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7292\n",
      "Epoch 339: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5382 - acc: 0.7291 - val_loss: 0.5451 - val_acc: 0.7264\n",
      "Epoch 340/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5389 - acc: 0.7318\n",
      "Epoch 340: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5403 - acc: 0.7313 - val_loss: 0.5449 - val_acc: 0.7268\n",
      "Epoch 341/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7303\n",
      "Epoch 341: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5379 - acc: 0.7306 - val_loss: 0.5450 - val_acc: 0.7277\n",
      "Epoch 342/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5377 - acc: 0.7311\n",
      "Epoch 342: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7314 - val_loss: 0.5450 - val_acc: 0.7272\n",
      "Epoch 343/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5380 - acc: 0.7348\n",
      "Epoch 343: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5385 - acc: 0.7349 - val_loss: 0.5447 - val_acc: 0.7268\n",
      "Epoch 344/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7346\n",
      "Epoch 344: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5360 - acc: 0.7346 - val_loss: 0.5442 - val_acc: 0.7264\n",
      "Epoch 345/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5315 - acc: 0.7354\n",
      "Epoch 345: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5328 - acc: 0.7345 - val_loss: 0.5439 - val_acc: 0.7264\n",
      "Epoch 346/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.7324\n",
      "Epoch 346: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5335 - acc: 0.7321 - val_loss: 0.5434 - val_acc: 0.7260\n",
      "Epoch 347/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7319\n",
      "Epoch 347: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5348 - acc: 0.7324 - val_loss: 0.5434 - val_acc: 0.7277\n",
      "Epoch 348/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.7325\n",
      "Epoch 348: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5391 - acc: 0.7318 - val_loss: 0.5435 - val_acc: 0.7272\n",
      "Epoch 349/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5348 - acc: 0.7333\n",
      "Epoch 349: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5352 - acc: 0.7332 - val_loss: 0.5433 - val_acc: 0.7277\n",
      "Epoch 350/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.7290\n",
      "Epoch 350: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5382 - acc: 0.7290 - val_loss: 0.5433 - val_acc: 0.7277\n",
      "Epoch 351/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5324 - acc: 0.7329\n",
      "Epoch 351: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7322 - val_loss: 0.5430 - val_acc: 0.7268\n",
      "Epoch 352/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7330\n",
      "Epoch 352: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5321 - acc: 0.7335 - val_loss: 0.5429 - val_acc: 0.7272\n",
      "Epoch 353/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7340\n",
      "Epoch 353: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5326 - acc: 0.7339 - val_loss: 0.5426 - val_acc: 0.7277\n",
      "Epoch 354/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5371 - acc: 0.7326\n",
      "Epoch 354: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5354 - acc: 0.7330 - val_loss: 0.5427 - val_acc: 0.7272\n",
      "Epoch 355/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7352\n",
      "Epoch 355: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5347 - acc: 0.7353 - val_loss: 0.5427 - val_acc: 0.7277\n",
      "Epoch 356/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5271 - acc: 0.7380\n",
      "Epoch 356: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5283 - acc: 0.7376 - val_loss: 0.5425 - val_acc: 0.7272\n",
      "Epoch 357/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5319 - acc: 0.7364\n",
      "Epoch 357: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5330 - acc: 0.7363 - val_loss: 0.5428 - val_acc: 0.7268\n",
      "Epoch 358/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5340 - acc: 0.7371\n",
      "Epoch 358: val_acc improved from 0.72766 to 0.72809, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5350 - acc: 0.7367 - val_loss: 0.5428 - val_acc: 0.7281\n",
      "Epoch 359/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5335 - acc: 0.7338\n",
      "Epoch 359: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5338 - acc: 0.7337 - val_loss: 0.5421 - val_acc: 0.7277\n",
      "Epoch 360/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5311 - acc: 0.7328\n",
      "Epoch 360: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5312 - acc: 0.7329 - val_loss: 0.5415 - val_acc: 0.7268\n",
      "Epoch 361/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.7328\n",
      "Epoch 361: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5318 - acc: 0.7329 - val_loss: 0.5416 - val_acc: 0.7281\n",
      "Epoch 362/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5360 - acc: 0.7294\n",
      "Epoch 362: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5356 - acc: 0.7297 - val_loss: 0.5417 - val_acc: 0.7277\n",
      "Epoch 363/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7344\n",
      "Epoch 363: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5334 - acc: 0.7337 - val_loss: 0.5417 - val_acc: 0.7281\n",
      "Epoch 364/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5361 - acc: 0.7326\n",
      "Epoch 364: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5355 - acc: 0.7321 - val_loss: 0.5416 - val_acc: 0.7281\n",
      "Epoch 365/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5348 - acc: 0.7323\n",
      "Epoch 365: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5347 - acc: 0.7321 - val_loss: 0.5418 - val_acc: 0.7281\n",
      "Epoch 366/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5312 - acc: 0.7360\n",
      "Epoch 366: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5324 - acc: 0.7348 - val_loss: 0.5416 - val_acc: 0.7281\n",
      "Epoch 367/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.7320\n",
      "Epoch 367: val_acc improved from 0.72809 to 0.72852, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5389 - acc: 0.7324 - val_loss: 0.5418 - val_acc: 0.7285\n",
      "Epoch 368/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5339 - acc: 0.7347\n",
      "Epoch 368: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5346 - acc: 0.7342 - val_loss: 0.5416 - val_acc: 0.7285\n",
      "Epoch 369/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5356 - acc: 0.7336\n",
      "Epoch 369: val_acc improved from 0.72852 to 0.72894, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5371 - acc: 0.7322 - val_loss: 0.5414 - val_acc: 0.7289\n",
      "Epoch 370/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5362 - acc: 0.7323\n",
      "Epoch 370: val_acc did not improve from 0.72894\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5368 - acc: 0.7319 - val_loss: 0.5413 - val_acc: 0.7289\n",
      "Epoch 371/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7290\n",
      "Epoch 371: val_acc improved from 0.72894 to 0.72980, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5399 - acc: 0.7290 - val_loss: 0.5413 - val_acc: 0.7298\n",
      "Epoch 372/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7371\n",
      "Epoch 372: val_acc improved from 0.72980 to 0.73023, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5301 - acc: 0.7373 - val_loss: 0.5411 - val_acc: 0.7302\n",
      "Epoch 373/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.7334\n",
      "Epoch 373: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5325 - acc: 0.7336 - val_loss: 0.5408 - val_acc: 0.7302\n",
      "Epoch 374/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5340 - acc: 0.7348\n",
      "Epoch 374: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5349 - acc: 0.7337 - val_loss: 0.5408 - val_acc: 0.7298\n",
      "Epoch 375/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5315 - acc: 0.7376\n",
      "Epoch 375: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5315 - acc: 0.7376 - val_loss: 0.5405 - val_acc: 0.7294\n",
      "Epoch 376/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7322\n",
      "Epoch 376: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5330 - acc: 0.7324 - val_loss: 0.5404 - val_acc: 0.7298\n",
      "Epoch 377/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5287 - acc: 0.7348\n",
      "Epoch 377: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5296 - acc: 0.7340 - val_loss: 0.5399 - val_acc: 0.7298\n",
      "Epoch 378/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7389\n",
      "Epoch 378: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5302 - acc: 0.7386 - val_loss: 0.5401 - val_acc: 0.7298\n",
      "Epoch 379/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7363\n",
      "Epoch 379: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5298 - acc: 0.7365 - val_loss: 0.5400 - val_acc: 0.7302\n",
      "Epoch 380/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.7336\n",
      "Epoch 380: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5313 - acc: 0.7339 - val_loss: 0.5399 - val_acc: 0.7294\n",
      "Epoch 381/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5343 - acc: 0.7349\n",
      "Epoch 381: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5331 - acc: 0.7347 - val_loss: 0.5397 - val_acc: 0.7294\n",
      "Epoch 382/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5330 - acc: 0.7329\n",
      "Epoch 382: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7319 - val_loss: 0.5398 - val_acc: 0.7294\n",
      "Epoch 383/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5328 - acc: 0.7329\n",
      "Epoch 383: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5338 - acc: 0.7322 - val_loss: 0.5395 - val_acc: 0.7298\n",
      "Epoch 384/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7324\n",
      "Epoch 384: val_acc improved from 0.73023 to 0.73065, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7311 - val_loss: 0.5392 - val_acc: 0.7307\n",
      "Epoch 385/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5376 - acc: 0.7349\n",
      "Epoch 385: val_acc improved from 0.73065 to 0.73151, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5375 - acc: 0.7344 - val_loss: 0.5392 - val_acc: 0.7315\n",
      "Epoch 386/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5291 - acc: 0.7381\n",
      "Epoch 386: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5290 - acc: 0.7380 - val_loss: 0.5392 - val_acc: 0.7315\n",
      "Epoch 387/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5312 - acc: 0.7361\n",
      "Epoch 387: val_acc improved from 0.73151 to 0.73279, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5318 - acc: 0.7351 - val_loss: 0.5392 - val_acc: 0.7328\n",
      "Epoch 388/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5333 - acc: 0.7336\n",
      "Epoch 388: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5329 - acc: 0.7338 - val_loss: 0.5393 - val_acc: 0.7328\n",
      "Epoch 389/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5279 - acc: 0.7332\n",
      "Epoch 389: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5288 - acc: 0.7323 - val_loss: 0.5392 - val_acc: 0.7319\n",
      "Epoch 390/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5250 - acc: 0.7378\n",
      "Epoch 390: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5272 - acc: 0.7366 - val_loss: 0.5391 - val_acc: 0.7328\n",
      "Epoch 391/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.7304\n",
      "Epoch 391: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5330 - acc: 0.7303 - val_loss: 0.5391 - val_acc: 0.7328\n",
      "Epoch 392/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5278 - acc: 0.7340\n",
      "Epoch 392: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5296 - acc: 0.7337 - val_loss: 0.5387 - val_acc: 0.7324\n",
      "Epoch 393/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7361\n",
      "Epoch 393: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5353 - acc: 0.7362 - val_loss: 0.5387 - val_acc: 0.7324\n",
      "Epoch 394/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5338 - acc: 0.7318\n",
      "Epoch 394: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5346 - acc: 0.7324 - val_loss: 0.5386 - val_acc: 0.7324\n",
      "Epoch 395/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7302\n",
      "Epoch 395: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5319 - acc: 0.7305 - val_loss: 0.5385 - val_acc: 0.7328\n",
      "Epoch 396/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7375\n",
      "Epoch 396: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5278 - acc: 0.7376 - val_loss: 0.5379 - val_acc: 0.7328\n",
      "Epoch 397/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5316 - acc: 0.7334\n",
      "Epoch 397: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5310 - acc: 0.7329 - val_loss: 0.5377 - val_acc: 0.7328\n",
      "Epoch 398/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7338\n",
      "Epoch 398: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5298 - acc: 0.7338 - val_loss: 0.5376 - val_acc: 0.7319\n",
      "Epoch 399/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5282 - acc: 0.7352\n",
      "Epoch 399: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5292 - acc: 0.7347 - val_loss: 0.5375 - val_acc: 0.7324\n",
      "Epoch 400/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7365\n",
      "Epoch 400: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5305 - acc: 0.7366 - val_loss: 0.5377 - val_acc: 0.7328\n",
      "Epoch 401/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5265 - acc: 0.7373\n",
      "Epoch 401: val_acc improved from 0.73279 to 0.73365, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5283 - acc: 0.7366 - val_loss: 0.5376 - val_acc: 0.7336\n",
      "Epoch 402/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5302 - acc: 0.7358\n",
      "Epoch 402: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5304 - acc: 0.7356 - val_loss: 0.5374 - val_acc: 0.7328\n",
      "Epoch 403/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7360\n",
      "Epoch 403: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5293 - acc: 0.7362 - val_loss: 0.5373 - val_acc: 0.7336\n",
      "Epoch 404/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7385\n",
      "Epoch 404: val_acc improved from 0.73365 to 0.73450, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5273 - acc: 0.7383 - val_loss: 0.5372 - val_acc: 0.7345\n",
      "Epoch 405/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5277 - acc: 0.7347\n",
      "Epoch 405: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5282 - acc: 0.7352 - val_loss: 0.5370 - val_acc: 0.7345\n",
      "Epoch 406/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5253 - acc: 0.7395\n",
      "Epoch 406: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5263 - acc: 0.7380 - val_loss: 0.5369 - val_acc: 0.7345\n",
      "Epoch 407/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5291 - acc: 0.7329\n",
      "Epoch 407: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5297 - acc: 0.7321 - val_loss: 0.5366 - val_acc: 0.7345\n",
      "Epoch 408/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.7310\n",
      "Epoch 408: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5293 - acc: 0.7305 - val_loss: 0.5365 - val_acc: 0.7345\n",
      "Epoch 409/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5313 - acc: 0.7364\n",
      "Epoch 409: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5323 - acc: 0.7360 - val_loss: 0.5363 - val_acc: 0.7345\n",
      "Epoch 410/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5251 - acc: 0.7350\n",
      "Epoch 410: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5258 - acc: 0.7343 - val_loss: 0.5361 - val_acc: 0.7336\n",
      "Epoch 411/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5256 - acc: 0.7378\n",
      "Epoch 411: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - acc: 0.7373 - val_loss: 0.5358 - val_acc: 0.7332\n",
      "Epoch 412/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5282 - acc: 0.7350\n",
      "Epoch 412: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5290 - acc: 0.7345 - val_loss: 0.5358 - val_acc: 0.7341\n",
      "Epoch 413/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5259 - acc: 0.7339\n",
      "Epoch 413: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - acc: 0.7340 - val_loss: 0.5356 - val_acc: 0.7341\n",
      "Epoch 414/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5258 - acc: 0.7337\n",
      "Epoch 414: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5271 - acc: 0.7336 - val_loss: 0.5357 - val_acc: 0.7341\n",
      "Epoch 415/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5276 - acc: 0.7334\n",
      "Epoch 415: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5270 - acc: 0.7334 - val_loss: 0.5356 - val_acc: 0.7345\n",
      "Epoch 416/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5296 - acc: 0.7355\n",
      "Epoch 416: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5298 - acc: 0.7353 - val_loss: 0.5354 - val_acc: 0.7341\n",
      "Epoch 417/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5242 - acc: 0.7373\n",
      "Epoch 417: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5240 - acc: 0.7370 - val_loss: 0.5354 - val_acc: 0.7336\n",
      "Epoch 418/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.7341\n",
      "Epoch 418: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5261 - acc: 0.7336 - val_loss: 0.5353 - val_acc: 0.7341\n",
      "Epoch 419/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5264 - acc: 0.7381\n",
      "Epoch 419: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5266 - acc: 0.7374 - val_loss: 0.5350 - val_acc: 0.7341\n",
      "Epoch 420/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7364\n",
      "Epoch 420: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5322 - acc: 0.7365 - val_loss: 0.5352 - val_acc: 0.7336\n",
      "Epoch 421/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7339\n",
      "Epoch 421: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5251 - acc: 0.7338 - val_loss: 0.5351 - val_acc: 0.7341\n",
      "Epoch 422/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5271 - acc: 0.7346\n",
      "Epoch 422: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5279 - acc: 0.7344 - val_loss: 0.5350 - val_acc: 0.7332\n",
      "Epoch 423/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5271 - acc: 0.7381\n",
      "Epoch 423: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5277 - acc: 0.7370 - val_loss: 0.5345 - val_acc: 0.7336\n",
      "Epoch 424/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7367\n",
      "Epoch 424: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5268 - acc: 0.7370 - val_loss: 0.5345 - val_acc: 0.7336\n",
      "Epoch 425/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5299 - acc: 0.7348\n",
      "Epoch 425: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5308 - acc: 0.7337 - val_loss: 0.5349 - val_acc: 0.7336\n",
      "Epoch 426/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5296 - acc: 0.7325\n",
      "Epoch 426: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5292 - acc: 0.7332 - val_loss: 0.5347 - val_acc: 0.7341\n",
      "Epoch 427/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5248 - acc: 0.7367\n",
      "Epoch 427: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5257 - acc: 0.7359 - val_loss: 0.5347 - val_acc: 0.7345\n",
      "Epoch 428/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5256 - acc: 0.7385\n",
      "Epoch 428: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5277 - acc: 0.7369 - val_loss: 0.5347 - val_acc: 0.7341\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 4.6304 - acc: 0.5914\n",
      "Epoch 1: val_acc improved from -inf to 0.70671, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 4.5872 - acc: 0.5908 - val_loss: 0.7978 - val_acc: 0.7067\n",
      "Epoch 2/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 2.5709 - acc: 0.6105\n",
      "Epoch 2: val_acc did not improve from 0.70671\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 2.5451 - acc: 0.6128 - val_loss: 0.6836 - val_acc: 0.6238\n",
      "Epoch 3/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 1.9393 - acc: 0.6042\n",
      "Epoch 3: val_acc did not improve from 0.70671\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.9323 - acc: 0.6044 - val_loss: 0.6512 - val_acc: 0.6011\n",
      "Epoch 4/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 1.5092 - acc: 0.6218\n",
      "Epoch 4: val_acc did not improve from 0.70671\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.5133 - acc: 0.6204 - val_loss: 0.6106 - val_acc: 0.6366\n",
      "Epoch 5/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 1.3596 - acc: 0.6175\n",
      "Epoch 5: val_acc did not improve from 0.70671\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.3544 - acc: 0.6169 - val_loss: 0.5966 - val_acc: 0.7041\n",
      "Epoch 6/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 1.2098 - acc: 0.6141\n",
      "Epoch 6: val_acc improved from 0.70671 to 0.71826, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.2125 - acc: 0.6150 - val_loss: 0.5823 - val_acc: 0.7183\n",
      "Epoch 7/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 1.1268 - acc: 0.6165\n",
      "Epoch 7: val_acc improved from 0.71826 to 0.72039, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.1239 - acc: 0.6171 - val_loss: 0.5767 - val_acc: 0.7204\n",
      "Epoch 8/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 1.0454 - acc: 0.6181\n",
      "Epoch 8: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.0453 - acc: 0.6183 - val_loss: 0.5794 - val_acc: 0.7187\n",
      "Epoch 9/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.9895 - acc: 0.6184\n",
      "Epoch 9: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.9894 - acc: 0.6184 - val_loss: 0.5811 - val_acc: 0.7204\n",
      "Epoch 10/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.9286 - acc: 0.6317\n",
      "Epoch 10: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.9243 - acc: 0.6325 - val_loss: 0.5812 - val_acc: 0.7187\n",
      "Epoch 11/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.8857 - acc: 0.6325\n",
      "Epoch 11: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8855 - acc: 0.6320 - val_loss: 0.5818 - val_acc: 0.7178\n",
      "Epoch 12/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.8560 - acc: 0.6388\n",
      "Epoch 12: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8552 - acc: 0.6382 - val_loss: 0.5834 - val_acc: 0.7165\n",
      "Epoch 13/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.8160 - acc: 0.6353\n",
      "Epoch 13: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8170 - acc: 0.6350 - val_loss: 0.5862 - val_acc: 0.7157\n",
      "Epoch 14/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.8168 - acc: 0.6361\n",
      "Epoch 14: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8145 - acc: 0.6374 - val_loss: 0.5894 - val_acc: 0.7148\n",
      "Epoch 15/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.7760 - acc: 0.6452\n",
      "Epoch 15: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7747 - acc: 0.6448 - val_loss: 0.5899 - val_acc: 0.7153\n",
      "Epoch 16/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.7764 - acc: 0.6387\n",
      "Epoch 16: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7764 - acc: 0.6384 - val_loss: 0.5922 - val_acc: 0.7123\n",
      "Epoch 17/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.7750 - acc: 0.6414\n",
      "Epoch 17: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7728 - acc: 0.6405 - val_loss: 0.5942 - val_acc: 0.7114\n",
      "Epoch 18/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.6513\n",
      "Epoch 18: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7520 - acc: 0.6528 - val_loss: 0.5959 - val_acc: 0.7106\n",
      "Epoch 19/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.7271 - acc: 0.6562\n",
      "Epoch 19: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7273 - acc: 0.6562 - val_loss: 0.5960 - val_acc: 0.7097\n",
      "Epoch 20/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.7242 - acc: 0.6502\n",
      "Epoch 20: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7227 - acc: 0.6510 - val_loss: 0.5965 - val_acc: 0.7097\n",
      "Epoch 21/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.6530\n",
      "Epoch 21: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7044 - acc: 0.6536 - val_loss: 0.5964 - val_acc: 0.7076\n",
      "Epoch 22/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.7055 - acc: 0.6561\n",
      "Epoch 22: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7068 - acc: 0.6543 - val_loss: 0.5967 - val_acc: 0.7084\n",
      "Epoch 23/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6947 - acc: 0.6571\n",
      "Epoch 23: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6948 - acc: 0.6570 - val_loss: 0.5974 - val_acc: 0.7093\n",
      "Epoch 24/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.7108 - acc: 0.6542\n",
      "Epoch 24: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7110 - acc: 0.6542 - val_loss: 0.5986 - val_acc: 0.7097\n",
      "Epoch 25/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6833 - acc: 0.6579\n",
      "Epoch 25: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6834 - acc: 0.6574 - val_loss: 0.5981 - val_acc: 0.7084\n",
      "Epoch 26/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.6684\n",
      "Epoch 26: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6758 - acc: 0.6680 - val_loss: 0.5976 - val_acc: 0.7084\n",
      "Epoch 27/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6757 - acc: 0.6663\n",
      "Epoch 27: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6762 - acc: 0.6654 - val_loss: 0.5985 - val_acc: 0.7088\n",
      "Epoch 28/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6942 - acc: 0.6569\n",
      "Epoch 28: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.6573 - val_loss: 0.5992 - val_acc: 0.7093\n",
      "Epoch 29/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6773 - acc: 0.6625\n",
      "Epoch 29: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6761 - acc: 0.6629 - val_loss: 0.5990 - val_acc: 0.7093\n",
      "Epoch 30/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6688 - acc: 0.6616\n",
      "Epoch 30: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6665 - acc: 0.6623 - val_loss: 0.5998 - val_acc: 0.7101\n",
      "Epoch 31/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6628 - acc: 0.6632\n",
      "Epoch 31: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6637 - acc: 0.6631 - val_loss: 0.6000 - val_acc: 0.7088\n",
      "Epoch 32/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6569 - acc: 0.6704\n",
      "Epoch 32: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6592 - acc: 0.6689 - val_loss: 0.6007 - val_acc: 0.7084\n",
      "Epoch 33/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6593 - acc: 0.6729\n",
      "Epoch 33: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6596 - acc: 0.6729 - val_loss: 0.6008 - val_acc: 0.7101\n",
      "Epoch 34/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6576 - acc: 0.6688\n",
      "Epoch 34: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6562 - acc: 0.6697 - val_loss: 0.6010 - val_acc: 0.7106\n",
      "Epoch 35/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6499 - acc: 0.6736\n",
      "Epoch 35: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6500 - acc: 0.6732 - val_loss: 0.6002 - val_acc: 0.7097\n",
      "Epoch 36/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6520 - acc: 0.6748\n",
      "Epoch 36: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6505 - acc: 0.6748 - val_loss: 0.6002 - val_acc: 0.7093\n",
      "Epoch 37/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.6729\n",
      "Epoch 37: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6474 - acc: 0.6720 - val_loss: 0.5998 - val_acc: 0.7093\n",
      "Epoch 38/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6425 - acc: 0.6775\n",
      "Epoch 38: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6405 - acc: 0.6778 - val_loss: 0.5996 - val_acc: 0.7093\n",
      "Epoch 39/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6336 - acc: 0.6805\n",
      "Epoch 39: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6336 - acc: 0.6805 - val_loss: 0.5996 - val_acc: 0.7088\n",
      "Epoch 40/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6346 - acc: 0.6781\n",
      "Epoch 40: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6340 - acc: 0.6786 - val_loss: 0.5997 - val_acc: 0.7093\n",
      "Epoch 41/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6286 - acc: 0.6816\n",
      "Epoch 41: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6287 - acc: 0.6816 - val_loss: 0.5990 - val_acc: 0.7093\n",
      "Epoch 42/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6274 - acc: 0.6850\n",
      "Epoch 42: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6273 - acc: 0.6847 - val_loss: 0.5990 - val_acc: 0.7097\n",
      "Epoch 43/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6357 - acc: 0.6763\n",
      "Epoch 43: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6356 - acc: 0.6763 - val_loss: 0.5986 - val_acc: 0.7101\n",
      "Epoch 44/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6329 - acc: 0.6782\n",
      "Epoch 44: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6323 - acc: 0.6781 - val_loss: 0.5980 - val_acc: 0.7088\n",
      "Epoch 45/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6297 - acc: 0.6800\n",
      "Epoch 45: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6290 - acc: 0.6797 - val_loss: 0.5978 - val_acc: 0.7088\n",
      "Epoch 46/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6258 - acc: 0.6821\n",
      "Epoch 46: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6262 - acc: 0.6815 - val_loss: 0.5971 - val_acc: 0.7093\n",
      "Epoch 47/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.6804\n",
      "Epoch 47: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6268 - acc: 0.6808 - val_loss: 0.5962 - val_acc: 0.7097\n",
      "Epoch 48/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6317 - acc: 0.6806\n",
      "Epoch 48: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6320 - acc: 0.6802 - val_loss: 0.5964 - val_acc: 0.7097\n",
      "Epoch 49/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6194 - acc: 0.6894\n",
      "Epoch 49: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6190 - acc: 0.6897 - val_loss: 0.5953 - val_acc: 0.7106\n",
      "Epoch 50/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6283 - acc: 0.6861\n",
      "Epoch 50: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6274 - acc: 0.6867 - val_loss: 0.5950 - val_acc: 0.7101\n",
      "Epoch 51/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6223 - acc: 0.6907\n",
      "Epoch 51: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6227 - acc: 0.6904 - val_loss: 0.5950 - val_acc: 0.7110\n",
      "Epoch 52/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6156 - acc: 0.6902\n",
      "Epoch 52: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6173 - acc: 0.6902 - val_loss: 0.5944 - val_acc: 0.7106\n",
      "Epoch 53/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.6888\n",
      "Epoch 53: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6224 - acc: 0.6889 - val_loss: 0.5952 - val_acc: 0.7118\n",
      "Epoch 54/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6133 - acc: 0.6897\n",
      "Epoch 54: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6136 - acc: 0.6895 - val_loss: 0.5950 - val_acc: 0.7118\n",
      "Epoch 55/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.6177 - acc: 0.6892\n",
      "Epoch 55: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6159 - acc: 0.6890 - val_loss: 0.5942 - val_acc: 0.7123\n",
      "Epoch 56/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6182 - acc: 0.6840\n",
      "Epoch 56: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6169 - acc: 0.6851 - val_loss: 0.5938 - val_acc: 0.7123\n",
      "Epoch 57/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.6867\n",
      "Epoch 57: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6165 - acc: 0.6875 - val_loss: 0.5939 - val_acc: 0.7123\n",
      "Epoch 58/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6167 - acc: 0.6878\n",
      "Epoch 58: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6159 - acc: 0.6874 - val_loss: 0.5934 - val_acc: 0.7131\n",
      "Epoch 59/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6145 - acc: 0.6907\n",
      "Epoch 59: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6137 - acc: 0.6903 - val_loss: 0.5928 - val_acc: 0.7131\n",
      "Epoch 60/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6166 - acc: 0.6874\n",
      "Epoch 60: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6159 - acc: 0.6873 - val_loss: 0.5932 - val_acc: 0.7131\n",
      "Epoch 61/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.6071 - acc: 0.6983\n",
      "Epoch 61: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6057 - acc: 0.6983 - val_loss: 0.5929 - val_acc: 0.7131\n",
      "Epoch 62/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6105 - acc: 0.6906\n",
      "Epoch 62: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6103 - acc: 0.6905 - val_loss: 0.5922 - val_acc: 0.7131\n",
      "Epoch 63/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.6920\n",
      "Epoch 63: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6146 - acc: 0.6920 - val_loss: 0.5924 - val_acc: 0.7136\n",
      "Epoch 64/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6082 - acc: 0.6957\n",
      "Epoch 64: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6082 - acc: 0.6957 - val_loss: 0.5923 - val_acc: 0.7140\n",
      "Epoch 65/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6112 - acc: 0.6864\n",
      "Epoch 65: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6092 - acc: 0.6882 - val_loss: 0.5918 - val_acc: 0.7140\n",
      "Epoch 66/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6125 - acc: 0.6866\n",
      "Epoch 66: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6111 - acc: 0.6869 - val_loss: 0.5911 - val_acc: 0.7148\n",
      "Epoch 67/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6052 - acc: 0.6923\n",
      "Epoch 67: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6054 - acc: 0.6919 - val_loss: 0.5901 - val_acc: 0.7153\n",
      "Epoch 68/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6051 - acc: 0.6885\n",
      "Epoch 68: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6046 - acc: 0.6887 - val_loss: 0.5902 - val_acc: 0.7148\n",
      "Epoch 69/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6093 - acc: 0.6945\n",
      "Epoch 69: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6100 - acc: 0.6940 - val_loss: 0.5904 - val_acc: 0.7148\n",
      "Epoch 70/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6025 - acc: 0.6911\n",
      "Epoch 70: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6020 - acc: 0.6905 - val_loss: 0.5899 - val_acc: 0.7148\n",
      "Epoch 71/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.6954\n",
      "Epoch 71: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6020 - acc: 0.6961 - val_loss: 0.5891 - val_acc: 0.7148\n",
      "Epoch 72/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.6947\n",
      "Epoch 72: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5977 - acc: 0.6950 - val_loss: 0.5886 - val_acc: 0.7148\n",
      "Epoch 73/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6026 - acc: 0.6921\n",
      "Epoch 73: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6028 - acc: 0.6924 - val_loss: 0.5885 - val_acc: 0.7148\n",
      "Epoch 74/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6077 - acc: 0.6915\n",
      "Epoch 74: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6081 - acc: 0.6914 - val_loss: 0.5889 - val_acc: 0.7148\n",
      "Epoch 75/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5967 - acc: 0.6947\n",
      "Epoch 75: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5968 - acc: 0.6944 - val_loss: 0.5885 - val_acc: 0.7148\n",
      "Epoch 76/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.7001\n",
      "Epoch 76: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6006 - acc: 0.7005 - val_loss: 0.5882 - val_acc: 0.7153\n",
      "Epoch 77/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.6952\n",
      "Epoch 77: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6008 - acc: 0.6960 - val_loss: 0.5884 - val_acc: 0.7148\n",
      "Epoch 78/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6031 - acc: 0.6960\n",
      "Epoch 78: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6006 - acc: 0.6973 - val_loss: 0.5877 - val_acc: 0.7148\n",
      "Epoch 79/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6011 - acc: 0.6976\n",
      "Epoch 79: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6010 - acc: 0.6975 - val_loss: 0.5876 - val_acc: 0.7153\n",
      "Epoch 80/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5978 - acc: 0.6938\n",
      "Epoch 80: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5983 - acc: 0.6928 - val_loss: 0.5879 - val_acc: 0.7157\n",
      "Epoch 81/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5949 - acc: 0.6960\n",
      "Epoch 81: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5945 - acc: 0.6966 - val_loss: 0.5870 - val_acc: 0.7157\n",
      "Epoch 82/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5975 - acc: 0.6955\n",
      "Epoch 82: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5966 - acc: 0.6960 - val_loss: 0.5869 - val_acc: 0.7157\n",
      "Epoch 83/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5981 - acc: 0.7008\n",
      "Epoch 83: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5976 - acc: 0.6995 - val_loss: 0.5873 - val_acc: 0.7161\n",
      "Epoch 84/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5977 - acc: 0.7008\n",
      "Epoch 84: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5956 - acc: 0.7017 - val_loss: 0.5866 - val_acc: 0.7161\n",
      "Epoch 85/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5934 - acc: 0.7042\n",
      "Epoch 85: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5924 - acc: 0.7043 - val_loss: 0.5862 - val_acc: 0.7165\n",
      "Epoch 86/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5954 - acc: 0.7013\n",
      "Epoch 86: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5947 - acc: 0.7019 - val_loss: 0.5856 - val_acc: 0.7165\n",
      "Epoch 87/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5956 - acc: 0.7057\n",
      "Epoch 87: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5944 - acc: 0.7052 - val_loss: 0.5851 - val_acc: 0.7165\n",
      "Epoch 88/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5904 - acc: 0.7014\n",
      "Epoch 88: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5895 - acc: 0.7019 - val_loss: 0.5848 - val_acc: 0.7165\n",
      "Epoch 89/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5915 - acc: 0.6999\n",
      "Epoch 89: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5907 - acc: 0.6997 - val_loss: 0.5843 - val_acc: 0.7165\n",
      "Epoch 90/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5896 - acc: 0.7075\n",
      "Epoch 90: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5899 - acc: 0.7063 - val_loss: 0.5843 - val_acc: 0.7170\n",
      "Epoch 91/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5908 - acc: 0.7057\n",
      "Epoch 91: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5908 - acc: 0.7057 - val_loss: 0.5841 - val_acc: 0.7178\n",
      "Epoch 92/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.7031\n",
      "Epoch 92: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5909 - acc: 0.7032 - val_loss: 0.5835 - val_acc: 0.7183\n",
      "Epoch 93/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.7043\n",
      "Epoch 93: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5941 - acc: 0.7048 - val_loss: 0.5832 - val_acc: 0.7174\n",
      "Epoch 94/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5906 - acc: 0.7026\n",
      "Epoch 94: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5901 - acc: 0.7025 - val_loss: 0.5830 - val_acc: 0.7178\n",
      "Epoch 95/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5964 - acc: 0.7014\n",
      "Epoch 95: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5952 - acc: 0.7013 - val_loss: 0.5828 - val_acc: 0.7183\n",
      "Epoch 96/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5872 - acc: 0.7041\n",
      "Epoch 96: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5867 - acc: 0.7039 - val_loss: 0.5824 - val_acc: 0.7178\n",
      "Epoch 97/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.7047\n",
      "Epoch 97: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5935 - acc: 0.7044 - val_loss: 0.5829 - val_acc: 0.7178\n",
      "Epoch 98/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.7051\n",
      "Epoch 98: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5882 - acc: 0.7051 - val_loss: 0.5825 - val_acc: 0.7183\n",
      "Epoch 99/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.6995\n",
      "Epoch 99: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5838 - acc: 0.6994 - val_loss: 0.5819 - val_acc: 0.7178\n",
      "Epoch 100/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.6993\n",
      "Epoch 100: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5953 - acc: 0.6987 - val_loss: 0.5815 - val_acc: 0.7174\n",
      "Epoch 101/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5902 - acc: 0.7038\n",
      "Epoch 101: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5902 - acc: 0.7036 - val_loss: 0.5816 - val_acc: 0.7178\n",
      "Epoch 102/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5900 - acc: 0.7047\n",
      "Epoch 102: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5904 - acc: 0.7040 - val_loss: 0.5814 - val_acc: 0.7183\n",
      "Epoch 103/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5848 - acc: 0.7034\n",
      "Epoch 103: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5856 - acc: 0.7023 - val_loss: 0.5808 - val_acc: 0.7183\n",
      "Epoch 104/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5933 - acc: 0.6995\n",
      "Epoch 104: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5934 - acc: 0.6991 - val_loss: 0.5806 - val_acc: 0.7183\n",
      "Epoch 105/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5851 - acc: 0.7005\n",
      "Epoch 105: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5849 - acc: 0.7006 - val_loss: 0.5803 - val_acc: 0.7183\n",
      "Epoch 106/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5871 - acc: 0.7054\n",
      "Epoch 106: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5874 - acc: 0.7057 - val_loss: 0.5800 - val_acc: 0.7183\n",
      "Epoch 107/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5833 - acc: 0.7077\n",
      "Epoch 107: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5826 - acc: 0.7088 - val_loss: 0.5802 - val_acc: 0.7174\n",
      "Epoch 108/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.7052\n",
      "Epoch 108: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5832 - acc: 0.7058 - val_loss: 0.5798 - val_acc: 0.7174\n",
      "Epoch 109/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5852 - acc: 0.7040\n",
      "Epoch 109: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5844 - acc: 0.7044 - val_loss: 0.5789 - val_acc: 0.7178\n",
      "Epoch 110/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5815 - acc: 0.7053\n",
      "Epoch 110: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5805 - acc: 0.7054 - val_loss: 0.5786 - val_acc: 0.7183\n",
      "Epoch 111/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5891 - acc: 0.7047\n",
      "Epoch 111: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5889 - acc: 0.7041 - val_loss: 0.5784 - val_acc: 0.7183\n",
      "Epoch 112/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.7000\n",
      "Epoch 112: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5903 - acc: 0.6999 - val_loss: 0.5782 - val_acc: 0.7183\n",
      "Epoch 113/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5823 - acc: 0.7094\n",
      "Epoch 113: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5821 - acc: 0.7091 - val_loss: 0.5781 - val_acc: 0.7178\n",
      "Epoch 114/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5855 - acc: 0.7028\n",
      "Epoch 114: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5855 - acc: 0.7026 - val_loss: 0.5780 - val_acc: 0.7187\n",
      "Epoch 115/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5799 - acc: 0.7085\n",
      "Epoch 115: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5794 - acc: 0.7090 - val_loss: 0.5774 - val_acc: 0.7183\n",
      "Epoch 116/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5861 - acc: 0.7105\n",
      "Epoch 116: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5835 - acc: 0.7108 - val_loss: 0.5774 - val_acc: 0.7200\n",
      "Epoch 117/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5842 - acc: 0.7072\n",
      "Epoch 117: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5852 - acc: 0.7064 - val_loss: 0.5773 - val_acc: 0.7195\n",
      "Epoch 118/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5828 - acc: 0.7082\n",
      "Epoch 118: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5822 - acc: 0.7089 - val_loss: 0.5769 - val_acc: 0.7195\n",
      "Epoch 119/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.7085\n",
      "Epoch 119: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5789 - acc: 0.7088 - val_loss: 0.5766 - val_acc: 0.7204\n",
      "Epoch 120/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5845 - acc: 0.7061\n",
      "Epoch 120: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5842 - acc: 0.7056 - val_loss: 0.5765 - val_acc: 0.7200\n",
      "Epoch 121/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5817 - acc: 0.7077\n",
      "Epoch 121: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5821 - acc: 0.7070 - val_loss: 0.5761 - val_acc: 0.7204\n",
      "Epoch 122/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5788 - acc: 0.7095\n",
      "Epoch 122: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5795 - acc: 0.7081 - val_loss: 0.5761 - val_acc: 0.7204\n",
      "Epoch 123/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5819 - acc: 0.7108\n",
      "Epoch 123: val_acc improved from 0.72039 to 0.72125, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5814 - acc: 0.7106 - val_loss: 0.5760 - val_acc: 0.7212\n",
      "Epoch 124/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5801 - acc: 0.7090\n",
      "Epoch 124: val_acc improved from 0.72125 to 0.72210, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5821 - acc: 0.7075 - val_loss: 0.5756 - val_acc: 0.7221\n",
      "Epoch 125/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5829 - acc: 0.7031\n",
      "Epoch 125: val_acc did not improve from 0.72210\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5808 - acc: 0.7046 - val_loss: 0.5753 - val_acc: 0.7221\n",
      "Epoch 126/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5821 - acc: 0.7044\n",
      "Epoch 126: val_acc did not improve from 0.72210\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5821 - acc: 0.7038 - val_loss: 0.5749 - val_acc: 0.7221\n",
      "Epoch 127/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5804 - acc: 0.7074\n",
      "Epoch 127: val_acc did not improve from 0.72210\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5806 - acc: 0.7070 - val_loss: 0.5744 - val_acc: 0.7217\n",
      "Epoch 128/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5817 - acc: 0.7078\n",
      "Epoch 128: val_acc did not improve from 0.72210\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5819 - acc: 0.7071 - val_loss: 0.5743 - val_acc: 0.7221\n",
      "Epoch 129/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.7018\n",
      "Epoch 129: val_acc improved from 0.72210 to 0.72253, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5842 - acc: 0.7018 - val_loss: 0.5747 - val_acc: 0.7225\n",
      "Epoch 130/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5802 - acc: 0.7109\n",
      "Epoch 130: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5803 - acc: 0.7108 - val_loss: 0.5744 - val_acc: 0.7225\n",
      "Epoch 131/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5825 - acc: 0.7100\n",
      "Epoch 131: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5827 - acc: 0.7099 - val_loss: 0.5740 - val_acc: 0.7221\n",
      "Epoch 132/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.7086\n",
      "Epoch 132: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5812 - acc: 0.7086 - val_loss: 0.5737 - val_acc: 0.7221\n",
      "Epoch 133/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5765 - acc: 0.7106\n",
      "Epoch 133: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5755 - acc: 0.7114 - val_loss: 0.5732 - val_acc: 0.7221\n",
      "Epoch 134/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5743 - acc: 0.7095\n",
      "Epoch 134: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5746 - acc: 0.7095 - val_loss: 0.5728 - val_acc: 0.7221\n",
      "Epoch 135/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5715 - acc: 0.7128\n",
      "Epoch 135: val_acc improved from 0.72253 to 0.72296, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5729 - acc: 0.7120 - val_loss: 0.5722 - val_acc: 0.7230\n",
      "Epoch 136/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5741 - acc: 0.7125\n",
      "Epoch 136: val_acc did not improve from 0.72296\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5738 - acc: 0.7127 - val_loss: 0.5718 - val_acc: 0.7230\n",
      "Epoch 137/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5711 - acc: 0.7120\n",
      "Epoch 137: val_acc improved from 0.72296 to 0.72339, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5703 - acc: 0.7123 - val_loss: 0.5714 - val_acc: 0.7234\n",
      "Epoch 138/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5771 - acc: 0.7113\n",
      "Epoch 138: val_acc did not improve from 0.72339\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5781 - acc: 0.7106 - val_loss: 0.5711 - val_acc: 0.7234\n",
      "Epoch 139/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5747 - acc: 0.7146\n",
      "Epoch 139: val_acc did not improve from 0.72339\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5740 - acc: 0.7147 - val_loss: 0.5712 - val_acc: 0.7234\n",
      "Epoch 140/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.7108\n",
      "Epoch 140: val_acc improved from 0.72339 to 0.72381, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5712 - acc: 0.7114 - val_loss: 0.5711 - val_acc: 0.7238\n",
      "Epoch 141/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5704 - acc: 0.7116\n",
      "Epoch 141: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5707 - acc: 0.7117 - val_loss: 0.5704 - val_acc: 0.7238\n",
      "Epoch 142/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5726 - acc: 0.7109\n",
      "Epoch 142: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5711 - acc: 0.7114 - val_loss: 0.5705 - val_acc: 0.7238\n",
      "Epoch 143/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5782 - acc: 0.7078\n",
      "Epoch 143: val_acc improved from 0.72381 to 0.72424, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5777 - acc: 0.7082 - val_loss: 0.5707 - val_acc: 0.7242\n",
      "Epoch 144/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5718 - acc: 0.7166\n",
      "Epoch 144: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5711 - acc: 0.7168 - val_loss: 0.5704 - val_acc: 0.7238\n",
      "Epoch 145/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5756 - acc: 0.7121\n",
      "Epoch 145: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5756 - acc: 0.7118 - val_loss: 0.5699 - val_acc: 0.7238\n",
      "Epoch 146/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.7105\n",
      "Epoch 146: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5741 - acc: 0.7112 - val_loss: 0.5695 - val_acc: 0.7238\n",
      "Epoch 147/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5719 - acc: 0.7111\n",
      "Epoch 147: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5708 - acc: 0.7118 - val_loss: 0.5689 - val_acc: 0.7238\n",
      "Epoch 148/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5806 - acc: 0.7121\n",
      "Epoch 148: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5805 - acc: 0.7114 - val_loss: 0.5690 - val_acc: 0.7242\n",
      "Epoch 149/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5734 - acc: 0.7141\n",
      "Epoch 149: val_acc improved from 0.72424 to 0.72510, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5734 - acc: 0.7141 - val_loss: 0.5689 - val_acc: 0.7251\n",
      "Epoch 150/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5761 - acc: 0.7107\n",
      "Epoch 150: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5751 - acc: 0.7111 - val_loss: 0.5689 - val_acc: 0.7247\n",
      "Epoch 151/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.7105\n",
      "Epoch 151: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5726 - acc: 0.7112 - val_loss: 0.5686 - val_acc: 0.7247\n",
      "Epoch 152/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.7118\n",
      "Epoch 152: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5723 - acc: 0.7115 - val_loss: 0.5684 - val_acc: 0.7251\n",
      "Epoch 153/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5667 - acc: 0.7146\n",
      "Epoch 153: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5666 - acc: 0.7139 - val_loss: 0.5680 - val_acc: 0.7251\n",
      "Epoch 154/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.7083\n",
      "Epoch 154: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5783 - acc: 0.7081 - val_loss: 0.5680 - val_acc: 0.7251\n",
      "Epoch 155/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5737 - acc: 0.7112\n",
      "Epoch 155: val_acc improved from 0.72510 to 0.72552, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5737 - acc: 0.7112 - val_loss: 0.5680 - val_acc: 0.7255\n",
      "Epoch 156/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.7163\n",
      "Epoch 156: val_acc improved from 0.72552 to 0.72595, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5678 - acc: 0.7160 - val_loss: 0.5676 - val_acc: 0.7260\n",
      "Epoch 157/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.7110\n",
      "Epoch 157: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5712 - acc: 0.7122 - val_loss: 0.5674 - val_acc: 0.7255\n",
      "Epoch 158/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.7130\n",
      "Epoch 158: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5689 - acc: 0.7127 - val_loss: 0.5667 - val_acc: 0.7255\n",
      "Epoch 159/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7156\n",
      "Epoch 159: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5694 - acc: 0.7158 - val_loss: 0.5669 - val_acc: 0.7255\n",
      "Epoch 160/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5755 - acc: 0.7115\n",
      "Epoch 160: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5756 - acc: 0.7119 - val_loss: 0.5672 - val_acc: 0.7260\n",
      "Epoch 161/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5762 - acc: 0.7100\n",
      "Epoch 161: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5758 - acc: 0.7103 - val_loss: 0.5670 - val_acc: 0.7260\n",
      "Epoch 162/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.7153\n",
      "Epoch 162: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5674 - acc: 0.7152 - val_loss: 0.5664 - val_acc: 0.7260\n",
      "Epoch 163/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5704 - acc: 0.7136\n",
      "Epoch 163: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5704 - acc: 0.7133 - val_loss: 0.5663 - val_acc: 0.7260\n",
      "Epoch 164/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5721 - acc: 0.7118\n",
      "Epoch 164: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5713 - acc: 0.7128 - val_loss: 0.5662 - val_acc: 0.7260\n",
      "Epoch 165/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5734 - acc: 0.7136\n",
      "Epoch 165: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5728 - acc: 0.7146 - val_loss: 0.5665 - val_acc: 0.7260\n",
      "Epoch 166/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5691 - acc: 0.7146\n",
      "Epoch 166: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5702 - acc: 0.7137 - val_loss: 0.5661 - val_acc: 0.7260\n",
      "Epoch 167/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5660 - acc: 0.7144\n",
      "Epoch 167: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5665 - acc: 0.7139 - val_loss: 0.5660 - val_acc: 0.7260\n",
      "Epoch 168/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5697 - acc: 0.7106\n",
      "Epoch 168: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5693 - acc: 0.7110 - val_loss: 0.5655 - val_acc: 0.7260\n",
      "Epoch 169/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5677 - acc: 0.7160\n",
      "Epoch 169: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5675 - acc: 0.7161 - val_loss: 0.5650 - val_acc: 0.7255\n",
      "Epoch 170/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5695 - acc: 0.7122\n",
      "Epoch 170: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5682 - acc: 0.7122 - val_loss: 0.5652 - val_acc: 0.7255\n",
      "Epoch 171/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.7149\n",
      "Epoch 171: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5702 - acc: 0.7148 - val_loss: 0.5650 - val_acc: 0.7255\n",
      "Epoch 172/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.7126\n",
      "Epoch 172: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5706 - acc: 0.7121 - val_loss: 0.5649 - val_acc: 0.7255\n",
      "Epoch 173/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.7113\n",
      "Epoch 173: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5671 - acc: 0.7118 - val_loss: 0.5647 - val_acc: 0.7251\n",
      "Epoch 174/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5649 - acc: 0.7148\n",
      "Epoch 174: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5649 - acc: 0.7149 - val_loss: 0.5644 - val_acc: 0.7255\n",
      "Epoch 175/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5652 - acc: 0.7135\n",
      "Epoch 175: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5659 - acc: 0.7133 - val_loss: 0.5645 - val_acc: 0.7255\n",
      "Epoch 176/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5647 - acc: 0.7138\n",
      "Epoch 176: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5649 - acc: 0.7137 - val_loss: 0.5640 - val_acc: 0.7255\n",
      "Epoch 177/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5650 - acc: 0.7133\n",
      "Epoch 177: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5652 - acc: 0.7129 - val_loss: 0.5637 - val_acc: 0.7260\n",
      "Epoch 178/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5710 - acc: 0.7148\n",
      "Epoch 178: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5711 - acc: 0.7143 - val_loss: 0.5638 - val_acc: 0.7260\n",
      "Epoch 179/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.7118\n",
      "Epoch 179: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5676 - acc: 0.7116 - val_loss: 0.5635 - val_acc: 0.7260\n",
      "Epoch 180/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7166\n",
      "Epoch 180: val_acc improved from 0.72595 to 0.72638, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5664 - acc: 0.7165 - val_loss: 0.5632 - val_acc: 0.7264\n",
      "Epoch 181/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.7144\n",
      "Epoch 181: val_acc did not improve from 0.72638\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5666 - acc: 0.7147 - val_loss: 0.5631 - val_acc: 0.7264\n",
      "Epoch 182/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5635 - acc: 0.7178\n",
      "Epoch 182: val_acc did not improve from 0.72638\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5630 - acc: 0.7175 - val_loss: 0.5628 - val_acc: 0.7264\n",
      "Epoch 183/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5654 - acc: 0.7117\n",
      "Epoch 183: val_acc did not improve from 0.72638\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5646 - acc: 0.7119 - val_loss: 0.5627 - val_acc: 0.7264\n",
      "Epoch 184/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5653 - acc: 0.7175\n",
      "Epoch 184: val_acc did not improve from 0.72638\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5647 - acc: 0.7167 - val_loss: 0.5623 - val_acc: 0.7264\n",
      "Epoch 185/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.7149\n",
      "Epoch 185: val_acc did not improve from 0.72638\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5626 - acc: 0.7152 - val_loss: 0.5620 - val_acc: 0.7264\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.7154\n",
      "Epoch 186: val_acc improved from 0.72638 to 0.72681, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5639 - acc: 0.7153 - val_loss: 0.5619 - val_acc: 0.7268\n",
      "Epoch 187/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.7176\n",
      "Epoch 187: val_acc improved from 0.72681 to 0.72723, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5594 - acc: 0.7176 - val_loss: 0.5619 - val_acc: 0.7272\n",
      "Epoch 188/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5665 - acc: 0.7162\n",
      "Epoch 188: val_acc improved from 0.72723 to 0.72766, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5658 - acc: 0.7150 - val_loss: 0.5619 - val_acc: 0.7277\n",
      "Epoch 189/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5622 - acc: 0.7184\n",
      "Epoch 189: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5628 - acc: 0.7185 - val_loss: 0.5617 - val_acc: 0.7272\n",
      "Epoch 190/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5623 - acc: 0.7182\n",
      "Epoch 190: val_acc improved from 0.72766 to 0.72809, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5624 - acc: 0.7174 - val_loss: 0.5614 - val_acc: 0.7281\n",
      "Epoch 191/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5654 - acc: 0.7195\n",
      "Epoch 191: val_acc did not improve from 0.72809\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5657 - acc: 0.7187 - val_loss: 0.5613 - val_acc: 0.7281\n",
      "Epoch 192/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5593 - acc: 0.7209\n",
      "Epoch 192: val_acc improved from 0.72809 to 0.72852, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5600 - acc: 0.7204 - val_loss: 0.5608 - val_acc: 0.7285\n",
      "Epoch 193/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.7154\n",
      "Epoch 193: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5631 - acc: 0.7154 - val_loss: 0.5605 - val_acc: 0.7281\n",
      "Epoch 194/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5605 - acc: 0.7193\n",
      "Epoch 194: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5613 - acc: 0.7177 - val_loss: 0.5603 - val_acc: 0.7285\n",
      "Epoch 195/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5638 - acc: 0.7148\n",
      "Epoch 195: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5636 - acc: 0.7149 - val_loss: 0.5604 - val_acc: 0.7281\n",
      "Epoch 196/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.7171\n",
      "Epoch 196: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5664 - acc: 0.7166 - val_loss: 0.5601 - val_acc: 0.7281\n",
      "Epoch 197/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5684 - acc: 0.7176\n",
      "Epoch 197: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5684 - acc: 0.7174 - val_loss: 0.5600 - val_acc: 0.7285\n",
      "Epoch 198/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5605 - acc: 0.7164\n",
      "Epoch 198: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5603 - acc: 0.7163 - val_loss: 0.5599 - val_acc: 0.7285\n",
      "Epoch 199/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5574 - acc: 0.7211\n",
      "Epoch 199: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5583 - acc: 0.7201 - val_loss: 0.5598 - val_acc: 0.7285\n",
      "Epoch 200/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7172\n",
      "Epoch 200: val_acc improved from 0.72852 to 0.72894, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5658 - acc: 0.7175 - val_loss: 0.5600 - val_acc: 0.7289\n",
      "Epoch 201/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5697 - acc: 0.7154\n",
      "Epoch 201: val_acc did not improve from 0.72894\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5696 - acc: 0.7156 - val_loss: 0.5597 - val_acc: 0.7289\n",
      "Epoch 202/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.7162\n",
      "Epoch 202: val_acc did not improve from 0.72894\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5619 - acc: 0.7161 - val_loss: 0.5595 - val_acc: 0.7289\n",
      "Epoch 203/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.7139\n",
      "Epoch 203: val_acc improved from 0.72894 to 0.72937, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5619 - acc: 0.7135 - val_loss: 0.5593 - val_acc: 0.7294\n",
      "Epoch 204/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.7136\n",
      "Epoch 204: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5644 - acc: 0.7136 - val_loss: 0.5592 - val_acc: 0.7294\n",
      "Epoch 205/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5574 - acc: 0.7180\n",
      "Epoch 205: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5552 - acc: 0.7190 - val_loss: 0.5590 - val_acc: 0.7294\n",
      "Epoch 206/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5589 - acc: 0.7186\n",
      "Epoch 206: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5588 - acc: 0.7191 - val_loss: 0.5587 - val_acc: 0.7294\n",
      "Epoch 207/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5612 - acc: 0.7179\n",
      "Epoch 207: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5615 - acc: 0.7178 - val_loss: 0.5583 - val_acc: 0.7294\n",
      "Epoch 208/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.7178\n",
      "Epoch 208: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5601 - acc: 0.7181 - val_loss: 0.5580 - val_acc: 0.7294\n",
      "Epoch 209/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7197\n",
      "Epoch 209: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5581 - acc: 0.7197 - val_loss: 0.5581 - val_acc: 0.7294\n",
      "Epoch 210/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5615 - acc: 0.7188\n",
      "Epoch 210: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5608 - acc: 0.7193 - val_loss: 0.5576 - val_acc: 0.7294\n",
      "Epoch 211/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5546 - acc: 0.7212\n",
      "Epoch 211: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5553 - acc: 0.7206 - val_loss: 0.5576 - val_acc: 0.7294\n",
      "Epoch 212/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5595 - acc: 0.7225\n",
      "Epoch 212: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5591 - acc: 0.7222 - val_loss: 0.5572 - val_acc: 0.7294\n",
      "Epoch 213/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5586 - acc: 0.7212\n",
      "Epoch 213: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5591 - acc: 0.7200 - val_loss: 0.5571 - val_acc: 0.7294\n",
      "Epoch 214/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5600 - acc: 0.7227\n",
      "Epoch 214: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5611 - acc: 0.7223 - val_loss: 0.5573 - val_acc: 0.7294\n",
      "Epoch 215/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5577 - acc: 0.7232\n",
      "Epoch 215: val_acc improved from 0.72937 to 0.72980, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5570 - acc: 0.7235 - val_loss: 0.5571 - val_acc: 0.7298\n",
      "Epoch 216/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5621 - acc: 0.7134\n",
      "Epoch 216: val_acc did not improve from 0.72980\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5624 - acc: 0.7131 - val_loss: 0.5570 - val_acc: 0.7294\n",
      "Epoch 217/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.7196\n",
      "Epoch 217: val_acc improved from 0.72980 to 0.73023, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5590 - acc: 0.7196 - val_loss: 0.5568 - val_acc: 0.7302\n",
      "Epoch 218/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5688 - acc: 0.7157\n",
      "Epoch 218: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5682 - acc: 0.7158 - val_loss: 0.5572 - val_acc: 0.7294\n",
      "Epoch 219/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7158\n",
      "Epoch 219: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5635 - acc: 0.7162 - val_loss: 0.5568 - val_acc: 0.7298\n",
      "Epoch 220/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5602 - acc: 0.7225\n",
      "Epoch 220: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5617 - acc: 0.7215 - val_loss: 0.5567 - val_acc: 0.7302\n",
      "Epoch 221/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7190\n",
      "Epoch 221: val_acc improved from 0.73023 to 0.73065, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5579 - acc: 0.7191 - val_loss: 0.5563 - val_acc: 0.7307\n",
      "Epoch 222/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.7205\n",
      "Epoch 222: val_acc did not improve from 0.73065\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5580 - acc: 0.7208 - val_loss: 0.5561 - val_acc: 0.7302\n",
      "Epoch 223/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.7162\n",
      "Epoch 223: val_acc did not improve from 0.73065\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5607 - acc: 0.7161 - val_loss: 0.5559 - val_acc: 0.7307\n",
      "Epoch 224/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5616 - acc: 0.7163\n",
      "Epoch 224: val_acc did not improve from 0.73065\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5616 - acc: 0.7164 - val_loss: 0.5558 - val_acc: 0.7298\n",
      "Epoch 225/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5578 - acc: 0.7201\n",
      "Epoch 225: val_acc did not improve from 0.73065\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5571 - acc: 0.7197 - val_loss: 0.5557 - val_acc: 0.7298\n",
      "Epoch 226/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5613 - acc: 0.7231\n",
      "Epoch 226: val_acc improved from 0.73065 to 0.73108, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5607 - acc: 0.7232 - val_loss: 0.5557 - val_acc: 0.7311\n",
      "Epoch 227/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5590 - acc: 0.7163\n",
      "Epoch 227: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5585 - acc: 0.7168 - val_loss: 0.5554 - val_acc: 0.7302\n",
      "Epoch 228/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5625 - acc: 0.7205\n",
      "Epoch 228: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5628 - acc: 0.7196 - val_loss: 0.5553 - val_acc: 0.7302\n",
      "Epoch 229/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5595 - acc: 0.7150\n",
      "Epoch 229: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5590 - acc: 0.7150 - val_loss: 0.5555 - val_acc: 0.7302\n",
      "Epoch 230/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5588 - acc: 0.7211\n",
      "Epoch 230: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5591 - acc: 0.7206 - val_loss: 0.5556 - val_acc: 0.7307\n",
      "Epoch 231/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5564 - acc: 0.7220\n",
      "Epoch 231: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5557 - acc: 0.7229 - val_loss: 0.5554 - val_acc: 0.7311\n",
      "Epoch 232/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5576 - acc: 0.7178\n",
      "Epoch 232: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5559 - acc: 0.7187 - val_loss: 0.5552 - val_acc: 0.7307\n",
      "Epoch 233/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5528 - acc: 0.7190\n",
      "Epoch 233: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5535 - acc: 0.7190 - val_loss: 0.5551 - val_acc: 0.7307\n",
      "Epoch 234/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5580 - acc: 0.7212\n",
      "Epoch 234: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5575 - acc: 0.7204 - val_loss: 0.5552 - val_acc: 0.7307\n",
      "Epoch 235/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.7175\n",
      "Epoch 235: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5555 - acc: 0.7166 - val_loss: 0.5550 - val_acc: 0.7307\n",
      "Epoch 236/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.7216\n",
      "Epoch 236: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5550 - acc: 0.7221 - val_loss: 0.5549 - val_acc: 0.7302\n",
      "Epoch 237/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5542 - acc: 0.7198\n",
      "Epoch 237: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5548 - acc: 0.7196 - val_loss: 0.5546 - val_acc: 0.7302\n",
      "Epoch 238/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5556 - acc: 0.7223\n",
      "Epoch 238: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5548 - acc: 0.7225 - val_loss: 0.5546 - val_acc: 0.7302\n",
      "Epoch 239/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.7233\n",
      "Epoch 239: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5553 - acc: 0.7227 - val_loss: 0.5545 - val_acc: 0.7302\n",
      "Epoch 240/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5532 - acc: 0.7207\n",
      "Epoch 240: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5532 - acc: 0.7207 - val_loss: 0.5542 - val_acc: 0.7302\n",
      "Epoch 241/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5537 - acc: 0.7213\n",
      "Epoch 241: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5533 - acc: 0.7219 - val_loss: 0.5539 - val_acc: 0.7311\n",
      "Epoch 242/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.7202\n",
      "Epoch 242: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5567 - acc: 0.7203 - val_loss: 0.5537 - val_acc: 0.7307\n",
      "Epoch 243/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5535 - acc: 0.7214\n",
      "Epoch 243: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5538 - acc: 0.7206 - val_loss: 0.5537 - val_acc: 0.7302\n",
      "Epoch 244/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5524 - acc: 0.7241\n",
      "Epoch 244: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5520 - acc: 0.7244 - val_loss: 0.5535 - val_acc: 0.7307\n",
      "Epoch 245/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5496 - acc: 0.7235\n",
      "Epoch 245: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5498 - acc: 0.7227 - val_loss: 0.5528 - val_acc: 0.7311\n",
      "Epoch 246/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5526 - acc: 0.7204\n",
      "Epoch 246: val_acc improved from 0.73108 to 0.73151, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5540 - acc: 0.7200 - val_loss: 0.5530 - val_acc: 0.7315\n",
      "Epoch 247/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5570 - acc: 0.7179\n",
      "Epoch 247: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5563 - acc: 0.7187 - val_loss: 0.5526 - val_acc: 0.7307\n",
      "Epoch 248/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.7184\n",
      "Epoch 248: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5557 - acc: 0.7190 - val_loss: 0.5529 - val_acc: 0.7311\n",
      "Epoch 249/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7194\n",
      "Epoch 249: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5543 - acc: 0.7194 - val_loss: 0.5526 - val_acc: 0.7311\n",
      "Epoch 250/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5573 - acc: 0.7228\n",
      "Epoch 250: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5589 - acc: 0.7215 - val_loss: 0.5525 - val_acc: 0.7307\n",
      "Epoch 251/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.7154\n",
      "Epoch 251: val_acc improved from 0.73151 to 0.73194, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5556 - acc: 0.7156 - val_loss: 0.5526 - val_acc: 0.7319\n",
      "Epoch 252/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.7237\n",
      "Epoch 252: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5540 - acc: 0.7231 - val_loss: 0.5521 - val_acc: 0.7315\n",
      "Epoch 253/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5566 - acc: 0.7174\n",
      "Epoch 253: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5553 - acc: 0.7191 - val_loss: 0.5521 - val_acc: 0.7319\n",
      "Epoch 254/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7225\n",
      "Epoch 254: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5504 - acc: 0.7222 - val_loss: 0.5518 - val_acc: 0.7319\n",
      "Epoch 255/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7202\n",
      "Epoch 255: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5578 - acc: 0.7203 - val_loss: 0.5517 - val_acc: 0.7319\n",
      "Epoch 256/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7201\n",
      "Epoch 256: val_acc improved from 0.73194 to 0.73236, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5523 - acc: 0.7201 - val_loss: 0.5516 - val_acc: 0.7324\n",
      "Epoch 257/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7204\n",
      "Epoch 257: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5497 - acc: 0.7209 - val_loss: 0.5514 - val_acc: 0.7324\n",
      "Epoch 258/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5556 - acc: 0.7220\n",
      "Epoch 258: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5564 - acc: 0.7207 - val_loss: 0.5516 - val_acc: 0.7324\n",
      "Epoch 259/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5546 - acc: 0.7185\n",
      "Epoch 259: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5540 - acc: 0.7191 - val_loss: 0.5513 - val_acc: 0.7324\n",
      "Epoch 260/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5567 - acc: 0.7207\n",
      "Epoch 260: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5567 - acc: 0.7207 - val_loss: 0.5513 - val_acc: 0.7324\n",
      "Epoch 261/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5586 - acc: 0.7159\n",
      "Epoch 261: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5594 - acc: 0.7168 - val_loss: 0.5511 - val_acc: 0.7319\n",
      "Epoch 262/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.7226\n",
      "Epoch 262: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5532 - acc: 0.7234 - val_loss: 0.5514 - val_acc: 0.7324\n",
      "Epoch 263/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5473 - acc: 0.7234\n",
      "Epoch 263: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5467 - acc: 0.7235 - val_loss: 0.5511 - val_acc: 0.7324\n",
      "Epoch 264/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.7236\n",
      "Epoch 264: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5528 - acc: 0.7229 - val_loss: 0.5509 - val_acc: 0.7315\n",
      "Epoch 265/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5502 - acc: 0.7264\n",
      "Epoch 265: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5516 - acc: 0.7251 - val_loss: 0.5507 - val_acc: 0.7319\n",
      "Epoch 266/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.7201\n",
      "Epoch 266: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5554 - acc: 0.7189 - val_loss: 0.5506 - val_acc: 0.7324\n",
      "Epoch 267/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5493 - acc: 0.7232\n",
      "Epoch 267: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5491 - acc: 0.7231 - val_loss: 0.5505 - val_acc: 0.7319\n",
      "Epoch 268/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.7241\n",
      "Epoch 268: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5482 - acc: 0.7247 - val_loss: 0.5502 - val_acc: 0.7319\n",
      "Epoch 269/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5490 - acc: 0.7237\n",
      "Epoch 269: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5498 - acc: 0.7234 - val_loss: 0.5502 - val_acc: 0.7319\n",
      "Epoch 270/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5494 - acc: 0.7233\n",
      "Epoch 270: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5495 - acc: 0.7230 - val_loss: 0.5499 - val_acc: 0.7319\n",
      "Epoch 271/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5488 - acc: 0.7240\n",
      "Epoch 271: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5499 - acc: 0.7223 - val_loss: 0.5495 - val_acc: 0.7319\n",
      "Epoch 272/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5502 - acc: 0.7220\n",
      "Epoch 272: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5500 - acc: 0.7216 - val_loss: 0.5492 - val_acc: 0.7319\n",
      "Epoch 273/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5487 - acc: 0.7228\n",
      "Epoch 273: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5482 - acc: 0.7234 - val_loss: 0.5491 - val_acc: 0.7319\n",
      "Epoch 274/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5509 - acc: 0.7238\n",
      "Epoch 274: val_acc improved from 0.73236 to 0.73279, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5516 - acc: 0.7226 - val_loss: 0.5490 - val_acc: 0.7328\n",
      "Epoch 275/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5498 - acc: 0.7248\n",
      "Epoch 275: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5480 - acc: 0.7254 - val_loss: 0.5488 - val_acc: 0.7328\n",
      "Epoch 276/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.7265\n",
      "Epoch 276: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5465 - acc: 0.7272 - val_loss: 0.5485 - val_acc: 0.7328\n",
      "Epoch 277/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.7195\n",
      "Epoch 277: val_acc improved from 0.73279 to 0.73322, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5539 - acc: 0.7204 - val_loss: 0.5487 - val_acc: 0.7332\n",
      "Epoch 278/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5508 - acc: 0.7237\n",
      "Epoch 278: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5519 - acc: 0.7228 - val_loss: 0.5489 - val_acc: 0.7332\n",
      "Epoch 279/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5468 - acc: 0.7274\n",
      "Epoch 279: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5470 - acc: 0.7271 - val_loss: 0.5486 - val_acc: 0.7332\n",
      "Epoch 280/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5482 - acc: 0.7267\n",
      "Epoch 280: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5492 - acc: 0.7265 - val_loss: 0.5484 - val_acc: 0.7332\n",
      "Epoch 281/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.7195\n",
      "Epoch 281: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5560 - acc: 0.7195 - val_loss: 0.5482 - val_acc: 0.7332\n",
      "Epoch 282/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5497 - acc: 0.7241\n",
      "Epoch 282: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5497 - acc: 0.7241 - val_loss: 0.5481 - val_acc: 0.7332\n",
      "Epoch 283/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.7218\n",
      "Epoch 283: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5513 - acc: 0.7225 - val_loss: 0.5479 - val_acc: 0.7332\n",
      "Epoch 284/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.7210\n",
      "Epoch 284: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5521 - acc: 0.7213 - val_loss: 0.5476 - val_acc: 0.7328\n",
      "Epoch 285/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.7268\n",
      "Epoch 285: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5513 - acc: 0.7259 - val_loss: 0.5476 - val_acc: 0.7328\n",
      "Epoch 286/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5555 - acc: 0.7180\n",
      "Epoch 286: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5553 - acc: 0.7175 - val_loss: 0.5476 - val_acc: 0.7332\n",
      "Epoch 287/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.7247\n",
      "Epoch 287: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5489 - acc: 0.7244 - val_loss: 0.5473 - val_acc: 0.7324\n",
      "Epoch 288/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.7280\n",
      "Epoch 288: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5474 - acc: 0.7277 - val_loss: 0.5472 - val_acc: 0.7332\n",
      "Epoch 289/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5495 - acc: 0.7211\n",
      "Epoch 289: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5496 - acc: 0.7209 - val_loss: 0.5471 - val_acc: 0.7332\n",
      "Epoch 290/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7248\n",
      "Epoch 290: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5485 - acc: 0.7251 - val_loss: 0.5470 - val_acc: 0.7332\n",
      "Epoch 291/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.7270\n",
      "Epoch 291: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5512 - acc: 0.7269 - val_loss: 0.5466 - val_acc: 0.7328\n",
      "Epoch 292/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7240\n",
      "Epoch 292: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5496 - acc: 0.7236 - val_loss: 0.5463 - val_acc: 0.7332\n",
      "Epoch 293/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7265\n",
      "Epoch 293: val_acc improved from 0.73322 to 0.73365, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5432 - acc: 0.7262 - val_loss: 0.5465 - val_acc: 0.7336\n",
      "Epoch 294/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5461 - acc: 0.7266\n",
      "Epoch 294: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5459 - acc: 0.7260 - val_loss: 0.5462 - val_acc: 0.7332\n",
      "Epoch 295/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7304\n",
      "Epoch 295: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5448 - acc: 0.7305 - val_loss: 0.5462 - val_acc: 0.7332\n",
      "Epoch 296/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.7235\n",
      "Epoch 296: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5521 - acc: 0.7237 - val_loss: 0.5460 - val_acc: 0.7332\n",
      "Epoch 297/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5470 - acc: 0.7226\n",
      "Epoch 297: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5474 - acc: 0.7223 - val_loss: 0.5462 - val_acc: 0.7336\n",
      "Epoch 298/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5532 - acc: 0.7194\n",
      "Epoch 298: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5523 - acc: 0.7191 - val_loss: 0.5465 - val_acc: 0.7336\n",
      "Epoch 299/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7259\n",
      "Epoch 299: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5447 - acc: 0.7260 - val_loss: 0.5462 - val_acc: 0.7336\n",
      "Epoch 300/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7212\n",
      "Epoch 300: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5480 - acc: 0.7215 - val_loss: 0.5457 - val_acc: 0.7336\n",
      "Epoch 301/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5464 - acc: 0.7250\n",
      "Epoch 301: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5464 - acc: 0.7250 - val_loss: 0.5457 - val_acc: 0.7336\n",
      "Epoch 302/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7290\n",
      "Epoch 302: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5452 - acc: 0.7288 - val_loss: 0.5455 - val_acc: 0.7336\n",
      "Epoch 303/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7221\n",
      "Epoch 303: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5449 - acc: 0.7223 - val_loss: 0.5458 - val_acc: 0.7332\n",
      "Epoch 304/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5425 - acc: 0.7227\n",
      "Epoch 304: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5428 - acc: 0.7219 - val_loss: 0.5453 - val_acc: 0.7332\n",
      "Epoch 305/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.7236\n",
      "Epoch 305: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5484 - acc: 0.7235 - val_loss: 0.5454 - val_acc: 0.7332\n",
      "Epoch 306/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7257\n",
      "Epoch 306: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5457 - acc: 0.7261 - val_loss: 0.5452 - val_acc: 0.7332\n",
      "Epoch 307/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7250\n",
      "Epoch 307: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5481 - acc: 0.7255 - val_loss: 0.5452 - val_acc: 0.7332\n",
      "Epoch 308/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7203\n",
      "Epoch 308: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5484 - acc: 0.7201 - val_loss: 0.5451 - val_acc: 0.7336\n",
      "Epoch 309/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7268\n",
      "Epoch 309: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5425 - acc: 0.7266 - val_loss: 0.5446 - val_acc: 0.7336\n",
      "Epoch 310/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5425 - acc: 0.7284\n",
      "Epoch 310: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5425 - acc: 0.7275 - val_loss: 0.5443 - val_acc: 0.7336\n",
      "Epoch 311/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.7265\n",
      "Epoch 311: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5453 - acc: 0.7265 - val_loss: 0.5443 - val_acc: 0.7336\n",
      "Epoch 312/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7206\n",
      "Epoch 312: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5473 - acc: 0.7210 - val_loss: 0.5444 - val_acc: 0.7336\n",
      "Epoch 313/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7242\n",
      "Epoch 313: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5459 - acc: 0.7241 - val_loss: 0.5445 - val_acc: 0.7336\n",
      "Epoch 314/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5432 - acc: 0.7287\n",
      "Epoch 314: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5440 - acc: 0.7275 - val_loss: 0.5440 - val_acc: 0.7336\n",
      "Epoch 315/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5410 - acc: 0.7248\n",
      "Epoch 315: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5424 - acc: 0.7243 - val_loss: 0.5437 - val_acc: 0.7336\n",
      "Epoch 316/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7229\n",
      "Epoch 316: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5487 - acc: 0.7234 - val_loss: 0.5440 - val_acc: 0.7336\n",
      "Epoch 317/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7208\n",
      "Epoch 317: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5451 - acc: 0.7211 - val_loss: 0.5439 - val_acc: 0.7336\n",
      "Epoch 318/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7251\n",
      "Epoch 318: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5422 - acc: 0.7249 - val_loss: 0.5439 - val_acc: 0.7336\n",
      "Epoch 319/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7219\n",
      "Epoch 319: val_acc improved from 0.73365 to 0.73407, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5472 - acc: 0.7222 - val_loss: 0.5438 - val_acc: 0.7341\n",
      "Epoch 320/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.7254\n",
      "Epoch 320: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5431 - acc: 0.7256 - val_loss: 0.5439 - val_acc: 0.7336\n",
      "Epoch 321/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7276\n",
      "Epoch 321: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5418 - acc: 0.7275 - val_loss: 0.5437 - val_acc: 0.7336\n",
      "Epoch 322/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7273\n",
      "Epoch 322: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5412 - acc: 0.7283 - val_loss: 0.5432 - val_acc: 0.7341\n",
      "Epoch 323/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5442 - acc: 0.7297\n",
      "Epoch 323: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5449 - acc: 0.7283 - val_loss: 0.5433 - val_acc: 0.7336\n",
      "Epoch 324/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.7272\n",
      "Epoch 324: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5448 - acc: 0.7270 - val_loss: 0.5430 - val_acc: 0.7341\n",
      "Epoch 325/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7284\n",
      "Epoch 325: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5384 - acc: 0.7291 - val_loss: 0.5425 - val_acc: 0.7336\n",
      "Epoch 326/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5399 - acc: 0.7297\n",
      "Epoch 326: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5399 - acc: 0.7297 - val_loss: 0.5425 - val_acc: 0.7336\n",
      "Epoch 327/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5444 - acc: 0.7245\n",
      "Epoch 327: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5439 - acc: 0.7251 - val_loss: 0.5425 - val_acc: 0.7336\n",
      "Epoch 328/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5433 - acc: 0.7292\n",
      "Epoch 328: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5436 - acc: 0.7275 - val_loss: 0.5427 - val_acc: 0.7336\n",
      "Epoch 329/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7296\n",
      "Epoch 329: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5428 - acc: 0.7298 - val_loss: 0.5424 - val_acc: 0.7332\n",
      "Epoch 330/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7275\n",
      "Epoch 330: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5415 - acc: 0.7280 - val_loss: 0.5423 - val_acc: 0.7336\n",
      "Epoch 331/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5497 - acc: 0.7221\n",
      "Epoch 331: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5492 - acc: 0.7220 - val_loss: 0.5422 - val_acc: 0.7332\n",
      "Epoch 332/2000\n",
      "268/293 [==========================>...] - ETA: 0s - loss: 0.5429 - acc: 0.7213\n",
      "Epoch 332: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5420 - acc: 0.7227 - val_loss: 0.5422 - val_acc: 0.7336\n",
      "Epoch 333/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5446 - acc: 0.7280\n",
      "Epoch 333: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5446 - acc: 0.7281 - val_loss: 0.5420 - val_acc: 0.7332\n",
      "Epoch 334/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5432 - acc: 0.7266\n",
      "Epoch 334: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5430 - acc: 0.7261 - val_loss: 0.5421 - val_acc: 0.7332\n",
      "Epoch 335/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5475 - acc: 0.7180\n",
      "Epoch 335: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5481 - acc: 0.7183 - val_loss: 0.5417 - val_acc: 0.7336\n",
      "Epoch 336/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5441 - acc: 0.7248\n",
      "Epoch 336: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5433 - acc: 0.7249 - val_loss: 0.5415 - val_acc: 0.7336\n",
      "Epoch 337/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5437 - acc: 0.7263\n",
      "Epoch 337: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5442 - acc: 0.7254 - val_loss: 0.5415 - val_acc: 0.7332\n",
      "Epoch 338/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5380 - acc: 0.7289\n",
      "Epoch 338: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5389 - acc: 0.7274 - val_loss: 0.5413 - val_acc: 0.7332\n",
      "Epoch 339/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.7282\n",
      "Epoch 339: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5417 - acc: 0.7281 - val_loss: 0.5413 - val_acc: 0.7336\n",
      "Epoch 340/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7239\n",
      "Epoch 340: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5417 - acc: 0.7240 - val_loss: 0.5410 - val_acc: 0.7336\n",
      "Epoch 341/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.7280\n",
      "Epoch 341: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5415 - acc: 0.7286 - val_loss: 0.5409 - val_acc: 0.7341\n",
      "Epoch 342/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5446 - acc: 0.7250\n",
      "Epoch 342: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5461 - acc: 0.7237 - val_loss: 0.5408 - val_acc: 0.7336\n",
      "Epoch 343/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5429 - acc: 0.7286\n",
      "Epoch 343: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5430 - acc: 0.7281 - val_loss: 0.5407 - val_acc: 0.7336\n",
      "Epoch 344/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.7243\n",
      "Epoch 344: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5391 - acc: 0.7249 - val_loss: 0.5406 - val_acc: 0.7336\n",
      "Epoch 345/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.7262\n",
      "Epoch 345: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5381 - acc: 0.7265 - val_loss: 0.5405 - val_acc: 0.7341\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 4.2677 - acc: 0.5799\n",
      "Epoch 1: val_acc improved from -inf to 0.72082, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 4.2622 - acc: 0.5795 - val_loss: 0.8803 - val_acc: 0.7208\n",
      "Epoch 2/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 2.3919 - acc: 0.6016\n",
      "Epoch 2: val_acc improved from 0.72082 to 0.72381, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 2.3868 - acc: 0.6018 - val_loss: 0.5885 - val_acc: 0.7238\n",
      "Epoch 3/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 1.8127 - acc: 0.6078\n",
      "Epoch 3: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.8016 - acc: 0.6086 - val_loss: 0.5716 - val_acc: 0.7191\n",
      "Epoch 4/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 1.5780 - acc: 0.6020\n",
      "Epoch 4: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.5622 - acc: 0.6041 - val_loss: 0.5790 - val_acc: 0.7183\n",
      "Epoch 5/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 1.3000 - acc: 0.6116\n",
      "Epoch 5: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.3000 - acc: 0.6111 - val_loss: 0.5880 - val_acc: 0.7140\n",
      "Epoch 6/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 1.1805 - acc: 0.6138\n",
      "Epoch 6: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.1802 - acc: 0.6139 - val_loss: 0.5911 - val_acc: 0.7187\n",
      "Epoch 7/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 1.0934 - acc: 0.6100\n",
      "Epoch 7: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.0864 - acc: 0.6118 - val_loss: 0.6060 - val_acc: 0.7148\n",
      "Epoch 8/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 1.0272 - acc: 0.6177\n",
      "Epoch 8: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.0218 - acc: 0.6182 - val_loss: 0.6132 - val_acc: 0.7165\n",
      "Epoch 9/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.9268 - acc: 0.6275\n",
      "Epoch 9: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.9292 - acc: 0.6265 - val_loss: 0.6137 - val_acc: 0.7187\n",
      "Epoch 10/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.9095 - acc: 0.6203\n",
      "Epoch 10: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.9061 - acc: 0.6217 - val_loss: 0.6186 - val_acc: 0.7200\n",
      "Epoch 11/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.8833 - acc: 0.6248\n",
      "Epoch 11: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8801 - acc: 0.6251 - val_loss: 0.6199 - val_acc: 0.7212\n",
      "Epoch 12/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.8447 - acc: 0.6322\n",
      "Epoch 12: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8430 - acc: 0.6322 - val_loss: 0.6213 - val_acc: 0.7204\n",
      "Epoch 13/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.8145 - acc: 0.6342\n",
      "Epoch 13: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.8106 - acc: 0.6356 - val_loss: 0.6213 - val_acc: 0.7187\n",
      "Epoch 14/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.7913 - acc: 0.6300\n",
      "Epoch 14: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7901 - acc: 0.6301 - val_loss: 0.6220 - val_acc: 0.7200\n",
      "Epoch 15/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.7898 - acc: 0.6350\n",
      "Epoch 15: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7878 - acc: 0.6362 - val_loss: 0.6219 - val_acc: 0.7157\n",
      "Epoch 16/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.7493 - acc: 0.6494\n",
      "Epoch 16: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7488 - acc: 0.6497 - val_loss: 0.6211 - val_acc: 0.7148\n",
      "Epoch 17/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.7502 - acc: 0.6448\n",
      "Epoch 17: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7505 - acc: 0.6450 - val_loss: 0.6218 - val_acc: 0.7170\n",
      "Epoch 18/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.7351 - acc: 0.6431\n",
      "Epoch 18: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7374 - acc: 0.6436 - val_loss: 0.6221 - val_acc: 0.7144\n",
      "Epoch 19/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.7196 - acc: 0.6486\n",
      "Epoch 19: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7178 - acc: 0.6498 - val_loss: 0.6214 - val_acc: 0.7144\n",
      "Epoch 20/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.7154 - acc: 0.6485\n",
      "Epoch 20: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7137 - acc: 0.6494 - val_loss: 0.6221 - val_acc: 0.7148\n",
      "Epoch 21/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.7108 - acc: 0.6549\n",
      "Epoch 21: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7084 - acc: 0.6560 - val_loss: 0.6223 - val_acc: 0.7136\n",
      "Epoch 22/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.7050 - acc: 0.6483\n",
      "Epoch 22: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7045 - acc: 0.6487 - val_loss: 0.6232 - val_acc: 0.7127\n",
      "Epoch 23/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6907 - acc: 0.6555\n",
      "Epoch 23: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6888 - acc: 0.6554 - val_loss: 0.6222 - val_acc: 0.7131\n",
      "Epoch 24/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6908 - acc: 0.6573\n",
      "Epoch 24: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6895 - acc: 0.6574 - val_loss: 0.6209 - val_acc: 0.7123\n",
      "Epoch 25/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6875 - acc: 0.6605\n",
      "Epoch 25: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6822 - acc: 0.6629 - val_loss: 0.6213 - val_acc: 0.7127\n",
      "Epoch 26/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6900 - acc: 0.6573\n",
      "Epoch 26: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6853 - acc: 0.6588 - val_loss: 0.6202 - val_acc: 0.7127\n",
      "Epoch 27/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6767 - acc: 0.6611\n",
      "Epoch 27: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6765 - acc: 0.6610 - val_loss: 0.6200 - val_acc: 0.7127\n",
      "Epoch 28/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6700 - acc: 0.6613\n",
      "Epoch 28: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6702 - acc: 0.6618 - val_loss: 0.6185 - val_acc: 0.7114\n",
      "Epoch 29/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6603 - acc: 0.6657\n",
      "Epoch 29: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6615 - acc: 0.6650 - val_loss: 0.6167 - val_acc: 0.7114\n",
      "Epoch 30/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6624 - acc: 0.6640\n",
      "Epoch 30: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6586 - acc: 0.6653 - val_loss: 0.6159 - val_acc: 0.7110\n",
      "Epoch 31/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6613 - acc: 0.6687\n",
      "Epoch 31: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6606 - acc: 0.6694 - val_loss: 0.6161 - val_acc: 0.7106\n",
      "Epoch 32/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6586 - acc: 0.6681\n",
      "Epoch 32: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6579 - acc: 0.6692 - val_loss: 0.6150 - val_acc: 0.7093\n",
      "Epoch 33/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6535 - acc: 0.6725\n",
      "Epoch 33: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6532 - acc: 0.6733 - val_loss: 0.6154 - val_acc: 0.7114\n",
      "Epoch 34/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6293 - acc: 0.6787\n",
      "Epoch 34: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6280 - acc: 0.6788 - val_loss: 0.6145 - val_acc: 0.7123\n",
      "Epoch 35/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6417 - acc: 0.6765\n",
      "Epoch 35: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6417 - acc: 0.6765 - val_loss: 0.6136 - val_acc: 0.7114\n",
      "Epoch 36/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.6744\n",
      "Epoch 36: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6363 - acc: 0.6746 - val_loss: 0.6132 - val_acc: 0.7114\n",
      "Epoch 37/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6433 - acc: 0.6812\n",
      "Epoch 37: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6423 - acc: 0.6815 - val_loss: 0.6120 - val_acc: 0.7114\n",
      "Epoch 38/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6419 - acc: 0.6737\n",
      "Epoch 38: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6413 - acc: 0.6740 - val_loss: 0.6110 - val_acc: 0.7114\n",
      "Epoch 39/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6461 - acc: 0.6722\n",
      "Epoch 39: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6455 - acc: 0.6720 - val_loss: 0.6093 - val_acc: 0.7114\n",
      "Epoch 40/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6429 - acc: 0.6772\n",
      "Epoch 40: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6412 - acc: 0.6781 - val_loss: 0.6085 - val_acc: 0.7093\n",
      "Epoch 41/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6346 - acc: 0.6843\n",
      "Epoch 41: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6334 - acc: 0.6848 - val_loss: 0.6088 - val_acc: 0.7127\n",
      "Epoch 42/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6239 - acc: 0.6824\n",
      "Epoch 42: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6239 - acc: 0.6824 - val_loss: 0.6076 - val_acc: 0.7127\n",
      "Epoch 43/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6260 - acc: 0.6812\n",
      "Epoch 43: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6235 - acc: 0.6822 - val_loss: 0.6066 - val_acc: 0.7123\n",
      "Epoch 44/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6202 - acc: 0.6865\n",
      "Epoch 44: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6195 - acc: 0.6877 - val_loss: 0.6070 - val_acc: 0.7131\n",
      "Epoch 45/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6275 - acc: 0.6835\n",
      "Epoch 45: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6263 - acc: 0.6847 - val_loss: 0.6056 - val_acc: 0.7127\n",
      "Epoch 46/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6254 - acc: 0.6835\n",
      "Epoch 46: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6254 - acc: 0.6834 - val_loss: 0.6056 - val_acc: 0.7127\n",
      "Epoch 47/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6280 - acc: 0.6854\n",
      "Epoch 47: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6281 - acc: 0.6852 - val_loss: 0.6060 - val_acc: 0.7131\n",
      "Epoch 48/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6238 - acc: 0.6854\n",
      "Epoch 48: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6219 - acc: 0.6864 - val_loss: 0.6053 - val_acc: 0.7136\n",
      "Epoch 49/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6239 - acc: 0.6767\n",
      "Epoch 49: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6230 - acc: 0.6778 - val_loss: 0.6052 - val_acc: 0.7131\n",
      "Epoch 50/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6204 - acc: 0.6837\n",
      "Epoch 50: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6206 - acc: 0.6837 - val_loss: 0.6035 - val_acc: 0.7131\n",
      "Epoch 51/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.6858\n",
      "Epoch 51: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6245 - acc: 0.6867 - val_loss: 0.6034 - val_acc: 0.7131\n",
      "Epoch 52/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6139 - acc: 0.6867\n",
      "Epoch 52: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6131 - acc: 0.6869 - val_loss: 0.6021 - val_acc: 0.7136\n",
      "Epoch 53/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6120 - acc: 0.6866\n",
      "Epoch 53: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6117 - acc: 0.6859 - val_loss: 0.6026 - val_acc: 0.7140\n",
      "Epoch 54/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6231 - acc: 0.6829\n",
      "Epoch 54: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6224 - acc: 0.6836 - val_loss: 0.6018 - val_acc: 0.7144\n",
      "Epoch 55/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.6086 - acc: 0.6947\n",
      "Epoch 55: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6074 - acc: 0.6950 - val_loss: 0.6010 - val_acc: 0.7148\n",
      "Epoch 56/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.6925\n",
      "Epoch 56: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6115 - acc: 0.6926 - val_loss: 0.6006 - val_acc: 0.7148\n",
      "Epoch 57/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6102 - acc: 0.6944\n",
      "Epoch 57: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6115 - acc: 0.6945 - val_loss: 0.5994 - val_acc: 0.7148\n",
      "Epoch 58/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6092 - acc: 0.6893\n",
      "Epoch 58: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6085 - acc: 0.6899 - val_loss: 0.5990 - val_acc: 0.7153\n",
      "Epoch 59/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6141 - acc: 0.6947\n",
      "Epoch 59: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6131 - acc: 0.6949 - val_loss: 0.5989 - val_acc: 0.7153\n",
      "Epoch 60/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.6957\n",
      "Epoch 60: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6060 - acc: 0.6961 - val_loss: 0.5981 - val_acc: 0.7153\n",
      "Epoch 61/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6095 - acc: 0.6918\n",
      "Epoch 61: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6089 - acc: 0.6917 - val_loss: 0.5974 - val_acc: 0.7157\n",
      "Epoch 62/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6071 - acc: 0.6961\n",
      "Epoch 62: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6058 - acc: 0.6980 - val_loss: 0.5964 - val_acc: 0.7161\n",
      "Epoch 63/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6111 - acc: 0.6946\n",
      "Epoch 63: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6106 - acc: 0.6952 - val_loss: 0.5957 - val_acc: 0.7157\n",
      "Epoch 64/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6070 - acc: 0.6976\n",
      "Epoch 64: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6072 - acc: 0.6973 - val_loss: 0.5951 - val_acc: 0.7157\n",
      "Epoch 65/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6084 - acc: 0.6938\n",
      "Epoch 65: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6083 - acc: 0.6939 - val_loss: 0.5950 - val_acc: 0.7157\n",
      "Epoch 66/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.6112 - acc: 0.6952\n",
      "Epoch 66: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6099 - acc: 0.6963 - val_loss: 0.5942 - val_acc: 0.7157\n",
      "Epoch 67/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6120 - acc: 0.6882\n",
      "Epoch 67: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6104 - acc: 0.6894 - val_loss: 0.5935 - val_acc: 0.7157\n",
      "Epoch 68/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6138 - acc: 0.6926\n",
      "Epoch 68: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6129 - acc: 0.6933 - val_loss: 0.5935 - val_acc: 0.7161\n",
      "Epoch 69/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.6030 - acc: 0.6944\n",
      "Epoch 69: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6021 - acc: 0.6952 - val_loss: 0.5931 - val_acc: 0.7161\n",
      "Epoch 70/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6041 - acc: 0.6939\n",
      "Epoch 70: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6028 - acc: 0.6955 - val_loss: 0.5928 - val_acc: 0.7161\n",
      "Epoch 71/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6026 - acc: 0.6990\n",
      "Epoch 71: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6020 - acc: 0.6996 - val_loss: 0.5920 - val_acc: 0.7165\n",
      "Epoch 72/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.6943\n",
      "Epoch 72: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6024 - acc: 0.6942 - val_loss: 0.5911 - val_acc: 0.7170\n",
      "Epoch 73/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6094 - acc: 0.6939\n",
      "Epoch 73: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6098 - acc: 0.6943 - val_loss: 0.5913 - val_acc: 0.7170\n",
      "Epoch 74/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.6928\n",
      "Epoch 74: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6010 - acc: 0.6933 - val_loss: 0.5905 - val_acc: 0.7174\n",
      "Epoch 75/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6043 - acc: 0.6921\n",
      "Epoch 75: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6043 - acc: 0.6920 - val_loss: 0.5894 - val_acc: 0.7165\n",
      "Epoch 76/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5983 - acc: 0.6944\n",
      "Epoch 76: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5985 - acc: 0.6945 - val_loss: 0.5893 - val_acc: 0.7178\n",
      "Epoch 77/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5968 - acc: 0.6951\n",
      "Epoch 77: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5968 - acc: 0.6951 - val_loss: 0.5891 - val_acc: 0.7178\n",
      "Epoch 78/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.6021 - acc: 0.6910\n",
      "Epoch 78: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6007 - acc: 0.6928 - val_loss: 0.5885 - val_acc: 0.7187\n",
      "Epoch 79/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5961 - acc: 0.7010\n",
      "Epoch 79: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5949 - acc: 0.7013 - val_loss: 0.5877 - val_acc: 0.7187\n",
      "Epoch 80/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5933 - acc: 0.7008\n",
      "Epoch 80: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5929 - acc: 0.7013 - val_loss: 0.5871 - val_acc: 0.7183\n",
      "Epoch 81/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5998 - acc: 0.6964\n",
      "Epoch 81: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5985 - acc: 0.6981 - val_loss: 0.5876 - val_acc: 0.7191\n",
      "Epoch 82/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5954 - acc: 0.6999\n",
      "Epoch 82: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5938 - acc: 0.7012 - val_loss: 0.5870 - val_acc: 0.7191\n",
      "Epoch 83/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5912 - acc: 0.7030\n",
      "Epoch 83: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5910 - acc: 0.7032 - val_loss: 0.5867 - val_acc: 0.7191\n",
      "Epoch 84/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5897 - acc: 0.7016\n",
      "Epoch 84: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5897 - acc: 0.7014 - val_loss: 0.5852 - val_acc: 0.7191\n",
      "Epoch 85/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.7007\n",
      "Epoch 85: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5970 - acc: 0.7017 - val_loss: 0.5850 - val_acc: 0.7191\n",
      "Epoch 86/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5949 - acc: 0.7006\n",
      "Epoch 86: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5949 - acc: 0.7006 - val_loss: 0.5850 - val_acc: 0.7204\n",
      "Epoch 87/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.7018\n",
      "Epoch 87: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5896 - acc: 0.7015 - val_loss: 0.5846 - val_acc: 0.7204\n",
      "Epoch 88/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5954 - acc: 0.7007\n",
      "Epoch 88: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5963 - acc: 0.7007 - val_loss: 0.5839 - val_acc: 0.7200\n",
      "Epoch 89/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.7051\n",
      "Epoch 89: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5848 - acc: 0.7052 - val_loss: 0.5835 - val_acc: 0.7195\n",
      "Epoch 90/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5882 - acc: 0.7023\n",
      "Epoch 90: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5875 - acc: 0.7037 - val_loss: 0.5827 - val_acc: 0.7195\n",
      "Epoch 91/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5908 - acc: 0.7014\n",
      "Epoch 91: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5907 - acc: 0.7013 - val_loss: 0.5826 - val_acc: 0.7200\n",
      "Epoch 92/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5890 - acc: 0.7046\n",
      "Epoch 92: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5897 - acc: 0.7044 - val_loss: 0.5823 - val_acc: 0.7200\n",
      "Epoch 93/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5928 - acc: 0.7039\n",
      "Epoch 93: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5903 - acc: 0.7061 - val_loss: 0.5819 - val_acc: 0.7204\n",
      "Epoch 94/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5876 - acc: 0.7008\n",
      "Epoch 94: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5880 - acc: 0.7014 - val_loss: 0.5817 - val_acc: 0.7200\n",
      "Epoch 95/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5896 - acc: 0.7020\n",
      "Epoch 95: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5890 - acc: 0.7028 - val_loss: 0.5810 - val_acc: 0.7208\n",
      "Epoch 96/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.7022\n",
      "Epoch 96: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5889 - acc: 0.7029 - val_loss: 0.5807 - val_acc: 0.7208\n",
      "Epoch 97/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5917 - acc: 0.7005\n",
      "Epoch 97: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5904 - acc: 0.7010 - val_loss: 0.5802 - val_acc: 0.7204\n",
      "Epoch 98/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5868 - acc: 0.7054\n",
      "Epoch 98: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5862 - acc: 0.7055 - val_loss: 0.5798 - val_acc: 0.7208\n",
      "Epoch 99/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5919 - acc: 0.7003\n",
      "Epoch 99: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5913 - acc: 0.7013 - val_loss: 0.5798 - val_acc: 0.7225\n",
      "Epoch 100/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5875 - acc: 0.7047\n",
      "Epoch 100: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5887 - acc: 0.7048 - val_loss: 0.5792 - val_acc: 0.7217\n",
      "Epoch 101/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.7036\n",
      "Epoch 101: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5879 - acc: 0.7036 - val_loss: 0.5786 - val_acc: 0.7221\n",
      "Epoch 102/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5893 - acc: 0.7023\n",
      "Epoch 102: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5894 - acc: 0.7023 - val_loss: 0.5782 - val_acc: 0.7212\n",
      "Epoch 103/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5855 - acc: 0.7031\n",
      "Epoch 103: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5846 - acc: 0.7038 - val_loss: 0.5780 - val_acc: 0.7212\n",
      "Epoch 104/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5863 - acc: 0.7021\n",
      "Epoch 104: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5859 - acc: 0.7028 - val_loss: 0.5774 - val_acc: 0.7217\n",
      "Epoch 105/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5886 - acc: 0.7020\n",
      "Epoch 105: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5883 - acc: 0.7024 - val_loss: 0.5771 - val_acc: 0.7225\n",
      "Epoch 106/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5917 - acc: 0.6991\n",
      "Epoch 106: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5904 - acc: 0.7006 - val_loss: 0.5770 - val_acc: 0.7225\n",
      "Epoch 107/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5871 - acc: 0.7073\n",
      "Epoch 107: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5871 - acc: 0.7076 - val_loss: 0.5767 - val_acc: 0.7225\n",
      "Epoch 108/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5914 - acc: 0.7056\n",
      "Epoch 108: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5925 - acc: 0.7048 - val_loss: 0.5774 - val_acc: 0.7230\n",
      "Epoch 109/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5893 - acc: 0.7070\n",
      "Epoch 109: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5893 - acc: 0.7070 - val_loss: 0.5764 - val_acc: 0.7225\n",
      "Epoch 110/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.7070\n",
      "Epoch 110: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5820 - acc: 0.7072 - val_loss: 0.5760 - val_acc: 0.7225\n",
      "Epoch 111/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5910 - acc: 0.6998\n",
      "Epoch 111: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5899 - acc: 0.7006 - val_loss: 0.5761 - val_acc: 0.7230\n",
      "Epoch 112/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.7072\n",
      "Epoch 112: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5828 - acc: 0.7079 - val_loss: 0.5755 - val_acc: 0.7234\n",
      "Epoch 113/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.7026\n",
      "Epoch 113: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5857 - acc: 0.7027 - val_loss: 0.5749 - val_acc: 0.7234\n",
      "Epoch 114/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5878 - acc: 0.7028\n",
      "Epoch 114: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5854 - acc: 0.7041 - val_loss: 0.5749 - val_acc: 0.7238\n",
      "Epoch 115/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5802 - acc: 0.7075\n",
      "Epoch 115: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5787 - acc: 0.7086 - val_loss: 0.5740 - val_acc: 0.7234\n",
      "Epoch 116/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5818 - acc: 0.7053\n",
      "Epoch 116: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5818 - acc: 0.7059 - val_loss: 0.5738 - val_acc: 0.7238\n",
      "Epoch 117/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5803 - acc: 0.7071\n",
      "Epoch 117: val_acc improved from 0.72381 to 0.72424, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5798 - acc: 0.7075 - val_loss: 0.5737 - val_acc: 0.7242\n",
      "Epoch 118/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5854 - acc: 0.7076\n",
      "Epoch 118: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5845 - acc: 0.7082 - val_loss: 0.5732 - val_acc: 0.7238\n",
      "Epoch 119/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.7037\n",
      "Epoch 119: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5861 - acc: 0.7040 - val_loss: 0.5730 - val_acc: 0.7238\n",
      "Epoch 120/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5822 - acc: 0.7078\n",
      "Epoch 120: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5823 - acc: 0.7079 - val_loss: 0.5729 - val_acc: 0.7238\n",
      "Epoch 121/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5751 - acc: 0.7106\n",
      "Epoch 121: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5751 - acc: 0.7106 - val_loss: 0.5721 - val_acc: 0.7242\n",
      "Epoch 122/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5804 - acc: 0.7056\n",
      "Epoch 122: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5799 - acc: 0.7061 - val_loss: 0.5717 - val_acc: 0.7242\n",
      "Epoch 123/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5830 - acc: 0.7091\n",
      "Epoch 123: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5824 - acc: 0.7097 - val_loss: 0.5716 - val_acc: 0.7242\n",
      "Epoch 124/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5829 - acc: 0.7068\n",
      "Epoch 124: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5826 - acc: 0.7068 - val_loss: 0.5721 - val_acc: 0.7242\n",
      "Epoch 125/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.7067\n",
      "Epoch 125: val_acc improved from 0.72424 to 0.72467, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5828 - acc: 0.7066 - val_loss: 0.5720 - val_acc: 0.7247\n",
      "Epoch 126/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5801 - acc: 0.7064\n",
      "Epoch 126: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5797 - acc: 0.7074 - val_loss: 0.5719 - val_acc: 0.7247\n",
      "Epoch 127/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5824 - acc: 0.7086\n",
      "Epoch 127: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5817 - acc: 0.7087 - val_loss: 0.5718 - val_acc: 0.7242\n",
      "Epoch 128/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5794 - acc: 0.7099\n",
      "Epoch 128: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5802 - acc: 0.7099 - val_loss: 0.5715 - val_acc: 0.7238\n",
      "Epoch 129/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5833 - acc: 0.7083\n",
      "Epoch 129: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5831 - acc: 0.7086 - val_loss: 0.5715 - val_acc: 0.7238\n",
      "Epoch 130/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5773 - acc: 0.7123\n",
      "Epoch 130: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5768 - acc: 0.7129 - val_loss: 0.5709 - val_acc: 0.7238\n",
      "Epoch 131/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.7070\n",
      "Epoch 131: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5815 - acc: 0.7070 - val_loss: 0.5705 - val_acc: 0.7238\n",
      "Epoch 132/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5739 - acc: 0.7074\n",
      "Epoch 132: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5734 - acc: 0.7084 - val_loss: 0.5701 - val_acc: 0.7238\n",
      "Epoch 133/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.7110\n",
      "Epoch 133: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5813 - acc: 0.7121 - val_loss: 0.5698 - val_acc: 0.7242\n",
      "Epoch 134/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5742 - acc: 0.7072\n",
      "Epoch 134: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5731 - acc: 0.7085 - val_loss: 0.5688 - val_acc: 0.7242\n",
      "Epoch 135/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5711 - acc: 0.7130\n",
      "Epoch 135: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5709 - acc: 0.7137 - val_loss: 0.5682 - val_acc: 0.7238\n",
      "Epoch 136/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5743 - acc: 0.7132\n",
      "Epoch 136: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5739 - acc: 0.7133 - val_loss: 0.5679 - val_acc: 0.7247\n",
      "Epoch 137/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5793 - acc: 0.7089\n",
      "Epoch 137: val_acc improved from 0.72467 to 0.72510, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5783 - acc: 0.7104 - val_loss: 0.5679 - val_acc: 0.7251\n",
      "Epoch 138/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.7102\n",
      "Epoch 138: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5771 - acc: 0.7103 - val_loss: 0.5679 - val_acc: 0.7251\n",
      "Epoch 139/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.7078\n",
      "Epoch 139: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5767 - acc: 0.7084 - val_loss: 0.5676 - val_acc: 0.7251\n",
      "Epoch 140/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.7064\n",
      "Epoch 140: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5768 - acc: 0.7067 - val_loss: 0.5669 - val_acc: 0.7251\n",
      "Epoch 141/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5797 - acc: 0.7076\n",
      "Epoch 141: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5791 - acc: 0.7083 - val_loss: 0.5663 - val_acc: 0.7251\n",
      "Epoch 142/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5783 - acc: 0.7112\n",
      "Epoch 142: val_acc improved from 0.72510 to 0.72595, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5767 - acc: 0.7131 - val_loss: 0.5666 - val_acc: 0.7260\n",
      "Epoch 143/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.7079\n",
      "Epoch 143: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5739 - acc: 0.7084 - val_loss: 0.5667 - val_acc: 0.7260\n",
      "Epoch 144/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5761 - acc: 0.7080\n",
      "Epoch 144: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5754 - acc: 0.7085 - val_loss: 0.5664 - val_acc: 0.7255\n",
      "Epoch 145/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.7120\n",
      "Epoch 145: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5777 - acc: 0.7122 - val_loss: 0.5661 - val_acc: 0.7255\n",
      "Epoch 146/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.7121\n",
      "Epoch 146: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5720 - acc: 0.7123 - val_loss: 0.5654 - val_acc: 0.7255\n",
      "Epoch 147/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5732 - acc: 0.7118\n",
      "Epoch 147: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5732 - acc: 0.7118 - val_loss: 0.5655 - val_acc: 0.7255\n",
      "Epoch 148/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.7111\n",
      "Epoch 148: val_acc did not improve from 0.72595\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5727 - acc: 0.7117 - val_loss: 0.5647 - val_acc: 0.7260\n",
      "Epoch 149/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5769 - acc: 0.7081\n",
      "Epoch 149: val_acc improved from 0.72595 to 0.72638, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5769 - acc: 0.7091 - val_loss: 0.5647 - val_acc: 0.7264\n",
      "Epoch 150/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5747 - acc: 0.7102\n",
      "Epoch 150: val_acc did not improve from 0.72638\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5742 - acc: 0.7108 - val_loss: 0.5644 - val_acc: 0.7260\n",
      "Epoch 151/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5792 - acc: 0.7086\n",
      "Epoch 151: val_acc did not improve from 0.72638\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5775 - acc: 0.7099 - val_loss: 0.5643 - val_acc: 0.7260\n",
      "Epoch 152/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5714 - acc: 0.7151\n",
      "Epoch 152: val_acc improved from 0.72638 to 0.72681, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5714 - acc: 0.7151 - val_loss: 0.5637 - val_acc: 0.7268\n",
      "Epoch 153/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5759 - acc: 0.7097\n",
      "Epoch 153: val_acc did not improve from 0.72681\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5748 - acc: 0.7103 - val_loss: 0.5637 - val_acc: 0.7264\n",
      "Epoch 154/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5765 - acc: 0.7092\n",
      "Epoch 154: val_acc did not improve from 0.72681\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5757 - acc: 0.7095 - val_loss: 0.5635 - val_acc: 0.7264\n",
      "Epoch 155/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5718 - acc: 0.7098\n",
      "Epoch 155: val_acc improved from 0.72681 to 0.72723, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5712 - acc: 0.7107 - val_loss: 0.5633 - val_acc: 0.7272\n",
      "Epoch 156/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5735 - acc: 0.7100\n",
      "Epoch 156: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5734 - acc: 0.7104 - val_loss: 0.5629 - val_acc: 0.7268\n",
      "Epoch 157/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.7138\n",
      "Epoch 157: val_acc improved from 0.72723 to 0.72766, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5693 - acc: 0.7138 - val_loss: 0.5625 - val_acc: 0.7277\n",
      "Epoch 158/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5762 - acc: 0.7100\n",
      "Epoch 158: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5750 - acc: 0.7107 - val_loss: 0.5628 - val_acc: 0.7277\n",
      "Epoch 159/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5736 - acc: 0.7107\n",
      "Epoch 159: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5734 - acc: 0.7112 - val_loss: 0.5621 - val_acc: 0.7272\n",
      "Epoch 160/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5713 - acc: 0.7106\n",
      "Epoch 160: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5709 - acc: 0.7112 - val_loss: 0.5617 - val_acc: 0.7272\n",
      "Epoch 161/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5680 - acc: 0.7160\n",
      "Epoch 161: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5678 - acc: 0.7165 - val_loss: 0.5620 - val_acc: 0.7272\n",
      "Epoch 162/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5789 - acc: 0.7076\n",
      "Epoch 162: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5781 - acc: 0.7085 - val_loss: 0.5616 - val_acc: 0.7272\n",
      "Epoch 163/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.7120\n",
      "Epoch 163: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5718 - acc: 0.7120 - val_loss: 0.5615 - val_acc: 0.7277\n",
      "Epoch 164/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7086\n",
      "Epoch 164: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5734 - acc: 0.7091 - val_loss: 0.5616 - val_acc: 0.7277\n",
      "Epoch 165/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5735 - acc: 0.7172\n",
      "Epoch 165: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5725 - acc: 0.7179 - val_loss: 0.5611 - val_acc: 0.7277\n",
      "Epoch 166/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5730 - acc: 0.7139\n",
      "Epoch 166: val_acc did not improve from 0.72766\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5727 - acc: 0.7138 - val_loss: 0.5611 - val_acc: 0.7277\n",
      "Epoch 167/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5692 - acc: 0.7157\n",
      "Epoch 167: val_acc improved from 0.72766 to 0.72809, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5684 - acc: 0.7164 - val_loss: 0.5607 - val_acc: 0.7281\n",
      "Epoch 168/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5698 - acc: 0.7132\n",
      "Epoch 168: val_acc improved from 0.72809 to 0.72937, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5690 - acc: 0.7144 - val_loss: 0.5609 - val_acc: 0.7294\n",
      "Epoch 169/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5729 - acc: 0.7156\n",
      "Epoch 169: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5744 - acc: 0.7154 - val_loss: 0.5608 - val_acc: 0.7294\n",
      "Epoch 170/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.7090\n",
      "Epoch 170: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5728 - acc: 0.7097 - val_loss: 0.5606 - val_acc: 0.7294\n",
      "Epoch 171/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7133\n",
      "Epoch 171: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5690 - acc: 0.7137 - val_loss: 0.5604 - val_acc: 0.7294\n",
      "Epoch 172/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5698 - acc: 0.7140\n",
      "Epoch 172: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5688 - acc: 0.7146 - val_loss: 0.5599 - val_acc: 0.7294\n",
      "Epoch 173/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5682 - acc: 0.7115\n",
      "Epoch 173: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5662 - acc: 0.7127 - val_loss: 0.5598 - val_acc: 0.7289\n",
      "Epoch 174/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5714 - acc: 0.7120\n",
      "Epoch 174: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5702 - acc: 0.7128 - val_loss: 0.5597 - val_acc: 0.7289\n",
      "Epoch 175/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5751 - acc: 0.7102\n",
      "Epoch 175: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5728 - acc: 0.7120 - val_loss: 0.5596 - val_acc: 0.7289\n",
      "Epoch 176/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5693 - acc: 0.7147\n",
      "Epoch 176: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5699 - acc: 0.7153 - val_loss: 0.5594 - val_acc: 0.7289\n",
      "Epoch 177/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5710 - acc: 0.7129\n",
      "Epoch 177: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5701 - acc: 0.7141 - val_loss: 0.5594 - val_acc: 0.7294\n",
      "Epoch 178/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5677 - acc: 0.7140\n",
      "Epoch 178: val_acc did not improve from 0.72937\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5669 - acc: 0.7159 - val_loss: 0.5592 - val_acc: 0.7294\n",
      "Epoch 179/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7166\n",
      "Epoch 179: val_acc improved from 0.72937 to 0.72980, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5663 - acc: 0.7166 - val_loss: 0.5589 - val_acc: 0.7298\n",
      "Epoch 180/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5665 - acc: 0.7154\n",
      "Epoch 180: val_acc did not improve from 0.72980\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5663 - acc: 0.7165 - val_loss: 0.5584 - val_acc: 0.7294\n",
      "Epoch 181/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5673 - acc: 0.7124\n",
      "Epoch 181: val_acc did not improve from 0.72980\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5665 - acc: 0.7131 - val_loss: 0.5583 - val_acc: 0.7294\n",
      "Epoch 182/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5725 - acc: 0.7132\n",
      "Epoch 182: val_acc did not improve from 0.72980\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5707 - acc: 0.7145 - val_loss: 0.5582 - val_acc: 0.7298\n",
      "Epoch 183/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5689 - acc: 0.7161\n",
      "Epoch 183: val_acc improved from 0.72980 to 0.73023, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5686 - acc: 0.7161 - val_loss: 0.5579 - val_acc: 0.7302\n",
      "Epoch 184/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.7122\n",
      "Epoch 184: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5704 - acc: 0.7121 - val_loss: 0.5571 - val_acc: 0.7298\n",
      "Epoch 185/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5689 - acc: 0.7166\n",
      "Epoch 185: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5693 - acc: 0.7165 - val_loss: 0.5576 - val_acc: 0.7298\n",
      "Epoch 186/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5667 - acc: 0.7145\n",
      "Epoch 186: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5672 - acc: 0.7149 - val_loss: 0.5574 - val_acc: 0.7294\n",
      "Epoch 187/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5711 - acc: 0.7131\n",
      "Epoch 187: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5707 - acc: 0.7133 - val_loss: 0.5574 - val_acc: 0.7298\n",
      "Epoch 188/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5648 - acc: 0.7144\n",
      "Epoch 188: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5645 - acc: 0.7161 - val_loss: 0.5569 - val_acc: 0.7302\n",
      "Epoch 189/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5669 - acc: 0.7158\n",
      "Epoch 189: val_acc improved from 0.73023 to 0.73065, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5668 - acc: 0.7159 - val_loss: 0.5570 - val_acc: 0.7307\n",
      "Epoch 190/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5656 - acc: 0.7150\n",
      "Epoch 190: val_acc improved from 0.73065 to 0.73108, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5656 - acc: 0.7150 - val_loss: 0.5566 - val_acc: 0.7311\n",
      "Epoch 191/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.7170\n",
      "Epoch 191: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5667 - acc: 0.7167 - val_loss: 0.5565 - val_acc: 0.7307\n",
      "Epoch 192/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.7094\n",
      "Epoch 192: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5698 - acc: 0.7105 - val_loss: 0.5564 - val_acc: 0.7307\n",
      "Epoch 193/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5652 - acc: 0.7196\n",
      "Epoch 193: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5646 - acc: 0.7198 - val_loss: 0.5561 - val_acc: 0.7307\n",
      "Epoch 194/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.7157\n",
      "Epoch 194: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5725 - acc: 0.7165 - val_loss: 0.5558 - val_acc: 0.7307\n",
      "Epoch 195/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5733 - acc: 0.7108\n",
      "Epoch 195: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5720 - acc: 0.7117 - val_loss: 0.5561 - val_acc: 0.7307\n",
      "Epoch 196/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.7142\n",
      "Epoch 196: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5670 - acc: 0.7149 - val_loss: 0.5558 - val_acc: 0.7302\n",
      "Epoch 197/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.7176\n",
      "Epoch 197: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5656 - acc: 0.7178 - val_loss: 0.5554 - val_acc: 0.7302\n",
      "Epoch 198/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5643 - acc: 0.7152\n",
      "Epoch 198: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5644 - acc: 0.7158 - val_loss: 0.5551 - val_acc: 0.7307\n",
      "Epoch 199/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.7148\n",
      "Epoch 199: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5656 - acc: 0.7161 - val_loss: 0.5549 - val_acc: 0.7307\n",
      "Epoch 200/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.7180\n",
      "Epoch 200: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5637 - acc: 0.7188 - val_loss: 0.5546 - val_acc: 0.7307\n",
      "Epoch 201/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5656 - acc: 0.7155\n",
      "Epoch 201: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5657 - acc: 0.7153 - val_loss: 0.5541 - val_acc: 0.7307\n",
      "Epoch 202/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.7143\n",
      "Epoch 202: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5658 - acc: 0.7143 - val_loss: 0.5543 - val_acc: 0.7311\n",
      "Epoch 203/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7153\n",
      "Epoch 203: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5598 - acc: 0.7158 - val_loss: 0.5541 - val_acc: 0.7311\n",
      "Epoch 204/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.7161\n",
      "Epoch 204: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5616 - acc: 0.7167 - val_loss: 0.5539 - val_acc: 0.7307\n",
      "Epoch 205/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5577 - acc: 0.7198\n",
      "Epoch 205: val_acc improved from 0.73108 to 0.73151, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5577 - acc: 0.7198 - val_loss: 0.5534 - val_acc: 0.7315\n",
      "Epoch 206/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.7189\n",
      "Epoch 206: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5620 - acc: 0.7194 - val_loss: 0.5530 - val_acc: 0.7311\n",
      "Epoch 207/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5656 - acc: 0.7176\n",
      "Epoch 207: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5648 - acc: 0.7187 - val_loss: 0.5530 - val_acc: 0.7311\n",
      "Epoch 208/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5592 - acc: 0.7189\n",
      "Epoch 208: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5580 - acc: 0.7192 - val_loss: 0.5524 - val_acc: 0.7311\n",
      "Epoch 209/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5668 - acc: 0.7177\n",
      "Epoch 209: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5665 - acc: 0.7179 - val_loss: 0.5528 - val_acc: 0.7315\n",
      "Epoch 210/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5653 - acc: 0.7173\n",
      "Epoch 210: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5651 - acc: 0.7177 - val_loss: 0.5528 - val_acc: 0.7315\n",
      "Epoch 211/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.7182\n",
      "Epoch 211: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5615 - acc: 0.7190 - val_loss: 0.5523 - val_acc: 0.7315\n",
      "Epoch 212/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.7163\n",
      "Epoch 212: val_acc improved from 0.73151 to 0.73194, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5629 - acc: 0.7167 - val_loss: 0.5524 - val_acc: 0.7319\n",
      "Epoch 213/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.7163\n",
      "Epoch 213: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5646 - acc: 0.7174 - val_loss: 0.5525 - val_acc: 0.7319\n",
      "Epoch 214/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5610 - acc: 0.7162\n",
      "Epoch 214: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5615 - acc: 0.7160 - val_loss: 0.5526 - val_acc: 0.7319\n",
      "Epoch 215/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5638 - acc: 0.7148\n",
      "Epoch 215: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5638 - acc: 0.7148 - val_loss: 0.5524 - val_acc: 0.7319\n",
      "Epoch 216/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.7202\n",
      "Epoch 216: val_acc improved from 0.73194 to 0.73236, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5554 - acc: 0.7203 - val_loss: 0.5520 - val_acc: 0.7324\n",
      "Epoch 217/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5596 - acc: 0.7156\n",
      "Epoch 217: val_acc improved from 0.73236 to 0.73322, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5587 - acc: 0.7164 - val_loss: 0.5518 - val_acc: 0.7332\n",
      "Epoch 218/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.7180\n",
      "Epoch 218: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5614 - acc: 0.7182 - val_loss: 0.5517 - val_acc: 0.7332\n",
      "Epoch 219/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5632 - acc: 0.7182\n",
      "Epoch 219: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5628 - acc: 0.7189 - val_loss: 0.5514 - val_acc: 0.7332\n",
      "Epoch 220/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.7195\n",
      "Epoch 220: val_acc did not improve from 0.73322\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5584 - acc: 0.7200 - val_loss: 0.5509 - val_acc: 0.7332\n",
      "Epoch 221/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5604 - acc: 0.7201\n",
      "Epoch 221: val_acc improved from 0.73322 to 0.73365, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5596 - acc: 0.7204 - val_loss: 0.5504 - val_acc: 0.7336\n",
      "Epoch 222/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5602 - acc: 0.7163\n",
      "Epoch 222: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5602 - acc: 0.7163 - val_loss: 0.5502 - val_acc: 0.7336\n",
      "Epoch 223/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5586 - acc: 0.7196\n",
      "Epoch 223: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5579 - acc: 0.7200 - val_loss: 0.5501 - val_acc: 0.7336\n",
      "Epoch 224/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.7144\n",
      "Epoch 224: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5619 - acc: 0.7152 - val_loss: 0.5501 - val_acc: 0.7336\n",
      "Epoch 225/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5605 - acc: 0.7178\n",
      "Epoch 225: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5602 - acc: 0.7193 - val_loss: 0.5499 - val_acc: 0.7336\n",
      "Epoch 226/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5598 - acc: 0.7182\n",
      "Epoch 226: val_acc improved from 0.73365 to 0.73407, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5592 - acc: 0.7190 - val_loss: 0.5498 - val_acc: 0.7341\n",
      "Epoch 227/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5615 - acc: 0.7127\n",
      "Epoch 227: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5611 - acc: 0.7130 - val_loss: 0.5495 - val_acc: 0.7336\n",
      "Epoch 228/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5601 - acc: 0.7216\n",
      "Epoch 228: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5605 - acc: 0.7213 - val_loss: 0.5492 - val_acc: 0.7341\n",
      "Epoch 229/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.7195\n",
      "Epoch 229: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5540 - acc: 0.7205 - val_loss: 0.5486 - val_acc: 0.7336\n",
      "Epoch 230/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.7172\n",
      "Epoch 230: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5575 - acc: 0.7177 - val_loss: 0.5484 - val_acc: 0.7336\n",
      "Epoch 231/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.7222\n",
      "Epoch 231: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5533 - acc: 0.7232 - val_loss: 0.5480 - val_acc: 0.7336\n",
      "Epoch 232/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5602 - acc: 0.7189\n",
      "Epoch 232: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5602 - acc: 0.7189 - val_loss: 0.5483 - val_acc: 0.7336\n",
      "Epoch 233/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5584 - acc: 0.7202\n",
      "Epoch 233: val_acc did not improve from 0.73407\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5579 - acc: 0.7201 - val_loss: 0.5483 - val_acc: 0.7336\n",
      "Epoch 234/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5608 - acc: 0.7163\n",
      "Epoch 234: val_acc improved from 0.73407 to 0.73450, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5608 - acc: 0.7168 - val_loss: 0.5484 - val_acc: 0.7345\n",
      "Epoch 235/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5597 - acc: 0.7189\n",
      "Epoch 235: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5598 - acc: 0.7194 - val_loss: 0.5485 - val_acc: 0.7341\n",
      "Epoch 236/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.7193\n",
      "Epoch 236: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5588 - acc: 0.7196 - val_loss: 0.5481 - val_acc: 0.7341\n",
      "Epoch 237/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.7165\n",
      "Epoch 237: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5612 - acc: 0.7166 - val_loss: 0.5482 - val_acc: 0.7345\n",
      "Epoch 238/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5606 - acc: 0.7170\n",
      "Epoch 238: val_acc did not improve from 0.73450\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5606 - acc: 0.7178 - val_loss: 0.5479 - val_acc: 0.7345\n",
      "Epoch 239/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.7169\n",
      "Epoch 239: val_acc improved from 0.73450 to 0.73493, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5582 - acc: 0.7168 - val_loss: 0.5479 - val_acc: 0.7349\n",
      "Epoch 240/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5601 - acc: 0.7211\n",
      "Epoch 240: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5600 - acc: 0.7220 - val_loss: 0.5476 - val_acc: 0.7345\n",
      "Epoch 241/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5593 - acc: 0.7171\n",
      "Epoch 241: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5585 - acc: 0.7180 - val_loss: 0.5476 - val_acc: 0.7341\n",
      "Epoch 242/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.7188\n",
      "Epoch 242: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5622 - acc: 0.7188 - val_loss: 0.5472 - val_acc: 0.7345\n",
      "Epoch 243/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7173\n",
      "Epoch 243: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5584 - acc: 0.7174 - val_loss: 0.5473 - val_acc: 0.7345\n",
      "Epoch 244/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.7156\n",
      "Epoch 244: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5583 - acc: 0.7162 - val_loss: 0.5471 - val_acc: 0.7341\n",
      "Epoch 245/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5590 - acc: 0.7200\n",
      "Epoch 245: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5583 - acc: 0.7199 - val_loss: 0.5471 - val_acc: 0.7345\n",
      "Epoch 246/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5633 - acc: 0.7176\n",
      "Epoch 246: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5630 - acc: 0.7175 - val_loss: 0.5469 - val_acc: 0.7341\n",
      "Epoch 247/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5525 - acc: 0.7213\n",
      "Epoch 247: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7213 - val_loss: 0.5467 - val_acc: 0.7349\n",
      "Epoch 248/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.7190\n",
      "Epoch 248: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5538 - acc: 0.7195 - val_loss: 0.5462 - val_acc: 0.7349\n",
      "Epoch 249/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.7204\n",
      "Epoch 249: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5549 - acc: 0.7208 - val_loss: 0.5460 - val_acc: 0.7349\n",
      "Epoch 250/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5570 - acc: 0.7180\n",
      "Epoch 250: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5567 - acc: 0.7174 - val_loss: 0.5456 - val_acc: 0.7349\n",
      "Epoch 251/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.7214\n",
      "Epoch 251: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5570 - acc: 0.7220 - val_loss: 0.5457 - val_acc: 0.7349\n",
      "Epoch 252/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.7197\n",
      "Epoch 252: val_acc improved from 0.73493 to 0.73536, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5616 - acc: 0.7200 - val_loss: 0.5458 - val_acc: 0.7354\n",
      "Epoch 253/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.7168\n",
      "Epoch 253: val_acc improved from 0.73536 to 0.73578, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5558 - acc: 0.7176 - val_loss: 0.5460 - val_acc: 0.7358\n",
      "Epoch 254/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.7191\n",
      "Epoch 254: val_acc improved from 0.73578 to 0.73621, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5588 - acc: 0.7192 - val_loss: 0.5461 - val_acc: 0.7362\n",
      "Epoch 255/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.7223\n",
      "Epoch 255: val_acc improved from 0.73621 to 0.73707, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5554 - acc: 0.7225 - val_loss: 0.5459 - val_acc: 0.7371\n",
      "Epoch 256/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5552 - acc: 0.7205\n",
      "Epoch 256: val_acc did not improve from 0.73707\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5540 - acc: 0.7215 - val_loss: 0.5457 - val_acc: 0.7366\n",
      "Epoch 257/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5550 - acc: 0.7182\n",
      "Epoch 257: val_acc improved from 0.73707 to 0.73749, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5546 - acc: 0.7187 - val_loss: 0.5454 - val_acc: 0.7375\n",
      "Epoch 258/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5580 - acc: 0.7175\n",
      "Epoch 258: val_acc improved from 0.73749 to 0.73835, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5578 - acc: 0.7183 - val_loss: 0.5455 - val_acc: 0.7383\n",
      "Epoch 259/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7243\n",
      "Epoch 259: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5553 - acc: 0.7241 - val_loss: 0.5456 - val_acc: 0.7383\n",
      "Epoch 260/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5553 - acc: 0.7222\n",
      "Epoch 260: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5553 - acc: 0.7222 - val_loss: 0.5452 - val_acc: 0.7383\n",
      "Epoch 261/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.7185\n",
      "Epoch 261: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5565 - acc: 0.7198 - val_loss: 0.5451 - val_acc: 0.7379\n",
      "Epoch 262/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.7188\n",
      "Epoch 262: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5588 - acc: 0.7195 - val_loss: 0.5450 - val_acc: 0.7379\n",
      "Epoch 263/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7207\n",
      "Epoch 263: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5520 - acc: 0.7211 - val_loss: 0.5448 - val_acc: 0.7383\n",
      "Epoch 264/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.7224\n",
      "Epoch 264: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5573 - acc: 0.7226 - val_loss: 0.5447 - val_acc: 0.7383\n",
      "Epoch 265/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5539 - acc: 0.7215\n",
      "Epoch 265: val_acc improved from 0.73835 to 0.73878, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5527 - acc: 0.7225 - val_loss: 0.5445 - val_acc: 0.7388\n",
      "Epoch 266/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5523 - acc: 0.7210\n",
      "Epoch 266: val_acc did not improve from 0.73878\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5505 - acc: 0.7224 - val_loss: 0.5440 - val_acc: 0.7388\n",
      "Epoch 267/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5558 - acc: 0.7196\n",
      "Epoch 267: val_acc did not improve from 0.73878\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5533 - acc: 0.7211 - val_loss: 0.5440 - val_acc: 0.7383\n",
      "Epoch 268/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.7175\n",
      "Epoch 268: val_acc improved from 0.73878 to 0.73920, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5533 - acc: 0.7181 - val_loss: 0.5440 - val_acc: 0.7392\n",
      "Epoch 269/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5554 - acc: 0.7173\n",
      "Epoch 269: val_acc did not improve from 0.73920\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5544 - acc: 0.7185 - val_loss: 0.5437 - val_acc: 0.7392\n",
      "Epoch 270/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5545 - acc: 0.7219\n",
      "Epoch 270: val_acc did not improve from 0.73920\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5538 - acc: 0.7226 - val_loss: 0.5435 - val_acc: 0.7392\n",
      "Epoch 271/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5499 - acc: 0.7247\n",
      "Epoch 271: val_acc improved from 0.73920 to 0.73963, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5495 - acc: 0.7258 - val_loss: 0.5429 - val_acc: 0.7396\n",
      "Epoch 272/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7260\n",
      "Epoch 272: val_acc did not improve from 0.73963\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5498 - acc: 0.7265 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 273/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5515 - acc: 0.7186\n",
      "Epoch 273: val_acc did not improve from 0.73963\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5513 - acc: 0.7183 - val_loss: 0.5430 - val_acc: 0.7396\n",
      "Epoch 274/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5554 - acc: 0.7209\n",
      "Epoch 274: val_acc did not improve from 0.73963\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5550 - acc: 0.7210 - val_loss: 0.5426 - val_acc: 0.7396\n",
      "Epoch 275/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5552 - acc: 0.7190\n",
      "Epoch 275: val_acc improved from 0.73963 to 0.74006, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5538 - acc: 0.7197 - val_loss: 0.5427 - val_acc: 0.7401\n",
      "Epoch 276/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7219\n",
      "Epoch 276: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5500 - acc: 0.7231 - val_loss: 0.5427 - val_acc: 0.7396\n",
      "Epoch 277/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.7266\n",
      "Epoch 277: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5495 - acc: 0.7267 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 278/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5559 - acc: 0.7207\n",
      "Epoch 278: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5537 - acc: 0.7211 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 279/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5539 - acc: 0.7217\n",
      "Epoch 279: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5537 - acc: 0.7225 - val_loss: 0.5424 - val_acc: 0.7396\n",
      "Epoch 280/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5525 - acc: 0.7228\n",
      "Epoch 280: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7228 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 281/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7212\n",
      "Epoch 281: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5504 - acc: 0.7210 - val_loss: 0.5426 - val_acc: 0.7396\n",
      "Epoch 282/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.7201\n",
      "Epoch 282: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5541 - acc: 0.7206 - val_loss: 0.5423 - val_acc: 0.7396\n",
      "Epoch 283/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7251\n",
      "Epoch 283: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5497 - acc: 0.7253 - val_loss: 0.5421 - val_acc: 0.7396\n",
      "Epoch 284/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5526 - acc: 0.7210\n",
      "Epoch 284: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5526 - acc: 0.7210 - val_loss: 0.5416 - val_acc: 0.7396\n",
      "Epoch 285/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5532 - acc: 0.7240\n",
      "Epoch 285: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5510 - acc: 0.7254 - val_loss: 0.5414 - val_acc: 0.7396\n",
      "Epoch 286/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5531 - acc: 0.7211\n",
      "Epoch 286: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5528 - acc: 0.7213 - val_loss: 0.5412 - val_acc: 0.7396\n",
      "Epoch 287/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5547 - acc: 0.7175\n",
      "Epoch 287: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5544 - acc: 0.7182 - val_loss: 0.5411 - val_acc: 0.7396\n",
      "Epoch 288/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.7236\n",
      "Epoch 288: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5475 - acc: 0.7237 - val_loss: 0.5410 - val_acc: 0.7396\n",
      "Epoch 289/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5485 - acc: 0.7190\n",
      "Epoch 289: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5485 - acc: 0.7190 - val_loss: 0.5407 - val_acc: 0.7401\n",
      "Epoch 290/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5536 - acc: 0.7195\n",
      "Epoch 290: val_acc improved from 0.74006 to 0.74049, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5530 - acc: 0.7199 - val_loss: 0.5408 - val_acc: 0.7405\n",
      "Epoch 291/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7223\n",
      "Epoch 291: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5537 - acc: 0.7228 - val_loss: 0.5409 - val_acc: 0.7405\n",
      "Epoch 292/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.7249\n",
      "Epoch 292: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5505 - acc: 0.7252 - val_loss: 0.5406 - val_acc: 0.7401\n",
      "Epoch 293/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5479 - acc: 0.7209\n",
      "Epoch 293: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5483 - acc: 0.7209 - val_loss: 0.5403 - val_acc: 0.7405\n",
      "Epoch 294/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.7203\n",
      "Epoch 294: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5488 - acc: 0.7208 - val_loss: 0.5402 - val_acc: 0.7405\n",
      "Epoch 295/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.7217\n",
      "Epoch 295: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5557 - acc: 0.7222 - val_loss: 0.5407 - val_acc: 0.7405\n",
      "Epoch 296/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5512 - acc: 0.7221\n",
      "Epoch 296: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5502 - acc: 0.7231 - val_loss: 0.5406 - val_acc: 0.7405\n",
      "Epoch 297/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.7255\n",
      "Epoch 297: val_acc improved from 0.74049 to 0.74091, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5474 - acc: 0.7255 - val_loss: 0.5400 - val_acc: 0.7409\n",
      "Epoch 298/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7200\n",
      "Epoch 298: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5500 - acc: 0.7214 - val_loss: 0.5400 - val_acc: 0.7405\n",
      "Epoch 299/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5487 - acc: 0.7260\n",
      "Epoch 299: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5487 - acc: 0.7260 - val_loss: 0.5395 - val_acc: 0.7405\n",
      "Epoch 300/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.7185\n",
      "Epoch 300: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5549 - acc: 0.7187 - val_loss: 0.5394 - val_acc: 0.7409\n",
      "Epoch 301/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7236\n",
      "Epoch 301: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5515 - acc: 0.7237 - val_loss: 0.5391 - val_acc: 0.7405\n",
      "Epoch 302/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5518 - acc: 0.7239\n",
      "Epoch 302: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5519 - acc: 0.7241 - val_loss: 0.5389 - val_acc: 0.7405\n",
      "Epoch 303/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5518 - acc: 0.7225\n",
      "Epoch 303: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5512 - acc: 0.7231 - val_loss: 0.5388 - val_acc: 0.7405\n",
      "Epoch 304/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5510 - acc: 0.7231\n",
      "Epoch 304: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5506 - acc: 0.7236 - val_loss: 0.5392 - val_acc: 0.7409\n",
      "Epoch 305/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5492 - acc: 0.7215\n",
      "Epoch 305: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5473 - acc: 0.7237 - val_loss: 0.5387 - val_acc: 0.7409\n",
      "Epoch 306/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5511 - acc: 0.7207\n",
      "Epoch 306: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5511 - acc: 0.7206 - val_loss: 0.5391 - val_acc: 0.7409\n",
      "Epoch 307/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.7219\n",
      "Epoch 307: val_acc did not improve from 0.74091\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5536 - acc: 0.7216 - val_loss: 0.5385 - val_acc: 0.7409\n",
      "Epoch 308/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7219\n",
      "Epoch 308: val_acc improved from 0.74091 to 0.74177, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5484 - acc: 0.7226 - val_loss: 0.5386 - val_acc: 0.7418\n",
      "Epoch 309/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5488 - acc: 0.7241\n",
      "Epoch 309: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5490 - acc: 0.7241 - val_loss: 0.5383 - val_acc: 0.7418\n",
      "Epoch 310/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5503 - acc: 0.7239\n",
      "Epoch 310: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5499 - acc: 0.7246 - val_loss: 0.5384 - val_acc: 0.7418\n",
      "Epoch 311/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5525 - acc: 0.7230\n",
      "Epoch 311: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7231 - val_loss: 0.5384 - val_acc: 0.7418\n",
      "Epoch 312/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5539 - acc: 0.7213\n",
      "Epoch 312: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5526 - acc: 0.7224 - val_loss: 0.5384 - val_acc: 0.7418\n",
      "Epoch 313/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5476 - acc: 0.7255\n",
      "Epoch 313: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5475 - acc: 0.7254 - val_loss: 0.5384 - val_acc: 0.7413\n",
      "Epoch 314/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7259\n",
      "Epoch 314: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5469 - acc: 0.7260 - val_loss: 0.5380 - val_acc: 0.7418\n",
      "Epoch 315/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5514 - acc: 0.7261\n",
      "Epoch 315: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5502 - acc: 0.7272 - val_loss: 0.5380 - val_acc: 0.7418\n",
      "Epoch 316/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7244\n",
      "Epoch 316: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5430 - acc: 0.7250 - val_loss: 0.5376 - val_acc: 0.7418\n",
      "Epoch 317/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5468 - acc: 0.7254\n",
      "Epoch 317: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5466 - acc: 0.7255 - val_loss: 0.5374 - val_acc: 0.7413\n",
      "Epoch 318/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7199\n",
      "Epoch 318: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5497 - acc: 0.7205 - val_loss: 0.5374 - val_acc: 0.7413\n",
      "Epoch 319/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5475 - acc: 0.7204\n",
      "Epoch 319: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5479 - acc: 0.7203 - val_loss: 0.5375 - val_acc: 0.7418\n",
      "Epoch 320/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5503 - acc: 0.7209\n",
      "Epoch 320: val_acc improved from 0.74177 to 0.74220, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5488 - acc: 0.7218 - val_loss: 0.5376 - val_acc: 0.7422\n",
      "Epoch 321/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5476 - acc: 0.7225\n",
      "Epoch 321: val_acc did not improve from 0.74220\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5474 - acc: 0.7237 - val_loss: 0.5373 - val_acc: 0.7422\n",
      "Epoch 322/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5462 - acc: 0.7230\n",
      "Epoch 322: val_acc did not improve from 0.74220\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5452 - acc: 0.7246 - val_loss: 0.5370 - val_acc: 0.7418\n",
      "Epoch 323/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5509 - acc: 0.7211\n",
      "Epoch 323: val_acc did not improve from 0.74220\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5511 - acc: 0.7215 - val_loss: 0.5373 - val_acc: 0.7422\n",
      "Epoch 324/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.7225\n",
      "Epoch 324: val_acc did not improve from 0.74220\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5494 - acc: 0.7236 - val_loss: 0.5371 - val_acc: 0.7422\n",
      "Epoch 325/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7245\n",
      "Epoch 325: val_acc improved from 0.74220 to 0.74263, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5452 - acc: 0.7247 - val_loss: 0.5366 - val_acc: 0.7426\n",
      "Epoch 326/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.7212\n",
      "Epoch 326: val_acc did not improve from 0.74263\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5483 - acc: 0.7216 - val_loss: 0.5365 - val_acc: 0.7426\n",
      "Epoch 327/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.7253\n",
      "Epoch 327: val_acc did not improve from 0.74263\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5496 - acc: 0.7245 - val_loss: 0.5366 - val_acc: 0.7426\n",
      "Epoch 328/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.7283\n",
      "Epoch 328: val_acc did not improve from 0.74263\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5457 - acc: 0.7288 - val_loss: 0.5362 - val_acc: 0.7426\n",
      "Epoch 329/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5443 - acc: 0.7246\n",
      "Epoch 329: val_acc improved from 0.74263 to 0.74305, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5427 - acc: 0.7256 - val_loss: 0.5361 - val_acc: 0.7431\n",
      "Epoch 330/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7240\n",
      "Epoch 330: val_acc improved from 0.74305 to 0.74391, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5500 - acc: 0.7249 - val_loss: 0.5363 - val_acc: 0.7439\n",
      "Epoch 331/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7291\n",
      "Epoch 331: val_acc did not improve from 0.74391\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5437 - acc: 0.7294 - val_loss: 0.5359 - val_acc: 0.7439\n",
      "Epoch 332/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.7269\n",
      "Epoch 332: val_acc improved from 0.74391 to 0.74434, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5453 - acc: 0.7273 - val_loss: 0.5356 - val_acc: 0.7443\n",
      "Epoch 333/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.7250\n",
      "Epoch 333: val_acc did not improve from 0.74434\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5478 - acc: 0.7252 - val_loss: 0.5357 - val_acc: 0.7443\n",
      "Epoch 334/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5415 - acc: 0.7266\n",
      "Epoch 334: val_acc did not improve from 0.74434\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5410 - acc: 0.7275 - val_loss: 0.5354 - val_acc: 0.7443\n",
      "Epoch 335/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.7254\n",
      "Epoch 335: val_acc improved from 0.74434 to 0.74476, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5475 - acc: 0.7260 - val_loss: 0.5356 - val_acc: 0.7448\n",
      "Epoch 336/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.7259\n",
      "Epoch 336: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5486 - acc: 0.7260 - val_loss: 0.5354 - val_acc: 0.7443\n",
      "Epoch 337/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5434 - acc: 0.7259\n",
      "Epoch 337: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5434 - acc: 0.7260 - val_loss: 0.5350 - val_acc: 0.7443\n",
      "Epoch 338/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.7247\n",
      "Epoch 338: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5465 - acc: 0.7253 - val_loss: 0.5347 - val_acc: 0.7443\n",
      "Epoch 339/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7247\n",
      "Epoch 339: val_acc improved from 0.74476 to 0.74519, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5460 - acc: 0.7255 - val_loss: 0.5345 - val_acc: 0.7452\n",
      "Epoch 340/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5422 - acc: 0.7281\n",
      "Epoch 340: val_acc did not improve from 0.74519\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5422 - acc: 0.7281 - val_loss: 0.5342 - val_acc: 0.7448\n",
      "Epoch 341/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.7232\n",
      "Epoch 341: val_acc improved from 0.74519 to 0.74562, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5491 - acc: 0.7232 - val_loss: 0.5343 - val_acc: 0.7456\n",
      "Epoch 342/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5458 - acc: 0.7283\n",
      "Epoch 342: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5462 - acc: 0.7286 - val_loss: 0.5342 - val_acc: 0.7452\n",
      "Epoch 343/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5466 - acc: 0.7236\n",
      "Epoch 343: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5454 - acc: 0.7247 - val_loss: 0.5344 - val_acc: 0.7443\n",
      "Epoch 344/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7282\n",
      "Epoch 344: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5435 - acc: 0.7283 - val_loss: 0.5341 - val_acc: 0.7452\n",
      "Epoch 345/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5433 - acc: 0.7284\n",
      "Epoch 345: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5433 - acc: 0.7284 - val_loss: 0.5339 - val_acc: 0.7448\n",
      "Epoch 346/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.7258\n",
      "Epoch 346: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5451 - acc: 0.7263 - val_loss: 0.5339 - val_acc: 0.7452\n",
      "Epoch 347/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.7294\n",
      "Epoch 347: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5431 - acc: 0.7299 - val_loss: 0.5335 - val_acc: 0.7452\n",
      "Epoch 348/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.7251\n",
      "Epoch 348: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5427 - acc: 0.7256 - val_loss: 0.5330 - val_acc: 0.7452\n",
      "Epoch 349/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7252\n",
      "Epoch 349: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5433 - acc: 0.7258 - val_loss: 0.5329 - val_acc: 0.7452\n",
      "Epoch 350/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5462 - acc: 0.7275\n",
      "Epoch 350: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5465 - acc: 0.7271 - val_loss: 0.5329 - val_acc: 0.7452\n",
      "Epoch 351/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5452 - acc: 0.7219\n",
      "Epoch 351: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5441 - acc: 0.7221 - val_loss: 0.5325 - val_acc: 0.7456\n",
      "Epoch 352/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7251\n",
      "Epoch 352: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5435 - acc: 0.7260 - val_loss: 0.5327 - val_acc: 0.7456\n",
      "Epoch 353/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.7229\n",
      "Epoch 353: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5470 - acc: 0.7238 - val_loss: 0.5330 - val_acc: 0.7456\n",
      "Epoch 354/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7283\n",
      "Epoch 354: val_acc did not improve from 0.74562\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5381 - acc: 0.7290 - val_loss: 0.5325 - val_acc: 0.7456\n",
      "Epoch 355/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7290\n",
      "Epoch 355: val_acc improved from 0.74562 to 0.74690, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5432 - acc: 0.7293 - val_loss: 0.5324 - val_acc: 0.7469\n",
      "Epoch 356/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7276\n",
      "Epoch 356: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5431 - acc: 0.7275 - val_loss: 0.5325 - val_acc: 0.7469\n",
      "Epoch 357/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5442 - acc: 0.7253\n",
      "Epoch 357: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5433 - acc: 0.7262 - val_loss: 0.5323 - val_acc: 0.7469\n",
      "Epoch 358/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7290\n",
      "Epoch 358: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5410 - acc: 0.7290 - val_loss: 0.5323 - val_acc: 0.7469\n",
      "Epoch 359/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.7213\n",
      "Epoch 359: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5490 - acc: 0.7212 - val_loss: 0.5324 - val_acc: 0.7469\n",
      "Epoch 360/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7242\n",
      "Epoch 360: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5483 - acc: 0.7247 - val_loss: 0.5323 - val_acc: 0.7469\n",
      "Epoch 361/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5486 - acc: 0.7255\n",
      "Epoch 361: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5476 - acc: 0.7257 - val_loss: 0.5322 - val_acc: 0.7465\n",
      "Epoch 362/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5430 - acc: 0.7269\n",
      "Epoch 362: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5436 - acc: 0.7261 - val_loss: 0.5319 - val_acc: 0.7469\n",
      "Epoch 363/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7291\n",
      "Epoch 363: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5395 - acc: 0.7291 - val_loss: 0.5317 - val_acc: 0.7465\n",
      "Epoch 364/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5395 - acc: 0.7305\n",
      "Epoch 364: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5388 - acc: 0.7311 - val_loss: 0.5313 - val_acc: 0.7469\n",
      "Epoch 365/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5414 - acc: 0.7259\n",
      "Epoch 365: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5408 - acc: 0.7267 - val_loss: 0.5311 - val_acc: 0.7469\n",
      "Epoch 366/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5433 - acc: 0.7278\n",
      "Epoch 366: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5433 - acc: 0.7274 - val_loss: 0.5309 - val_acc: 0.7465\n",
      "Epoch 367/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5404 - acc: 0.7262\n",
      "Epoch 367: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5396 - acc: 0.7270 - val_loss: 0.5310 - val_acc: 0.7469\n",
      "Epoch 368/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7257\n",
      "Epoch 368: val_acc did not improve from 0.74690\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5473 - acc: 0.7256 - val_loss: 0.5310 - val_acc: 0.7469\n",
      "Epoch 369/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.7261\n",
      "Epoch 369: val_acc improved from 0.74690 to 0.74733, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5444 - acc: 0.7261 - val_loss: 0.5310 - val_acc: 0.7473\n",
      "Epoch 370/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5477 - acc: 0.7255\n",
      "Epoch 370: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5459 - acc: 0.7268 - val_loss: 0.5311 - val_acc: 0.7473\n",
      "Epoch 371/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7285\n",
      "Epoch 371: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5429 - acc: 0.7296 - val_loss: 0.5309 - val_acc: 0.7473\n",
      "Epoch 372/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.7256\n",
      "Epoch 372: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5412 - acc: 0.7256 - val_loss: 0.5309 - val_acc: 0.7473\n",
      "Epoch 373/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7228\n",
      "Epoch 373: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5477 - acc: 0.7234 - val_loss: 0.5305 - val_acc: 0.7465\n",
      "Epoch 374/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5433 - acc: 0.7276\n",
      "Epoch 374: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5416 - acc: 0.7284 - val_loss: 0.5307 - val_acc: 0.7469\n",
      "Epoch 375/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5425 - acc: 0.7258\n",
      "Epoch 375: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5425 - acc: 0.7259 - val_loss: 0.5306 - val_acc: 0.7469\n",
      "Epoch 376/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5440 - acc: 0.7256\n",
      "Epoch 376: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5430 - acc: 0.7271 - val_loss: 0.5307 - val_acc: 0.7473\n",
      "Epoch 377/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5417 - acc: 0.7275\n",
      "Epoch 377: val_acc did not improve from 0.74733\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5417 - acc: 0.7276 - val_loss: 0.5305 - val_acc: 0.7469\n",
      "Epoch 378/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7276\n",
      "Epoch 378: val_acc improved from 0.74733 to 0.74776, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5451 - acc: 0.7283 - val_loss: 0.5307 - val_acc: 0.7478\n",
      "Epoch 379/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.7265\n",
      "Epoch 379: val_acc improved from 0.74776 to 0.74818, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5419 - acc: 0.7262 - val_loss: 0.5307 - val_acc: 0.7482\n",
      "Epoch 380/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7256\n",
      "Epoch 380: val_acc did not improve from 0.74818\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5402 - acc: 0.7259 - val_loss: 0.5303 - val_acc: 0.7478\n",
      "Epoch 381/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7281\n",
      "Epoch 381: val_acc did not improve from 0.74818\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5460 - acc: 0.7281 - val_loss: 0.5302 - val_acc: 0.7473\n",
      "Epoch 382/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5432 - acc: 0.7237\n",
      "Epoch 382: val_acc did not improve from 0.74818\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5428 - acc: 0.7244 - val_loss: 0.5302 - val_acc: 0.7482\n",
      "Epoch 383/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.7262\n",
      "Epoch 383: val_acc did not improve from 0.74818\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5366 - acc: 0.7267 - val_loss: 0.5302 - val_acc: 0.7482\n",
      "Epoch 384/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7254\n",
      "Epoch 384: val_acc improved from 0.74818 to 0.74861, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5423 - acc: 0.7255 - val_loss: 0.5300 - val_acc: 0.7486\n",
      "Epoch 385/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5462 - acc: 0.7264\n",
      "Epoch 385: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5445 - acc: 0.7268 - val_loss: 0.5298 - val_acc: 0.7478\n",
      "Epoch 386/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.7272\n",
      "Epoch 386: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5410 - acc: 0.7273 - val_loss: 0.5297 - val_acc: 0.7478\n",
      "Epoch 387/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7244\n",
      "Epoch 387: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5436 - acc: 0.7250 - val_loss: 0.5297 - val_acc: 0.7478\n",
      "Epoch 388/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5440 - acc: 0.7249\n",
      "Epoch 388: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5434 - acc: 0.7254 - val_loss: 0.5294 - val_acc: 0.7478\n",
      "Epoch 389/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.7297\n",
      "Epoch 389: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5374 - acc: 0.7300 - val_loss: 0.5292 - val_acc: 0.7478\n",
      "Epoch 390/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.7272\n",
      "Epoch 390: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5403 - acc: 0.7277 - val_loss: 0.5288 - val_acc: 0.7478\n",
      "Epoch 391/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5419 - acc: 0.7273\n",
      "Epoch 391: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5408 - acc: 0.7277 - val_loss: 0.5290 - val_acc: 0.7482\n",
      "Epoch 392/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5432 - acc: 0.7233\n",
      "Epoch 392: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5422 - acc: 0.7237 - val_loss: 0.5293 - val_acc: 0.7486\n",
      "Epoch 393/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5377 - acc: 0.7295\n",
      "Epoch 393: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5377 - acc: 0.7301 - val_loss: 0.5292 - val_acc: 0.7478\n",
      "Epoch 394/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5420 - acc: 0.7270\n",
      "Epoch 394: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5426 - acc: 0.7268 - val_loss: 0.5290 - val_acc: 0.7478\n",
      "Epoch 395/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.7292\n",
      "Epoch 395: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5375 - acc: 0.7294 - val_loss: 0.5289 - val_acc: 0.7478\n",
      "Epoch 396/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5340 - acc: 0.7320\n",
      "Epoch 396: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5340 - acc: 0.7318 - val_loss: 0.5284 - val_acc: 0.7482\n",
      "Epoch 397/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7263\n",
      "Epoch 397: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5367 - acc: 0.7268 - val_loss: 0.5284 - val_acc: 0.7478\n",
      "Epoch 398/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5391 - acc: 0.7304\n",
      "Epoch 398: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5391 - acc: 0.7304 - val_loss: 0.5283 - val_acc: 0.7482\n",
      "Epoch 399/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5394 - acc: 0.7295\n",
      "Epoch 399: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5403 - acc: 0.7293 - val_loss: 0.5283 - val_acc: 0.7486\n",
      "Epoch 400/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5434 - acc: 0.7272\n",
      "Epoch 400: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5425 - acc: 0.7276 - val_loss: 0.5282 - val_acc: 0.7482\n",
      "Epoch 401/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7285\n",
      "Epoch 401: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5394 - acc: 0.7288 - val_loss: 0.5281 - val_acc: 0.7482\n",
      "Epoch 402/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5410 - acc: 0.7247\n",
      "Epoch 402: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5393 - acc: 0.7255 - val_loss: 0.5278 - val_acc: 0.7478\n",
      "Epoch 403/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5354 - acc: 0.7294\n",
      "Epoch 403: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5343 - acc: 0.7304 - val_loss: 0.5274 - val_acc: 0.7486\n",
      "Epoch 404/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.7292\n",
      "Epoch 404: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7296 - val_loss: 0.5271 - val_acc: 0.7486\n",
      "Epoch 405/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7304\n",
      "Epoch 405: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5389 - acc: 0.7306 - val_loss: 0.5270 - val_acc: 0.7482\n",
      "Epoch 406/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5427 - acc: 0.7280\n",
      "Epoch 406: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5420 - acc: 0.7283 - val_loss: 0.5268 - val_acc: 0.7482\n",
      "Epoch 407/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5441 - acc: 0.7268\n",
      "Epoch 407: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5431 - acc: 0.7276 - val_loss: 0.5270 - val_acc: 0.7478\n",
      "Epoch 408/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7270\n",
      "Epoch 408: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5383 - acc: 0.7280 - val_loss: 0.5270 - val_acc: 0.7473\n",
      "Epoch 409/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7305\n",
      "Epoch 409: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5406 - acc: 0.7304 - val_loss: 0.5266 - val_acc: 0.7478\n",
      "Epoch 410/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5365 - acc: 0.7310\n",
      "Epoch 410: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5364 - acc: 0.7318 - val_loss: 0.5266 - val_acc: 0.7478\n",
      "Epoch 411/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5365 - acc: 0.7304\n",
      "Epoch 411: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5365 - acc: 0.7301 - val_loss: 0.5263 - val_acc: 0.7473\n",
      "Epoch 412/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7293\n",
      "Epoch 412: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5388 - acc: 0.7296 - val_loss: 0.5261 - val_acc: 0.7478\n",
      "Epoch 413/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7336\n",
      "Epoch 413: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5343 - acc: 0.7342 - val_loss: 0.5260 - val_acc: 0.7478\n",
      "Epoch 414/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5388 - acc: 0.7273\n",
      "Epoch 414: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5391 - acc: 0.7272 - val_loss: 0.5256 - val_acc: 0.7478\n",
      "Epoch 415/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.7261\n",
      "Epoch 415: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5385 - acc: 0.7269 - val_loss: 0.5256 - val_acc: 0.7478\n",
      "Epoch 416/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5343 - acc: 0.7350\n",
      "Epoch 416: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5336 - acc: 0.7359 - val_loss: 0.5255 - val_acc: 0.7482\n",
      "Epoch 417/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7317\n",
      "Epoch 417: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5368 - acc: 0.7323 - val_loss: 0.5256 - val_acc: 0.7482\n",
      "Epoch 418/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7298\n",
      "Epoch 418: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5358 - acc: 0.7301 - val_loss: 0.5256 - val_acc: 0.7482\n",
      "Epoch 419/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5393 - acc: 0.7271\n",
      "Epoch 419: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5393 - acc: 0.7271 - val_loss: 0.5255 - val_acc: 0.7486\n",
      "Epoch 420/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5399 - acc: 0.7275\n",
      "Epoch 420: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5392 - acc: 0.7286 - val_loss: 0.5253 - val_acc: 0.7482\n",
      "Epoch 421/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7282\n",
      "Epoch 421: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5390 - acc: 0.7284 - val_loss: 0.5251 - val_acc: 0.7482\n",
      "Epoch 422/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5376 - acc: 0.7302\n",
      "Epoch 422: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5364 - acc: 0.7317 - val_loss: 0.5253 - val_acc: 0.7482\n",
      "Epoch 423/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7329\n",
      "Epoch 423: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5377 - acc: 0.7336 - val_loss: 0.5251 - val_acc: 0.7478\n",
      "Epoch 424/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7297\n",
      "Epoch 424: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5406 - acc: 0.7301 - val_loss: 0.5253 - val_acc: 0.7478\n",
      "Epoch 425/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5310 - acc: 0.7326\n",
      "Epoch 425: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5310 - acc: 0.7332 - val_loss: 0.5251 - val_acc: 0.7478\n",
      "Epoch 426/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5354 - acc: 0.7314\n",
      "Epoch 426: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5354 - acc: 0.7314 - val_loss: 0.5249 - val_acc: 0.7482\n",
      "Epoch 427/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7266\n",
      "Epoch 427: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5393 - acc: 0.7269 - val_loss: 0.5250 - val_acc: 0.7482\n",
      "Epoch 428/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7318\n",
      "Epoch 428: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5361 - acc: 0.7320 - val_loss: 0.5248 - val_acc: 0.7482\n",
      "Epoch 429/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5383 - acc: 0.7282\n",
      "Epoch 429: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5364 - acc: 0.7296 - val_loss: 0.5247 - val_acc: 0.7482\n",
      "Epoch 430/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7311\n",
      "Epoch 430: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5399 - acc: 0.7318 - val_loss: 0.5250 - val_acc: 0.7482\n",
      "Epoch 431/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7266\n",
      "Epoch 431: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5380 - acc: 0.7266 - val_loss: 0.5247 - val_acc: 0.7482\n",
      "Epoch 432/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5394 - acc: 0.7280\n",
      "Epoch 432: val_acc did not improve from 0.74861\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5379 - acc: 0.7299 - val_loss: 0.5247 - val_acc: 0.7482\n",
      "Epoch 433/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5381 - acc: 0.7305\n",
      "Epoch 433: val_acc improved from 0.74861 to 0.74947, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5362 - acc: 0.7309 - val_loss: 0.5252 - val_acc: 0.7495\n",
      "Epoch 434/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7341\n",
      "Epoch 434: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5352 - acc: 0.7339 - val_loss: 0.5250 - val_acc: 0.7490\n",
      "Epoch 435/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5414 - acc: 0.7297\n",
      "Epoch 435: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5408 - acc: 0.7303 - val_loss: 0.5252 - val_acc: 0.7495\n",
      "Epoch 436/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7291\n",
      "Epoch 436: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5375 - acc: 0.7293 - val_loss: 0.5249 - val_acc: 0.7495\n",
      "Epoch 437/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7310\n",
      "Epoch 437: val_acc improved from 0.74947 to 0.75032, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5371 - acc: 0.7311 - val_loss: 0.5250 - val_acc: 0.7503\n",
      "Epoch 438/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7319\n",
      "Epoch 438: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7318 - val_loss: 0.5247 - val_acc: 0.7490\n",
      "Epoch 439/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7279\n",
      "Epoch 439: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5376 - acc: 0.7285 - val_loss: 0.5244 - val_acc: 0.7482\n",
      "Epoch 440/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7311\n",
      "Epoch 440: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5356 - acc: 0.7313 - val_loss: 0.5245 - val_acc: 0.7495\n",
      "Epoch 441/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7300\n",
      "Epoch 441: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5368 - acc: 0.7303 - val_loss: 0.5241 - val_acc: 0.7490\n",
      "Epoch 442/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.7317\n",
      "Epoch 442: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5363 - acc: 0.7317 - val_loss: 0.5240 - val_acc: 0.7490\n",
      "Epoch 443/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7301\n",
      "Epoch 443: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5377 - acc: 0.7303 - val_loss: 0.5242 - val_acc: 0.7503\n",
      "Epoch 444/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5373 - acc: 0.7306\n",
      "Epoch 444: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5367 - acc: 0.7314 - val_loss: 0.5238 - val_acc: 0.7499\n",
      "Epoch 445/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7295\n",
      "Epoch 445: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5363 - acc: 0.7304 - val_loss: 0.5235 - val_acc: 0.7495\n",
      "Epoch 446/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7330\n",
      "Epoch 446: val_acc did not improve from 0.75032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5326 - acc: 0.7342 - val_loss: 0.5231 - val_acc: 0.7499\n",
      "Epoch 447/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5315 - acc: 0.7334\n",
      "Epoch 447: val_acc improved from 0.75032 to 0.75075, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5315 - acc: 0.7334 - val_loss: 0.5230 - val_acc: 0.7507\n",
      "Epoch 448/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7303\n",
      "Epoch 448: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5373 - acc: 0.7309 - val_loss: 0.5229 - val_acc: 0.7503\n",
      "Epoch 449/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5291 - acc: 0.7334\n",
      "Epoch 449: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5295 - acc: 0.7330 - val_loss: 0.5222 - val_acc: 0.7499\n",
      "Epoch 450/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.7323\n",
      "Epoch 450: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5345 - acc: 0.7323 - val_loss: 0.5222 - val_acc: 0.7499\n",
      "Epoch 451/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5376 - acc: 0.7317\n",
      "Epoch 451: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5377 - acc: 0.7318 - val_loss: 0.5227 - val_acc: 0.7499\n",
      "Epoch 452/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7312\n",
      "Epoch 452: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5337 - acc: 0.7309 - val_loss: 0.5225 - val_acc: 0.7499\n",
      "Epoch 453/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7324\n",
      "Epoch 453: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5356 - acc: 0.7327 - val_loss: 0.5225 - val_acc: 0.7503\n",
      "Epoch 454/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5340 - acc: 0.7316\n",
      "Epoch 454: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5326 - acc: 0.7335 - val_loss: 0.5221 - val_acc: 0.7499\n",
      "Epoch 455/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.7307\n",
      "Epoch 455: val_acc improved from 0.75075 to 0.75160, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5327 - acc: 0.7312 - val_loss: 0.5223 - val_acc: 0.7516\n",
      "Epoch 456/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5357 - acc: 0.7278\n",
      "Epoch 456: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5364 - acc: 0.7274 - val_loss: 0.5223 - val_acc: 0.7507\n",
      "Epoch 457/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.7356\n",
      "Epoch 457: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5323 - acc: 0.7356 - val_loss: 0.5224 - val_acc: 0.7512\n",
      "Epoch 458/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7309\n",
      "Epoch 458: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5367 - acc: 0.7308 - val_loss: 0.5221 - val_acc: 0.7516\n",
      "Epoch 459/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5358 - acc: 0.7295\n",
      "Epoch 459: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5354 - acc: 0.7299 - val_loss: 0.5219 - val_acc: 0.7507\n",
      "Epoch 460/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7331\n",
      "Epoch 460: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5334 - acc: 0.7332 - val_loss: 0.5217 - val_acc: 0.7512\n",
      "Epoch 461/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5342 - acc: 0.7313\n",
      "Epoch 461: val_acc improved from 0.75160 to 0.75203, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5343 - acc: 0.7315 - val_loss: 0.5219 - val_acc: 0.7520\n",
      "Epoch 462/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5335 - acc: 0.7349\n",
      "Epoch 462: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5335 - acc: 0.7349 - val_loss: 0.5215 - val_acc: 0.7520\n",
      "Epoch 463/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.7311\n",
      "Epoch 463: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5349 - acc: 0.7317 - val_loss: 0.5211 - val_acc: 0.7516\n",
      "Epoch 464/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5333 - acc: 0.7326\n",
      "Epoch 464: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5335 - acc: 0.7321 - val_loss: 0.5209 - val_acc: 0.7516\n",
      "Epoch 465/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7368\n",
      "Epoch 465: val_acc improved from 0.75203 to 0.75374, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5302 - acc: 0.7369 - val_loss: 0.5210 - val_acc: 0.7537\n",
      "Epoch 466/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7303\n",
      "Epoch 466: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7312 - val_loss: 0.5208 - val_acc: 0.7533\n",
      "Epoch 467/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5362 - acc: 0.7299\n",
      "Epoch 467: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5366 - acc: 0.7297 - val_loss: 0.5209 - val_acc: 0.7533\n",
      "Epoch 468/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5321 - acc: 0.7330\n",
      "Epoch 468: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5321 - acc: 0.7330 - val_loss: 0.5207 - val_acc: 0.7525\n",
      "Epoch 469/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5351 - acc: 0.7259\n",
      "Epoch 469: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5348 - acc: 0.7260 - val_loss: 0.5205 - val_acc: 0.7520\n",
      "Epoch 470/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7296\n",
      "Epoch 470: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7297 - val_loss: 0.5206 - val_acc: 0.7525\n",
      "Epoch 471/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5359 - acc: 0.7288\n",
      "Epoch 471: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5346 - acc: 0.7300 - val_loss: 0.5203 - val_acc: 0.7525\n",
      "Epoch 472/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5351 - acc: 0.7289\n",
      "Epoch 472: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5358 - acc: 0.7287 - val_loss: 0.5206 - val_acc: 0.7533\n",
      "Epoch 473/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7306\n",
      "Epoch 473: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5287 - acc: 0.7309 - val_loss: 0.5201 - val_acc: 0.7529\n",
      "Epoch 474/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5315 - acc: 0.7347\n",
      "Epoch 474: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5314 - acc: 0.7346 - val_loss: 0.5200 - val_acc: 0.7520\n",
      "Epoch 475/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5363 - acc: 0.7288\n",
      "Epoch 475: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5363 - acc: 0.7288 - val_loss: 0.5204 - val_acc: 0.7529\n",
      "Epoch 476/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7291\n",
      "Epoch 476: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5326 - acc: 0.7303 - val_loss: 0.5201 - val_acc: 0.7529\n",
      "Epoch 477/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.7312\n",
      "Epoch 477: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5317 - acc: 0.7327 - val_loss: 0.5201 - val_acc: 0.7533\n",
      "Epoch 478/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5334 - acc: 0.7355\n",
      "Epoch 478: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5326 - acc: 0.7352 - val_loss: 0.5198 - val_acc: 0.7533\n",
      "Epoch 479/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.7306\n",
      "Epoch 479: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7312 - val_loss: 0.5198 - val_acc: 0.7529\n",
      "Epoch 480/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5345 - acc: 0.7297\n",
      "Epoch 480: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5336 - acc: 0.7302 - val_loss: 0.5196 - val_acc: 0.7516\n",
      "Epoch 481/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7312\n",
      "Epoch 481: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5373 - acc: 0.7312 - val_loss: 0.5197 - val_acc: 0.7516\n",
      "Epoch 482/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5279 - acc: 0.7338\n",
      "Epoch 482: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5277 - acc: 0.7335 - val_loss: 0.5192 - val_acc: 0.7520\n",
      "Epoch 483/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5327 - acc: 0.7314\n",
      "Epoch 483: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5315 - acc: 0.7325 - val_loss: 0.5190 - val_acc: 0.7516\n",
      "Epoch 484/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5363 - acc: 0.7341\n",
      "Epoch 484: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5347 - acc: 0.7346 - val_loss: 0.5191 - val_acc: 0.7516\n",
      "Epoch 485/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7351\n",
      "Epoch 485: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5314 - acc: 0.7355 - val_loss: 0.5190 - val_acc: 0.7520\n",
      "Epoch 486/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7327\n",
      "Epoch 486: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5284 - acc: 0.7329 - val_loss: 0.5188 - val_acc: 0.7529\n",
      "Epoch 487/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5340 - acc: 0.7344\n",
      "Epoch 487: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5337 - acc: 0.7346 - val_loss: 0.5188 - val_acc: 0.7529\n",
      "Epoch 488/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5298 - acc: 0.7335\n",
      "Epoch 488: val_acc did not improve from 0.75374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5297 - acc: 0.7333 - val_loss: 0.5188 - val_acc: 0.7529\n",
      "Epoch 489/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5358 - acc: 0.7276\n",
      "Epoch 489: val_acc improved from 0.75374 to 0.75417, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5348 - acc: 0.7290 - val_loss: 0.5190 - val_acc: 0.7542\n",
      "Epoch 490/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7322\n",
      "Epoch 490: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5340 - acc: 0.7329 - val_loss: 0.5188 - val_acc: 0.7537\n",
      "Epoch 491/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5305 - acc: 0.7312\n",
      "Epoch 491: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5304 - acc: 0.7322 - val_loss: 0.5186 - val_acc: 0.7537\n",
      "Epoch 492/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7317\n",
      "Epoch 492: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5305 - acc: 0.7319 - val_loss: 0.5184 - val_acc: 0.7533\n",
      "Epoch 493/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5280 - acc: 0.7320\n",
      "Epoch 493: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5282 - acc: 0.7327 - val_loss: 0.5181 - val_acc: 0.7533\n",
      "Epoch 494/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7317\n",
      "Epoch 494: val_acc improved from 0.75417 to 0.75460, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5291 - acc: 0.7329 - val_loss: 0.5183 - val_acc: 0.7546\n",
      "Epoch 495/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7292\n",
      "Epoch 495: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5348 - acc: 0.7292 - val_loss: 0.5184 - val_acc: 0.7533\n",
      "Epoch 496/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7325\n",
      "Epoch 496: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5321 - acc: 0.7330 - val_loss: 0.5186 - val_acc: 0.7546\n",
      "Epoch 497/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5337 - acc: 0.7309\n",
      "Epoch 497: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5329 - acc: 0.7312 - val_loss: 0.5185 - val_acc: 0.7542\n",
      "Epoch 498/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5302 - acc: 0.7331\n",
      "Epoch 498: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5300 - acc: 0.7333 - val_loss: 0.5181 - val_acc: 0.7542\n",
      "Epoch 499/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7349\n",
      "Epoch 499: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5273 - acc: 0.7353 - val_loss: 0.5178 - val_acc: 0.7546\n",
      "Epoch 500/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.7317\n",
      "Epoch 500: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5303 - acc: 0.7322 - val_loss: 0.5178 - val_acc: 0.7546\n",
      "Epoch 501/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7326\n",
      "Epoch 501: val_acc improved from 0.75460 to 0.75545, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5324 - acc: 0.7332 - val_loss: 0.5182 - val_acc: 0.7555\n",
      "Epoch 502/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5314 - acc: 0.7321\n",
      "Epoch 502: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5310 - acc: 0.7323 - val_loss: 0.5180 - val_acc: 0.7550\n",
      "Epoch 503/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.7322\n",
      "Epoch 503: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5282 - acc: 0.7321 - val_loss: 0.5172 - val_acc: 0.7546\n",
      "Epoch 504/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5312 - acc: 0.7344\n",
      "Epoch 504: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5308 - acc: 0.7350 - val_loss: 0.5172 - val_acc: 0.7546\n",
      "Epoch 505/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5255 - acc: 0.7366\n",
      "Epoch 505: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5252 - acc: 0.7376 - val_loss: 0.5168 - val_acc: 0.7542\n",
      "Epoch 506/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5263 - acc: 0.7367\n",
      "Epoch 506: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5261 - acc: 0.7369 - val_loss: 0.5166 - val_acc: 0.7546\n",
      "Epoch 507/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5326 - acc: 0.7334\n",
      "Epoch 507: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5311 - acc: 0.7340 - val_loss: 0.5167 - val_acc: 0.7546\n",
      "Epoch 508/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5275 - acc: 0.7356\n",
      "Epoch 508: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5270 - acc: 0.7364 - val_loss: 0.5164 - val_acc: 0.7550\n",
      "Epoch 509/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5311 - acc: 0.7337\n",
      "Epoch 509: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5311 - acc: 0.7337 - val_loss: 0.5164 - val_acc: 0.7546\n",
      "Epoch 510/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.7307\n",
      "Epoch 510: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5275 - acc: 0.7312 - val_loss: 0.5162 - val_acc: 0.7537\n",
      "Epoch 511/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5271 - acc: 0.7372\n",
      "Epoch 511: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5267 - acc: 0.7376 - val_loss: 0.5158 - val_acc: 0.7546\n",
      "Epoch 512/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.7384\n",
      "Epoch 512: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - acc: 0.7387 - val_loss: 0.5157 - val_acc: 0.7546\n",
      "Epoch 513/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.7376\n",
      "Epoch 513: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5272 - acc: 0.7382 - val_loss: 0.5156 - val_acc: 0.7546\n",
      "Epoch 514/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5288 - acc: 0.7344\n",
      "Epoch 514: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5275 - acc: 0.7358 - val_loss: 0.5157 - val_acc: 0.7537\n",
      "Epoch 515/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5316 - acc: 0.7349\n",
      "Epoch 515: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5307 - acc: 0.7353 - val_loss: 0.5156 - val_acc: 0.7546\n",
      "Epoch 516/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7348\n",
      "Epoch 516: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5245 - acc: 0.7349 - val_loss: 0.5147 - val_acc: 0.7546\n",
      "Epoch 517/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5293 - acc: 0.7338\n",
      "Epoch 517: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5285 - acc: 0.7347 - val_loss: 0.5146 - val_acc: 0.7546\n",
      "Epoch 518/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7344\n",
      "Epoch 518: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5243 - acc: 0.7352 - val_loss: 0.5149 - val_acc: 0.7542\n",
      "Epoch 519/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7371\n",
      "Epoch 519: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5318 - acc: 0.7375 - val_loss: 0.5150 - val_acc: 0.7550\n",
      "Epoch 520/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.7382\n",
      "Epoch 520: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5260 - acc: 0.7381 - val_loss: 0.5148 - val_acc: 0.7537\n",
      "Epoch 521/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.7317\n",
      "Epoch 521: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5270 - acc: 0.7322 - val_loss: 0.5145 - val_acc: 0.7537\n",
      "Epoch 522/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5346 - acc: 0.7314\n",
      "Epoch 522: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5338 - acc: 0.7327 - val_loss: 0.5147 - val_acc: 0.7537\n",
      "Epoch 523/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7371\n",
      "Epoch 523: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5270 - acc: 0.7374 - val_loss: 0.5148 - val_acc: 0.7542\n",
      "Epoch 524/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.7349\n",
      "Epoch 524: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5312 - acc: 0.7350 - val_loss: 0.5148 - val_acc: 0.7550\n",
      "Epoch 525/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7331\n",
      "Epoch 525: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - acc: 0.7331 - val_loss: 0.5148 - val_acc: 0.7550\n",
      "Epoch 526/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.7330\n",
      "Epoch 526: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5337 - acc: 0.7333 - val_loss: 0.5148 - val_acc: 0.7550\n",
      "Epoch 527/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7358\n",
      "Epoch 527: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5277 - acc: 0.7361 - val_loss: 0.5146 - val_acc: 0.7546\n",
      "Epoch 528/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7332\n",
      "Epoch 528: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5277 - acc: 0.7333 - val_loss: 0.5146 - val_acc: 0.7542\n",
      "Epoch 529/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7314\n",
      "Epoch 529: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5327 - acc: 0.7315 - val_loss: 0.5146 - val_acc: 0.7550\n",
      "Epoch 530/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5301 - acc: 0.7349\n",
      "Epoch 530: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5296 - acc: 0.7350 - val_loss: 0.5146 - val_acc: 0.7550\n",
      "Epoch 531/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.7351\n",
      "Epoch 531: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5267 - acc: 0.7358 - val_loss: 0.5143 - val_acc: 0.7542\n",
      "Epoch 532/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7394\n",
      "Epoch 532: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5240 - acc: 0.7406 - val_loss: 0.5141 - val_acc: 0.7550\n",
      "Epoch 533/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5244 - acc: 0.7362\n",
      "Epoch 533: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5244 - acc: 0.7365 - val_loss: 0.5140 - val_acc: 0.7546\n",
      "Epoch 534/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5280 - acc: 0.7319\n",
      "Epoch 534: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5261 - acc: 0.7334 - val_loss: 0.5140 - val_acc: 0.7542\n",
      "Epoch 535/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5295 - acc: 0.7347\n",
      "Epoch 535: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5297 - acc: 0.7342 - val_loss: 0.5136 - val_acc: 0.7550\n",
      "Epoch 536/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5277 - acc: 0.7336\n",
      "Epoch 536: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5270 - acc: 0.7340 - val_loss: 0.5138 - val_acc: 0.7542\n",
      "Epoch 537/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5243 - acc: 0.7363\n",
      "Epoch 537: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5245 - acc: 0.7362 - val_loss: 0.5137 - val_acc: 0.7542\n",
      "Epoch 538/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7369\n",
      "Epoch 538: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5269 - acc: 0.7373 - val_loss: 0.5136 - val_acc: 0.7537\n",
      "Epoch 539/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5343 - acc: 0.7297\n",
      "Epoch 539: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5334 - acc: 0.7307 - val_loss: 0.5138 - val_acc: 0.7537\n",
      "Epoch 540/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7345\n",
      "Epoch 540: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5272 - acc: 0.7350 - val_loss: 0.5140 - val_acc: 0.7550\n",
      "Epoch 541/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7362\n",
      "Epoch 541: val_acc improved from 0.75545 to 0.75588, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5270 - acc: 0.7364 - val_loss: 0.5140 - val_acc: 0.7559\n",
      "Epoch 542/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7317\n",
      "Epoch 542: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5293 - acc: 0.7319 - val_loss: 0.5137 - val_acc: 0.7542\n",
      "Epoch 543/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5265 - acc: 0.7344\n",
      "Epoch 543: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5251 - acc: 0.7353 - val_loss: 0.5135 - val_acc: 0.7550\n",
      "Epoch 544/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.7329\n",
      "Epoch 544: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5261 - acc: 0.7330 - val_loss: 0.5133 - val_acc: 0.7546\n",
      "Epoch 545/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7316\n",
      "Epoch 545: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5288 - acc: 0.7319 - val_loss: 0.5135 - val_acc: 0.7555\n",
      "Epoch 546/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7343\n",
      "Epoch 546: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5244 - acc: 0.7340 - val_loss: 0.5133 - val_acc: 0.7559\n",
      "Epoch 547/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7370\n",
      "Epoch 547: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5269 - acc: 0.7369 - val_loss: 0.5135 - val_acc: 0.7559\n",
      "Epoch 548/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5249 - acc: 0.7331\n",
      "Epoch 548: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5238 - acc: 0.7335 - val_loss: 0.5136 - val_acc: 0.7546\n",
      "Epoch 549/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7364\n",
      "Epoch 549: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5233 - acc: 0.7366 - val_loss: 0.5132 - val_acc: 0.7550\n",
      "Epoch 550/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5275 - acc: 0.7310\n",
      "Epoch 550: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - acc: 0.7328 - val_loss: 0.5127 - val_acc: 0.7550\n",
      "Epoch 551/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7385\n",
      "Epoch 551: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5285 - acc: 0.7383 - val_loss: 0.5126 - val_acc: 0.7550\n",
      "Epoch 552/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5241 - acc: 0.7369\n",
      "Epoch 552: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - acc: 0.7368 - val_loss: 0.5124 - val_acc: 0.7550\n",
      "Epoch 553/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5271 - acc: 0.7398\n",
      "Epoch 553: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5281 - acc: 0.7396 - val_loss: 0.5124 - val_acc: 0.7546\n",
      "Epoch 554/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5292 - acc: 0.7355\n",
      "Epoch 554: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5279 - acc: 0.7359 - val_loss: 0.5123 - val_acc: 0.7546\n",
      "Epoch 555/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5231 - acc: 0.7365\n",
      "Epoch 555: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - acc: 0.7371 - val_loss: 0.5121 - val_acc: 0.7542\n",
      "Epoch 556/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.7342\n",
      "Epoch 556: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5260 - acc: 0.7339 - val_loss: 0.5121 - val_acc: 0.7550\n",
      "Epoch 557/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.7353\n",
      "Epoch 557: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - acc: 0.7355 - val_loss: 0.5122 - val_acc: 0.7550\n",
      "Epoch 558/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5257 - acc: 0.7357\n",
      "Epoch 558: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5258 - acc: 0.7362 - val_loss: 0.5118 - val_acc: 0.7546\n",
      "Epoch 559/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7329\n",
      "Epoch 559: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - acc: 0.7333 - val_loss: 0.5117 - val_acc: 0.7537\n",
      "Epoch 560/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5276 - acc: 0.7331\n",
      "Epoch 560: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5280 - acc: 0.7330 - val_loss: 0.5120 - val_acc: 0.7542\n",
      "Epoch 561/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.7383\n",
      "Epoch 561: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5230 - acc: 0.7383 - val_loss: 0.5119 - val_acc: 0.7546\n",
      "Epoch 562/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.7352\n",
      "Epoch 562: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5247 - acc: 0.7352 - val_loss: 0.5116 - val_acc: 0.7546\n",
      "Epoch 563/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5255 - acc: 0.7389\n",
      "Epoch 563: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5258 - acc: 0.7386 - val_loss: 0.5115 - val_acc: 0.7546\n",
      "Epoch 564/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7391\n",
      "Epoch 564: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - acc: 0.7390 - val_loss: 0.5116 - val_acc: 0.7555\n",
      "Epoch 565/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7383\n",
      "Epoch 565: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5227 - acc: 0.7386 - val_loss: 0.5113 - val_acc: 0.7555\n",
      "Epoch 566/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5245 - acc: 0.7376\n",
      "Epoch 566: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5238 - acc: 0.7376 - val_loss: 0.5109 - val_acc: 0.7555\n",
      "Epoch 567/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5235 - acc: 0.7412\n",
      "Epoch 567: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5235 - acc: 0.7412 - val_loss: 0.5108 - val_acc: 0.7555\n",
      "Epoch 568/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7357\n",
      "Epoch 568: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5272 - acc: 0.7358 - val_loss: 0.5107 - val_acc: 0.7555\n",
      "Epoch 569/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7353\n",
      "Epoch 569: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5265 - acc: 0.7364 - val_loss: 0.5108 - val_acc: 0.7555\n",
      "Epoch 570/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5264 - acc: 0.7348\n",
      "Epoch 570: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - acc: 0.7352 - val_loss: 0.5110 - val_acc: 0.7550\n",
      "Epoch 571/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5200 - acc: 0.7391\n",
      "Epoch 571: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5203 - acc: 0.7393 - val_loss: 0.5108 - val_acc: 0.7546\n",
      "Epoch 572/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.7361\n",
      "Epoch 572: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5261 - acc: 0.7364 - val_loss: 0.5107 - val_acc: 0.7550\n",
      "Epoch 573/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.7382\n",
      "Epoch 573: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - acc: 0.7389 - val_loss: 0.5106 - val_acc: 0.7555\n",
      "Epoch 574/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5277 - acc: 0.7373\n",
      "Epoch 574: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5291 - acc: 0.7371 - val_loss: 0.5105 - val_acc: 0.7555\n",
      "Epoch 575/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.7358\n",
      "Epoch 575: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - acc: 0.7362 - val_loss: 0.5104 - val_acc: 0.7550\n",
      "Epoch 576/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5324 - acc: 0.7352\n",
      "Epoch 576: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5315 - acc: 0.7356 - val_loss: 0.5105 - val_acc: 0.7550\n",
      "Epoch 577/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5224 - acc: 0.7364\n",
      "Epoch 577: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5223 - acc: 0.7367 - val_loss: 0.5102 - val_acc: 0.7546\n",
      "Epoch 578/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5245 - acc: 0.7333\n",
      "Epoch 578: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5249 - acc: 0.7333 - val_loss: 0.5098 - val_acc: 0.7546\n",
      "Epoch 579/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5238 - acc: 0.7409\n",
      "Epoch 579: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5234 - acc: 0.7414 - val_loss: 0.5104 - val_acc: 0.7550\n",
      "Epoch 580/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.7403\n",
      "Epoch 580: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - acc: 0.7404 - val_loss: 0.5100 - val_acc: 0.7550\n",
      "Epoch 581/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5265 - acc: 0.7356\n",
      "Epoch 581: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5255 - acc: 0.7366 - val_loss: 0.5098 - val_acc: 0.7550\n",
      "Epoch 582/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7410\n",
      "Epoch 582: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5208 - acc: 0.7413 - val_loss: 0.5097 - val_acc: 0.7555\n",
      "Epoch 583/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5201 - acc: 0.7398\n",
      "Epoch 583: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5202 - acc: 0.7399 - val_loss: 0.5093 - val_acc: 0.7555\n",
      "Epoch 584/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.7338\n",
      "Epoch 584: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5243 - acc: 0.7346 - val_loss: 0.5094 - val_acc: 0.7555\n",
      "Epoch 585/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5243 - acc: 0.7375\n",
      "Epoch 585: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - acc: 0.7385 - val_loss: 0.5089 - val_acc: 0.7550\n",
      "Epoch 586/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7364\n",
      "Epoch 586: val_acc did not improve from 0.75588\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - acc: 0.7370 - val_loss: 0.5091 - val_acc: 0.7559\n",
      "Epoch 587/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.7374\n",
      "Epoch 587: val_acc improved from 0.75588 to 0.75631, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - acc: 0.7378 - val_loss: 0.5091 - val_acc: 0.7563\n",
      "Epoch 588/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7375\n",
      "Epoch 588: val_acc did not improve from 0.75631\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5223 - acc: 0.7375 - val_loss: 0.5089 - val_acc: 0.7563\n",
      "Epoch 589/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5223 - acc: 0.7379\n",
      "Epoch 589: val_acc improved from 0.75631 to 0.75673, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - acc: 0.7379 - val_loss: 0.5092 - val_acc: 0.7567\n",
      "Epoch 590/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5187 - acc: 0.7401\n",
      "Epoch 590: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - acc: 0.7397 - val_loss: 0.5087 - val_acc: 0.7567\n",
      "Epoch 591/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7347\n",
      "Epoch 591: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5240 - acc: 0.7351 - val_loss: 0.5089 - val_acc: 0.7567\n",
      "Epoch 592/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5239 - acc: 0.7379\n",
      "Epoch 592: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5229 - acc: 0.7383 - val_loss: 0.5088 - val_acc: 0.7567\n",
      "Epoch 593/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5249 - acc: 0.7365\n",
      "Epoch 593: val_acc improved from 0.75673 to 0.75716, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5240 - acc: 0.7377 - val_loss: 0.5089 - val_acc: 0.7572\n",
      "Epoch 594/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7347\n",
      "Epoch 594: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5244 - acc: 0.7351 - val_loss: 0.5089 - val_acc: 0.7567\n",
      "Epoch 595/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5242 - acc: 0.7385\n",
      "Epoch 595: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - acc: 0.7385 - val_loss: 0.5087 - val_acc: 0.7572\n",
      "Epoch 596/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5218 - acc: 0.7399\n",
      "Epoch 596: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5216 - acc: 0.7395 - val_loss: 0.5086 - val_acc: 0.7567\n",
      "Epoch 597/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5204 - acc: 0.7386\n",
      "Epoch 597: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - acc: 0.7391 - val_loss: 0.5087 - val_acc: 0.7567\n",
      "Epoch 598/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7378\n",
      "Epoch 598: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - acc: 0.7378 - val_loss: 0.5084 - val_acc: 0.7567\n",
      "Epoch 599/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.7389\n",
      "Epoch 599: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5198 - acc: 0.7396 - val_loss: 0.5082 - val_acc: 0.7567\n",
      "Epoch 600/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5235 - acc: 0.7395\n",
      "Epoch 600: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - acc: 0.7408 - val_loss: 0.5080 - val_acc: 0.7567\n",
      "Epoch 601/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5224 - acc: 0.7404\n",
      "Epoch 601: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - acc: 0.7402 - val_loss: 0.5077 - val_acc: 0.7567\n",
      "Epoch 602/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5230 - acc: 0.7358\n",
      "Epoch 602: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - acc: 0.7358 - val_loss: 0.5076 - val_acc: 0.7567\n",
      "Epoch 603/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5239 - acc: 0.7380\n",
      "Epoch 603: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5231 - acc: 0.7391 - val_loss: 0.5076 - val_acc: 0.7567\n",
      "Epoch 604/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7387\n",
      "Epoch 604: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5158 - acc: 0.7391 - val_loss: 0.5073 - val_acc: 0.7563\n",
      "Epoch 605/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5183 - acc: 0.7447\n",
      "Epoch 605: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - acc: 0.7441 - val_loss: 0.5073 - val_acc: 0.7572\n",
      "Epoch 606/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7381\n",
      "Epoch 606: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - acc: 0.7385 - val_loss: 0.5069 - val_acc: 0.7567\n",
      "Epoch 607/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7388\n",
      "Epoch 607: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - acc: 0.7392 - val_loss: 0.5068 - val_acc: 0.7567\n",
      "Epoch 608/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5245 - acc: 0.7347\n",
      "Epoch 608: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - acc: 0.7353 - val_loss: 0.5067 - val_acc: 0.7567\n",
      "Epoch 609/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5257 - acc: 0.7383\n",
      "Epoch 609: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5257 - acc: 0.7383 - val_loss: 0.5070 - val_acc: 0.7567\n",
      "Epoch 610/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5202 - acc: 0.7366\n",
      "Epoch 610: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5196 - acc: 0.7375 - val_loss: 0.5071 - val_acc: 0.7572\n",
      "Epoch 611/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5182 - acc: 0.7377\n",
      "Epoch 611: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - acc: 0.7382 - val_loss: 0.5070 - val_acc: 0.7572\n",
      "Epoch 612/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5187 - acc: 0.7410\n",
      "Epoch 612: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - acc: 0.7408 - val_loss: 0.5068 - val_acc: 0.7559\n",
      "Epoch 613/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5205 - acc: 0.7409\n",
      "Epoch 613: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5208 - acc: 0.7415 - val_loss: 0.5069 - val_acc: 0.7559\n",
      "Epoch 614/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.7397\n",
      "Epoch 614: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - acc: 0.7396 - val_loss: 0.5060 - val_acc: 0.7572\n",
      "Epoch 615/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7395\n",
      "Epoch 615: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5189 - acc: 0.7397 - val_loss: 0.5062 - val_acc: 0.7559\n",
      "Epoch 616/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7371\n",
      "Epoch 616: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5199 - acc: 0.7378 - val_loss: 0.5061 - val_acc: 0.7555\n",
      "Epoch 617/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5223 - acc: 0.7397\n",
      "Epoch 617: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - acc: 0.7396 - val_loss: 0.5060 - val_acc: 0.7555\n",
      "Epoch 618/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7361\n",
      "Epoch 618: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - acc: 0.7361 - val_loss: 0.5057 - val_acc: 0.7555\n",
      "Epoch 619/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5198 - acc: 0.7426\n",
      "Epoch 619: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - acc: 0.7425 - val_loss: 0.5060 - val_acc: 0.7555\n",
      "Epoch 620/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5225 - acc: 0.7389\n",
      "Epoch 620: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5223 - acc: 0.7391 - val_loss: 0.5058 - val_acc: 0.7555\n",
      "Epoch 621/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5183 - acc: 0.7385\n",
      "Epoch 621: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - acc: 0.7394 - val_loss: 0.5058 - val_acc: 0.7559\n",
      "Epoch 622/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.7357\n",
      "Epoch 622: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5201 - acc: 0.7359 - val_loss: 0.5060 - val_acc: 0.7559\n",
      "Epoch 623/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7405\n",
      "Epoch 623: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - acc: 0.7404 - val_loss: 0.5056 - val_acc: 0.7559\n",
      "Epoch 624/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5217 - acc: 0.7357\n",
      "Epoch 624: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - acc: 0.7371 - val_loss: 0.5059 - val_acc: 0.7559\n",
      "Epoch 625/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7378\n",
      "Epoch 625: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5176 - acc: 0.7385 - val_loss: 0.5056 - val_acc: 0.7559\n",
      "Epoch 626/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5235 - acc: 0.7405\n",
      "Epoch 626: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - acc: 0.7406 - val_loss: 0.5055 - val_acc: 0.7559\n",
      "Epoch 627/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.7393\n",
      "Epoch 627: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - acc: 0.7393 - val_loss: 0.5051 - val_acc: 0.7559\n",
      "Epoch 628/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5178 - acc: 0.7379\n",
      "Epoch 628: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - acc: 0.7389 - val_loss: 0.5053 - val_acc: 0.7559\n",
      "Epoch 629/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5192 - acc: 0.7407\n",
      "Epoch 629: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - acc: 0.7405 - val_loss: 0.5050 - val_acc: 0.7559\n",
      "Epoch 630/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5156 - acc: 0.7412\n",
      "Epoch 630: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5143 - acc: 0.7421 - val_loss: 0.5047 - val_acc: 0.7559\n",
      "Epoch 631/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5169 - acc: 0.7412\n",
      "Epoch 631: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5159 - acc: 0.7415 - val_loss: 0.5047 - val_acc: 0.7555\n",
      "Epoch 632/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5208 - acc: 0.7418\n",
      "Epoch 632: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5211 - acc: 0.7416 - val_loss: 0.5049 - val_acc: 0.7555\n",
      "Epoch 633/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5270 - acc: 0.7369\n",
      "Epoch 633: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - acc: 0.7370 - val_loss: 0.5048 - val_acc: 0.7555\n",
      "Epoch 634/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5127 - acc: 0.7400\n",
      "Epoch 634: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - acc: 0.7394 - val_loss: 0.5043 - val_acc: 0.7559\n",
      "Epoch 635/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7428\n",
      "Epoch 635: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - acc: 0.7428 - val_loss: 0.5043 - val_acc: 0.7559\n",
      "Epoch 636/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.7382\n",
      "Epoch 636: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5182 - acc: 0.7384 - val_loss: 0.5043 - val_acc: 0.7559\n",
      "Epoch 637/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5179 - acc: 0.7432\n",
      "Epoch 637: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - acc: 0.7436 - val_loss: 0.5046 - val_acc: 0.7555\n",
      "Epoch 638/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5164 - acc: 0.7396\n",
      "Epoch 638: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5168 - acc: 0.7392 - val_loss: 0.5048 - val_acc: 0.7555\n",
      "Epoch 639/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.7384\n",
      "Epoch 639: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - acc: 0.7384 - val_loss: 0.5045 - val_acc: 0.7555\n",
      "Epoch 640/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5247 - acc: 0.7355\n",
      "Epoch 640: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5243 - acc: 0.7356 - val_loss: 0.5046 - val_acc: 0.7555\n",
      "Epoch 641/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.7390\n",
      "Epoch 641: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5118 - acc: 0.7391 - val_loss: 0.5042 - val_acc: 0.7555\n",
      "Epoch 642/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5190 - acc: 0.7399\n",
      "Epoch 642: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - acc: 0.7394 - val_loss: 0.5040 - val_acc: 0.7555\n",
      "Epoch 643/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7389\n",
      "Epoch 643: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5193 - acc: 0.7392 - val_loss: 0.5038 - val_acc: 0.7555\n",
      "Epoch 644/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5184 - acc: 0.7433\n",
      "Epoch 644: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - acc: 0.7433 - val_loss: 0.5037 - val_acc: 0.7555\n",
      "Epoch 645/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7350\n",
      "Epoch 645: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5216 - acc: 0.7354 - val_loss: 0.5039 - val_acc: 0.7555\n",
      "Epoch 646/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7382\n",
      "Epoch 646: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5199 - acc: 0.7387 - val_loss: 0.5042 - val_acc: 0.7563\n",
      "Epoch 647/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7413\n",
      "Epoch 647: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - acc: 0.7415 - val_loss: 0.5040 - val_acc: 0.7563\n",
      "Epoch 648/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7356\n",
      "Epoch 648: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - acc: 0.7365 - val_loss: 0.5038 - val_acc: 0.7559\n",
      "Epoch 649/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.7426\n",
      "Epoch 649: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - acc: 0.7429 - val_loss: 0.5041 - val_acc: 0.7563\n",
      "Epoch 650/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5159 - acc: 0.7400\n",
      "Epoch 650: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - acc: 0.7396 - val_loss: 0.5036 - val_acc: 0.7563\n",
      "Epoch 651/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5166 - acc: 0.7390\n",
      "Epoch 651: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5162 - acc: 0.7396 - val_loss: 0.5033 - val_acc: 0.7563\n",
      "Epoch 652/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5190 - acc: 0.7404\n",
      "Epoch 652: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - acc: 0.7394 - val_loss: 0.5030 - val_acc: 0.7563\n",
      "Epoch 653/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5189 - acc: 0.7408\n",
      "Epoch 653: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5189 - acc: 0.7408 - val_loss: 0.5029 - val_acc: 0.7563\n",
      "Epoch 654/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7412\n",
      "Epoch 654: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - acc: 0.7415 - val_loss: 0.5031 - val_acc: 0.7567\n",
      "Epoch 655/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7441\n",
      "Epoch 655: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - acc: 0.7445 - val_loss: 0.5031 - val_acc: 0.7567\n",
      "Epoch 656/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5198 - acc: 0.7397\n",
      "Epoch 656: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5197 - acc: 0.7394 - val_loss: 0.5027 - val_acc: 0.7567\n",
      "Epoch 657/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5186 - acc: 0.7397\n",
      "Epoch 657: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5186 - acc: 0.7397 - val_loss: 0.5029 - val_acc: 0.7563\n",
      "Epoch 658/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5178 - acc: 0.7397\n",
      "Epoch 658: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - acc: 0.7400 - val_loss: 0.5028 - val_acc: 0.7563\n",
      "Epoch 659/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.7375\n",
      "Epoch 659: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - acc: 0.7380 - val_loss: 0.5026 - val_acc: 0.7563\n",
      "Epoch 660/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5142 - acc: 0.7407\n",
      "Epoch 660: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - acc: 0.7411 - val_loss: 0.5024 - val_acc: 0.7563\n",
      "Epoch 661/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5185 - acc: 0.7397\n",
      "Epoch 661: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5185 - acc: 0.7396 - val_loss: 0.5027 - val_acc: 0.7567\n",
      "Epoch 662/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7388\n",
      "Epoch 662: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5204 - acc: 0.7393 - val_loss: 0.5028 - val_acc: 0.7567\n",
      "Epoch 663/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.7385\n",
      "Epoch 663: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5190 - acc: 0.7384 - val_loss: 0.5029 - val_acc: 0.7567\n",
      "Epoch 664/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5145 - acc: 0.7441\n",
      "Epoch 664: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5147 - acc: 0.7436 - val_loss: 0.5029 - val_acc: 0.7567\n",
      "Epoch 665/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.7401\n",
      "Epoch 665: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - acc: 0.7402 - val_loss: 0.5025 - val_acc: 0.7567\n",
      "Epoch 666/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.7415\n",
      "Epoch 666: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5144 - acc: 0.7415 - val_loss: 0.5021 - val_acc: 0.7567\n",
      "Epoch 667/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5188 - acc: 0.7394\n",
      "Epoch 667: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5188 - acc: 0.7387 - val_loss: 0.5020 - val_acc: 0.7572\n",
      "Epoch 668/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5153 - acc: 0.7444\n",
      "Epoch 668: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5153 - acc: 0.7444 - val_loss: 0.5022 - val_acc: 0.7567\n",
      "Epoch 669/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5187 - acc: 0.7428\n",
      "Epoch 669: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - acc: 0.7439 - val_loss: 0.5019 - val_acc: 0.7567\n",
      "Epoch 670/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5205 - acc: 0.7381\n",
      "Epoch 670: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5201 - acc: 0.7381 - val_loss: 0.5023 - val_acc: 0.7572\n",
      "Epoch 671/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5163 - acc: 0.7407\n",
      "Epoch 671: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - acc: 0.7426 - val_loss: 0.5022 - val_acc: 0.7572\n",
      "Epoch 672/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5140 - acc: 0.7417\n",
      "Epoch 672: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5140 - acc: 0.7417 - val_loss: 0.5019 - val_acc: 0.7567\n",
      "Epoch 673/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5193 - acc: 0.7370\n",
      "Epoch 673: val_acc improved from 0.75716 to 0.75759, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5193 - acc: 0.7370 - val_loss: 0.5019 - val_acc: 0.7576\n",
      "Epoch 674/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.7410\n",
      "Epoch 674: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - acc: 0.7416 - val_loss: 0.5017 - val_acc: 0.7576\n",
      "Epoch 675/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.7370\n",
      "Epoch 675: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - acc: 0.7371 - val_loss: 0.5018 - val_acc: 0.7572\n",
      "Epoch 676/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7421\n",
      "Epoch 676: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - acc: 0.7424 - val_loss: 0.5017 - val_acc: 0.7576\n",
      "Epoch 677/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.7398\n",
      "Epoch 677: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5185 - acc: 0.7400 - val_loss: 0.5018 - val_acc: 0.7572\n",
      "Epoch 678/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5118 - acc: 0.7420\n",
      "Epoch 678: val_acc improved from 0.75759 to 0.75802, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5112 - acc: 0.7430 - val_loss: 0.5014 - val_acc: 0.7580\n",
      "Epoch 679/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5167 - acc: 0.7412\n",
      "Epoch 679: val_acc did not improve from 0.75802\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5158 - acc: 0.7410 - val_loss: 0.5011 - val_acc: 0.7576\n",
      "Epoch 680/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7416\n",
      "Epoch 680: val_acc did not improve from 0.75802\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - acc: 0.7421 - val_loss: 0.5014 - val_acc: 0.7576\n",
      "Epoch 681/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7379\n",
      "Epoch 681: val_acc did not improve from 0.75802\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5187 - acc: 0.7385 - val_loss: 0.5012 - val_acc: 0.7576\n",
      "Epoch 682/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5158 - acc: 0.7423\n",
      "Epoch 682: val_acc improved from 0.75802 to 0.75844, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - acc: 0.7435 - val_loss: 0.5008 - val_acc: 0.7584\n",
      "Epoch 683/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7424\n",
      "Epoch 683: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - acc: 0.7426 - val_loss: 0.5008 - val_acc: 0.7584\n",
      "Epoch 684/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.7425\n",
      "Epoch 684: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5150 - acc: 0.7429 - val_loss: 0.5007 - val_acc: 0.7580\n",
      "Epoch 685/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.7431\n",
      "Epoch 685: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5135 - acc: 0.7433 - val_loss: 0.5006 - val_acc: 0.7576\n",
      "Epoch 686/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.7356\n",
      "Epoch 686: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5187 - acc: 0.7361 - val_loss: 0.5006 - val_acc: 0.7572\n",
      "Epoch 687/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.7399\n",
      "Epoch 687: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - acc: 0.7401 - val_loss: 0.5008 - val_acc: 0.7572\n",
      "Epoch 688/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7424\n",
      "Epoch 688: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5177 - acc: 0.7430 - val_loss: 0.5007 - val_acc: 0.7567\n",
      "Epoch 689/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.7436\n",
      "Epoch 689: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5135 - acc: 0.7445 - val_loss: 0.5005 - val_acc: 0.7567\n",
      "Epoch 690/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5165 - acc: 0.7448\n",
      "Epoch 690: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5176 - acc: 0.7438 - val_loss: 0.5007 - val_acc: 0.7572\n",
      "Epoch 691/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.7429\n",
      "Epoch 691: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5178 - acc: 0.7435 - val_loss: 0.5005 - val_acc: 0.7576\n",
      "Epoch 692/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5154 - acc: 0.7391\n",
      "Epoch 692: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - acc: 0.7390 - val_loss: 0.5003 - val_acc: 0.7572\n",
      "Epoch 693/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5188 - acc: 0.7417\n",
      "Epoch 693: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5181 - acc: 0.7422 - val_loss: 0.5003 - val_acc: 0.7580\n",
      "Epoch 694/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.7399\n",
      "Epoch 694: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5161 - acc: 0.7404 - val_loss: 0.5002 - val_acc: 0.7580\n",
      "Epoch 695/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5143 - acc: 0.7444\n",
      "Epoch 695: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5137 - acc: 0.7454 - val_loss: 0.5002 - val_acc: 0.7584\n",
      "Epoch 696/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.7437\n",
      "Epoch 696: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5119 - acc: 0.7444 - val_loss: 0.4996 - val_acc: 0.7584\n",
      "Epoch 697/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5114 - acc: 0.7430\n",
      "Epoch 697: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - acc: 0.7435 - val_loss: 0.4995 - val_acc: 0.7584\n",
      "Epoch 698/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.7435\n",
      "Epoch 698: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - acc: 0.7438 - val_loss: 0.4998 - val_acc: 0.7584\n",
      "Epoch 699/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.7417\n",
      "Epoch 699: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5160 - acc: 0.7418 - val_loss: 0.4999 - val_acc: 0.7576\n",
      "Epoch 700/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5142 - acc: 0.7418\n",
      "Epoch 700: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5143 - acc: 0.7412 - val_loss: 0.4997 - val_acc: 0.7572\n",
      "Epoch 701/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5143 - acc: 0.7446\n",
      "Epoch 701: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - acc: 0.7452 - val_loss: 0.4995 - val_acc: 0.7576\n",
      "Epoch 702/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5145 - acc: 0.7439\n",
      "Epoch 702: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5142 - acc: 0.7444 - val_loss: 0.4993 - val_acc: 0.7580\n",
      "Epoch 703/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.7411\n",
      "Epoch 703: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - acc: 0.7412 - val_loss: 0.4992 - val_acc: 0.7576\n",
      "Epoch 704/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.7436\n",
      "Epoch 704: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - acc: 0.7440 - val_loss: 0.4990 - val_acc: 0.7576\n",
      "Epoch 705/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7389\n",
      "Epoch 705: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5162 - acc: 0.7389 - val_loss: 0.4990 - val_acc: 0.7576\n",
      "Epoch 706/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.7428\n",
      "Epoch 706: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - acc: 0.7431 - val_loss: 0.4992 - val_acc: 0.7576\n",
      "Epoch 707/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5179 - acc: 0.7419\n",
      "Epoch 707: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - acc: 0.7412 - val_loss: 0.4991 - val_acc: 0.7572\n",
      "Epoch 708/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5158 - acc: 0.7427\n",
      "Epoch 708: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - acc: 0.7425 - val_loss: 0.4993 - val_acc: 0.7576\n",
      "Epoch 709/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5182 - acc: 0.7407\n",
      "Epoch 709: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - acc: 0.7416 - val_loss: 0.4993 - val_acc: 0.7576\n",
      "Epoch 710/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5124 - acc: 0.7439\n",
      "Epoch 710: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5134 - acc: 0.7433 - val_loss: 0.4991 - val_acc: 0.7584\n",
      "Epoch 711/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5104 - acc: 0.7445\n",
      "Epoch 711: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5103 - acc: 0.7445 - val_loss: 0.4987 - val_acc: 0.7584\n",
      "Epoch 712/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.7444\n",
      "Epoch 712: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5116 - acc: 0.7445 - val_loss: 0.4986 - val_acc: 0.7580\n",
      "Epoch 713/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7418\n",
      "Epoch 713: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - acc: 0.7417 - val_loss: 0.4984 - val_acc: 0.7584\n",
      "Epoch 714/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5127 - acc: 0.7403\n",
      "Epoch 714: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - acc: 0.7414 - val_loss: 0.4981 - val_acc: 0.7580\n",
      "Epoch 715/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7408\n",
      "Epoch 715: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - acc: 0.7408 - val_loss: 0.4984 - val_acc: 0.7584\n",
      "Epoch 716/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.7419\n",
      "Epoch 716: val_acc did not improve from 0.75844\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5134 - acc: 0.7418 - val_loss: 0.4980 - val_acc: 0.7584\n",
      "Epoch 717/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7402\n",
      "Epoch 717: val_acc improved from 0.75844 to 0.75887, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5146 - acc: 0.7406 - val_loss: 0.4981 - val_acc: 0.7589\n",
      "Epoch 718/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5116 - acc: 0.7453\n",
      "Epoch 718: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5114 - acc: 0.7461 - val_loss: 0.4979 - val_acc: 0.7589\n",
      "Epoch 719/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.7465\n",
      "Epoch 719: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - acc: 0.7469 - val_loss: 0.4977 - val_acc: 0.7584\n",
      "Epoch 720/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5158 - acc: 0.7409\n",
      "Epoch 720: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5159 - acc: 0.7407 - val_loss: 0.4980 - val_acc: 0.7589\n",
      "Epoch 721/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5073 - acc: 0.7455\n",
      "Epoch 721: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5081 - acc: 0.7448 - val_loss: 0.4977 - val_acc: 0.7589\n",
      "Epoch 722/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7380\n",
      "Epoch 722: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5176 - acc: 0.7382 - val_loss: 0.4981 - val_acc: 0.7584\n",
      "Epoch 723/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5127 - acc: 0.7432\n",
      "Epoch 723: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - acc: 0.7426 - val_loss: 0.4977 - val_acc: 0.7589\n",
      "Epoch 724/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7384\n",
      "Epoch 724: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5163 - acc: 0.7395 - val_loss: 0.4980 - val_acc: 0.7580\n",
      "Epoch 725/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.7427\n",
      "Epoch 725: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5135 - acc: 0.7427 - val_loss: 0.4979 - val_acc: 0.7580\n",
      "Epoch 726/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5139 - acc: 0.7436\n",
      "Epoch 726: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5143 - acc: 0.7429 - val_loss: 0.4981 - val_acc: 0.7580\n",
      "Epoch 727/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7400\n",
      "Epoch 727: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5142 - acc: 0.7407 - val_loss: 0.4977 - val_acc: 0.7589\n",
      "Epoch 728/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.7458\n",
      "Epoch 728: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - acc: 0.7458 - val_loss: 0.4975 - val_acc: 0.7589\n",
      "Epoch 729/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.7429\n",
      "Epoch 729: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5157 - acc: 0.7430 - val_loss: 0.4974 - val_acc: 0.7589\n",
      "Epoch 730/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.7452\n",
      "Epoch 730: val_acc improved from 0.75887 to 0.75930, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5087 - acc: 0.7460 - val_loss: 0.4974 - val_acc: 0.7593\n",
      "Epoch 731/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.7409\n",
      "Epoch 731: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - acc: 0.7412 - val_loss: 0.4973 - val_acc: 0.7593\n",
      "Epoch 732/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5139 - acc: 0.7422\n",
      "Epoch 732: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5136 - acc: 0.7427 - val_loss: 0.4972 - val_acc: 0.7593\n",
      "Epoch 733/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.7432\n",
      "Epoch 733: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5139 - acc: 0.7436 - val_loss: 0.4972 - val_acc: 0.7593\n",
      "Epoch 734/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5114 - acc: 0.7429\n",
      "Epoch 734: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - acc: 0.7430 - val_loss: 0.4969 - val_acc: 0.7593\n",
      "Epoch 735/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5128 - acc: 0.7407\n",
      "Epoch 735: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - acc: 0.7399 - val_loss: 0.4968 - val_acc: 0.7589\n",
      "Epoch 736/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5122 - acc: 0.7468\n",
      "Epoch 736: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5110 - acc: 0.7474 - val_loss: 0.4968 - val_acc: 0.7593\n",
      "Epoch 737/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5120 - acc: 0.7435\n",
      "Epoch 737: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5107 - acc: 0.7447 - val_loss: 0.4969 - val_acc: 0.7589\n",
      "Epoch 738/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5146 - acc: 0.7413\n",
      "Epoch 738: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - acc: 0.7413 - val_loss: 0.4968 - val_acc: 0.7593\n",
      "Epoch 739/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5108 - acc: 0.7470\n",
      "Epoch 739: val_acc improved from 0.75930 to 0.76015, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - acc: 0.7474 - val_loss: 0.4969 - val_acc: 0.7602\n",
      "Epoch 740/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.7420\n",
      "Epoch 740: val_acc did not improve from 0.76015\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5039 - acc: 0.7430 - val_loss: 0.4963 - val_acc: 0.7593\n",
      "Epoch 741/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5075 - acc: 0.7443\n",
      "Epoch 741: val_acc did not improve from 0.76015\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5054 - acc: 0.7460 - val_loss: 0.4959 - val_acc: 0.7589\n",
      "Epoch 742/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5143 - acc: 0.7427\n",
      "Epoch 742: val_acc did not improve from 0.76015\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - acc: 0.7427 - val_loss: 0.4958 - val_acc: 0.7589\n",
      "Epoch 743/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.7433\n",
      "Epoch 743: val_acc improved from 0.76015 to 0.76058, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - acc: 0.7436 - val_loss: 0.4962 - val_acc: 0.7606\n",
      "Epoch 744/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7429\n",
      "Epoch 744: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5127 - acc: 0.7431 - val_loss: 0.4962 - val_acc: 0.7606\n",
      "Epoch 745/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7471\n",
      "Epoch 745: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5089 - acc: 0.7471 - val_loss: 0.4956 - val_acc: 0.7602\n",
      "Epoch 746/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.7479\n",
      "Epoch 746: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5068 - acc: 0.7484 - val_loss: 0.4955 - val_acc: 0.7606\n",
      "Epoch 747/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5110 - acc: 0.7449\n",
      "Epoch 747: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5112 - acc: 0.7452 - val_loss: 0.4952 - val_acc: 0.7597\n",
      "Epoch 748/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.7466\n",
      "Epoch 748: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - acc: 0.7470 - val_loss: 0.4952 - val_acc: 0.7602\n",
      "Epoch 749/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.7398\n",
      "Epoch 749: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - acc: 0.7397 - val_loss: 0.4954 - val_acc: 0.7606\n",
      "Epoch 750/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7452\n",
      "Epoch 750: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5105 - acc: 0.7454 - val_loss: 0.4952 - val_acc: 0.7602\n",
      "Epoch 751/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5134 - acc: 0.7424\n",
      "Epoch 751: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5129 - acc: 0.7430 - val_loss: 0.4953 - val_acc: 0.7602\n",
      "Epoch 752/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5059 - acc: 0.7445\n",
      "Epoch 752: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5061 - acc: 0.7447 - val_loss: 0.4950 - val_acc: 0.7606\n",
      "Epoch 753/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7420\n",
      "Epoch 753: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - acc: 0.7421 - val_loss: 0.4949 - val_acc: 0.7602\n",
      "Epoch 754/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.7439\n",
      "Epoch 754: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - acc: 0.7439 - val_loss: 0.4951 - val_acc: 0.7606\n",
      "Epoch 755/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.7454\n",
      "Epoch 755: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5127 - acc: 0.7455 - val_loss: 0.4956 - val_acc: 0.7606\n",
      "Epoch 756/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.7441\n",
      "Epoch 756: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - acc: 0.7445 - val_loss: 0.4955 - val_acc: 0.7602\n",
      "Epoch 757/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5073 - acc: 0.7503\n",
      "Epoch 757: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5070 - acc: 0.7503 - val_loss: 0.4951 - val_acc: 0.7606\n",
      "Epoch 758/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7477\n",
      "Epoch 758: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - acc: 0.7480 - val_loss: 0.4951 - val_acc: 0.7606\n",
      "Epoch 759/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5131 - acc: 0.7451\n",
      "Epoch 759: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - acc: 0.7467 - val_loss: 0.4951 - val_acc: 0.7602\n",
      "Epoch 760/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5089 - acc: 0.7465\n",
      "Epoch 760: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5085 - acc: 0.7469 - val_loss: 0.4945 - val_acc: 0.7606\n",
      "Epoch 761/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5150 - acc: 0.7400\n",
      "Epoch 761: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - acc: 0.7423 - val_loss: 0.4945 - val_acc: 0.7602\n",
      "Epoch 762/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.7490\n",
      "Epoch 762: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5080 - acc: 0.7495 - val_loss: 0.4947 - val_acc: 0.7597\n",
      "Epoch 763/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.7500\n",
      "Epoch 763: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5058 - acc: 0.7502 - val_loss: 0.4942 - val_acc: 0.7597\n",
      "Epoch 764/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5125 - acc: 0.7438\n",
      "Epoch 764: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - acc: 0.7433 - val_loss: 0.4943 - val_acc: 0.7597\n",
      "Epoch 765/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5142 - acc: 0.7444\n",
      "Epoch 765: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - acc: 0.7448 - val_loss: 0.4945 - val_acc: 0.7606\n",
      "Epoch 766/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5068 - acc: 0.7454\n",
      "Epoch 766: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5068 - acc: 0.7454 - val_loss: 0.4942 - val_acc: 0.7602\n",
      "Epoch 767/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5087 - acc: 0.7448\n",
      "Epoch 767: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - acc: 0.7454 - val_loss: 0.4937 - val_acc: 0.7602\n",
      "Epoch 768/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7441\n",
      "Epoch 768: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5079 - acc: 0.7441 - val_loss: 0.4939 - val_acc: 0.7602\n",
      "Epoch 769/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5100 - acc: 0.7440\n",
      "Epoch 769: val_acc improved from 0.76058 to 0.76101, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5094 - acc: 0.7451 - val_loss: 0.4940 - val_acc: 0.7610\n",
      "Epoch 770/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5135 - acc: 0.7414\n",
      "Epoch 770: val_acc improved from 0.76101 to 0.76144, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - acc: 0.7420 - val_loss: 0.4942 - val_acc: 0.7614\n",
      "Epoch 771/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7461\n",
      "Epoch 771: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5070 - acc: 0.7469 - val_loss: 0.4938 - val_acc: 0.7606\n",
      "Epoch 772/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5121 - acc: 0.7466\n",
      "Epoch 772: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - acc: 0.7464 - val_loss: 0.4936 - val_acc: 0.7602\n",
      "Epoch 773/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.7474\n",
      "Epoch 773: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5046 - acc: 0.7474 - val_loss: 0.4929 - val_acc: 0.7610\n",
      "Epoch 774/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5090 - acc: 0.7457\n",
      "Epoch 774: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5096 - acc: 0.7448 - val_loss: 0.4930 - val_acc: 0.7606\n",
      "Epoch 775/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.7460\n",
      "Epoch 775: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - acc: 0.7467 - val_loss: 0.4929 - val_acc: 0.7610\n",
      "Epoch 776/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.7471\n",
      "Epoch 776: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5057 - acc: 0.7474 - val_loss: 0.4927 - val_acc: 0.7610\n",
      "Epoch 777/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.7470\n",
      "Epoch 777: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5054 - acc: 0.7470 - val_loss: 0.4927 - val_acc: 0.7610\n",
      "Epoch 778/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5073 - acc: 0.7443\n",
      "Epoch 778: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5073 - acc: 0.7443 - val_loss: 0.4930 - val_acc: 0.7610\n",
      "Epoch 779/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5093 - acc: 0.7440\n",
      "Epoch 779: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5101 - acc: 0.7435 - val_loss: 0.4927 - val_acc: 0.7606\n",
      "Epoch 780/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5124 - acc: 0.7411\n",
      "Epoch 780: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - acc: 0.7417 - val_loss: 0.4928 - val_acc: 0.7606\n",
      "Epoch 781/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.7450\n",
      "Epoch 781: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - acc: 0.7456 - val_loss: 0.4925 - val_acc: 0.7606\n",
      "Epoch 782/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7457\n",
      "Epoch 782: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5073 - acc: 0.7464 - val_loss: 0.4929 - val_acc: 0.7614\n",
      "Epoch 783/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.7445\n",
      "Epoch 783: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5078 - acc: 0.7445 - val_loss: 0.4928 - val_acc: 0.7614\n",
      "Epoch 784/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5114 - acc: 0.7439\n",
      "Epoch 784: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - acc: 0.7441 - val_loss: 0.4925 - val_acc: 0.7610\n",
      "Epoch 785/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5038 - acc: 0.7496\n",
      "Epoch 785: val_acc improved from 0.76144 to 0.76186, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5046 - acc: 0.7493 - val_loss: 0.4926 - val_acc: 0.7619\n",
      "Epoch 786/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.7515\n",
      "Epoch 786: val_acc improved from 0.76186 to 0.76229, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5052 - acc: 0.7517 - val_loss: 0.4924 - val_acc: 0.7623\n",
      "Epoch 787/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5067 - acc: 0.7466\n",
      "Epoch 787: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5061 - acc: 0.7473 - val_loss: 0.4921 - val_acc: 0.7619\n",
      "Epoch 788/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.7405\n",
      "Epoch 788: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5106 - acc: 0.7410 - val_loss: 0.4923 - val_acc: 0.7614\n",
      "Epoch 789/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5048 - acc: 0.7471\n",
      "Epoch 789: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5043 - acc: 0.7475 - val_loss: 0.4923 - val_acc: 0.7619\n",
      "Epoch 790/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5038 - acc: 0.7479\n",
      "Epoch 790: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5034 - acc: 0.7483 - val_loss: 0.4923 - val_acc: 0.7610\n",
      "Epoch 791/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5063 - acc: 0.7464\n",
      "Epoch 791: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5063 - acc: 0.7464 - val_loss: 0.4921 - val_acc: 0.7610\n",
      "Epoch 792/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5025 - acc: 0.7504\n",
      "Epoch 792: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5019 - acc: 0.7506 - val_loss: 0.4920 - val_acc: 0.7614\n",
      "Epoch 793/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5053 - acc: 0.7449\n",
      "Epoch 793: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5053 - acc: 0.7449 - val_loss: 0.4918 - val_acc: 0.7614\n",
      "Epoch 794/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.7450\n",
      "Epoch 794: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - acc: 0.7455 - val_loss: 0.4919 - val_acc: 0.7619\n",
      "Epoch 795/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.7434\n",
      "Epoch 795: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - acc: 0.7431 - val_loss: 0.4919 - val_acc: 0.7619\n",
      "Epoch 796/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.7495\n",
      "Epoch 796: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5056 - acc: 0.7491 - val_loss: 0.4918 - val_acc: 0.7619\n",
      "Epoch 797/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.7509\n",
      "Epoch 797: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5058 - acc: 0.7513 - val_loss: 0.4915 - val_acc: 0.7614\n",
      "Epoch 798/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.7450\n",
      "Epoch 798: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - acc: 0.7451 - val_loss: 0.4910 - val_acc: 0.7614\n",
      "Epoch 799/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5113 - acc: 0.7436\n",
      "Epoch 799: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - acc: 0.7429 - val_loss: 0.4914 - val_acc: 0.7623\n",
      "Epoch 800/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7446\n",
      "Epoch 800: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - acc: 0.7452 - val_loss: 0.4912 - val_acc: 0.7606\n",
      "Epoch 801/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.7452\n",
      "Epoch 801: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5069 - acc: 0.7458 - val_loss: 0.4914 - val_acc: 0.7623\n",
      "Epoch 802/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5088 - acc: 0.7486\n",
      "Epoch 802: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5091 - acc: 0.7483 - val_loss: 0.4914 - val_acc: 0.7623\n",
      "Epoch 803/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5077 - acc: 0.7470\n",
      "Epoch 803: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5077 - acc: 0.7470 - val_loss: 0.4914 - val_acc: 0.7619\n",
      "Epoch 804/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.7469\n",
      "Epoch 804: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5047 - acc: 0.7471 - val_loss: 0.4914 - val_acc: 0.7623\n",
      "Epoch 805/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.7503\n",
      "Epoch 805: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5068 - acc: 0.7507 - val_loss: 0.4913 - val_acc: 0.7619\n",
      "Epoch 806/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7491\n",
      "Epoch 806: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5027 - acc: 0.7491 - val_loss: 0.4911 - val_acc: 0.7619\n",
      "Epoch 807/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5045 - acc: 0.7481\n",
      "Epoch 807: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5045 - acc: 0.7489 - val_loss: 0.4911 - val_acc: 0.7623\n",
      "Epoch 808/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5039 - acc: 0.7469\n",
      "Epoch 808: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5037 - acc: 0.7466 - val_loss: 0.4907 - val_acc: 0.7619\n",
      "Epoch 809/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5049 - acc: 0.7435\n",
      "Epoch 809: val_acc improved from 0.76229 to 0.76315, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5046 - acc: 0.7441 - val_loss: 0.4906 - val_acc: 0.7631\n",
      "Epoch 810/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5031 - acc: 0.7518\n",
      "Epoch 810: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5032 - acc: 0.7510 - val_loss: 0.4901 - val_acc: 0.7631\n",
      "Epoch 811/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5062 - acc: 0.7473\n",
      "Epoch 811: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5062 - acc: 0.7473 - val_loss: 0.4904 - val_acc: 0.7627\n",
      "Epoch 812/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5098 - acc: 0.7471\n",
      "Epoch 812: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5073 - acc: 0.7504 - val_loss: 0.4907 - val_acc: 0.7627\n",
      "Epoch 813/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.7452\n",
      "Epoch 813: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - acc: 0.7456 - val_loss: 0.4905 - val_acc: 0.7631\n",
      "Epoch 814/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.7462\n",
      "Epoch 814: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5131 - acc: 0.7472 - val_loss: 0.4904 - val_acc: 0.7623\n",
      "Epoch 815/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.7447\n",
      "Epoch 815: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5061 - acc: 0.7446 - val_loss: 0.4904 - val_acc: 0.7627\n",
      "Epoch 816/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5069 - acc: 0.7485\n",
      "Epoch 816: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5071 - acc: 0.7482 - val_loss: 0.4903 - val_acc: 0.7627\n",
      "Epoch 817/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5045 - acc: 0.7473\n",
      "Epoch 817: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5040 - acc: 0.7486 - val_loss: 0.4901 - val_acc: 0.7623\n",
      "Epoch 818/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5081 - acc: 0.7460\n",
      "Epoch 818: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5068 - acc: 0.7472 - val_loss: 0.4900 - val_acc: 0.7619\n",
      "Epoch 819/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5064 - acc: 0.7473\n",
      "Epoch 819: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5057 - acc: 0.7479 - val_loss: 0.4901 - val_acc: 0.7614\n",
      "Epoch 820/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5063 - acc: 0.7435\n",
      "Epoch 820: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5068 - acc: 0.7428 - val_loss: 0.4898 - val_acc: 0.7619\n",
      "Epoch 821/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5014 - acc: 0.7487\n",
      "Epoch 821: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5017 - acc: 0.7489 - val_loss: 0.4896 - val_acc: 0.7614\n",
      "Epoch 822/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5093 - acc: 0.7421\n",
      "Epoch 822: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - acc: 0.7423 - val_loss: 0.4894 - val_acc: 0.7619\n",
      "Epoch 823/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.7472\n",
      "Epoch 823: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5072 - acc: 0.7473 - val_loss: 0.4894 - val_acc: 0.7614\n",
      "Epoch 824/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5066 - acc: 0.7487\n",
      "Epoch 824: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5065 - acc: 0.7495 - val_loss: 0.4896 - val_acc: 0.7602\n",
      "Epoch 825/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.7468\n",
      "Epoch 825: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5058 - acc: 0.7469 - val_loss: 0.4899 - val_acc: 0.7619\n",
      "Epoch 826/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.7468\n",
      "Epoch 826: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5057 - acc: 0.7466 - val_loss: 0.4899 - val_acc: 0.7623\n",
      "Epoch 827/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5076 - acc: 0.7473\n",
      "Epoch 827: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5084 - acc: 0.7467 - val_loss: 0.4897 - val_acc: 0.7623\n",
      "Epoch 828/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5085 - acc: 0.7446\n",
      "Epoch 828: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5071 - acc: 0.7461 - val_loss: 0.4897 - val_acc: 0.7619\n",
      "Epoch 829/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5081 - acc: 0.7470\n",
      "Epoch 829: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5065 - acc: 0.7482 - val_loss: 0.4895 - val_acc: 0.7623\n",
      "Epoch 830/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5081 - acc: 0.7444\n",
      "Epoch 830: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5072 - acc: 0.7448 - val_loss: 0.4894 - val_acc: 0.7623\n",
      "Epoch 831/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.7476\n",
      "Epoch 831: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - acc: 0.7477 - val_loss: 0.4892 - val_acc: 0.7627\n",
      "Epoch 832/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.7504\n",
      "Epoch 832: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5028 - acc: 0.7514 - val_loss: 0.4890 - val_acc: 0.7623\n",
      "Epoch 833/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.7445\n",
      "Epoch 833: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5090 - acc: 0.7452 - val_loss: 0.4893 - val_acc: 0.7623\n",
      "Epoch 834/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5093 - acc: 0.7448\n",
      "Epoch 834: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - acc: 0.7452 - val_loss: 0.4896 - val_acc: 0.7623\n",
      "Epoch 835/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5027 - acc: 0.7491\n",
      "Epoch 835: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5025 - acc: 0.7489 - val_loss: 0.4891 - val_acc: 0.7623\n",
      "Epoch 836/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5028 - acc: 0.7483\n",
      "Epoch 836: val_acc did not improve from 0.76315\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5031 - acc: 0.7482 - val_loss: 0.4893 - val_acc: 0.7627\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 4.4227 - acc: 0.5854\n",
      "Epoch 1: val_acc improved from -inf to 0.73097, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 4ms/step - loss: 4.3740 - acc: 0.5859 - val_loss: 0.8508 - val_acc: 0.7310\n",
      "Epoch 2/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 2.5095 - acc: 0.5922\n",
      "Epoch 2: val_acc did not improve from 0.73097\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 2.5029 - acc: 0.5925 - val_loss: 0.6244 - val_acc: 0.7310\n",
      "Epoch 3/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 1.8772 - acc: 0.6059\n",
      "Epoch 3: val_acc did not improve from 0.73097\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.8801 - acc: 0.6055 - val_loss: 0.5696 - val_acc: 0.7263\n",
      "Epoch 4/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 1.5813 - acc: 0.6045\n",
      "Epoch 4: val_acc did not improve from 0.73097\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.5768 - acc: 0.6039 - val_loss: 0.5630 - val_acc: 0.7288\n",
      "Epoch 5/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 1.3928 - acc: 0.6057\n",
      "Epoch 5: val_acc did not improve from 0.73097\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.3942 - acc: 0.6052 - val_loss: 0.5633 - val_acc: 0.7254\n",
      "Epoch 6/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 1.2512 - acc: 0.6042\n",
      "Epoch 6: val_acc did not improve from 0.73097\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.2506 - acc: 0.6031 - val_loss: 0.5668 - val_acc: 0.7275\n",
      "Epoch 7/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 1.1133 - acc: 0.6113\n",
      "Epoch 7: val_acc did not improve from 0.73097\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 1.1077 - acc: 0.6127 - val_loss: 0.5713 - val_acc: 0.7297\n",
      "Epoch 8/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 1.0416 - acc: 0.6258\n",
      "Epoch 8: val_acc improved from 0.73097 to 0.73268, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 1.0401 - acc: 0.6264 - val_loss: 0.5733 - val_acc: 0.7327\n",
      "Epoch 9/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.9712 - acc: 0.6229\n",
      "Epoch 9: val_acc improved from 0.73268 to 0.73439, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.9701 - acc: 0.6230 - val_loss: 0.5803 - val_acc: 0.7344\n",
      "Epoch 10/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.9365 - acc: 0.6244\n",
      "Epoch 10: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.9377 - acc: 0.6238 - val_loss: 0.5841 - val_acc: 0.7322\n",
      "Epoch 11/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.9366 - acc: 0.6200\n",
      "Epoch 11: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.9343 - acc: 0.6199 - val_loss: 0.5881 - val_acc: 0.7318\n",
      "Epoch 12/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8817 - acc: 0.6199\n",
      "Epoch 12: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.8817 - acc: 0.6199 - val_loss: 0.5925 - val_acc: 0.7297\n",
      "Epoch 13/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.8275 - acc: 0.6367\n",
      "Epoch 13: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.8290 - acc: 0.6364 - val_loss: 0.5954 - val_acc: 0.7293\n",
      "Epoch 14/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.8162 - acc: 0.6380\n",
      "Epoch 14: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.8128 - acc: 0.6391 - val_loss: 0.5959 - val_acc: 0.7275\n",
      "Epoch 15/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.7956 - acc: 0.6340\n",
      "Epoch 15: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7953 - acc: 0.6339 - val_loss: 0.5979 - val_acc: 0.7263\n",
      "Epoch 16/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.7839 - acc: 0.6372\n",
      "Epoch 16: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7843 - acc: 0.6377 - val_loss: 0.6017 - val_acc: 0.7297\n",
      "Epoch 17/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.7631 - acc: 0.6391\n",
      "Epoch 17: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7603 - acc: 0.6384 - val_loss: 0.6021 - val_acc: 0.7288\n",
      "Epoch 18/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.7493 - acc: 0.6366\n",
      "Epoch 18: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.7492 - acc: 0.6365 - val_loss: 0.6034 - val_acc: 0.7280\n",
      "Epoch 19/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.7362 - acc: 0.6410\n",
      "Epoch 19: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.7355 - acc: 0.6414 - val_loss: 0.6045 - val_acc: 0.7271\n",
      "Epoch 20/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.7317 - acc: 0.6406\n",
      "Epoch 20: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7318 - acc: 0.6407 - val_loss: 0.6054 - val_acc: 0.7280\n",
      "Epoch 21/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.7211 - acc: 0.6560\n",
      "Epoch 21: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7245 - acc: 0.6535 - val_loss: 0.6052 - val_acc: 0.7275\n",
      "Epoch 22/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.7180 - acc: 0.6436\n",
      "Epoch 22: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7175 - acc: 0.6440 - val_loss: 0.6072 - val_acc: 0.7275\n",
      "Epoch 23/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.7030 - acc: 0.6514\n",
      "Epoch 23: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7024 - acc: 0.6519 - val_loss: 0.6059 - val_acc: 0.7271\n",
      "Epoch 24/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6972 - acc: 0.6551\n",
      "Epoch 24: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6960 - acc: 0.6551 - val_loss: 0.6059 - val_acc: 0.7284\n",
      "Epoch 25/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.7001 - acc: 0.6552\n",
      "Epoch 25: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.7037 - acc: 0.6546 - val_loss: 0.6072 - val_acc: 0.7293\n",
      "Epoch 26/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6948 - acc: 0.6513\n",
      "Epoch 26: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6942 - acc: 0.6508 - val_loss: 0.6076 - val_acc: 0.7284\n",
      "Epoch 27/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.6506\n",
      "Epoch 27: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6937 - acc: 0.6500 - val_loss: 0.6069 - val_acc: 0.7288\n",
      "Epoch 28/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6762 - acc: 0.6566\n",
      "Epoch 28: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6772 - acc: 0.6559 - val_loss: 0.6043 - val_acc: 0.7267\n",
      "Epoch 29/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6746 - acc: 0.6636\n",
      "Epoch 29: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6746 - acc: 0.6631 - val_loss: 0.6036 - val_acc: 0.7275\n",
      "Epoch 30/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6756 - acc: 0.6562\n",
      "Epoch 30: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6771 - acc: 0.6562 - val_loss: 0.6044 - val_acc: 0.7267\n",
      "Epoch 31/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.6620\n",
      "Epoch 31: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6647 - acc: 0.6620 - val_loss: 0.6032 - val_acc: 0.7267\n",
      "Epoch 32/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6616 - acc: 0.6698\n",
      "Epoch 32: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6613 - acc: 0.6694 - val_loss: 0.6034 - val_acc: 0.7258\n",
      "Epoch 33/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6598 - acc: 0.6703\n",
      "Epoch 33: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6609 - acc: 0.6684 - val_loss: 0.6025 - val_acc: 0.7271\n",
      "Epoch 34/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6581 - acc: 0.6681\n",
      "Epoch 34: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6592 - acc: 0.6666 - val_loss: 0.6019 - val_acc: 0.7267\n",
      "Epoch 35/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6667 - acc: 0.6628\n",
      "Epoch 35: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6658 - acc: 0.6634 - val_loss: 0.6017 - val_acc: 0.7267\n",
      "Epoch 36/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6544 - acc: 0.6667\n",
      "Epoch 36: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6550 - acc: 0.6660 - val_loss: 0.6014 - val_acc: 0.7250\n",
      "Epoch 37/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6437 - acc: 0.6760\n",
      "Epoch 37: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6430 - acc: 0.6763 - val_loss: 0.6013 - val_acc: 0.7263\n",
      "Epoch 38/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.6756\n",
      "Epoch 38: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6448 - acc: 0.6756 - val_loss: 0.6007 - val_acc: 0.7263\n",
      "Epoch 39/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.6692\n",
      "Epoch 39: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6555 - acc: 0.6692 - val_loss: 0.6017 - val_acc: 0.7258\n",
      "Epoch 40/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.6779\n",
      "Epoch 40: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6447 - acc: 0.6775 - val_loss: 0.6010 - val_acc: 0.7254\n",
      "Epoch 41/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.6749\n",
      "Epoch 41: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6432 - acc: 0.6748 - val_loss: 0.5997 - val_acc: 0.7263\n",
      "Epoch 42/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.6785\n",
      "Epoch 42: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6357 - acc: 0.6791 - val_loss: 0.6005 - val_acc: 0.7271\n",
      "Epoch 43/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6328 - acc: 0.6793\n",
      "Epoch 43: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6328 - acc: 0.6791 - val_loss: 0.5984 - val_acc: 0.7246\n",
      "Epoch 44/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6507 - acc: 0.6713\n",
      "Epoch 44: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6523 - acc: 0.6687 - val_loss: 0.5982 - val_acc: 0.7263\n",
      "Epoch 45/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.6759\n",
      "Epoch 45: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6353 - acc: 0.6755 - val_loss: 0.5982 - val_acc: 0.7267\n",
      "Epoch 46/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6375 - acc: 0.6757\n",
      "Epoch 46: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6369 - acc: 0.6764 - val_loss: 0.5976 - val_acc: 0.7267\n",
      "Epoch 47/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6294 - acc: 0.6812\n",
      "Epoch 47: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6302 - acc: 0.6832 - val_loss: 0.5970 - val_acc: 0.7271\n",
      "Epoch 48/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6275 - acc: 0.6758\n",
      "Epoch 48: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6263 - acc: 0.6764 - val_loss: 0.5964 - val_acc: 0.7293\n",
      "Epoch 49/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.6257 - acc: 0.6760\n",
      "Epoch 49: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6244 - acc: 0.6773 - val_loss: 0.5961 - val_acc: 0.7293\n",
      "Epoch 50/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6220 - acc: 0.6823\n",
      "Epoch 50: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6209 - acc: 0.6829 - val_loss: 0.5952 - val_acc: 0.7293\n",
      "Epoch 51/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6240 - acc: 0.6848\n",
      "Epoch 51: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6246 - acc: 0.6843 - val_loss: 0.5937 - val_acc: 0.7275\n",
      "Epoch 52/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6222 - acc: 0.6845\n",
      "Epoch 52: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6226 - acc: 0.6836 - val_loss: 0.5945 - val_acc: 0.7293\n",
      "Epoch 53/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6307 - acc: 0.6816\n",
      "Epoch 53: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6302 - acc: 0.6819 - val_loss: 0.5943 - val_acc: 0.7288\n",
      "Epoch 54/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6240 - acc: 0.6832\n",
      "Epoch 54: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6244 - acc: 0.6831 - val_loss: 0.5937 - val_acc: 0.7297\n",
      "Epoch 55/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.6851\n",
      "Epoch 55: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6225 - acc: 0.6852 - val_loss: 0.5936 - val_acc: 0.7293\n",
      "Epoch 56/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.6217 - acc: 0.6892\n",
      "Epoch 56: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6222 - acc: 0.6881 - val_loss: 0.5924 - val_acc: 0.7293\n",
      "Epoch 57/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6190 - acc: 0.6856\n",
      "Epoch 57: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6191 - acc: 0.6847 - val_loss: 0.5926 - val_acc: 0.7301\n",
      "Epoch 58/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6146 - acc: 0.6850\n",
      "Epoch 58: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6154 - acc: 0.6848 - val_loss: 0.5918 - val_acc: 0.7301\n",
      "Epoch 59/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6175 - acc: 0.6898\n",
      "Epoch 59: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6186 - acc: 0.6886 - val_loss: 0.5919 - val_acc: 0.7297\n",
      "Epoch 60/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6190 - acc: 0.6826\n",
      "Epoch 60: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6194 - acc: 0.6827 - val_loss: 0.5914 - val_acc: 0.7301\n",
      "Epoch 61/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6171 - acc: 0.6824\n",
      "Epoch 61: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6179 - acc: 0.6821 - val_loss: 0.5910 - val_acc: 0.7301\n",
      "Epoch 62/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6074 - acc: 0.6879\n",
      "Epoch 62: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6074 - acc: 0.6879 - val_loss: 0.5910 - val_acc: 0.7301\n",
      "Epoch 63/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6113 - acc: 0.6902\n",
      "Epoch 63: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6103 - acc: 0.6901 - val_loss: 0.5903 - val_acc: 0.7301\n",
      "Epoch 64/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6150 - acc: 0.6878\n",
      "Epoch 64: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6137 - acc: 0.6882 - val_loss: 0.5904 - val_acc: 0.7318\n",
      "Epoch 65/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6100 - acc: 0.6871\n",
      "Epoch 65: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6100 - acc: 0.6870 - val_loss: 0.5892 - val_acc: 0.7318\n",
      "Epoch 66/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6114 - acc: 0.6917\n",
      "Epoch 66: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6116 - acc: 0.6911 - val_loss: 0.5894 - val_acc: 0.7335\n",
      "Epoch 67/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6071 - acc: 0.6928\n",
      "Epoch 67: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6072 - acc: 0.6927 - val_loss: 0.5883 - val_acc: 0.7335\n",
      "Epoch 68/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6091 - acc: 0.6862\n",
      "Epoch 68: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6093 - acc: 0.6867 - val_loss: 0.5873 - val_acc: 0.7335\n",
      "Epoch 69/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.6919\n",
      "Epoch 69: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6057 - acc: 0.6920 - val_loss: 0.5865 - val_acc: 0.7335\n",
      "Epoch 70/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6116 - acc: 0.6861\n",
      "Epoch 70: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6119 - acc: 0.6859 - val_loss: 0.5861 - val_acc: 0.7335\n",
      "Epoch 71/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.6043 - acc: 0.6937\n",
      "Epoch 71: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6034 - acc: 0.6940 - val_loss: 0.5853 - val_acc: 0.7340\n",
      "Epoch 72/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6066 - acc: 0.6904\n",
      "Epoch 72: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6063 - acc: 0.6898 - val_loss: 0.5854 - val_acc: 0.7340\n",
      "Epoch 73/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6069 - acc: 0.6878\n",
      "Epoch 73: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6078 - acc: 0.6884 - val_loss: 0.5852 - val_acc: 0.7344\n",
      "Epoch 74/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6055 - acc: 0.6923\n",
      "Epoch 74: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6058 - acc: 0.6921 - val_loss: 0.5851 - val_acc: 0.7344\n",
      "Epoch 75/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.6908\n",
      "Epoch 75: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6024 - acc: 0.6907 - val_loss: 0.5842 - val_acc: 0.7344\n",
      "Epoch 76/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5998 - acc: 0.6910\n",
      "Epoch 76: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5975 - acc: 0.6925 - val_loss: 0.5834 - val_acc: 0.7344\n",
      "Epoch 77/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6046 - acc: 0.6902\n",
      "Epoch 77: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6043 - acc: 0.6906 - val_loss: 0.5828 - val_acc: 0.7344\n",
      "Epoch 78/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5991 - acc: 0.6995\n",
      "Epoch 78: val_acc improved from 0.73439 to 0.73482, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6004 - acc: 0.6993 - val_loss: 0.5825 - val_acc: 0.7348\n",
      "Epoch 79/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5997 - acc: 0.6979\n",
      "Epoch 79: val_acc did not improve from 0.73482\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5982 - acc: 0.6974 - val_loss: 0.5815 - val_acc: 0.7348\n",
      "Epoch 80/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.6008 - acc: 0.6928\n",
      "Epoch 80: val_acc did not improve from 0.73482\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6012 - acc: 0.6921 - val_loss: 0.5815 - val_acc: 0.7348\n",
      "Epoch 81/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5950 - acc: 0.6936\n",
      "Epoch 81: val_acc did not improve from 0.73482\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5953 - acc: 0.6937 - val_loss: 0.5810 - val_acc: 0.7348\n",
      "Epoch 82/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5983 - acc: 0.6972\n",
      "Epoch 82: val_acc improved from 0.73482 to 0.73524, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5980 - acc: 0.6974 - val_loss: 0.5806 - val_acc: 0.7352\n",
      "Epoch 83/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5950 - acc: 0.6945\n",
      "Epoch 83: val_acc did not improve from 0.73524\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5961 - acc: 0.6944 - val_loss: 0.5797 - val_acc: 0.7352\n",
      "Epoch 84/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6065 - acc: 0.6978\n",
      "Epoch 84: val_acc did not improve from 0.73524\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6055 - acc: 0.6977 - val_loss: 0.5802 - val_acc: 0.7352\n",
      "Epoch 85/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5990 - acc: 0.6928\n",
      "Epoch 85: val_acc did not improve from 0.73524\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5989 - acc: 0.6928 - val_loss: 0.5800 - val_acc: 0.7352\n",
      "Epoch 86/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5981 - acc: 0.6958\n",
      "Epoch 86: val_acc did not improve from 0.73524\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5982 - acc: 0.6953 - val_loss: 0.5803 - val_acc: 0.7352\n",
      "Epoch 87/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5927 - acc: 0.6962\n",
      "Epoch 87: val_acc did not improve from 0.73524\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5927 - acc: 0.6968 - val_loss: 0.5795 - val_acc: 0.7352\n",
      "Epoch 88/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.6971\n",
      "Epoch 88: val_acc improved from 0.73524 to 0.73567, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5962 - acc: 0.6973 - val_loss: 0.5792 - val_acc: 0.7357\n",
      "Epoch 89/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.6029 - acc: 0.6944\n",
      "Epoch 89: val_acc did not improve from 0.73567\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.6033 - acc: 0.6943 - val_loss: 0.5787 - val_acc: 0.7357\n",
      "Epoch 90/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5914 - acc: 0.7017\n",
      "Epoch 90: val_acc improved from 0.73567 to 0.73610, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5908 - acc: 0.7017 - val_loss: 0.5787 - val_acc: 0.7361\n",
      "Epoch 91/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.6962\n",
      "Epoch 91: val_acc did not improve from 0.73610\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5964 - acc: 0.6956 - val_loss: 0.5778 - val_acc: 0.7361\n",
      "Epoch 92/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5952 - acc: 0.6984\n",
      "Epoch 92: val_acc improved from 0.73610 to 0.73653, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5953 - acc: 0.6981 - val_loss: 0.5780 - val_acc: 0.7365\n",
      "Epoch 93/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.6979\n",
      "Epoch 93: val_acc did not improve from 0.73653\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5923 - acc: 0.6983 - val_loss: 0.5777 - val_acc: 0.7365\n",
      "Epoch 94/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5961 - acc: 0.6943\n",
      "Epoch 94: val_acc did not improve from 0.73653\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5955 - acc: 0.6948 - val_loss: 0.5772 - val_acc: 0.7361\n",
      "Epoch 95/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5949 - acc: 0.7010\n",
      "Epoch 95: val_acc did not improve from 0.73653\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5952 - acc: 0.7000 - val_loss: 0.5766 - val_acc: 0.7361\n",
      "Epoch 96/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5913 - acc: 0.7021\n",
      "Epoch 96: val_acc did not improve from 0.73653\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5924 - acc: 0.7018 - val_loss: 0.5769 - val_acc: 0.7361\n",
      "Epoch 97/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5868 - acc: 0.7035\n",
      "Epoch 97: val_acc did not improve from 0.73653\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5873 - acc: 0.7029 - val_loss: 0.5764 - val_acc: 0.7357\n",
      "Epoch 98/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5881 - acc: 0.7008\n",
      "Epoch 98: val_acc did not improve from 0.73653\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5878 - acc: 0.7016 - val_loss: 0.5757 - val_acc: 0.7357\n",
      "Epoch 99/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5870 - acc: 0.6995\n",
      "Epoch 99: val_acc did not improve from 0.73653\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5865 - acc: 0.7009 - val_loss: 0.5750 - val_acc: 0.7365\n",
      "Epoch 100/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.7024\n",
      "Epoch 100: val_acc improved from 0.73653 to 0.73695, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5904 - acc: 0.7030 - val_loss: 0.5745 - val_acc: 0.7370\n",
      "Epoch 101/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.7012\n",
      "Epoch 101: val_acc did not improve from 0.73695\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5880 - acc: 0.7015 - val_loss: 0.5745 - val_acc: 0.7370\n",
      "Epoch 102/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5891 - acc: 0.7048\n",
      "Epoch 102: val_acc improved from 0.73695 to 0.73738, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5881 - acc: 0.7050 - val_loss: 0.5739 - val_acc: 0.7374\n",
      "Epoch 103/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5941 - acc: 0.6978\n",
      "Epoch 103: val_acc did not improve from 0.73738\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5936 - acc: 0.6986 - val_loss: 0.5741 - val_acc: 0.7370\n",
      "Epoch 104/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.6965\n",
      "Epoch 104: val_acc did not improve from 0.73738\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5899 - acc: 0.6968 - val_loss: 0.5735 - val_acc: 0.7374\n",
      "Epoch 105/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5920 - acc: 0.6996\n",
      "Epoch 105: val_acc did not improve from 0.73738\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5917 - acc: 0.6992 - val_loss: 0.5734 - val_acc: 0.7374\n",
      "Epoch 106/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5907 - acc: 0.6975\n",
      "Epoch 106: val_acc did not improve from 0.73738\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5899 - acc: 0.6983 - val_loss: 0.5734 - val_acc: 0.7374\n",
      "Epoch 107/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5891 - acc: 0.7003\n",
      "Epoch 107: val_acc did not improve from 0.73738\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5897 - acc: 0.6999 - val_loss: 0.5733 - val_acc: 0.7370\n",
      "Epoch 108/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.7010\n",
      "Epoch 108: val_acc improved from 0.73738 to 0.73781, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5898 - acc: 0.7009 - val_loss: 0.5729 - val_acc: 0.7378\n",
      "Epoch 109/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5928 - acc: 0.6994\n",
      "Epoch 109: val_acc did not improve from 0.73781\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5928 - acc: 0.6994 - val_loss: 0.5718 - val_acc: 0.7370\n",
      "Epoch 110/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5915 - acc: 0.7035\n",
      "Epoch 110: val_acc did not improve from 0.73781\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5909 - acc: 0.7037 - val_loss: 0.5714 - val_acc: 0.7378\n",
      "Epoch 111/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5854 - acc: 0.7039\n",
      "Epoch 111: val_acc did not improve from 0.73781\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5849 - acc: 0.7038 - val_loss: 0.5710 - val_acc: 0.7374\n",
      "Epoch 112/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5828 - acc: 0.7039\n",
      "Epoch 112: val_acc did not improve from 0.73781\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5829 - acc: 0.7040 - val_loss: 0.5706 - val_acc: 0.7374\n",
      "Epoch 113/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5902 - acc: 0.7014\n",
      "Epoch 113: val_acc did not improve from 0.73781\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5900 - acc: 0.7018 - val_loss: 0.5701 - val_acc: 0.7378\n",
      "Epoch 114/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.7075\n",
      "Epoch 114: val_acc did not improve from 0.73781\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5788 - acc: 0.7076 - val_loss: 0.5696 - val_acc: 0.7370\n",
      "Epoch 115/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.7083\n",
      "Epoch 115: val_acc improved from 0.73781 to 0.73824, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5819 - acc: 0.7085 - val_loss: 0.5695 - val_acc: 0.7382\n",
      "Epoch 116/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.6968\n",
      "Epoch 116: val_acc did not improve from 0.73824\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5878 - acc: 0.6965 - val_loss: 0.5685 - val_acc: 0.7361\n",
      "Epoch 117/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.7043\n",
      "Epoch 117: val_acc did not improve from 0.73824\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5829 - acc: 0.7044 - val_loss: 0.5690 - val_acc: 0.7382\n",
      "Epoch 118/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.7070\n",
      "Epoch 118: val_acc did not improve from 0.73824\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5769 - acc: 0.7067 - val_loss: 0.5683 - val_acc: 0.7374\n",
      "Epoch 119/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5855 - acc: 0.7028\n",
      "Epoch 119: val_acc improved from 0.73824 to 0.73867, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5857 - acc: 0.7029 - val_loss: 0.5683 - val_acc: 0.7387\n",
      "Epoch 120/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5808 - acc: 0.7077\n",
      "Epoch 120: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5810 - acc: 0.7076 - val_loss: 0.5688 - val_acc: 0.7374\n",
      "Epoch 121/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5858 - acc: 0.7054\n",
      "Epoch 121: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5866 - acc: 0.7054 - val_loss: 0.5688 - val_acc: 0.7374\n",
      "Epoch 122/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5832 - acc: 0.7048\n",
      "Epoch 122: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5829 - acc: 0.7048 - val_loss: 0.5684 - val_acc: 0.7378\n",
      "Epoch 123/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.7075\n",
      "Epoch 123: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5782 - acc: 0.7075 - val_loss: 0.5682 - val_acc: 0.7378\n",
      "Epoch 124/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5836 - acc: 0.7053\n",
      "Epoch 124: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5848 - acc: 0.7053 - val_loss: 0.5681 - val_acc: 0.7382\n",
      "Epoch 125/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5793 - acc: 0.7094\n",
      "Epoch 125: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5799 - acc: 0.7091 - val_loss: 0.5678 - val_acc: 0.7378\n",
      "Epoch 126/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.7092\n",
      "Epoch 126: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5768 - acc: 0.7092 - val_loss: 0.5676 - val_acc: 0.7378\n",
      "Epoch 127/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.7079\n",
      "Epoch 127: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5796 - acc: 0.7075 - val_loss: 0.5668 - val_acc: 0.7378\n",
      "Epoch 128/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.7089\n",
      "Epoch 128: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5784 - acc: 0.7083 - val_loss: 0.5666 - val_acc: 0.7387\n",
      "Epoch 129/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7114\n",
      "Epoch 129: val_acc did not improve from 0.73867\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5741 - acc: 0.7111 - val_loss: 0.5665 - val_acc: 0.7378\n",
      "Epoch 130/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5745 - acc: 0.7105\n",
      "Epoch 130: val_acc improved from 0.73867 to 0.73952, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5753 - acc: 0.7096 - val_loss: 0.5667 - val_acc: 0.7395\n",
      "Epoch 131/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.7069\n",
      "Epoch 131: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5791 - acc: 0.7072 - val_loss: 0.5664 - val_acc: 0.7395\n",
      "Epoch 132/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5793 - acc: 0.7089\n",
      "Epoch 132: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5799 - acc: 0.7081 - val_loss: 0.5663 - val_acc: 0.7391\n",
      "Epoch 133/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5787 - acc: 0.7056\n",
      "Epoch 133: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5800 - acc: 0.7050 - val_loss: 0.5659 - val_acc: 0.7391\n",
      "Epoch 134/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5834 - acc: 0.7015\n",
      "Epoch 134: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5823 - acc: 0.7023 - val_loss: 0.5653 - val_acc: 0.7395\n",
      "Epoch 135/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.7070\n",
      "Epoch 135: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5816 - acc: 0.7066 - val_loss: 0.5649 - val_acc: 0.7395\n",
      "Epoch 136/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5803 - acc: 0.7073\n",
      "Epoch 136: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5802 - acc: 0.7069 - val_loss: 0.5649 - val_acc: 0.7395\n",
      "Epoch 137/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5793 - acc: 0.7075\n",
      "Epoch 137: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5801 - acc: 0.7071 - val_loss: 0.5650 - val_acc: 0.7395\n",
      "Epoch 138/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.7036\n",
      "Epoch 138: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5742 - acc: 0.7035 - val_loss: 0.5645 - val_acc: 0.7391\n",
      "Epoch 139/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5752 - acc: 0.7125\n",
      "Epoch 139: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5755 - acc: 0.7116 - val_loss: 0.5643 - val_acc: 0.7395\n",
      "Epoch 140/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5699 - acc: 0.7119\n",
      "Epoch 140: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5728 - acc: 0.7101 - val_loss: 0.5640 - val_acc: 0.7391\n",
      "Epoch 141/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5730 - acc: 0.7090\n",
      "Epoch 141: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5736 - acc: 0.7080 - val_loss: 0.5631 - val_acc: 0.7395\n",
      "Epoch 142/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5773 - acc: 0.7101\n",
      "Epoch 142: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5776 - acc: 0.7100 - val_loss: 0.5624 - val_acc: 0.7395\n",
      "Epoch 143/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7077\n",
      "Epoch 143: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5744 - acc: 0.7075 - val_loss: 0.5620 - val_acc: 0.7395\n",
      "Epoch 144/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5711 - acc: 0.7119\n",
      "Epoch 144: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5717 - acc: 0.7114 - val_loss: 0.5623 - val_acc: 0.7395\n",
      "Epoch 145/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.7061\n",
      "Epoch 145: val_acc improved from 0.73952 to 0.74038, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5702 - acc: 0.7061 - val_loss: 0.5622 - val_acc: 0.7404\n",
      "Epoch 146/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5733 - acc: 0.7100\n",
      "Epoch 146: val_acc improved from 0.74038 to 0.74123, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5740 - acc: 0.7095 - val_loss: 0.5625 - val_acc: 0.7412\n",
      "Epoch 147/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5748 - acc: 0.7111\n",
      "Epoch 147: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5740 - acc: 0.7112 - val_loss: 0.5618 - val_acc: 0.7412\n",
      "Epoch 148/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5696 - acc: 0.7140\n",
      "Epoch 148: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5696 - acc: 0.7140 - val_loss: 0.5614 - val_acc: 0.7399\n",
      "Epoch 149/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.7083\n",
      "Epoch 149: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5735 - acc: 0.7082 - val_loss: 0.5608 - val_acc: 0.7399\n",
      "Epoch 150/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5777 - acc: 0.7024\n",
      "Epoch 150: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5776 - acc: 0.7024 - val_loss: 0.5610 - val_acc: 0.7404\n",
      "Epoch 151/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.7083\n",
      "Epoch 151: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5731 - acc: 0.7086 - val_loss: 0.5608 - val_acc: 0.7404\n",
      "Epoch 152/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.5736 - acc: 0.7091\n",
      "Epoch 152: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5742 - acc: 0.7099 - val_loss: 0.5605 - val_acc: 0.7404\n",
      "Epoch 153/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.7143\n",
      "Epoch 153: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5663 - acc: 0.7141 - val_loss: 0.5601 - val_acc: 0.7408\n",
      "Epoch 154/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5706 - acc: 0.7079\n",
      "Epoch 154: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5728 - acc: 0.7074 - val_loss: 0.5602 - val_acc: 0.7412\n",
      "Epoch 155/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.7118\n",
      "Epoch 155: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5720 - acc: 0.7126 - val_loss: 0.5598 - val_acc: 0.7408\n",
      "Epoch 156/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5680 - acc: 0.7137\n",
      "Epoch 156: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5683 - acc: 0.7130 - val_loss: 0.5596 - val_acc: 0.7399\n",
      "Epoch 157/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.7151\n",
      "Epoch 157: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5692 - acc: 0.7147 - val_loss: 0.5595 - val_acc: 0.7404\n",
      "Epoch 158/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.7123\n",
      "Epoch 158: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5702 - acc: 0.7123 - val_loss: 0.5590 - val_acc: 0.7408\n",
      "Epoch 159/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.7091\n",
      "Epoch 159: val_acc did not improve from 0.74123\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5738 - acc: 0.7091 - val_loss: 0.5590 - val_acc: 0.7412\n",
      "Epoch 160/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5689 - acc: 0.7108\n",
      "Epoch 160: val_acc improved from 0.74123 to 0.74166, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5694 - acc: 0.7102 - val_loss: 0.5590 - val_acc: 0.7417\n",
      "Epoch 161/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.7125\n",
      "Epoch 161: val_acc did not improve from 0.74166\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5669 - acc: 0.7125 - val_loss: 0.5583 - val_acc: 0.7408\n",
      "Epoch 162/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5715 - acc: 0.7108\n",
      "Epoch 162: val_acc did not improve from 0.74166\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5728 - acc: 0.7101 - val_loss: 0.5581 - val_acc: 0.7417\n",
      "Epoch 163/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.7084\n",
      "Epoch 163: val_acc did not improve from 0.74166\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5719 - acc: 0.7079 - val_loss: 0.5579 - val_acc: 0.7412\n",
      "Epoch 164/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.7127\n",
      "Epoch 164: val_acc did not improve from 0.74166\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5674 - acc: 0.7123 - val_loss: 0.5573 - val_acc: 0.7412\n",
      "Epoch 165/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5708 - acc: 0.7116\n",
      "Epoch 165: val_acc did not improve from 0.74166\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5709 - acc: 0.7108 - val_loss: 0.5572 - val_acc: 0.7417\n",
      "Epoch 166/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5663 - acc: 0.7124\n",
      "Epoch 166: val_acc improved from 0.74166 to 0.74209, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5663 - acc: 0.7124 - val_loss: 0.5571 - val_acc: 0.7421\n",
      "Epoch 167/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.7119\n",
      "Epoch 167: val_acc did not improve from 0.74209\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5689 - acc: 0.7115 - val_loss: 0.5570 - val_acc: 0.7417\n",
      "Epoch 168/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.7169\n",
      "Epoch 168: val_acc did not improve from 0.74209\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5621 - acc: 0.7165 - val_loss: 0.5563 - val_acc: 0.7417\n",
      "Epoch 169/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.7160\n",
      "Epoch 169: val_acc did not improve from 0.74209\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5671 - acc: 0.7159 - val_loss: 0.5560 - val_acc: 0.7417\n",
      "Epoch 170/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5692 - acc: 0.7124\n",
      "Epoch 170: val_acc did not improve from 0.74209\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5700 - acc: 0.7126 - val_loss: 0.5561 - val_acc: 0.7417\n",
      "Epoch 171/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.7153\n",
      "Epoch 171: val_acc did not improve from 0.74209\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5690 - acc: 0.7150 - val_loss: 0.5557 - val_acc: 0.7417\n",
      "Epoch 172/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.7155\n",
      "Epoch 172: val_acc did not improve from 0.74209\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5645 - acc: 0.7155 - val_loss: 0.5559 - val_acc: 0.7421\n",
      "Epoch 173/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.7112\n",
      "Epoch 173: val_acc improved from 0.74209 to 0.74251, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5672 - acc: 0.7107 - val_loss: 0.5556 - val_acc: 0.7425\n",
      "Epoch 174/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5691 - acc: 0.7114\n",
      "Epoch 174: val_acc did not improve from 0.74251\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5690 - acc: 0.7121 - val_loss: 0.5555 - val_acc: 0.7425\n",
      "Epoch 175/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5637 - acc: 0.7149\n",
      "Epoch 175: val_acc improved from 0.74251 to 0.74380, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5645 - acc: 0.7144 - val_loss: 0.5560 - val_acc: 0.7438\n",
      "Epoch 176/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5681 - acc: 0.7147\n",
      "Epoch 176: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5681 - acc: 0.7147 - val_loss: 0.5559 - val_acc: 0.7429\n",
      "Epoch 177/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5721 - acc: 0.7122\n",
      "Epoch 177: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5714 - acc: 0.7126 - val_loss: 0.5557 - val_acc: 0.7425\n",
      "Epoch 178/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5615 - acc: 0.7090\n",
      "Epoch 178: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5628 - acc: 0.7083 - val_loss: 0.5551 - val_acc: 0.7425\n",
      "Epoch 179/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.7092\n",
      "Epoch 179: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5673 - acc: 0.7095 - val_loss: 0.5551 - val_acc: 0.7425\n",
      "Epoch 180/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5694 - acc: 0.7143\n",
      "Epoch 180: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5692 - acc: 0.7141 - val_loss: 0.5551 - val_acc: 0.7425\n",
      "Epoch 181/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.7134\n",
      "Epoch 181: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5705 - acc: 0.7129 - val_loss: 0.5550 - val_acc: 0.7429\n",
      "Epoch 182/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5630 - acc: 0.7153\n",
      "Epoch 182: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5626 - acc: 0.7159 - val_loss: 0.5548 - val_acc: 0.7438\n",
      "Epoch 183/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5697 - acc: 0.7125\n",
      "Epoch 183: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5705 - acc: 0.7117 - val_loss: 0.5543 - val_acc: 0.7429\n",
      "Epoch 184/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5710 - acc: 0.7121\n",
      "Epoch 184: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5700 - acc: 0.7126 - val_loss: 0.5543 - val_acc: 0.7429\n",
      "Epoch 185/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5646 - acc: 0.7110\n",
      "Epoch 185: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5639 - acc: 0.7117 - val_loss: 0.5541 - val_acc: 0.7438\n",
      "Epoch 186/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.7158\n",
      "Epoch 186: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5685 - acc: 0.7157 - val_loss: 0.5542 - val_acc: 0.7438\n",
      "Epoch 187/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5668 - acc: 0.7122\n",
      "Epoch 187: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5666 - acc: 0.7125 - val_loss: 0.5540 - val_acc: 0.7434\n",
      "Epoch 188/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.7134\n",
      "Epoch 188: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5639 - acc: 0.7132 - val_loss: 0.5537 - val_acc: 0.7434\n",
      "Epoch 189/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5699 - acc: 0.7134\n",
      "Epoch 189: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5698 - acc: 0.7133 - val_loss: 0.5537 - val_acc: 0.7434\n",
      "Epoch 190/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5629 - acc: 0.7142\n",
      "Epoch 190: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5629 - acc: 0.7134 - val_loss: 0.5533 - val_acc: 0.7429\n",
      "Epoch 191/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5626 - acc: 0.7127\n",
      "Epoch 191: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5633 - acc: 0.7123 - val_loss: 0.5534 - val_acc: 0.7429\n",
      "Epoch 192/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5636 - acc: 0.7157\n",
      "Epoch 192: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5646 - acc: 0.7142 - val_loss: 0.5532 - val_acc: 0.7434\n",
      "Epoch 193/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5619 - acc: 0.7137\n",
      "Epoch 193: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5626 - acc: 0.7140 - val_loss: 0.5529 - val_acc: 0.7434\n",
      "Epoch 194/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5646 - acc: 0.7149\n",
      "Epoch 194: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5645 - acc: 0.7149 - val_loss: 0.5524 - val_acc: 0.7434\n",
      "Epoch 195/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.7147\n",
      "Epoch 195: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5648 - acc: 0.7149 - val_loss: 0.5520 - val_acc: 0.7434\n",
      "Epoch 196/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5686 - acc: 0.7118\n",
      "Epoch 196: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5697 - acc: 0.7121 - val_loss: 0.5520 - val_acc: 0.7438\n",
      "Epoch 197/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5686 - acc: 0.7133\n",
      "Epoch 197: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5693 - acc: 0.7130 - val_loss: 0.5520 - val_acc: 0.7434\n",
      "Epoch 198/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.7153\n",
      "Epoch 198: val_acc did not improve from 0.74380\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5576 - acc: 0.7157 - val_loss: 0.5514 - val_acc: 0.7438\n",
      "Epoch 199/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.7197\n",
      "Epoch 199: val_acc improved from 0.74380 to 0.74423, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5605 - acc: 0.7194 - val_loss: 0.5514 - val_acc: 0.7442\n",
      "Epoch 200/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5621 - acc: 0.7177\n",
      "Epoch 200: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5631 - acc: 0.7171 - val_loss: 0.5510 - val_acc: 0.7442\n",
      "Epoch 201/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5665 - acc: 0.7118\n",
      "Epoch 201: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5679 - acc: 0.7114 - val_loss: 0.5505 - val_acc: 0.7442\n",
      "Epoch 202/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5602 - acc: 0.7182\n",
      "Epoch 202: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5605 - acc: 0.7181 - val_loss: 0.5504 - val_acc: 0.7442\n",
      "Epoch 203/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.7210\n",
      "Epoch 203: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5595 - acc: 0.7207 - val_loss: 0.5506 - val_acc: 0.7438\n",
      "Epoch 204/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.7154\n",
      "Epoch 204: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5586 - acc: 0.7152 - val_loss: 0.5501 - val_acc: 0.7438\n",
      "Epoch 205/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.7132\n",
      "Epoch 205: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5631 - acc: 0.7132 - val_loss: 0.5500 - val_acc: 0.7442\n",
      "Epoch 206/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5600 - acc: 0.7145\n",
      "Epoch 206: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5600 - acc: 0.7145 - val_loss: 0.5499 - val_acc: 0.7442\n",
      "Epoch 207/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5629 - acc: 0.7138\n",
      "Epoch 207: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5624 - acc: 0.7141 - val_loss: 0.5495 - val_acc: 0.7442\n",
      "Epoch 208/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5666 - acc: 0.7122\n",
      "Epoch 208: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5667 - acc: 0.7124 - val_loss: 0.5498 - val_acc: 0.7442\n",
      "Epoch 209/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.7160\n",
      "Epoch 209: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5662 - acc: 0.7161 - val_loss: 0.5497 - val_acc: 0.7442\n",
      "Epoch 210/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.7182\n",
      "Epoch 210: val_acc improved from 0.74423 to 0.74508, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5631 - acc: 0.7180 - val_loss: 0.5499 - val_acc: 0.7451\n",
      "Epoch 211/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.7145\n",
      "Epoch 211: val_acc did not improve from 0.74508\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5648 - acc: 0.7142 - val_loss: 0.5496 - val_acc: 0.7451\n",
      "Epoch 212/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.7141\n",
      "Epoch 212: val_acc did not improve from 0.74508\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5587 - acc: 0.7148 - val_loss: 0.5492 - val_acc: 0.7447\n",
      "Epoch 213/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5560 - acc: 0.7208\n",
      "Epoch 213: val_acc did not improve from 0.74508\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5560 - acc: 0.7205 - val_loss: 0.5485 - val_acc: 0.7447\n",
      "Epoch 214/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5576 - acc: 0.7155\n",
      "Epoch 214: val_acc did not improve from 0.74508\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5575 - acc: 0.7154 - val_loss: 0.5480 - val_acc: 0.7451\n",
      "Epoch 215/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5568 - acc: 0.7192\n",
      "Epoch 215: val_acc did not improve from 0.74508\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5571 - acc: 0.7195 - val_loss: 0.5481 - val_acc: 0.7451\n",
      "Epoch 216/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5561 - acc: 0.7198\n",
      "Epoch 216: val_acc did not improve from 0.74508\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5566 - acc: 0.7188 - val_loss: 0.5478 - val_acc: 0.7451\n",
      "Epoch 217/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.7171\n",
      "Epoch 217: val_acc improved from 0.74508 to 0.74551, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5540 - acc: 0.7162 - val_loss: 0.5478 - val_acc: 0.7455\n",
      "Epoch 218/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.7170\n",
      "Epoch 218: val_acc improved from 0.74551 to 0.74594, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5575 - acc: 0.7165 - val_loss: 0.5479 - val_acc: 0.7459\n",
      "Epoch 219/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5573 - acc: 0.7177\n",
      "Epoch 219: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5584 - acc: 0.7179 - val_loss: 0.5481 - val_acc: 0.7459\n",
      "Epoch 220/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5576 - acc: 0.7193\n",
      "Epoch 220: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5576 - acc: 0.7193 - val_loss: 0.5475 - val_acc: 0.7459\n",
      "Epoch 221/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5622 - acc: 0.7157\n",
      "Epoch 221: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5602 - acc: 0.7150 - val_loss: 0.5471 - val_acc: 0.7459\n",
      "Epoch 222/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5619 - acc: 0.7165\n",
      "Epoch 222: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5621 - acc: 0.7164 - val_loss: 0.5474 - val_acc: 0.7459\n",
      "Epoch 223/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5592 - acc: 0.7199\n",
      "Epoch 223: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5605 - acc: 0.7190 - val_loss: 0.5476 - val_acc: 0.7459\n",
      "Epoch 224/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.7160\n",
      "Epoch 224: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5609 - acc: 0.7156 - val_loss: 0.5475 - val_acc: 0.7459\n",
      "Epoch 225/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5600 - acc: 0.7202\n",
      "Epoch 225: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5606 - acc: 0.7199 - val_loss: 0.5472 - val_acc: 0.7459\n",
      "Epoch 226/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5561 - acc: 0.7157\n",
      "Epoch 226: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5567 - acc: 0.7152 - val_loss: 0.5465 - val_acc: 0.7459\n",
      "Epoch 227/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5556 - acc: 0.7195\n",
      "Epoch 227: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5552 - acc: 0.7196 - val_loss: 0.5462 - val_acc: 0.7459\n",
      "Epoch 228/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7186\n",
      "Epoch 228: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5523 - acc: 0.7185 - val_loss: 0.5461 - val_acc: 0.7459\n",
      "Epoch 229/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5565 - acc: 0.7188\n",
      "Epoch 229: val_acc improved from 0.74594 to 0.74636, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5567 - acc: 0.7188 - val_loss: 0.5459 - val_acc: 0.7464\n",
      "Epoch 230/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.7151\n",
      "Epoch 230: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5557 - acc: 0.7152 - val_loss: 0.5458 - val_acc: 0.7464\n",
      "Epoch 231/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.7200\n",
      "Epoch 231: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5522 - acc: 0.7205 - val_loss: 0.5455 - val_acc: 0.7464\n",
      "Epoch 232/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.7201\n",
      "Epoch 232: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5530 - acc: 0.7206 - val_loss: 0.5455 - val_acc: 0.7464\n",
      "Epoch 233/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5550 - acc: 0.7180\n",
      "Epoch 233: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5552 - acc: 0.7178 - val_loss: 0.5450 - val_acc: 0.7464\n",
      "Epoch 234/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.7211\n",
      "Epoch 234: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5531 - acc: 0.7209 - val_loss: 0.5448 - val_acc: 0.7464\n",
      "Epoch 235/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.7250\n",
      "Epoch 235: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7252 - val_loss: 0.5449 - val_acc: 0.7464\n",
      "Epoch 236/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5539 - acc: 0.7219\n",
      "Epoch 236: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5551 - acc: 0.7211 - val_loss: 0.5446 - val_acc: 0.7464\n",
      "Epoch 237/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5507 - acc: 0.7215\n",
      "Epoch 237: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5513 - acc: 0.7210 - val_loss: 0.5443 - val_acc: 0.7464\n",
      "Epoch 238/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.7200\n",
      "Epoch 238: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5580 - acc: 0.7199 - val_loss: 0.5444 - val_acc: 0.7464\n",
      "Epoch 239/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5510 - acc: 0.7239\n",
      "Epoch 239: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5513 - acc: 0.7234 - val_loss: 0.5437 - val_acc: 0.7464\n",
      "Epoch 240/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5551 - acc: 0.7215\n",
      "Epoch 240: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5567 - acc: 0.7210 - val_loss: 0.5435 - val_acc: 0.7464\n",
      "Epoch 241/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5506 - acc: 0.7200\n",
      "Epoch 241: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5513 - acc: 0.7199 - val_loss: 0.5434 - val_acc: 0.7464\n",
      "Epoch 242/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5571 - acc: 0.7185\n",
      "Epoch 242: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5580 - acc: 0.7175 - val_loss: 0.5430 - val_acc: 0.7464\n",
      "Epoch 243/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5531 - acc: 0.7249\n",
      "Epoch 243: val_acc did not improve from 0.74636\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5540 - acc: 0.7245 - val_loss: 0.5434 - val_acc: 0.7464\n",
      "Epoch 244/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5517 - acc: 0.7203\n",
      "Epoch 244: val_acc improved from 0.74636 to 0.74679, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7203 - val_loss: 0.5431 - val_acc: 0.7468\n",
      "Epoch 245/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.7172\n",
      "Epoch 245: val_acc did not improve from 0.74679\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5570 - acc: 0.7171 - val_loss: 0.5428 - val_acc: 0.7468\n",
      "Epoch 246/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.7192\n",
      "Epoch 246: val_acc did not improve from 0.74679\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5561 - acc: 0.7194 - val_loss: 0.5432 - val_acc: 0.7468\n",
      "Epoch 247/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.7207\n",
      "Epoch 247: val_acc did not improve from 0.74679\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5517 - acc: 0.7203 - val_loss: 0.5428 - val_acc: 0.7468\n",
      "Epoch 248/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.7191\n",
      "Epoch 248: val_acc did not improve from 0.74679\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5536 - acc: 0.7190 - val_loss: 0.5424 - val_acc: 0.7468\n",
      "Epoch 249/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.7213\n",
      "Epoch 249: val_acc did not improve from 0.74679\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5561 - acc: 0.7211 - val_loss: 0.5426 - val_acc: 0.7468\n",
      "Epoch 250/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7226\n",
      "Epoch 250: val_acc improved from 0.74679 to 0.74765, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5524 - acc: 0.7225 - val_loss: 0.5422 - val_acc: 0.7476\n",
      "Epoch 251/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5515 - acc: 0.7188\n",
      "Epoch 251: val_acc did not improve from 0.74765\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5524 - acc: 0.7187 - val_loss: 0.5420 - val_acc: 0.7476\n",
      "Epoch 252/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5518 - acc: 0.7230\n",
      "Epoch 252: val_acc did not improve from 0.74765\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5532 - acc: 0.7216 - val_loss: 0.5419 - val_acc: 0.7476\n",
      "Epoch 253/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5471 - acc: 0.7236\n",
      "Epoch 253: val_acc did not improve from 0.74765\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5474 - acc: 0.7229 - val_loss: 0.5422 - val_acc: 0.7476\n",
      "Epoch 254/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5535 - acc: 0.7209\n",
      "Epoch 254: val_acc did not improve from 0.74765\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5525 - acc: 0.7206 - val_loss: 0.5420 - val_acc: 0.7476\n",
      "Epoch 255/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5502 - acc: 0.7198\n",
      "Epoch 255: val_acc did not improve from 0.74765\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5505 - acc: 0.7194 - val_loss: 0.5419 - val_acc: 0.7476\n",
      "Epoch 256/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5569 - acc: 0.7188\n",
      "Epoch 256: val_acc improved from 0.74765 to 0.74808, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5566 - acc: 0.7186 - val_loss: 0.5416 - val_acc: 0.7481\n",
      "Epoch 257/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5525 - acc: 0.7198\n",
      "Epoch 257: val_acc did not improve from 0.74808\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5521 - acc: 0.7205 - val_loss: 0.5411 - val_acc: 0.7476\n",
      "Epoch 258/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5554 - acc: 0.7210\n",
      "Epoch 258: val_acc did not improve from 0.74808\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5554 - acc: 0.7210 - val_loss: 0.5413 - val_acc: 0.7476\n",
      "Epoch 259/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7221\n",
      "Epoch 259: val_acc did not improve from 0.74808\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5517 - acc: 0.7211 - val_loss: 0.5412 - val_acc: 0.7481\n",
      "Epoch 260/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.7218\n",
      "Epoch 260: val_acc did not improve from 0.74808\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5526 - acc: 0.7236 - val_loss: 0.5410 - val_acc: 0.7481\n",
      "Epoch 261/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5483 - acc: 0.7211\n",
      "Epoch 261: val_acc did not improve from 0.74808\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5496 - acc: 0.7208 - val_loss: 0.5407 - val_acc: 0.7481\n",
      "Epoch 262/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5470 - acc: 0.7249\n",
      "Epoch 262: val_acc did not improve from 0.74808\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5481 - acc: 0.7235 - val_loss: 0.5401 - val_acc: 0.7481\n",
      "Epoch 263/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5502 - acc: 0.7232\n",
      "Epoch 263: val_acc did not improve from 0.74808\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5519 - acc: 0.7220 - val_loss: 0.5402 - val_acc: 0.7481\n",
      "Epoch 264/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5562 - acc: 0.7179\n",
      "Epoch 264: val_acc improved from 0.74808 to 0.74850, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5566 - acc: 0.7175 - val_loss: 0.5402 - val_acc: 0.7485\n",
      "Epoch 265/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.7256\n",
      "Epoch 265: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5510 - acc: 0.7255 - val_loss: 0.5398 - val_acc: 0.7481\n",
      "Epoch 266/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5512 - acc: 0.7191\n",
      "Epoch 266: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5517 - acc: 0.7196 - val_loss: 0.5397 - val_acc: 0.7485\n",
      "Epoch 267/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5561 - acc: 0.7204\n",
      "Epoch 267: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5562 - acc: 0.7205 - val_loss: 0.5397 - val_acc: 0.7485\n",
      "Epoch 268/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5489 - acc: 0.7209\n",
      "Epoch 268: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5500 - acc: 0.7199 - val_loss: 0.5391 - val_acc: 0.7485\n",
      "Epoch 269/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.7178\n",
      "Epoch 269: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5490 - acc: 0.7179 - val_loss: 0.5390 - val_acc: 0.7485\n",
      "Epoch 270/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5540 - acc: 0.7226\n",
      "Epoch 270: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5541 - acc: 0.7219 - val_loss: 0.5393 - val_acc: 0.7485\n",
      "Epoch 271/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5533 - acc: 0.7198\n",
      "Epoch 271: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5528 - acc: 0.7205 - val_loss: 0.5396 - val_acc: 0.7481\n",
      "Epoch 272/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.7175\n",
      "Epoch 272: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5556 - acc: 0.7174 - val_loss: 0.5397 - val_acc: 0.7485\n",
      "Epoch 273/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.7186\n",
      "Epoch 273: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5508 - acc: 0.7190 - val_loss: 0.5396 - val_acc: 0.7485\n",
      "Epoch 274/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.7248\n",
      "Epoch 274: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5489 - acc: 0.7248 - val_loss: 0.5392 - val_acc: 0.7485\n",
      "Epoch 275/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5503 - acc: 0.7226\n",
      "Epoch 275: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5508 - acc: 0.7218 - val_loss: 0.5393 - val_acc: 0.7485\n",
      "Epoch 276/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5514 - acc: 0.7203\n",
      "Epoch 276: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5501 - acc: 0.7211 - val_loss: 0.5386 - val_acc: 0.7485\n",
      "Epoch 277/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7224\n",
      "Epoch 277: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5456 - acc: 0.7221 - val_loss: 0.5381 - val_acc: 0.7485\n",
      "Epoch 278/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.7237\n",
      "Epoch 278: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5505 - acc: 0.7237 - val_loss: 0.5383 - val_acc: 0.7485\n",
      "Epoch 279/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5515 - acc: 0.7208\n",
      "Epoch 279: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5510 - acc: 0.7212 - val_loss: 0.5381 - val_acc: 0.7485\n",
      "Epoch 280/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5500 - acc: 0.7203\n",
      "Epoch 280: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5514 - acc: 0.7193 - val_loss: 0.5380 - val_acc: 0.7485\n",
      "Epoch 281/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5505 - acc: 0.7237\n",
      "Epoch 281: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5499 - acc: 0.7232 - val_loss: 0.5379 - val_acc: 0.7485\n",
      "Epoch 282/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5445 - acc: 0.7253\n",
      "Epoch 282: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5450 - acc: 0.7245 - val_loss: 0.5378 - val_acc: 0.7485\n",
      "Epoch 283/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5498 - acc: 0.7200\n",
      "Epoch 283: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5496 - acc: 0.7205 - val_loss: 0.5379 - val_acc: 0.7481\n",
      "Epoch 284/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5470 - acc: 0.7252\n",
      "Epoch 284: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5475 - acc: 0.7249 - val_loss: 0.5381 - val_acc: 0.7476\n",
      "Epoch 285/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.7164\n",
      "Epoch 285: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5502 - acc: 0.7161 - val_loss: 0.5377 - val_acc: 0.7476\n",
      "Epoch 286/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5479 - acc: 0.7224\n",
      "Epoch 286: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5488 - acc: 0.7210 - val_loss: 0.5374 - val_acc: 0.7476\n",
      "Epoch 287/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7195\n",
      "Epoch 287: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5506 - acc: 0.7194 - val_loss: 0.5369 - val_acc: 0.7481\n",
      "Epoch 288/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.7178\n",
      "Epoch 288: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5470 - acc: 0.7180 - val_loss: 0.5371 - val_acc: 0.7476\n",
      "Epoch 289/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7180\n",
      "Epoch 289: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5543 - acc: 0.7179 - val_loss: 0.5371 - val_acc: 0.7476\n",
      "Epoch 290/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5498 - acc: 0.7237\n",
      "Epoch 290: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5494 - acc: 0.7233 - val_loss: 0.5372 - val_acc: 0.7481\n",
      "Epoch 291/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5456 - acc: 0.7232\n",
      "Epoch 291: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5464 - acc: 0.7233 - val_loss: 0.5371 - val_acc: 0.7476\n",
      "Epoch 292/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7222\n",
      "Epoch 292: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5499 - acc: 0.7225 - val_loss: 0.5367 - val_acc: 0.7481\n",
      "Epoch 293/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5495 - acc: 0.7249\n",
      "Epoch 293: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5500 - acc: 0.7246 - val_loss: 0.5363 - val_acc: 0.7472\n",
      "Epoch 294/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7200\n",
      "Epoch 294: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5513 - acc: 0.7196 - val_loss: 0.5359 - val_acc: 0.7476\n",
      "Epoch 295/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.7200\n",
      "Epoch 295: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5506 - acc: 0.7203 - val_loss: 0.5363 - val_acc: 0.7472\n",
      "Epoch 296/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5439 - acc: 0.7285\n",
      "Epoch 296: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5442 - acc: 0.7283 - val_loss: 0.5361 - val_acc: 0.7472\n",
      "Epoch 297/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5515 - acc: 0.7230\n",
      "Epoch 297: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5518 - acc: 0.7221 - val_loss: 0.5360 - val_acc: 0.7476\n",
      "Epoch 298/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7236\n",
      "Epoch 298: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5455 - acc: 0.7233 - val_loss: 0.5357 - val_acc: 0.7485\n",
      "Epoch 299/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5426 - acc: 0.7264\n",
      "Epoch 299: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5426 - acc: 0.7258 - val_loss: 0.5354 - val_acc: 0.7485\n",
      "Epoch 300/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.7188\n",
      "Epoch 300: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5535 - acc: 0.7189 - val_loss: 0.5354 - val_acc: 0.7485\n",
      "Epoch 301/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7255\n",
      "Epoch 301: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5433 - acc: 0.7253 - val_loss: 0.5351 - val_acc: 0.7485\n",
      "Epoch 302/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5445 - acc: 0.7228\n",
      "Epoch 302: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5449 - acc: 0.7223 - val_loss: 0.5347 - val_acc: 0.7485\n",
      "Epoch 303/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.7231\n",
      "Epoch 303: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5486 - acc: 0.7231 - val_loss: 0.5353 - val_acc: 0.7485\n",
      "Epoch 304/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5439 - acc: 0.7288\n",
      "Epoch 304: val_acc improved from 0.74850 to 0.74893, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5452 - acc: 0.7280 - val_loss: 0.5350 - val_acc: 0.7489\n",
      "Epoch 305/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.7224\n",
      "Epoch 305: val_acc improved from 0.74893 to 0.74936, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5463 - acc: 0.7219 - val_loss: 0.5346 - val_acc: 0.7494\n",
      "Epoch 306/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.7257\n",
      "Epoch 306: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5432 - acc: 0.7252 - val_loss: 0.5344 - val_acc: 0.7494\n",
      "Epoch 307/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7221\n",
      "Epoch 307: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5455 - acc: 0.7223 - val_loss: 0.5346 - val_acc: 0.7494\n",
      "Epoch 308/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7236\n",
      "Epoch 308: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5459 - acc: 0.7234 - val_loss: 0.5341 - val_acc: 0.7489\n",
      "Epoch 309/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.7254\n",
      "Epoch 309: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5432 - acc: 0.7254 - val_loss: 0.5340 - val_acc: 0.7489\n",
      "Epoch 310/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7269\n",
      "Epoch 310: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5449 - acc: 0.7265 - val_loss: 0.5344 - val_acc: 0.7485\n",
      "Epoch 311/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.7241\n",
      "Epoch 311: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5449 - acc: 0.7241 - val_loss: 0.5342 - val_acc: 0.7485\n",
      "Epoch 312/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.7241\n",
      "Epoch 312: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5470 - acc: 0.7237 - val_loss: 0.5344 - val_acc: 0.7489\n",
      "Epoch 313/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5480 - acc: 0.7285\n",
      "Epoch 313: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5480 - acc: 0.7285 - val_loss: 0.5342 - val_acc: 0.7485\n",
      "Epoch 314/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5438 - acc: 0.7258\n",
      "Epoch 314: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5434 - acc: 0.7267 - val_loss: 0.5339 - val_acc: 0.7485\n",
      "Epoch 315/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.7235\n",
      "Epoch 315: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5448 - acc: 0.7236 - val_loss: 0.5332 - val_acc: 0.7485\n",
      "Epoch 316/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5446 - acc: 0.7259\n",
      "Epoch 316: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5450 - acc: 0.7253 - val_loss: 0.5333 - val_acc: 0.7476\n",
      "Epoch 317/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5407 - acc: 0.7297\n",
      "Epoch 317: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5406 - acc: 0.7294 - val_loss: 0.5330 - val_acc: 0.7481\n",
      "Epoch 318/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5394 - acc: 0.7304\n",
      "Epoch 318: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5396 - acc: 0.7296 - val_loss: 0.5327 - val_acc: 0.7485\n",
      "Epoch 319/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5450 - acc: 0.7264\n",
      "Epoch 319: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5450 - acc: 0.7264 - val_loss: 0.5322 - val_acc: 0.7485\n",
      "Epoch 320/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5412 - acc: 0.7272\n",
      "Epoch 320: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5421 - acc: 0.7263 - val_loss: 0.5323 - val_acc: 0.7481\n",
      "Epoch 321/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7238\n",
      "Epoch 321: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5407 - acc: 0.7237 - val_loss: 0.5322 - val_acc: 0.7485\n",
      "Epoch 322/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7263\n",
      "Epoch 322: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5413 - acc: 0.7269 - val_loss: 0.5318 - val_acc: 0.7481\n",
      "Epoch 323/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7291\n",
      "Epoch 323: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5404 - acc: 0.7286 - val_loss: 0.5321 - val_acc: 0.7489\n",
      "Epoch 324/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5422 - acc: 0.7258\n",
      "Epoch 324: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5422 - acc: 0.7258 - val_loss: 0.5314 - val_acc: 0.7481\n",
      "Epoch 325/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5448 - acc: 0.7201\n",
      "Epoch 325: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5438 - acc: 0.7207 - val_loss: 0.5314 - val_acc: 0.7481\n",
      "Epoch 326/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7265\n",
      "Epoch 326: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5438 - acc: 0.7265 - val_loss: 0.5315 - val_acc: 0.7489\n",
      "Epoch 327/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5472 - acc: 0.7254\n",
      "Epoch 327: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5473 - acc: 0.7254 - val_loss: 0.5315 - val_acc: 0.7489\n",
      "Epoch 328/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5481 - acc: 0.7206\n",
      "Epoch 328: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5494 - acc: 0.7201 - val_loss: 0.5317 - val_acc: 0.7494\n",
      "Epoch 329/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7230\n",
      "Epoch 329: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5453 - acc: 0.7226 - val_loss: 0.5315 - val_acc: 0.7485\n",
      "Epoch 330/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7246\n",
      "Epoch 330: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5459 - acc: 0.7251 - val_loss: 0.5316 - val_acc: 0.7485\n",
      "Epoch 331/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.7273\n",
      "Epoch 331: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5442 - acc: 0.7281 - val_loss: 0.5311 - val_acc: 0.7481\n",
      "Epoch 332/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5421 - acc: 0.7221\n",
      "Epoch 332: val_acc did not improve from 0.74936\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5434 - acc: 0.7211 - val_loss: 0.5309 - val_acc: 0.7485\n",
      "Epoch 333/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.7245\n",
      "Epoch 333: val_acc improved from 0.74936 to 0.75021, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5476 - acc: 0.7251 - val_loss: 0.5311 - val_acc: 0.7502\n",
      "Epoch 334/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5447 - acc: 0.7255\n",
      "Epoch 334: val_acc did not improve from 0.75021\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5447 - acc: 0.7253 - val_loss: 0.5315 - val_acc: 0.7498\n",
      "Epoch 335/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5385 - acc: 0.7308\n",
      "Epoch 335: val_acc did not improve from 0.75021\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5383 - acc: 0.7304 - val_loss: 0.5311 - val_acc: 0.7502\n",
      "Epoch 336/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5386 - acc: 0.7247\n",
      "Epoch 336: val_acc did not improve from 0.75021\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5398 - acc: 0.7236 - val_loss: 0.5308 - val_acc: 0.7502\n",
      "Epoch 337/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5408 - acc: 0.7257\n",
      "Epoch 337: val_acc improved from 0.75021 to 0.75107, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5421 - acc: 0.7257 - val_loss: 0.5310 - val_acc: 0.7511\n",
      "Epoch 338/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.7260\n",
      "Epoch 338: val_acc did not improve from 0.75107\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5388 - acc: 0.7258 - val_loss: 0.5307 - val_acc: 0.7502\n",
      "Epoch 339/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5336 - acc: 0.7300\n",
      "Epoch 339: val_acc did not improve from 0.75107\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5335 - acc: 0.7295 - val_loss: 0.5301 - val_acc: 0.7511\n",
      "Epoch 340/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5406 - acc: 0.7286\n",
      "Epoch 340: val_acc improved from 0.75107 to 0.75150, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5421 - acc: 0.7273 - val_loss: 0.5301 - val_acc: 0.7515\n",
      "Epoch 341/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5412 - acc: 0.7253\n",
      "Epoch 341: val_acc did not improve from 0.75150\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5413 - acc: 0.7247 - val_loss: 0.5300 - val_acc: 0.7515\n",
      "Epoch 342/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.7269\n",
      "Epoch 342: val_acc did not improve from 0.75150\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5381 - acc: 0.7267 - val_loss: 0.5295 - val_acc: 0.7515\n",
      "Epoch 343/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5397 - acc: 0.7289\n",
      "Epoch 343: val_acc did not improve from 0.75150\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5404 - acc: 0.7293 - val_loss: 0.5295 - val_acc: 0.7515\n",
      "Epoch 344/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7250\n",
      "Epoch 344: val_acc did not improve from 0.75150\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5420 - acc: 0.7252 - val_loss: 0.5294 - val_acc: 0.7515\n",
      "Epoch 345/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.7266\n",
      "Epoch 345: val_acc did not improve from 0.75150\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5395 - acc: 0.7266 - val_loss: 0.5294 - val_acc: 0.7515\n",
      "Epoch 346/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5468 - acc: 0.7231\n",
      "Epoch 346: val_acc did not improve from 0.75150\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5472 - acc: 0.7227 - val_loss: 0.5294 - val_acc: 0.7515\n",
      "Epoch 347/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.7253\n",
      "Epoch 347: val_acc did not improve from 0.75150\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5448 - acc: 0.7251 - val_loss: 0.5298 - val_acc: 0.7515\n",
      "Epoch 348/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5414 - acc: 0.7249\n",
      "Epoch 348: val_acc improved from 0.75150 to 0.75192, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5415 - acc: 0.7247 - val_loss: 0.5296 - val_acc: 0.7519\n",
      "Epoch 349/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5408 - acc: 0.7321\n",
      "Epoch 349: val_acc did not improve from 0.75192\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5403 - acc: 0.7316 - val_loss: 0.5294 - val_acc: 0.7519\n",
      "Epoch 350/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7278\n",
      "Epoch 350: val_acc did not improve from 0.75192\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5393 - acc: 0.7279 - val_loss: 0.5291 - val_acc: 0.7519\n",
      "Epoch 351/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.7287\n",
      "Epoch 351: val_acc improved from 0.75192 to 0.75235, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5366 - acc: 0.7282 - val_loss: 0.5292 - val_acc: 0.7524\n",
      "Epoch 352/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5378 - acc: 0.7285\n",
      "Epoch 352: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5384 - acc: 0.7280 - val_loss: 0.5286 - val_acc: 0.7524\n",
      "Epoch 353/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7250\n",
      "Epoch 353: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5416 - acc: 0.7255 - val_loss: 0.5285 - val_acc: 0.7519\n",
      "Epoch 354/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7253\n",
      "Epoch 354: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5420 - acc: 0.7252 - val_loss: 0.5282 - val_acc: 0.7524\n",
      "Epoch 355/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.7283\n",
      "Epoch 355: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5407 - acc: 0.7280 - val_loss: 0.5282 - val_acc: 0.7524\n",
      "Epoch 356/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5395 - acc: 0.7264\n",
      "Epoch 356: val_acc improved from 0.75235 to 0.75278, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5399 - acc: 0.7263 - val_loss: 0.5280 - val_acc: 0.7528\n",
      "Epoch 357/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.7271\n",
      "Epoch 357: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7272 - val_loss: 0.5278 - val_acc: 0.7528\n",
      "Epoch 358/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5418 - acc: 0.7274\n",
      "Epoch 358: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5418 - acc: 0.7276 - val_loss: 0.5282 - val_acc: 0.7528\n",
      "Epoch 359/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7279\n",
      "Epoch 359: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5390 - acc: 0.7276 - val_loss: 0.5277 - val_acc: 0.7524\n",
      "Epoch 360/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7271\n",
      "Epoch 360: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5368 - acc: 0.7269 - val_loss: 0.5271 - val_acc: 0.7524\n",
      "Epoch 361/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5388 - acc: 0.7295\n",
      "Epoch 361: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5400 - acc: 0.7283 - val_loss: 0.5273 - val_acc: 0.7519\n",
      "Epoch 362/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5387 - acc: 0.7302\n",
      "Epoch 362: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5398 - acc: 0.7293 - val_loss: 0.5271 - val_acc: 0.7519\n",
      "Epoch 363/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5322 - acc: 0.7311\n",
      "Epoch 363: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7307 - val_loss: 0.5270 - val_acc: 0.7519\n",
      "Epoch 364/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7270\n",
      "Epoch 364: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5372 - acc: 0.7268 - val_loss: 0.5271 - val_acc: 0.7524\n",
      "Epoch 365/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5378 - acc: 0.7285\n",
      "Epoch 365: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5392 - acc: 0.7276 - val_loss: 0.5269 - val_acc: 0.7524\n",
      "Epoch 366/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5349 - acc: 0.7340\n",
      "Epoch 366: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5348 - acc: 0.7333 - val_loss: 0.5269 - val_acc: 0.7528\n",
      "Epoch 367/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5395 - acc: 0.7248\n",
      "Epoch 367: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5405 - acc: 0.7247 - val_loss: 0.5268 - val_acc: 0.7524\n",
      "Epoch 368/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7300\n",
      "Epoch 368: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5366 - acc: 0.7298 - val_loss: 0.5266 - val_acc: 0.7524\n",
      "Epoch 369/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5364 - acc: 0.7285\n",
      "Epoch 369: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5368 - acc: 0.7282 - val_loss: 0.5264 - val_acc: 0.7519\n",
      "Epoch 370/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5380 - acc: 0.7266\n",
      "Epoch 370: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5383 - acc: 0.7263 - val_loss: 0.5265 - val_acc: 0.7528\n",
      "Epoch 371/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7301\n",
      "Epoch 371: val_acc did not improve from 0.75278\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5304 - acc: 0.7299 - val_loss: 0.5263 - val_acc: 0.7528\n",
      "Epoch 372/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5355 - acc: 0.7289\n",
      "Epoch 372: val_acc improved from 0.75278 to 0.75321, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5366 - acc: 0.7279 - val_loss: 0.5264 - val_acc: 0.7532\n",
      "Epoch 373/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5388 - acc: 0.7293\n",
      "Epoch 373: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5399 - acc: 0.7291 - val_loss: 0.5268 - val_acc: 0.7532\n",
      "Epoch 374/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5393 - acc: 0.7296\n",
      "Epoch 374: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5393 - acc: 0.7296 - val_loss: 0.5267 - val_acc: 0.7532\n",
      "Epoch 375/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7278\n",
      "Epoch 375: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5346 - acc: 0.7276 - val_loss: 0.5263 - val_acc: 0.7532\n",
      "Epoch 376/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5360 - acc: 0.7293\n",
      "Epoch 376: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5352 - acc: 0.7296 - val_loss: 0.5259 - val_acc: 0.7532\n",
      "Epoch 377/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.7316\n",
      "Epoch 377: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5339 - acc: 0.7317 - val_loss: 0.5260 - val_acc: 0.7532\n",
      "Epoch 378/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5413 - acc: 0.7245\n",
      "Epoch 378: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5426 - acc: 0.7242 - val_loss: 0.5263 - val_acc: 0.7532\n",
      "Epoch 379/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7280\n",
      "Epoch 379: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5382 - acc: 0.7277 - val_loss: 0.5264 - val_acc: 0.7532\n",
      "Epoch 380/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5363 - acc: 0.7294\n",
      "Epoch 380: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5378 - acc: 0.7283 - val_loss: 0.5258 - val_acc: 0.7532\n",
      "Epoch 381/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5387 - acc: 0.7282\n",
      "Epoch 381: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5394 - acc: 0.7274 - val_loss: 0.5254 - val_acc: 0.7532\n",
      "Epoch 382/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7261\n",
      "Epoch 382: val_acc did not improve from 0.75321\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5329 - acc: 0.7260 - val_loss: 0.5249 - val_acc: 0.7532\n",
      "Epoch 383/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7279\n",
      "Epoch 383: val_acc improved from 0.75321 to 0.75364, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5377 - acc: 0.7277 - val_loss: 0.5250 - val_acc: 0.7536\n",
      "Epoch 384/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5354 - acc: 0.7264\n",
      "Epoch 384: val_acc did not improve from 0.75364\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5354 - acc: 0.7258 - val_loss: 0.5249 - val_acc: 0.7532\n",
      "Epoch 385/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.7265\n",
      "Epoch 385: val_acc did not improve from 0.75364\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5382 - acc: 0.7263 - val_loss: 0.5248 - val_acc: 0.7532\n",
      "Epoch 386/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5343 - acc: 0.7267\n",
      "Epoch 386: val_acc did not improve from 0.75364\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5339 - acc: 0.7279 - val_loss: 0.5246 - val_acc: 0.7532\n",
      "Epoch 387/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5323 - acc: 0.7327\n",
      "Epoch 387: val_acc did not improve from 0.75364\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5325 - acc: 0.7325 - val_loss: 0.5244 - val_acc: 0.7528\n",
      "Epoch 388/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7283\n",
      "Epoch 388: val_acc did not improve from 0.75364\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7291 - val_loss: 0.5243 - val_acc: 0.7532\n",
      "Epoch 389/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5362 - acc: 0.7276\n",
      "Epoch 389: val_acc did not improve from 0.75364\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5374 - acc: 0.7270 - val_loss: 0.5244 - val_acc: 0.7536\n",
      "Epoch 390/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7229\n",
      "Epoch 390: val_acc improved from 0.75364 to 0.75406, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5359 - acc: 0.7236 - val_loss: 0.5242 - val_acc: 0.7541\n",
      "Epoch 391/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7295\n",
      "Epoch 391: val_acc did not improve from 0.75406\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5338 - acc: 0.7293 - val_loss: 0.5241 - val_acc: 0.7541\n",
      "Epoch 392/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5320 - acc: 0.7258\n",
      "Epoch 392: val_acc did not improve from 0.75406\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7252 - val_loss: 0.5239 - val_acc: 0.7541\n",
      "Epoch 393/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5369 - acc: 0.7303\n",
      "Epoch 393: val_acc did not improve from 0.75406\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5369 - acc: 0.7303 - val_loss: 0.5236 - val_acc: 0.7541\n",
      "Epoch 394/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7297\n",
      "Epoch 394: val_acc did not improve from 0.75406\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5363 - acc: 0.7294 - val_loss: 0.5242 - val_acc: 0.7536\n",
      "Epoch 395/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5344 - acc: 0.7309\n",
      "Epoch 395: val_acc did not improve from 0.75406\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5332 - acc: 0.7315 - val_loss: 0.5238 - val_acc: 0.7536\n",
      "Epoch 396/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5325 - acc: 0.7319\n",
      "Epoch 396: val_acc did not improve from 0.75406\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5328 - acc: 0.7315 - val_loss: 0.5235 - val_acc: 0.7536\n",
      "Epoch 397/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5351 - acc: 0.7293\n",
      "Epoch 397: val_acc did not improve from 0.75406\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5354 - acc: 0.7291 - val_loss: 0.5233 - val_acc: 0.7541\n",
      "Epoch 398/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5298 - acc: 0.7312\n",
      "Epoch 398: val_acc improved from 0.75406 to 0.75449, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5317 - acc: 0.7302 - val_loss: 0.5233 - val_acc: 0.7545\n",
      "Epoch 399/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.7309\n",
      "Epoch 399: val_acc did not improve from 0.75449\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5339 - acc: 0.7312 - val_loss: 0.5232 - val_acc: 0.7545\n",
      "Epoch 400/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5313 - acc: 0.7335\n",
      "Epoch 400: val_acc did not improve from 0.75449\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5315 - acc: 0.7331 - val_loss: 0.5228 - val_acc: 0.7541\n",
      "Epoch 401/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5316 - acc: 0.7354\n",
      "Epoch 401: val_acc did not improve from 0.75449\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5320 - acc: 0.7347 - val_loss: 0.5227 - val_acc: 0.7545\n",
      "Epoch 402/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7295\n",
      "Epoch 402: val_acc did not improve from 0.75449\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5340 - acc: 0.7303 - val_loss: 0.5226 - val_acc: 0.7545\n",
      "Epoch 403/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7258\n",
      "Epoch 403: val_acc did not improve from 0.75449\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5377 - acc: 0.7253 - val_loss: 0.5228 - val_acc: 0.7545\n",
      "Epoch 404/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7345\n",
      "Epoch 404: val_acc improved from 0.75449 to 0.75492, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5323 - acc: 0.7342 - val_loss: 0.5228 - val_acc: 0.7549\n",
      "Epoch 405/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7334\n",
      "Epoch 405: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5342 - acc: 0.7320 - val_loss: 0.5223 - val_acc: 0.7545\n",
      "Epoch 406/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5378 - acc: 0.7247\n",
      "Epoch 406: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5382 - acc: 0.7237 - val_loss: 0.5221 - val_acc: 0.7545\n",
      "Epoch 407/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5313 - acc: 0.7301\n",
      "Epoch 407: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5313 - acc: 0.7301 - val_loss: 0.5219 - val_acc: 0.7545\n",
      "Epoch 408/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.7316\n",
      "Epoch 408: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5335 - acc: 0.7316 - val_loss: 0.5218 - val_acc: 0.7545\n",
      "Epoch 409/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5310 - acc: 0.7331\n",
      "Epoch 409: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - acc: 0.7326 - val_loss: 0.5218 - val_acc: 0.7549\n",
      "Epoch 410/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7312\n",
      "Epoch 410: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5287 - acc: 0.7313 - val_loss: 0.5216 - val_acc: 0.7549\n",
      "Epoch 411/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5343 - acc: 0.7298\n",
      "Epoch 411: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5343 - acc: 0.7298 - val_loss: 0.5218 - val_acc: 0.7549\n",
      "Epoch 412/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7287\n",
      "Epoch 412: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5394 - acc: 0.7283 - val_loss: 0.5215 - val_acc: 0.7549\n",
      "Epoch 413/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5313 - acc: 0.7302\n",
      "Epoch 413: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5313 - acc: 0.7302 - val_loss: 0.5211 - val_acc: 0.7545\n",
      "Epoch 414/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5298 - acc: 0.7320\n",
      "Epoch 414: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5301 - acc: 0.7316 - val_loss: 0.5209 - val_acc: 0.7549\n",
      "Epoch 415/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5347 - acc: 0.7280\n",
      "Epoch 415: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5354 - acc: 0.7277 - val_loss: 0.5211 - val_acc: 0.7549\n",
      "Epoch 416/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.7316\n",
      "Epoch 416: val_acc improved from 0.75492 to 0.75535, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5301 - acc: 0.7315 - val_loss: 0.5211 - val_acc: 0.7553\n",
      "Epoch 417/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5300 - acc: 0.7316\n",
      "Epoch 417: val_acc did not improve from 0.75535\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5315 - acc: 0.7315 - val_loss: 0.5207 - val_acc: 0.7553\n",
      "Epoch 418/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5341 - acc: 0.7269\n",
      "Epoch 418: val_acc did not improve from 0.75535\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5355 - acc: 0.7263 - val_loss: 0.5209 - val_acc: 0.7553\n",
      "Epoch 419/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7348\n",
      "Epoch 419: val_acc did not improve from 0.75535\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5296 - acc: 0.7347 - val_loss: 0.5207 - val_acc: 0.7553\n",
      "Epoch 420/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7303\n",
      "Epoch 420: val_acc improved from 0.75535 to 0.75663, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5330 - acc: 0.7302 - val_loss: 0.5211 - val_acc: 0.7566\n",
      "Epoch 421/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.7336\n",
      "Epoch 421: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5303 - acc: 0.7332 - val_loss: 0.5208 - val_acc: 0.7558\n",
      "Epoch 422/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7310\n",
      "Epoch 422: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5346 - acc: 0.7309 - val_loss: 0.5207 - val_acc: 0.7553\n",
      "Epoch 423/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7313\n",
      "Epoch 423: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5329 - acc: 0.7313 - val_loss: 0.5208 - val_acc: 0.7566\n",
      "Epoch 424/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5324 - acc: 0.7343\n",
      "Epoch 424: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5330 - acc: 0.7334 - val_loss: 0.5203 - val_acc: 0.7566\n",
      "Epoch 425/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7322\n",
      "Epoch 425: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5335 - acc: 0.7325 - val_loss: 0.5202 - val_acc: 0.7558\n",
      "Epoch 426/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5299 - acc: 0.7314\n",
      "Epoch 426: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5300 - acc: 0.7315 - val_loss: 0.5198 - val_acc: 0.7558\n",
      "Epoch 427/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7295\n",
      "Epoch 427: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5282 - acc: 0.7293 - val_loss: 0.5198 - val_acc: 0.7562\n",
      "Epoch 428/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5309 - acc: 0.7301\n",
      "Epoch 428: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5313 - acc: 0.7303 - val_loss: 0.5197 - val_acc: 0.7566\n",
      "Epoch 429/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7309\n",
      "Epoch 429: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5284 - acc: 0.7308 - val_loss: 0.5198 - val_acc: 0.7562\n",
      "Epoch 430/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.7339\n",
      "Epoch 430: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5266 - acc: 0.7342 - val_loss: 0.5195 - val_acc: 0.7562\n",
      "Epoch 431/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7332\n",
      "Epoch 431: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5305 - acc: 0.7324 - val_loss: 0.5197 - val_acc: 0.7562\n",
      "Epoch 432/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5256 - acc: 0.7364\n",
      "Epoch 432: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - acc: 0.7364 - val_loss: 0.5195 - val_acc: 0.7566\n",
      "Epoch 433/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.7309\n",
      "Epoch 433: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5265 - acc: 0.7304 - val_loss: 0.5188 - val_acc: 0.7566\n",
      "Epoch 434/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7335\n",
      "Epoch 434: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5262 - acc: 0.7332 - val_loss: 0.5184 - val_acc: 0.7566\n",
      "Epoch 435/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5324 - acc: 0.7354\n",
      "Epoch 435: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5323 - acc: 0.7354 - val_loss: 0.5186 - val_acc: 0.7566\n",
      "Epoch 436/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5251 - acc: 0.7349\n",
      "Epoch 436: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5254 - acc: 0.7344 - val_loss: 0.5183 - val_acc: 0.7566\n",
      "Epoch 437/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7369\n",
      "Epoch 437: val_acc did not improve from 0.75663\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5297 - acc: 0.7369 - val_loss: 0.5185 - val_acc: 0.7566\n",
      "Epoch 438/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7310\n",
      "Epoch 438: val_acc improved from 0.75663 to 0.75706, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5322 - acc: 0.7309 - val_loss: 0.5183 - val_acc: 0.7571\n",
      "Epoch 439/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5308 - acc: 0.7350\n",
      "Epoch 439: val_acc did not improve from 0.75706\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5299 - acc: 0.7354 - val_loss: 0.5184 - val_acc: 0.7566\n",
      "Epoch 440/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5278 - acc: 0.7350\n",
      "Epoch 440: val_acc improved from 0.75706 to 0.75749, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5290 - acc: 0.7342 - val_loss: 0.5183 - val_acc: 0.7575\n",
      "Epoch 441/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7331\n",
      "Epoch 441: val_acc improved from 0.75749 to 0.75791, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5265 - acc: 0.7330 - val_loss: 0.5183 - val_acc: 0.7579\n",
      "Epoch 442/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5228 - acc: 0.7358\n",
      "Epoch 442: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5241 - acc: 0.7354 - val_loss: 0.5184 - val_acc: 0.7579\n",
      "Epoch 443/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5321 - acc: 0.7307\n",
      "Epoch 443: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5326 - acc: 0.7294 - val_loss: 0.5183 - val_acc: 0.7579\n",
      "Epoch 444/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5308 - acc: 0.7320\n",
      "Epoch 444: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5326 - acc: 0.7315 - val_loss: 0.5182 - val_acc: 0.7575\n",
      "Epoch 445/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7342\n",
      "Epoch 445: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5301 - acc: 0.7332 - val_loss: 0.5183 - val_acc: 0.7579\n",
      "Epoch 446/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.7346\n",
      "Epoch 446: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5320 - acc: 0.7344 - val_loss: 0.5184 - val_acc: 0.7579\n",
      "Epoch 447/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7338\n",
      "Epoch 447: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5282 - acc: 0.7332 - val_loss: 0.5183 - val_acc: 0.7575\n",
      "Epoch 448/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5235 - acc: 0.7354\n",
      "Epoch 448: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5240 - acc: 0.7346 - val_loss: 0.5179 - val_acc: 0.7575\n",
      "Epoch 449/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5293 - acc: 0.7304\n",
      "Epoch 449: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5306 - acc: 0.7294 - val_loss: 0.5178 - val_acc: 0.7571\n",
      "Epoch 450/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.7345\n",
      "Epoch 450: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5271 - acc: 0.7335 - val_loss: 0.5177 - val_acc: 0.7575\n",
      "Epoch 451/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.7386\n",
      "Epoch 451: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - acc: 0.7386 - val_loss: 0.5178 - val_acc: 0.7575\n",
      "Epoch 452/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7331\n",
      "Epoch 452: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5264 - acc: 0.7336 - val_loss: 0.5176 - val_acc: 0.7575\n",
      "Epoch 453/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5253 - acc: 0.7348\n",
      "Epoch 453: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5258 - acc: 0.7348 - val_loss: 0.5175 - val_acc: 0.7575\n",
      "Epoch 454/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7361\n",
      "Epoch 454: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - acc: 0.7360 - val_loss: 0.5174 - val_acc: 0.7571\n",
      "Epoch 455/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5278 - acc: 0.7344\n",
      "Epoch 455: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5263 - acc: 0.7355 - val_loss: 0.5174 - val_acc: 0.7571\n",
      "Epoch 456/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7297\n",
      "Epoch 456: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5313 - acc: 0.7296 - val_loss: 0.5175 - val_acc: 0.7575\n",
      "Epoch 457/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.7306\n",
      "Epoch 457: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5253 - acc: 0.7303 - val_loss: 0.5171 - val_acc: 0.7575\n",
      "Epoch 458/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7368\n",
      "Epoch 458: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5245 - acc: 0.7364 - val_loss: 0.5171 - val_acc: 0.7571\n",
      "Epoch 459/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5276 - acc: 0.7383\n",
      "Epoch 459: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5278 - acc: 0.7388 - val_loss: 0.5170 - val_acc: 0.7575\n",
      "Epoch 460/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.7334\n",
      "Epoch 460: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5310 - acc: 0.7331 - val_loss: 0.5171 - val_acc: 0.7575\n",
      "Epoch 461/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5245 - acc: 0.7340\n",
      "Epoch 461: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5250 - acc: 0.7335 - val_loss: 0.5165 - val_acc: 0.7575\n",
      "Epoch 462/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5212 - acc: 0.7365\n",
      "Epoch 462: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - acc: 0.7365 - val_loss: 0.5162 - val_acc: 0.7571\n",
      "Epoch 463/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5265 - acc: 0.7372\n",
      "Epoch 463: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5265 - acc: 0.7371 - val_loss: 0.5165 - val_acc: 0.7575\n",
      "Epoch 464/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5254 - acc: 0.7342\n",
      "Epoch 464: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5264 - acc: 0.7325 - val_loss: 0.5165 - val_acc: 0.7579\n",
      "Epoch 465/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5268 - acc: 0.7340\n",
      "Epoch 465: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5292 - acc: 0.7330 - val_loss: 0.5163 - val_acc: 0.7575\n",
      "Epoch 466/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.7352\n",
      "Epoch 466: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5250 - acc: 0.7347 - val_loss: 0.5162 - val_acc: 0.7575\n",
      "Epoch 467/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5232 - acc: 0.7357\n",
      "Epoch 467: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5239 - acc: 0.7350 - val_loss: 0.5160 - val_acc: 0.7575\n",
      "Epoch 468/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7350\n",
      "Epoch 468: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5275 - acc: 0.7349 - val_loss: 0.5159 - val_acc: 0.7579\n",
      "Epoch 469/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5267 - acc: 0.7324\n",
      "Epoch 469: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5264 - acc: 0.7315 - val_loss: 0.5162 - val_acc: 0.7575\n",
      "Epoch 470/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.7343\n",
      "Epoch 470: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - acc: 0.7340 - val_loss: 0.5159 - val_acc: 0.7579\n",
      "Epoch 471/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5304 - acc: 0.7346\n",
      "Epoch 471: val_acc did not improve from 0.75791\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5310 - acc: 0.7339 - val_loss: 0.5163 - val_acc: 0.7571\n",
      "Epoch 472/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7348\n",
      "Epoch 472: val_acc improved from 0.75791 to 0.75834, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5300 - acc: 0.7346 - val_loss: 0.5161 - val_acc: 0.7583\n",
      "Epoch 473/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5222 - acc: 0.7341\n",
      "Epoch 473: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - acc: 0.7329 - val_loss: 0.5158 - val_acc: 0.7579\n",
      "Epoch 474/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5311 - acc: 0.7309\n",
      "Epoch 474: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5319 - acc: 0.7300 - val_loss: 0.5159 - val_acc: 0.7575\n",
      "Epoch 475/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7382\n",
      "Epoch 475: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5238 - acc: 0.7381 - val_loss: 0.5159 - val_acc: 0.7571\n",
      "Epoch 476/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5253 - acc: 0.7349\n",
      "Epoch 476: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5259 - acc: 0.7342 - val_loss: 0.5153 - val_acc: 0.7575\n",
      "Epoch 477/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.7359\n",
      "Epoch 477: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5289 - acc: 0.7356 - val_loss: 0.5152 - val_acc: 0.7575\n",
      "Epoch 478/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7338\n",
      "Epoch 478: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5225 - acc: 0.7341 - val_loss: 0.5146 - val_acc: 0.7575\n",
      "Epoch 479/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5257 - acc: 0.7283\n",
      "Epoch 479: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5258 - acc: 0.7295 - val_loss: 0.5148 - val_acc: 0.7575\n",
      "Epoch 480/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7291\n",
      "Epoch 480: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5283 - acc: 0.7294 - val_loss: 0.5146 - val_acc: 0.7571\n",
      "Epoch 481/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5278 - acc: 0.7322\n",
      "Epoch 481: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5282 - acc: 0.7315 - val_loss: 0.5148 - val_acc: 0.7562\n",
      "Epoch 482/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7354\n",
      "Epoch 482: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - acc: 0.7355 - val_loss: 0.5146 - val_acc: 0.7562\n",
      "Epoch 483/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5273 - acc: 0.7335\n",
      "Epoch 483: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5268 - acc: 0.7333 - val_loss: 0.5147 - val_acc: 0.7566\n",
      "Epoch 484/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7362\n",
      "Epoch 484: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - acc: 0.7360 - val_loss: 0.5142 - val_acc: 0.7566\n",
      "Epoch 485/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.7367\n",
      "Epoch 485: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5255 - acc: 0.7363 - val_loss: 0.5142 - val_acc: 0.7566\n",
      "Epoch 486/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7309\n",
      "Epoch 486: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5233 - acc: 0.7320 - val_loss: 0.5140 - val_acc: 0.7571\n",
      "Epoch 487/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5248 - acc: 0.7400\n",
      "Epoch 487: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5248 - acc: 0.7400 - val_loss: 0.5139 - val_acc: 0.7566\n",
      "Epoch 488/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5283 - acc: 0.7329\n",
      "Epoch 488: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5293 - acc: 0.7327 - val_loss: 0.5141 - val_acc: 0.7566\n",
      "Epoch 489/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5203 - acc: 0.7378\n",
      "Epoch 489: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - acc: 0.7375 - val_loss: 0.5138 - val_acc: 0.7566\n",
      "Epoch 490/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7360\n",
      "Epoch 490: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - acc: 0.7359 - val_loss: 0.5138 - val_acc: 0.7566\n",
      "Epoch 491/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5297 - acc: 0.7329\n",
      "Epoch 491: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5296 - acc: 0.7329 - val_loss: 0.5141 - val_acc: 0.7571\n",
      "Epoch 492/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5251 - acc: 0.7345\n",
      "Epoch 492: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5249 - acc: 0.7347 - val_loss: 0.5142 - val_acc: 0.7571\n",
      "Epoch 493/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.7327\n",
      "Epoch 493: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - acc: 0.7327 - val_loss: 0.5141 - val_acc: 0.7571\n",
      "Epoch 494/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7343\n",
      "Epoch 494: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5239 - acc: 0.7342 - val_loss: 0.5140 - val_acc: 0.7575\n",
      "Epoch 495/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7328\n",
      "Epoch 495: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5283 - acc: 0.7338 - val_loss: 0.5138 - val_acc: 0.7571\n",
      "Epoch 496/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.7381\n",
      "Epoch 496: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - acc: 0.7371 - val_loss: 0.5136 - val_acc: 0.7575\n",
      "Epoch 497/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.7369\n",
      "Epoch 497: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5234 - acc: 0.7369 - val_loss: 0.5132 - val_acc: 0.7571\n",
      "Epoch 498/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7401\n",
      "Epoch 498: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5155 - acc: 0.7402 - val_loss: 0.5131 - val_acc: 0.7571\n",
      "Epoch 499/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5210 - acc: 0.7351\n",
      "Epoch 499: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5215 - acc: 0.7345 - val_loss: 0.5130 - val_acc: 0.7579\n",
      "Epoch 500/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.7344\n",
      "Epoch 500: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5268 - acc: 0.7343 - val_loss: 0.5132 - val_acc: 0.7579\n",
      "Epoch 501/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.7370\n",
      "Epoch 501: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - acc: 0.7370 - val_loss: 0.5130 - val_acc: 0.7583\n",
      "Epoch 502/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7328\n",
      "Epoch 502: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - acc: 0.7329 - val_loss: 0.5128 - val_acc: 0.7579\n",
      "Epoch 503/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5213 - acc: 0.7362\n",
      "Epoch 503: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5224 - acc: 0.7350 - val_loss: 0.5126 - val_acc: 0.7575\n",
      "Epoch 504/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5235 - acc: 0.7380\n",
      "Epoch 504: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5243 - acc: 0.7374 - val_loss: 0.5129 - val_acc: 0.7579\n",
      "Epoch 505/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7385\n",
      "Epoch 505: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5235 - acc: 0.7385 - val_loss: 0.5130 - val_acc: 0.7579\n",
      "Epoch 506/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5203 - acc: 0.7382\n",
      "Epoch 506: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - acc: 0.7375 - val_loss: 0.5124 - val_acc: 0.7575\n",
      "Epoch 507/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5245 - acc: 0.7363\n",
      "Epoch 507: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - acc: 0.7340 - val_loss: 0.5127 - val_acc: 0.7575\n",
      "Epoch 508/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.7364\n",
      "Epoch 508: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5246 - acc: 0.7363 - val_loss: 0.5128 - val_acc: 0.7575\n",
      "Epoch 509/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.7339\n",
      "Epoch 509: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5254 - acc: 0.7346 - val_loss: 0.5132 - val_acc: 0.7583\n",
      "Epoch 510/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5235 - acc: 0.7375\n",
      "Epoch 510: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5235 - acc: 0.7369 - val_loss: 0.5126 - val_acc: 0.7575\n",
      "Epoch 511/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.7325\n",
      "Epoch 511: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5252 - acc: 0.7322 - val_loss: 0.5124 - val_acc: 0.7583\n",
      "Epoch 512/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7382\n",
      "Epoch 512: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5192 - acc: 0.7379 - val_loss: 0.5118 - val_acc: 0.7579\n",
      "Epoch 513/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5165 - acc: 0.7354\n",
      "Epoch 513: val_acc did not improve from 0.75834\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5165 - acc: 0.7354 - val_loss: 0.5116 - val_acc: 0.7583\n",
      "Epoch 514/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5214 - acc: 0.7358\n",
      "Epoch 514: val_acc improved from 0.75834 to 0.75877, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5231 - acc: 0.7345 - val_loss: 0.5118 - val_acc: 0.7588\n",
      "Epoch 515/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5210 - acc: 0.7338\n",
      "Epoch 515: val_acc did not improve from 0.75877\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - acc: 0.7340 - val_loss: 0.5117 - val_acc: 0.7588\n",
      "Epoch 516/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.7381\n",
      "Epoch 516: val_acc did not improve from 0.75877\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - acc: 0.7382 - val_loss: 0.5117 - val_acc: 0.7588\n",
      "Epoch 517/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5180 - acc: 0.7383\n",
      "Epoch 517: val_acc did not improve from 0.75877\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - acc: 0.7376 - val_loss: 0.5119 - val_acc: 0.7588\n",
      "Epoch 518/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5215 - acc: 0.7385\n",
      "Epoch 518: val_acc did not improve from 0.75877\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5229 - acc: 0.7379 - val_loss: 0.5117 - val_acc: 0.7588\n",
      "Epoch 519/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.7418\n",
      "Epoch 519: val_acc did not improve from 0.75877\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - acc: 0.7413 - val_loss: 0.5115 - val_acc: 0.7588\n",
      "Epoch 520/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5169 - acc: 0.7404\n",
      "Epoch 520: val_acc did not improve from 0.75877\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5182 - acc: 0.7389 - val_loss: 0.5113 - val_acc: 0.7588\n",
      "Epoch 521/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7356\n",
      "Epoch 521: val_acc did not improve from 0.75877\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5217 - acc: 0.7354 - val_loss: 0.5111 - val_acc: 0.7588\n",
      "Epoch 522/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5212 - acc: 0.7386\n",
      "Epoch 522: val_acc improved from 0.75877 to 0.76005, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - acc: 0.7383 - val_loss: 0.5113 - val_acc: 0.7601\n",
      "Epoch 523/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.7347\n",
      "Epoch 523: val_acc did not improve from 0.76005\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5232 - acc: 0.7343 - val_loss: 0.5116 - val_acc: 0.7601\n",
      "Epoch 524/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5234 - acc: 0.7357\n",
      "Epoch 524: val_acc improved from 0.76005 to 0.76048, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5236 - acc: 0.7356 - val_loss: 0.5118 - val_acc: 0.7605\n",
      "Epoch 525/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5225 - acc: 0.7358\n",
      "Epoch 525: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5234 - acc: 0.7348 - val_loss: 0.5113 - val_acc: 0.7605\n",
      "Epoch 526/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.7358\n",
      "Epoch 526: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - acc: 0.7360 - val_loss: 0.5112 - val_acc: 0.7601\n",
      "Epoch 527/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5233 - acc: 0.7344\n",
      "Epoch 527: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5218 - acc: 0.7357 - val_loss: 0.5112 - val_acc: 0.7605\n",
      "Epoch 528/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7378\n",
      "Epoch 528: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - acc: 0.7381 - val_loss: 0.5107 - val_acc: 0.7601\n",
      "Epoch 529/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5223 - acc: 0.7359\n",
      "Epoch 529: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5224 - acc: 0.7356 - val_loss: 0.5107 - val_acc: 0.7601\n",
      "Epoch 530/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5241 - acc: 0.7317\n",
      "Epoch 530: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - acc: 0.7315 - val_loss: 0.5106 - val_acc: 0.7596\n",
      "Epoch 531/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5256 - acc: 0.7358\n",
      "Epoch 531: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - acc: 0.7358 - val_loss: 0.5105 - val_acc: 0.7601\n",
      "Epoch 532/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5194 - acc: 0.7367\n",
      "Epoch 532: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - acc: 0.7364 - val_loss: 0.5103 - val_acc: 0.7601\n",
      "Epoch 533/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5224 - acc: 0.7314\n",
      "Epoch 533: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5229 - acc: 0.7305 - val_loss: 0.5106 - val_acc: 0.7601\n",
      "Epoch 534/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.7382\n",
      "Epoch 534: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - acc: 0.7382 - val_loss: 0.5103 - val_acc: 0.7596\n",
      "Epoch 535/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7416\n",
      "Epoch 535: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - acc: 0.7414 - val_loss: 0.5104 - val_acc: 0.7601\n",
      "Epoch 536/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5195 - acc: 0.7408\n",
      "Epoch 536: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - acc: 0.7407 - val_loss: 0.5106 - val_acc: 0.7601\n",
      "Epoch 537/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5213 - acc: 0.7372\n",
      "Epoch 537: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5214 - acc: 0.7362 - val_loss: 0.5105 - val_acc: 0.7605\n",
      "Epoch 538/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.7375\n",
      "Epoch 538: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5174 - acc: 0.7372 - val_loss: 0.5103 - val_acc: 0.7601\n",
      "Epoch 539/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5246 - acc: 0.7340\n",
      "Epoch 539: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5251 - acc: 0.7331 - val_loss: 0.5103 - val_acc: 0.7596\n",
      "Epoch 540/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5171 - acc: 0.7395\n",
      "Epoch 540: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5179 - acc: 0.7381 - val_loss: 0.5102 - val_acc: 0.7605\n",
      "Epoch 541/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5150 - acc: 0.7396\n",
      "Epoch 541: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - acc: 0.7392 - val_loss: 0.5100 - val_acc: 0.7601\n",
      "Epoch 542/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5196 - acc: 0.7380\n",
      "Epoch 542: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5203 - acc: 0.7374 - val_loss: 0.5098 - val_acc: 0.7601\n",
      "Epoch 543/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5202 - acc: 0.7395\n",
      "Epoch 543: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - acc: 0.7390 - val_loss: 0.5099 - val_acc: 0.7601\n",
      "Epoch 544/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7404\n",
      "Epoch 544: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - acc: 0.7406 - val_loss: 0.5096 - val_acc: 0.7596\n",
      "Epoch 545/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5224 - acc: 0.7349\n",
      "Epoch 545: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5229 - acc: 0.7343 - val_loss: 0.5096 - val_acc: 0.7592\n",
      "Epoch 546/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.7403\n",
      "Epoch 546: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - acc: 0.7402 - val_loss: 0.5091 - val_acc: 0.7596\n",
      "Epoch 547/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5208 - acc: 0.7337\n",
      "Epoch 547: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - acc: 0.7347 - val_loss: 0.5092 - val_acc: 0.7601\n",
      "Epoch 548/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5179 - acc: 0.7395\n",
      "Epoch 548: val_acc did not improve from 0.76048\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - acc: 0.7395 - val_loss: 0.5091 - val_acc: 0.7601\n",
      "Epoch 549/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5201 - acc: 0.7377\n",
      "Epoch 549: val_acc improved from 0.76048 to 0.76133, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - acc: 0.7358 - val_loss: 0.5091 - val_acc: 0.7613\n",
      "Epoch 550/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5177 - acc: 0.7387\n",
      "Epoch 550: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - acc: 0.7385 - val_loss: 0.5089 - val_acc: 0.7613\n",
      "Epoch 551/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.7389\n",
      "Epoch 551: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - acc: 0.7388 - val_loss: 0.5088 - val_acc: 0.7609\n",
      "Epoch 552/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5202 - acc: 0.7357\n",
      "Epoch 552: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - acc: 0.7349 - val_loss: 0.5089 - val_acc: 0.7601\n",
      "Epoch 553/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5176 - acc: 0.7429\n",
      "Epoch 553: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5173 - acc: 0.7418 - val_loss: 0.5089 - val_acc: 0.7605\n",
      "Epoch 554/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7397\n",
      "Epoch 554: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5127 - acc: 0.7390 - val_loss: 0.5089 - val_acc: 0.7609\n",
      "Epoch 555/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5160 - acc: 0.7417\n",
      "Epoch 555: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5163 - acc: 0.7413 - val_loss: 0.5085 - val_acc: 0.7609\n",
      "Epoch 556/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7390\n",
      "Epoch 556: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5152 - acc: 0.7395 - val_loss: 0.5085 - val_acc: 0.7609\n",
      "Epoch 557/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5175 - acc: 0.7396\n",
      "Epoch 557: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - acc: 0.7394 - val_loss: 0.5082 - val_acc: 0.7609\n",
      "Epoch 558/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5196 - acc: 0.7333\n",
      "Epoch 558: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - acc: 0.7336 - val_loss: 0.5081 - val_acc: 0.7609\n",
      "Epoch 559/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5152 - acc: 0.7385\n",
      "Epoch 559: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - acc: 0.7377 - val_loss: 0.5079 - val_acc: 0.7609\n",
      "Epoch 560/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5198 - acc: 0.7374\n",
      "Epoch 560: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - acc: 0.7375 - val_loss: 0.5081 - val_acc: 0.7609\n",
      "Epoch 561/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5124 - acc: 0.7425\n",
      "Epoch 561: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - acc: 0.7416 - val_loss: 0.5079 - val_acc: 0.7609\n",
      "Epoch 562/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7451\n",
      "Epoch 562: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - acc: 0.7448 - val_loss: 0.5074 - val_acc: 0.7609\n",
      "Epoch 563/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.7358\n",
      "Epoch 563: val_acc improved from 0.76133 to 0.76176, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - acc: 0.7356 - val_loss: 0.5078 - val_acc: 0.7618\n",
      "Epoch 564/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.7338\n",
      "Epoch 564: val_acc did not improve from 0.76176\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - acc: 0.7339 - val_loss: 0.5077 - val_acc: 0.7618\n",
      "Epoch 565/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7401\n",
      "Epoch 565: val_acc did not improve from 0.76176\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - acc: 0.7393 - val_loss: 0.5078 - val_acc: 0.7618\n",
      "Epoch 566/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5136 - acc: 0.7427\n",
      "Epoch 566: val_acc did not improve from 0.76176\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - acc: 0.7427 - val_loss: 0.5075 - val_acc: 0.7618\n",
      "Epoch 567/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5173 - acc: 0.7375\n",
      "Epoch 567: val_acc did not improve from 0.76176\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - acc: 0.7371 - val_loss: 0.5074 - val_acc: 0.7618\n",
      "Epoch 568/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5192 - acc: 0.7384\n",
      "Epoch 568: val_acc improved from 0.76176 to 0.76305, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - acc: 0.7379 - val_loss: 0.5077 - val_acc: 0.7630\n",
      "Epoch 569/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7368\n",
      "Epoch 569: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - acc: 0.7366 - val_loss: 0.5073 - val_acc: 0.7618\n",
      "Epoch 570/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5164 - acc: 0.7388\n",
      "Epoch 570: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5166 - acc: 0.7389 - val_loss: 0.5075 - val_acc: 0.7626\n",
      "Epoch 571/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5105 - acc: 0.7394\n",
      "Epoch 571: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5119 - acc: 0.7373 - val_loss: 0.5075 - val_acc: 0.7626\n",
      "Epoch 572/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5184 - acc: 0.7364\n",
      "Epoch 572: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5189 - acc: 0.7359 - val_loss: 0.5075 - val_acc: 0.7626\n",
      "Epoch 573/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5131 - acc: 0.7428\n",
      "Epoch 573: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5141 - acc: 0.7417 - val_loss: 0.5073 - val_acc: 0.7618\n",
      "Epoch 574/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5183 - acc: 0.7368\n",
      "Epoch 574: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - acc: 0.7369 - val_loss: 0.5072 - val_acc: 0.7618\n",
      "Epoch 575/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5205 - acc: 0.7329\n",
      "Epoch 575: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - acc: 0.7336 - val_loss: 0.5078 - val_acc: 0.7622\n",
      "Epoch 576/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7383\n",
      "Epoch 576: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - acc: 0.7386 - val_loss: 0.5074 - val_acc: 0.7622\n",
      "Epoch 577/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.7403\n",
      "Epoch 577: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - acc: 0.7400 - val_loss: 0.5073 - val_acc: 0.7626\n",
      "Epoch 578/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.7372\n",
      "Epoch 578: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - acc: 0.7370 - val_loss: 0.5070 - val_acc: 0.7622\n",
      "Epoch 579/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.7383\n",
      "Epoch 579: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - acc: 0.7379 - val_loss: 0.5066 - val_acc: 0.7626\n",
      "Epoch 580/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.7433\n",
      "Epoch 580: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - acc: 0.7436 - val_loss: 0.5067 - val_acc: 0.7626\n",
      "Epoch 581/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.7417\n",
      "Epoch 581: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - acc: 0.7416 - val_loss: 0.5066 - val_acc: 0.7626\n",
      "Epoch 582/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.7398\n",
      "Epoch 582: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - acc: 0.7394 - val_loss: 0.5068 - val_acc: 0.7622\n",
      "Epoch 583/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7408\n",
      "Epoch 583: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - acc: 0.7406 - val_loss: 0.5068 - val_acc: 0.7626\n",
      "Epoch 584/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5187 - acc: 0.7392\n",
      "Epoch 584: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5200 - acc: 0.7388 - val_loss: 0.5071 - val_acc: 0.7630\n",
      "Epoch 585/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.7371\n",
      "Epoch 585: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - acc: 0.7375 - val_loss: 0.5069 - val_acc: 0.7626\n",
      "Epoch 586/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7419\n",
      "Epoch 586: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - acc: 0.7413 - val_loss: 0.5065 - val_acc: 0.7622\n",
      "Epoch 587/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5170 - acc: 0.7398\n",
      "Epoch 587: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - acc: 0.7397 - val_loss: 0.5066 - val_acc: 0.7630\n",
      "Epoch 588/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5165 - acc: 0.7457\n",
      "Epoch 588: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5168 - acc: 0.7447 - val_loss: 0.5066 - val_acc: 0.7630\n",
      "Epoch 589/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5142 - acc: 0.7431\n",
      "Epoch 589: val_acc did not improve from 0.76305\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5143 - acc: 0.7429 - val_loss: 0.5065 - val_acc: 0.7630\n",
      "Epoch 590/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5147 - acc: 0.7379\n",
      "Epoch 590: val_acc improved from 0.76305 to 0.76347, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5145 - acc: 0.7376 - val_loss: 0.5061 - val_acc: 0.7635\n",
      "Epoch 591/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.7413\n",
      "Epoch 591: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5152 - acc: 0.7409 - val_loss: 0.5063 - val_acc: 0.7635\n",
      "Epoch 592/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5156 - acc: 0.7394\n",
      "Epoch 592: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - acc: 0.7389 - val_loss: 0.5064 - val_acc: 0.7635\n",
      "Epoch 593/2000\n",
      "271/293 [==========================>...] - ETA: 0s - loss: 0.5137 - acc: 0.7387\n",
      "Epoch 593: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - acc: 0.7402 - val_loss: 0.5061 - val_acc: 0.7635\n",
      "Epoch 594/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5156 - acc: 0.7382\n",
      "Epoch 594: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - acc: 0.7386 - val_loss: 0.5061 - val_acc: 0.7635\n",
      "Epoch 595/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5125 - acc: 0.7416\n",
      "Epoch 595: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - acc: 0.7407 - val_loss: 0.5059 - val_acc: 0.7635\n",
      "Epoch 596/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.7399\n",
      "Epoch 596: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - acc: 0.7396 - val_loss: 0.5055 - val_acc: 0.7630\n",
      "Epoch 597/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5170 - acc: 0.7433\n",
      "Epoch 597: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - acc: 0.7431 - val_loss: 0.5057 - val_acc: 0.7635\n",
      "Epoch 598/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5142 - acc: 0.7405\n",
      "Epoch 598: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - acc: 0.7406 - val_loss: 0.5057 - val_acc: 0.7635\n",
      "Epoch 599/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5129 - acc: 0.7418\n",
      "Epoch 599: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - acc: 0.7416 - val_loss: 0.5053 - val_acc: 0.7630\n",
      "Epoch 600/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7391\n",
      "Epoch 600: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - acc: 0.7394 - val_loss: 0.5054 - val_acc: 0.7635\n",
      "Epoch 601/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.7383\n",
      "Epoch 601: val_acc did not improve from 0.76347\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - acc: 0.7387 - val_loss: 0.5052 - val_acc: 0.7635\n",
      "Epoch 602/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7437\n",
      "Epoch 602: val_acc improved from 0.76347 to 0.76390, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - acc: 0.7434 - val_loss: 0.5050 - val_acc: 0.7639\n",
      "Epoch 603/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.7422\n",
      "Epoch 603: val_acc did not improve from 0.76390\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - acc: 0.7420 - val_loss: 0.5045 - val_acc: 0.7635\n",
      "Epoch 604/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5123 - acc: 0.7384\n",
      "Epoch 604: val_acc did not improve from 0.76390\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - acc: 0.7375 - val_loss: 0.5046 - val_acc: 0.7639\n",
      "Epoch 605/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7437\n",
      "Epoch 605: val_acc did not improve from 0.76390\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - acc: 0.7439 - val_loss: 0.5041 - val_acc: 0.7630\n",
      "Epoch 606/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5126 - acc: 0.7442\n",
      "Epoch 606: val_acc did not improve from 0.76390\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - acc: 0.7438 - val_loss: 0.5043 - val_acc: 0.7639\n",
      "Epoch 607/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.7380\n",
      "Epoch 607: val_acc did not improve from 0.76390\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - acc: 0.7377 - val_loss: 0.5042 - val_acc: 0.7630\n",
      "Epoch 608/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.7421\n",
      "Epoch 608: val_acc did not improve from 0.76390\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5141 - acc: 0.7422 - val_loss: 0.5045 - val_acc: 0.7639\n",
      "Epoch 609/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.7456\n",
      "Epoch 609: val_acc improved from 0.76390 to 0.76433, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5100 - acc: 0.7458 - val_loss: 0.5043 - val_acc: 0.7643\n",
      "Epoch 610/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5133 - acc: 0.7430\n",
      "Epoch 610: val_acc did not improve from 0.76433\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5135 - acc: 0.7429 - val_loss: 0.5041 - val_acc: 0.7639\n",
      "Epoch 611/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5118 - acc: 0.7457\n",
      "Epoch 611: val_acc did not improve from 0.76433\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5127 - acc: 0.7449 - val_loss: 0.5041 - val_acc: 0.7643\n",
      "Epoch 612/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5093 - acc: 0.7425\n",
      "Epoch 612: val_acc did not improve from 0.76433\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5090 - acc: 0.7422 - val_loss: 0.5040 - val_acc: 0.7635\n",
      "Epoch 613/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5131 - acc: 0.7420\n",
      "Epoch 613: val_acc did not improve from 0.76433\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - acc: 0.7408 - val_loss: 0.5040 - val_acc: 0.7639\n",
      "Epoch 614/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.7428\n",
      "Epoch 614: val_acc did not improve from 0.76433\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - acc: 0.7424 - val_loss: 0.5037 - val_acc: 0.7643\n",
      "Epoch 615/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.7419\n",
      "Epoch 615: val_acc improved from 0.76433 to 0.76476, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - acc: 0.7422 - val_loss: 0.5038 - val_acc: 0.7648\n",
      "Epoch 616/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5083 - acc: 0.7433\n",
      "Epoch 616: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - acc: 0.7419 - val_loss: 0.5035 - val_acc: 0.7648\n",
      "Epoch 617/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.7404\n",
      "Epoch 617: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - acc: 0.7400 - val_loss: 0.5035 - val_acc: 0.7643\n",
      "Epoch 618/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7456\n",
      "Epoch 618: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - acc: 0.7451 - val_loss: 0.5037 - val_acc: 0.7648\n",
      "Epoch 619/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5152 - acc: 0.7446\n",
      "Epoch 619: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - acc: 0.7431 - val_loss: 0.5037 - val_acc: 0.7648\n",
      "Epoch 620/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5088 - acc: 0.7436\n",
      "Epoch 620: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - acc: 0.7437 - val_loss: 0.5038 - val_acc: 0.7648\n",
      "Epoch 621/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5116 - acc: 0.7417\n",
      "Epoch 621: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - acc: 0.7417 - val_loss: 0.5037 - val_acc: 0.7648\n",
      "Epoch 622/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5110 - acc: 0.7420\n",
      "Epoch 622: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - acc: 0.7420 - val_loss: 0.5033 - val_acc: 0.7648\n",
      "Epoch 623/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5145 - acc: 0.7396\n",
      "Epoch 623: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - acc: 0.7395 - val_loss: 0.5032 - val_acc: 0.7643\n",
      "Epoch 624/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.7375\n",
      "Epoch 624: val_acc did not improve from 0.76476\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - acc: 0.7369 - val_loss: 0.5031 - val_acc: 0.7648\n",
      "Epoch 625/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.7405\n",
      "Epoch 625: val_acc improved from 0.76476 to 0.76518, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - acc: 0.7401 - val_loss: 0.5028 - val_acc: 0.7652\n",
      "Epoch 626/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5116 - acc: 0.7413\n",
      "Epoch 626: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - acc: 0.7398 - val_loss: 0.5028 - val_acc: 0.7652\n",
      "Epoch 627/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.7419\n",
      "Epoch 627: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5082 - acc: 0.7418 - val_loss: 0.5025 - val_acc: 0.7652\n",
      "Epoch 628/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5093 - acc: 0.7414\n",
      "Epoch 628: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - acc: 0.7411 - val_loss: 0.5022 - val_acc: 0.7648\n",
      "Epoch 629/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5128 - acc: 0.7456\n",
      "Epoch 629: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - acc: 0.7444 - val_loss: 0.5021 - val_acc: 0.7648\n",
      "Epoch 630/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5072 - acc: 0.7461\n",
      "Epoch 630: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5086 - acc: 0.7457 - val_loss: 0.5018 - val_acc: 0.7648\n",
      "Epoch 631/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.7424\n",
      "Epoch 631: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5104 - acc: 0.7424 - val_loss: 0.5017 - val_acc: 0.7652\n",
      "Epoch 632/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.7409\n",
      "Epoch 632: val_acc improved from 0.76518 to 0.76561, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5140 - acc: 0.7406 - val_loss: 0.5020 - val_acc: 0.7656\n",
      "Epoch 633/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7399\n",
      "Epoch 633: val_acc improved from 0.76561 to 0.76604, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5095 - acc: 0.7397 - val_loss: 0.5021 - val_acc: 0.7660\n",
      "Epoch 634/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5139 - acc: 0.7404\n",
      "Epoch 634: val_acc did not improve from 0.76604\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5138 - acc: 0.7398 - val_loss: 0.5021 - val_acc: 0.7660\n",
      "Epoch 635/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.7478\n",
      "Epoch 635: val_acc did not improve from 0.76604\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5075 - acc: 0.7478 - val_loss: 0.5019 - val_acc: 0.7660\n",
      "Epoch 636/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.7432\n",
      "Epoch 636: val_acc improved from 0.76604 to 0.76689, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5085 - acc: 0.7428 - val_loss: 0.5016 - val_acc: 0.7669\n",
      "Epoch 637/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5118 - acc: 0.7451\n",
      "Epoch 637: val_acc did not improve from 0.76689\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - acc: 0.7457 - val_loss: 0.5014 - val_acc: 0.7669\n",
      "Epoch 638/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.7473\n",
      "Epoch 638: val_acc improved from 0.76689 to 0.76775, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5074 - acc: 0.7468 - val_loss: 0.5015 - val_acc: 0.7678\n",
      "Epoch 639/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.7443\n",
      "Epoch 639: val_acc improved from 0.76775 to 0.76903, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - acc: 0.7443 - val_loss: 0.5015 - val_acc: 0.7690\n",
      "Epoch 640/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.7403\n",
      "Epoch 640: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - acc: 0.7400 - val_loss: 0.5013 - val_acc: 0.7669\n",
      "Epoch 641/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5084 - acc: 0.7445\n",
      "Epoch 641: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - acc: 0.7429 - val_loss: 0.5009 - val_acc: 0.7669\n",
      "Epoch 642/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.7390\n",
      "Epoch 642: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - acc: 0.7398 - val_loss: 0.5008 - val_acc: 0.7669\n",
      "Epoch 643/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.7414\n",
      "Epoch 643: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - acc: 0.7418 - val_loss: 0.5008 - val_acc: 0.7669\n",
      "Epoch 644/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5134 - acc: 0.7416\n",
      "Epoch 644: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - acc: 0.7413 - val_loss: 0.5010 - val_acc: 0.7669\n",
      "Epoch 645/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5056 - acc: 0.7440\n",
      "Epoch 645: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5053 - acc: 0.7448 - val_loss: 0.5007 - val_acc: 0.7682\n",
      "Epoch 646/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.7454\n",
      "Epoch 646: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5058 - acc: 0.7456 - val_loss: 0.5002 - val_acc: 0.7673\n",
      "Epoch 647/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.7462\n",
      "Epoch 647: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5080 - acc: 0.7468 - val_loss: 0.4999 - val_acc: 0.7665\n",
      "Epoch 648/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.7450\n",
      "Epoch 648: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5064 - acc: 0.7449 - val_loss: 0.5001 - val_acc: 0.7682\n",
      "Epoch 649/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.7450\n",
      "Epoch 649: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5027 - acc: 0.7452 - val_loss: 0.4998 - val_acc: 0.7678\n",
      "Epoch 650/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.7441\n",
      "Epoch 650: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - acc: 0.7440 - val_loss: 0.4998 - val_acc: 0.7682\n",
      "Epoch 651/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.7406\n",
      "Epoch 651: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - acc: 0.7408 - val_loss: 0.4997 - val_acc: 0.7682\n",
      "Epoch 652/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.7449\n",
      "Epoch 652: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5082 - acc: 0.7448 - val_loss: 0.4995 - val_acc: 0.7678\n",
      "Epoch 653/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5108 - acc: 0.7404\n",
      "Epoch 653: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - acc: 0.7403 - val_loss: 0.4994 - val_acc: 0.7678\n",
      "Epoch 654/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.7411\n",
      "Epoch 654: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5118 - acc: 0.7409 - val_loss: 0.4996 - val_acc: 0.7673\n",
      "Epoch 655/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.7469\n",
      "Epoch 655: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5062 - acc: 0.7467 - val_loss: 0.4993 - val_acc: 0.7673\n",
      "Epoch 656/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5102 - acc: 0.7425\n",
      "Epoch 656: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5103 - acc: 0.7424 - val_loss: 0.4997 - val_acc: 0.7686\n",
      "Epoch 657/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.7446\n",
      "Epoch 657: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5044 - acc: 0.7444 - val_loss: 0.4996 - val_acc: 0.7690\n",
      "Epoch 658/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.7453\n",
      "Epoch 658: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5079 - acc: 0.7453 - val_loss: 0.4993 - val_acc: 0.7682\n",
      "Epoch 659/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5101 - acc: 0.7425\n",
      "Epoch 659: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5112 - acc: 0.7418 - val_loss: 0.4995 - val_acc: 0.7682\n",
      "Epoch 660/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.7452\n",
      "Epoch 660: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - acc: 0.7449 - val_loss: 0.4993 - val_acc: 0.7678\n",
      "Epoch 661/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.7465\n",
      "Epoch 661: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5064 - acc: 0.7463 - val_loss: 0.4993 - val_acc: 0.7682\n",
      "Epoch 662/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.7414\n",
      "Epoch 662: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - acc: 0.7416 - val_loss: 0.4990 - val_acc: 0.7682\n",
      "Epoch 663/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.7445\n",
      "Epoch 663: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - acc: 0.7441 - val_loss: 0.4994 - val_acc: 0.7690\n",
      "Epoch 664/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5042 - acc: 0.7433\n",
      "Epoch 664: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5045 - acc: 0.7432 - val_loss: 0.4992 - val_acc: 0.7682\n",
      "Epoch 665/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5085 - acc: 0.7438\n",
      "Epoch 665: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5081 - acc: 0.7447 - val_loss: 0.4989 - val_acc: 0.7678\n",
      "Epoch 666/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.7463\n",
      "Epoch 666: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5022 - acc: 0.7459 - val_loss: 0.4987 - val_acc: 0.7678\n",
      "Epoch 667/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7440\n",
      "Epoch 667: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - acc: 0.7439 - val_loss: 0.4987 - val_acc: 0.7690\n",
      "Epoch 668/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5051 - acc: 0.7486\n",
      "Epoch 668: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5067 - acc: 0.7476 - val_loss: 0.4986 - val_acc: 0.7690\n",
      "Epoch 669/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5115 - acc: 0.7452\n",
      "Epoch 669: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - acc: 0.7449 - val_loss: 0.4993 - val_acc: 0.7690\n",
      "Epoch 670/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5095 - acc: 0.7459\n",
      "Epoch 670: val_acc did not improve from 0.76903\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - acc: 0.7462 - val_loss: 0.4992 - val_acc: 0.7690\n",
      "Epoch 671/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5061 - acc: 0.7444\n",
      "Epoch 671: val_acc improved from 0.76903 to 0.76946, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5062 - acc: 0.7449 - val_loss: 0.4989 - val_acc: 0.7695\n",
      "Epoch 672/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.7433\n",
      "Epoch 672: val_acc did not improve from 0.76946\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5027 - acc: 0.7426 - val_loss: 0.4986 - val_acc: 0.7695\n",
      "Epoch 673/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.7454\n",
      "Epoch 673: val_acc did not improve from 0.76946\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5039 - acc: 0.7449 - val_loss: 0.4985 - val_acc: 0.7695\n",
      "Epoch 674/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7434\n",
      "Epoch 674: val_acc did not improve from 0.76946\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - acc: 0.7436 - val_loss: 0.4985 - val_acc: 0.7695\n",
      "Epoch 675/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7444\n",
      "Epoch 675: val_acc improved from 0.76946 to 0.76989, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5114 - acc: 0.7439 - val_loss: 0.4988 - val_acc: 0.7699\n",
      "Epoch 676/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5051 - acc: 0.7440\n",
      "Epoch 676: val_acc did not improve from 0.76989\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5051 - acc: 0.7440 - val_loss: 0.4985 - val_acc: 0.7699\n",
      "Epoch 677/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7479\n",
      "Epoch 677: val_acc improved from 0.76989 to 0.77032, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5077 - acc: 0.7478 - val_loss: 0.4987 - val_acc: 0.7703\n",
      "Epoch 678/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5095 - acc: 0.7473\n",
      "Epoch 678: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - acc: 0.7460 - val_loss: 0.4985 - val_acc: 0.7690\n",
      "Epoch 679/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.7423\n",
      "Epoch 679: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5129 - acc: 0.7425 - val_loss: 0.4983 - val_acc: 0.7695\n",
      "Epoch 680/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5119 - acc: 0.7451\n",
      "Epoch 680: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5126 - acc: 0.7450 - val_loss: 0.4983 - val_acc: 0.7690\n",
      "Epoch 681/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.7380\n",
      "Epoch 681: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5118 - acc: 0.7377 - val_loss: 0.4980 - val_acc: 0.7699\n",
      "Epoch 682/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.7437\n",
      "Epoch 682: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5026 - acc: 0.7436 - val_loss: 0.4975 - val_acc: 0.7699\n",
      "Epoch 683/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5049 - acc: 0.7434\n",
      "Epoch 683: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5052 - acc: 0.7427 - val_loss: 0.4975 - val_acc: 0.7699\n",
      "Epoch 684/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.7494\n",
      "Epoch 684: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5082 - acc: 0.7496 - val_loss: 0.4975 - val_acc: 0.7695\n",
      "Epoch 685/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.7439\n",
      "Epoch 685: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5057 - acc: 0.7441 - val_loss: 0.4976 - val_acc: 0.7699\n",
      "Epoch 686/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5036 - acc: 0.7445\n",
      "Epoch 686: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5036 - acc: 0.7445 - val_loss: 0.4976 - val_acc: 0.7699\n",
      "Epoch 687/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7428\n",
      "Epoch 687: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5078 - acc: 0.7428 - val_loss: 0.4975 - val_acc: 0.7695\n",
      "Epoch 688/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.5029 - acc: 0.7455\n",
      "Epoch 688: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5037 - acc: 0.7456 - val_loss: 0.4975 - val_acc: 0.7695\n",
      "Epoch 689/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5055 - acc: 0.7409\n",
      "Epoch 689: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5055 - acc: 0.7409 - val_loss: 0.4972 - val_acc: 0.7695\n",
      "Epoch 690/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.7463\n",
      "Epoch 690: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5063 - acc: 0.7462 - val_loss: 0.4969 - val_acc: 0.7699\n",
      "Epoch 691/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5042 - acc: 0.7528\n",
      "Epoch 691: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5042 - acc: 0.7528 - val_loss: 0.4969 - val_acc: 0.7703\n",
      "Epoch 692/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5034 - acc: 0.7451\n",
      "Epoch 692: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5034 - acc: 0.7451 - val_loss: 0.4968 - val_acc: 0.7703\n",
      "Epoch 693/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.7416\n",
      "Epoch 693: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5080 - acc: 0.7418 - val_loss: 0.4972 - val_acc: 0.7703\n",
      "Epoch 694/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.7468\n",
      "Epoch 694: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5048 - acc: 0.7467 - val_loss: 0.4972 - val_acc: 0.7703\n",
      "Epoch 695/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5038 - acc: 0.7490\n",
      "Epoch 695: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5038 - acc: 0.7489 - val_loss: 0.4971 - val_acc: 0.7703\n",
      "Epoch 696/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5102 - acc: 0.7460\n",
      "Epoch 696: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - acc: 0.7452 - val_loss: 0.4971 - val_acc: 0.7703\n",
      "Epoch 697/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.7412\n",
      "Epoch 697: val_acc improved from 0.77032 to 0.77074, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5047 - acc: 0.7417 - val_loss: 0.4970 - val_acc: 0.7707\n",
      "Epoch 698/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5050 - acc: 0.7487\n",
      "Epoch 698: val_acc did not improve from 0.77074\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5050 - acc: 0.7482 - val_loss: 0.4972 - val_acc: 0.7699\n",
      "Epoch 699/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5099 - acc: 0.7444\n",
      "Epoch 699: val_acc did not improve from 0.77074\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - acc: 0.7438 - val_loss: 0.4970 - val_acc: 0.7699\n",
      "Epoch 700/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5029 - acc: 0.7480\n",
      "Epoch 700: val_acc did not improve from 0.77074\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5034 - acc: 0.7481 - val_loss: 0.4969 - val_acc: 0.7703\n",
      "Epoch 701/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.7454\n",
      "Epoch 701: val_acc did not improve from 0.77074\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5061 - acc: 0.7454 - val_loss: 0.4968 - val_acc: 0.7703\n",
      "Epoch 702/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5073 - acc: 0.7469\n",
      "Epoch 702: val_acc improved from 0.77074 to 0.77117, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5074 - acc: 0.7473 - val_loss: 0.4967 - val_acc: 0.7712\n",
      "Epoch 703/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4991 - acc: 0.7497\n",
      "Epoch 703: val_acc did not improve from 0.77117\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5001 - acc: 0.7487 - val_loss: 0.4963 - val_acc: 0.7712\n",
      "Epoch 704/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5029 - acc: 0.7478\n",
      "Epoch 704: val_acc did not improve from 0.77117\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5024 - acc: 0.7479 - val_loss: 0.4961 - val_acc: 0.7712\n",
      "Epoch 705/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.7472\n",
      "Epoch 705: val_acc did not improve from 0.77117\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5054 - acc: 0.7473 - val_loss: 0.4962 - val_acc: 0.7712\n",
      "Epoch 706/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5049 - acc: 0.7454\n",
      "Epoch 706: val_acc did not improve from 0.77117\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5058 - acc: 0.7444 - val_loss: 0.4965 - val_acc: 0.7703\n",
      "Epoch 707/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.7468\n",
      "Epoch 707: val_acc did not improve from 0.77117\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5007 - acc: 0.7469 - val_loss: 0.4959 - val_acc: 0.7703\n",
      "Epoch 708/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5008 - acc: 0.7483\n",
      "Epoch 708: val_acc did not improve from 0.77117\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5015 - acc: 0.7480 - val_loss: 0.4960 - val_acc: 0.7695\n",
      "Epoch 709/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.7457\n",
      "Epoch 709: val_acc did not improve from 0.77117\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5067 - acc: 0.7451 - val_loss: 0.4961 - val_acc: 0.7703\n",
      "Epoch 710/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5041 - acc: 0.7427\n",
      "Epoch 710: val_acc improved from 0.77117 to 0.77160, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5045 - acc: 0.7424 - val_loss: 0.4959 - val_acc: 0.7716\n",
      "Epoch 711/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5038 - acc: 0.7470\n",
      "Epoch 711: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5061 - acc: 0.7455 - val_loss: 0.4959 - val_acc: 0.7716\n",
      "Epoch 712/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.7441\n",
      "Epoch 712: val_acc improved from 0.77160 to 0.77203, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5063 - acc: 0.7441 - val_loss: 0.4959 - val_acc: 0.7720\n",
      "Epoch 713/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5077 - acc: 0.7410\n",
      "Epoch 713: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5088 - acc: 0.7401 - val_loss: 0.4958 - val_acc: 0.7707\n",
      "Epoch 714/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5018 - acc: 0.7450\n",
      "Epoch 714: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5019 - acc: 0.7451 - val_loss: 0.4956 - val_acc: 0.7712\n",
      "Epoch 715/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5034 - acc: 0.7453\n",
      "Epoch 715: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5033 - acc: 0.7462 - val_loss: 0.4958 - val_acc: 0.7720\n",
      "Epoch 716/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.7526\n",
      "Epoch 716: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4980 - acc: 0.7529 - val_loss: 0.4954 - val_acc: 0.7712\n",
      "Epoch 717/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7495\n",
      "Epoch 717: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5023 - acc: 0.7497 - val_loss: 0.4956 - val_acc: 0.7712\n",
      "Epoch 718/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4964 - acc: 0.7515\n",
      "Epoch 718: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4979 - acc: 0.7505 - val_loss: 0.4954 - val_acc: 0.7712\n",
      "Epoch 719/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5052 - acc: 0.7486\n",
      "Epoch 719: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5060 - acc: 0.7486 - val_loss: 0.4952 - val_acc: 0.7712\n",
      "Epoch 720/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5097 - acc: 0.7467\n",
      "Epoch 720: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - acc: 0.7453 - val_loss: 0.4952 - val_acc: 0.7707\n",
      "Epoch 721/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.7519\n",
      "Epoch 721: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5021 - acc: 0.7514 - val_loss: 0.4952 - val_acc: 0.7707\n",
      "Epoch 722/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.7433\n",
      "Epoch 722: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5047 - acc: 0.7436 - val_loss: 0.4950 - val_acc: 0.7716\n",
      "Epoch 723/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4985 - acc: 0.7494\n",
      "Epoch 723: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5000 - acc: 0.7488 - val_loss: 0.4950 - val_acc: 0.7716\n",
      "Epoch 724/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4987 - acc: 0.7500\n",
      "Epoch 724: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4993 - acc: 0.7489 - val_loss: 0.4949 - val_acc: 0.7720\n",
      "Epoch 725/2000\n",
      "269/293 [==========================>...] - ETA: 0s - loss: 0.4988 - acc: 0.7519\n",
      "Epoch 725: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4987 - acc: 0.7518 - val_loss: 0.4948 - val_acc: 0.7720\n",
      "Epoch 726/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.5032 - acc: 0.7467\n",
      "Epoch 726: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5024 - acc: 0.7470 - val_loss: 0.4946 - val_acc: 0.7720\n",
      "Epoch 727/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.4977 - acc: 0.7509\n",
      "Epoch 727: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4989 - acc: 0.7499 - val_loss: 0.4946 - val_acc: 0.7720\n",
      "Epoch 728/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5100 - acc: 0.7430\n",
      "Epoch 728: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - acc: 0.7431 - val_loss: 0.4949 - val_acc: 0.7716\n",
      "Epoch 729/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.7452\n",
      "Epoch 729: val_acc improved from 0.77203 to 0.77288, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5076 - acc: 0.7450 - val_loss: 0.4945 - val_acc: 0.7729\n",
      "Epoch 730/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5063 - acc: 0.7476\n",
      "Epoch 730: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5069 - acc: 0.7473 - val_loss: 0.4944 - val_acc: 0.7720\n",
      "Epoch 731/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5059 - acc: 0.7475\n",
      "Epoch 731: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5073 - acc: 0.7465 - val_loss: 0.4940 - val_acc: 0.7720\n",
      "Epoch 732/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.7452\n",
      "Epoch 732: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5039 - acc: 0.7451 - val_loss: 0.4945 - val_acc: 0.7716\n",
      "Epoch 733/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.7486\n",
      "Epoch 733: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5052 - acc: 0.7491 - val_loss: 0.4949 - val_acc: 0.7720\n",
      "Epoch 734/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5014 - acc: 0.7418\n",
      "Epoch 734: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5014 - acc: 0.7418 - val_loss: 0.4946 - val_acc: 0.7725\n",
      "Epoch 735/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7472\n",
      "Epoch 735: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5026 - acc: 0.7472 - val_loss: 0.4948 - val_acc: 0.7725\n",
      "Epoch 736/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5073 - acc: 0.7418\n",
      "Epoch 736: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5068 - acc: 0.7417 - val_loss: 0.4945 - val_acc: 0.7720\n",
      "Epoch 737/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.5000 - acc: 0.7517\n",
      "Epoch 737: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5005 - acc: 0.7509 - val_loss: 0.4944 - val_acc: 0.7720\n",
      "Epoch 738/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5014 - acc: 0.7463\n",
      "Epoch 738: val_acc improved from 0.77288 to 0.77331, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5034 - acc: 0.7443 - val_loss: 0.4941 - val_acc: 0.7733\n",
      "Epoch 739/2000\n",
      "276/293 [===========================>..] - ETA: 0s - loss: 0.5053 - acc: 0.7467\n",
      "Epoch 739: val_acc did not improve from 0.77331\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5045 - acc: 0.7471 - val_loss: 0.4937 - val_acc: 0.7729\n",
      "Epoch 740/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4973 - acc: 0.7502\n",
      "Epoch 740: val_acc did not improve from 0.77331\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4976 - acc: 0.7501 - val_loss: 0.4936 - val_acc: 0.7725\n",
      "Epoch 741/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.7480\n",
      "Epoch 741: val_acc did not improve from 0.77331\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5018 - acc: 0.7479 - val_loss: 0.4936 - val_acc: 0.7720\n",
      "Epoch 742/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5011 - acc: 0.7500\n",
      "Epoch 742: val_acc did not improve from 0.77331\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5016 - acc: 0.7497 - val_loss: 0.4936 - val_acc: 0.7720\n",
      "Epoch 743/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5005 - acc: 0.7476\n",
      "Epoch 743: val_acc did not improve from 0.77331\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5005 - acc: 0.7476 - val_loss: 0.4935 - val_acc: 0.7720\n",
      "Epoch 744/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.7470\n",
      "Epoch 744: val_acc did not improve from 0.77331\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4972 - acc: 0.7469 - val_loss: 0.4930 - val_acc: 0.7729\n",
      "Epoch 745/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5016 - acc: 0.7486\n",
      "Epoch 745: val_acc did not improve from 0.77331\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5018 - acc: 0.7474 - val_loss: 0.4932 - val_acc: 0.7733\n",
      "Epoch 746/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.4975 - acc: 0.7528\n",
      "Epoch 746: val_acc improved from 0.77331 to 0.77374, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4987 - acc: 0.7517 - val_loss: 0.4930 - val_acc: 0.7737\n",
      "Epoch 747/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.7513\n",
      "Epoch 747: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4947 - acc: 0.7510 - val_loss: 0.4930 - val_acc: 0.7733\n",
      "Epoch 748/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5012 - acc: 0.7458\n",
      "Epoch 748: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5014 - acc: 0.7457 - val_loss: 0.4930 - val_acc: 0.7729\n",
      "Epoch 749/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5025 - acc: 0.7490\n",
      "Epoch 749: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5026 - acc: 0.7491 - val_loss: 0.4931 - val_acc: 0.7729\n",
      "Epoch 750/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.7489\n",
      "Epoch 750: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4977 - acc: 0.7487 - val_loss: 0.4933 - val_acc: 0.7725\n",
      "Epoch 751/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4929 - acc: 0.7533\n",
      "Epoch 751: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4947 - acc: 0.7518 - val_loss: 0.4932 - val_acc: 0.7725\n",
      "Epoch 752/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.4986 - acc: 0.7500\n",
      "Epoch 752: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4999 - acc: 0.7491 - val_loss: 0.4930 - val_acc: 0.7725\n",
      "Epoch 753/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.7529\n",
      "Epoch 753: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4971 - acc: 0.7533 - val_loss: 0.4926 - val_acc: 0.7725\n",
      "Epoch 754/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5001 - acc: 0.7496\n",
      "Epoch 754: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5010 - acc: 0.7489 - val_loss: 0.4926 - val_acc: 0.7729\n",
      "Epoch 755/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.7442\n",
      "Epoch 755: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5007 - acc: 0.7440 - val_loss: 0.4926 - val_acc: 0.7725\n",
      "Epoch 756/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.4961 - acc: 0.7469\n",
      "Epoch 756: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4963 - acc: 0.7469 - val_loss: 0.4923 - val_acc: 0.7725\n",
      "Epoch 757/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.4985 - acc: 0.7526\n",
      "Epoch 757: val_acc did not improve from 0.77374\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4989 - acc: 0.7515 - val_loss: 0.4925 - val_acc: 0.7725\n",
      "Epoch 758/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.5011 - acc: 0.7484\n",
      "Epoch 758: val_acc improved from 0.77374 to 0.77417, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5019 - acc: 0.7474 - val_loss: 0.4929 - val_acc: 0.7742\n",
      "Epoch 759/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.7464\n",
      "Epoch 759: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4986 - acc: 0.7463 - val_loss: 0.4924 - val_acc: 0.7733\n",
      "Epoch 760/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.7449\n",
      "Epoch 760: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5048 - acc: 0.7445 - val_loss: 0.4927 - val_acc: 0.7737\n",
      "Epoch 761/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.4995 - acc: 0.7483\n",
      "Epoch 761: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5009 - acc: 0.7472 - val_loss: 0.4929 - val_acc: 0.7742\n",
      "Epoch 762/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4940 - acc: 0.7510\n",
      "Epoch 762: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4940 - acc: 0.7510 - val_loss: 0.4924 - val_acc: 0.7737\n",
      "Epoch 763/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.4999 - acc: 0.7445\n",
      "Epoch 763: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5008 - acc: 0.7439 - val_loss: 0.4923 - val_acc: 0.7737\n",
      "Epoch 764/2000\n",
      "275/293 [===========================>..] - ETA: 0s - loss: 0.5028 - acc: 0.7500\n",
      "Epoch 764: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5023 - acc: 0.7497 - val_loss: 0.4924 - val_acc: 0.7742\n",
      "Epoch 765/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.7440\n",
      "Epoch 765: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5010 - acc: 0.7439 - val_loss: 0.4922 - val_acc: 0.7725\n",
      "Epoch 766/2000\n",
      "278/293 [===========================>..] - ETA: 0s - loss: 0.4985 - acc: 0.7491\n",
      "Epoch 766: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4990 - acc: 0.7484 - val_loss: 0.4919 - val_acc: 0.7725\n",
      "Epoch 767/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.4975 - acc: 0.7469\n",
      "Epoch 767: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4983 - acc: 0.7460 - val_loss: 0.4917 - val_acc: 0.7725\n",
      "Epoch 768/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4960 - acc: 0.7530\n",
      "Epoch 768: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4967 - acc: 0.7519 - val_loss: 0.4914 - val_acc: 0.7725\n",
      "Epoch 769/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.7461\n",
      "Epoch 769: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4988 - acc: 0.7460 - val_loss: 0.4916 - val_acc: 0.7737\n",
      "Epoch 770/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.7501\n",
      "Epoch 770: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5002 - acc: 0.7499 - val_loss: 0.4919 - val_acc: 0.7742\n",
      "Epoch 771/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4996 - acc: 0.7507\n",
      "Epoch 771: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5000 - acc: 0.7501 - val_loss: 0.4916 - val_acc: 0.7742\n",
      "Epoch 772/2000\n",
      "272/293 [==========================>...] - ETA: 0s - loss: 0.5004 - acc: 0.7484\n",
      "Epoch 772: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5001 - acc: 0.7475 - val_loss: 0.4914 - val_acc: 0.7742\n",
      "Epoch 773/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.4960 - acc: 0.7500\n",
      "Epoch 773: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4960 - acc: 0.7500 - val_loss: 0.4912 - val_acc: 0.7733\n",
      "Epoch 774/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.7501\n",
      "Epoch 774: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4999 - acc: 0.7505 - val_loss: 0.4910 - val_acc: 0.7733\n",
      "Epoch 775/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.7497\n",
      "Epoch 775: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4995 - acc: 0.7496 - val_loss: 0.4912 - val_acc: 0.7742\n",
      "Epoch 776/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.7446\n",
      "Epoch 776: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4998 - acc: 0.7447 - val_loss: 0.4913 - val_acc: 0.7737\n",
      "Epoch 777/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5007 - acc: 0.7469\n",
      "Epoch 777: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5013 - acc: 0.7471 - val_loss: 0.4914 - val_acc: 0.7737\n",
      "Epoch 778/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.4955 - acc: 0.7516\n",
      "Epoch 778: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4961 - acc: 0.7509 - val_loss: 0.4911 - val_acc: 0.7733\n",
      "Epoch 779/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4990 - acc: 0.7530\n",
      "Epoch 779: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4989 - acc: 0.7531 - val_loss: 0.4909 - val_acc: 0.7737\n",
      "Epoch 780/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4996 - acc: 0.7479\n",
      "Epoch 780: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4991 - acc: 0.7482 - val_loss: 0.4908 - val_acc: 0.7733\n",
      "Epoch 781/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4998 - acc: 0.7486\n",
      "Epoch 781: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5008 - acc: 0.7476 - val_loss: 0.4907 - val_acc: 0.7733\n",
      "Epoch 782/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7473\n",
      "Epoch 782: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5022 - acc: 0.7476 - val_loss: 0.4908 - val_acc: 0.7742\n",
      "Epoch 783/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.7479\n",
      "Epoch 783: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4948 - acc: 0.7479 - val_loss: 0.4908 - val_acc: 0.7737\n",
      "Epoch 784/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4997 - acc: 0.7482\n",
      "Epoch 784: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4998 - acc: 0.7482 - val_loss: 0.4906 - val_acc: 0.7742\n",
      "Epoch 785/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.4966 - acc: 0.7482\n",
      "Epoch 785: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4987 - acc: 0.7469 - val_loss: 0.4905 - val_acc: 0.7737\n",
      "Epoch 786/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.7461\n",
      "Epoch 786: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4962 - acc: 0.7458 - val_loss: 0.4907 - val_acc: 0.7737\n",
      "Epoch 787/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.7529\n",
      "Epoch 787: val_acc did not improve from 0.77417\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4938 - acc: 0.7530 - val_loss: 0.4908 - val_acc: 0.7733\n",
      "Epoch 788/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.7478\n",
      "Epoch 788: val_acc improved from 0.77417 to 0.77459, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4982 - acc: 0.7478 - val_loss: 0.4903 - val_acc: 0.7746\n",
      "Epoch 789/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.7468\n",
      "Epoch 789: val_acc improved from 0.77459 to 0.77502, saving model to train_logs/logs7/RWB_ANN_512_256_Adagrad\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5005 - acc: 0.7465 - val_loss: 0.4904 - val_acc: 0.7750\n",
      "Epoch 790/2000\n",
      "277/293 [===========================>..] - ETA: 0s - loss: 0.4940 - acc: 0.7498\n",
      "Epoch 790: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4942 - acc: 0.7485 - val_loss: 0.4902 - val_acc: 0.7742\n",
      "Epoch 791/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.7503\n",
      "Epoch 791: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4986 - acc: 0.7502 - val_loss: 0.4899 - val_acc: 0.7742\n",
      "Epoch 792/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4988 - acc: 0.7478\n",
      "Epoch 792: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4986 - acc: 0.7478 - val_loss: 0.4899 - val_acc: 0.7746\n",
      "Epoch 793/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.4973 - acc: 0.7481\n",
      "Epoch 793: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4974 - acc: 0.7484 - val_loss: 0.4897 - val_acc: 0.7742\n",
      "Epoch 794/2000\n",
      "270/293 [==========================>...] - ETA: 0s - loss: 0.5023 - acc: 0.7491\n",
      "Epoch 794: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.5032 - acc: 0.7476 - val_loss: 0.4898 - val_acc: 0.7737\n",
      "Epoch 795/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.7491\n",
      "Epoch 795: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4977 - acc: 0.7490 - val_loss: 0.4896 - val_acc: 0.7742\n",
      "Epoch 796/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.7483\n",
      "Epoch 796: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4977 - acc: 0.7484 - val_loss: 0.4896 - val_acc: 0.7737\n",
      "Epoch 797/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4948 - acc: 0.7498\n",
      "Epoch 797: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4946 - acc: 0.7498 - val_loss: 0.4898 - val_acc: 0.7746\n",
      "Epoch 798/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.7530\n",
      "Epoch 798: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4945 - acc: 0.7522 - val_loss: 0.4899 - val_acc: 0.7750\n",
      "Epoch 799/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.7496\n",
      "Epoch 799: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4964 - acc: 0.7495 - val_loss: 0.4894 - val_acc: 0.7737\n",
      "Epoch 800/2000\n",
      "274/293 [===========================>..] - ETA: 0s - loss: 0.4965 - acc: 0.7502\n",
      "Epoch 800: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4981 - acc: 0.7494 - val_loss: 0.4894 - val_acc: 0.7737\n",
      "Epoch 801/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4956 - acc: 0.7494\n",
      "Epoch 801: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4960 - acc: 0.7490 - val_loss: 0.4896 - val_acc: 0.7746\n",
      "Epoch 802/2000\n",
      "273/293 [==========================>...] - ETA: 0s - loss: 0.4958 - acc: 0.7522\n",
      "Epoch 802: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4965 - acc: 0.7526 - val_loss: 0.4894 - val_acc: 0.7746\n",
      "Epoch 803/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4928 - acc: 0.7524\n",
      "Epoch 803: val_acc did not improve from 0.77502\n",
      "293/293 [==============================] - 1s 3ms/step - loss: 0.4927 - acc: 0.7522 - val_loss: 0.4893 - val_acc: 0.7742\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4995 - acc: 0.7562\n"
     ]
    }
   ],
   "source": [
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=range(1, 6))\n",
    "    training_time = pd.DataFrame(index=lags, columns=[f'CPU_Time_{i}' for i in range(1, 6)] + [f'Wall_Time_{i}' for i in range(1, 6)])\n",
    "\n",
    "    for lag in lags:\n",
    "        train_temp_dir = train_dir + '_' + str(lag)\n",
    "        train = tf.data.Dataset.load(train_temp_dir)\n",
    "        flattened_train = train.unbatch()\n",
    "\n",
    "        train_data = list(flattened_train.as_numpy_iterator())\n",
    "        train_size = len(list(train_data))\n",
    "\n",
    "        kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_data), 1):\n",
    "            train_fold_data = ([train_data[i][0] for i in train_index], [train_data[i][1] for i in train_index])\n",
    "            val_fold_data = ([train_data[i][0] for i in val_index], [train_data[i][1] for i in val_index])\n",
    "\n",
    "            train_fold = tf.data.Dataset.from_tensor_slices(train_fold_data).batch(32)\n",
    "            val_fold = tf.data.Dataset.from_tensor_slices(val_fold_data).batch(32)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adagrad(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_fold, epochs=epochs, validation_data=val_fold, callbacks=myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "\n",
    "            training_time.loc[lag, f'CPU_Time_{fold}'] = cpu_time\n",
    "            training_time.loc[lag, f'Wall_Time_{fold}'] = wall_time\n",
    "\n",
    "            recap.loc[lag, fold] = history.history['acc'][-1]\n",
    "\n",
    "\n",
    "    # Evaluate on the test dataset after cross-validation\n",
    "    test_temp_dir = test_dir + '_' + str(lag)\n",
    "    test_ds = tf.data.Dataset.load(test_temp_dir)\n",
    "    results = model.evaluate(test_ds, callbacks=myCallbacks(log_path))\n",
    "\n",
    "    recap[f'test_{lag}'] = results[1]\n",
    "\n",
    "    log_recap_dir = os.path.join(log_dir, 'Recap')\n",
    "    if not os.path.exists(log_recap_dir):\n",
    "        os.makedirs(log_recap_dir)\n",
    "\n",
    "    recap.to_csv(os.path.join(log_recap_dir, 'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_recap_dir, 'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "LAG = 256\n",
    "\n",
    "test_dir = f\"datasets/tf_batch/RWB/segment_1 seconds/test_{LAG}\"\n",
    "test_ds = tf.data.Dataset.load(test_dir)\n",
    "model_dir = [f\"train_logs/logs7/RWB_ANN_512_256_Adagrad/{i}/{LAG}/best_model.h5\" for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_test = test_ds.unbatch()\n",
    "test_data = list(flattened_test.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_value = np.array([test_data[i][0] for i in range(len(test_data))])\n",
    "test_data_label = np.array([test_data[i][1] for i in range(len(test_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 96)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_value.reshape(test_data_value.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s 2ms/step - loss: 0.5381 - acc: 0.7404\n",
      "0.5380551815032959 0.7404240965843201\n",
      "92/92 [==============================] - 0s 2ms/step\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.5221 - acc: 0.7507\n",
      "0.5221185088157654 0.7506840229034424\n",
      "92/92 [==============================] - 0s 1ms/step\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.5368 - acc: 0.7373\n",
      "0.5368096828460693 0.7373461127281189\n",
      "92/92 [==============================] - 0s 1ms/step\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4887 - acc: 0.7722\n",
      "0.48868995904922485 0.7722298502922058\n",
      "92/92 [==============================] - 0s 2ms/step\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.4790 - acc: 0.7698\n",
      "0.47900891304016113 0.7698358297348022\n",
      "92/92 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, model_path in enumerate(model_dir):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(loss, acc)\n",
    "    pred = model.predict(test_data_value.reshape(test_data_value.shape[0], -1))\n",
    "    results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_data_label.astype(bool), pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'autism'])\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Result: RWB ANN (512,256)(Adagrad) lag 256 for model in fold {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: RWB ANN (512,256)(Adagrad) lag 256 for model in fold 1\n",
      "Precision: 0.728725380899294\n",
      "Recall: 0.9854271356783919\n",
      "Accuracy: 0.7404240766073872\n",
      "F1-score: 0.837855159154027\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(Adagrad) lag 256 for model in fold 2\n",
      "Precision: 0.7364079490063742\n",
      "Recall: 0.9869346733668342\n",
      "Accuracy: 0.7506839945280438\n",
      "F1-score: 0.8434614558728796\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(Adagrad) lag 256 for model in fold 3\n",
      "Precision: 0.7236456808199122\n",
      "Recall: 0.9934673366834171\n",
      "Accuracy: 0.7373461012311902\n",
      "F1-score: 0.8373570520965693\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(Adagrad) lag 256 for model in fold 4\n",
      "Precision: 0.7604248623131392\n",
      "Recall: 0.971356783919598\n",
      "Accuracy: 0.7722298221614227\n",
      "F1-score: 0.8530450132391879\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(Adagrad) lag 256 for model in fold 5\n",
      "Precision: 0.7577299412915851\n",
      "Recall: 0.9728643216080402\n",
      "Accuracy: 0.7698358413132695\n",
      "F1-score: 0.8519251925192519\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    test_data_label = test_data_label.astype(bool)  \n",
    "\n",
    "    print(f\"Result: RWB ANN (512,256)(Adagrad) lag 256 for model in fold {i}\")\n",
    "\n",
    "    TP = np.sum((test_data_label == True) & (pred == True))\n",
    "    FP = np.sum((test_data_label == False) & (pred == True))\n",
    "    TN = np.sum((test_data_label == False) & (pred == False))\n",
    "    FN = np.sum((test_data_label == True) & (pred == False))  \n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for log_dir in log_dirs:\n",
    "#     recap = pd.DataFrame(index=lags, columns=folds)\n",
    "#     training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "#     for fold in range(1,3):\n",
    "#         for lag in lags:\n",
    "#             if fold == 2:\n",
    "#                 train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "#             train_temp_dir = train_dir + '_' + str(lag)\n",
    "#             test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "#             train = get_batch(train_temp_dir)\n",
    "#             test_ds = get_batch(test_temp_dir)\n",
    "\n",
    "#             train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "#             train_ds = train.take(train_size)\n",
    "#             val_ds = train.skip(train_size)\n",
    "\n",
    "#             log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "#             model = create_model()\n",
    "#             model.summary()\n",
    "\n",
    "#             model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "#             cpu_start = time.process_time()\n",
    "#             wt_start = time.time()\n",
    "\n",
    "#             history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             wt_end = time.time()\n",
    "#             cpu_end = time.process_time()\n",
    "#             wall_time = wt_end - wt_start\n",
    "#             cpu_time = cpu_end - cpu_start\n",
    "#             training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "#             training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "#             results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "#             recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "#             recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "#     log_dir = os.path.join(log_dir,'Recap')\n",
    "#     if not os.path.exists(log_dir):\n",
    "#         os.makedirs(log_dir)\n",
    "#     recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "#     training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir c:/Users/farra/Documents/Pribadi/EEG/EEG-Autism-Classification --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farra\\Documents\\Pribadi\\EEG\\EEG-Autism-Classification\n"
     ]
    }
   ],
   "source": [
    "! cd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
