{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, lag, excluded_name=[]):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):        \n",
    "        folder = os.path.join(directory, foldername)\n",
    "        # print(folder)\n",
    "        if str(lag) in folder:\n",
    "            # print(os.listdir(folder))\n",
    "            for name in os.listdir(folder):\n",
    "                if name in excluded_name:\n",
    "                    # print(name)\n",
    "                    continue\n",
    "                filename = os.path.join(folder, name)\n",
    "                # print(filename)\n",
    "                for files in os.listdir(filename):\n",
    "                    rel_path = os.path.join(filename, files)\n",
    "                    # print(rel_path)\n",
    "                    temp_label = folder\n",
    "                    if \"autism\" in temp_label:\n",
    "                        label = 'autism'\n",
    "                    else:\n",
    "                        label = 'normal'\n",
    "\n",
    "                    temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                    rwb = np.load(rel_path)\n",
    "                    rwb.astype(np.float64).reshape(-1,1)\n",
    "                                    \n",
    "                    temp_data.loc[0, \"data\"] = rwb\n",
    "                    temp_data['label'] = label\n",
    "                    data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"autism\": 1, \"normal\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_value(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "    missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "    clean_data = data.drop(index=data.index[missing_indices])\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "# rwb = np.load(\"datasets/features/rwb/segment_1 seconds/autism_256/bader/Bader_segment_100.csv_bispectrum.npy\")\n",
    "# rwb.astype(np.float64).reshape(-1,1)\n",
    "# temp_data.loc[0, \"data\"] = rwb\n",
    "# temp_data['label'] = \"autism\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data, des_path):\n",
    "    if not os.path.exists(des_path):\n",
    "        os.makedirs(des_path)\n",
    "    data.save(des_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, train_split: float):\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "        data['data'],\n",
    "        data[['label', 'label_map']],\n",
    "        train_size=train_split,\n",
    "        stratify=data['label_map']\n",
    "    )\n",
    "\n",
    "    train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "    test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "    train_df[\"data\"] = train_x\n",
    "    train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "    test_df[\"data\"] = test_x\n",
    "    test_df[['label', 'label_map']] = test_y\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    # loading extracted feature & label\n",
    "    # x = get_dataset(path, lag, excluded_name)\n",
    "\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = data[\"label_map\"].values\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list))\n",
    "\n",
    "    # train_size = int(train_split * len(labels_list))  \n",
    "    # test_size = len(labels_list) - train_size  \n",
    "\n",
    "    # train_dataset = dataset.take(train_size)\n",
    "    # test_dataset = dataset.skip(train_size)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "#         data['data'],\n",
    "#         data[['label', 'label_map']],\n",
    "#         train_size=0.8,\n",
    "#         stratify=data['label_map']\n",
    "#     )\n",
    "\n",
    "# train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "# test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "# train_df[\"data\"] = train_x\n",
    "# train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "# test_df[\"data\"] = test_x\n",
    "# test_df[['label', 'label_map']] = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = [\"zyad\"]\n",
    "# data = get_dataset(data_dir, 256, excluded_name=excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = data[\"data\"].values\n",
    "# series_list = np.vstack(temp)\n",
    "# series_list = series_list.reshape(-1, 96, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/features/rwb/segment_1 seconds\"\n",
    "\n",
    "train_dir = \"datasets/tf_batch/rwb/segment_1 seconds/train\"\n",
    "test_dir = \"datasets/tf_batch/rwb/segment_1 seconds/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = [\"zyad\"]\n",
    "# data = get_dataset(data_dir, 256, excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_list = np.vstack(data[\"data\"].values)\n",
    "# labels_list = data[\"label_map\"].values    \n",
    "# missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "# clean_data = data.drop(index=data.index[missing_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15434, 3)\n",
      "(14618, 3)\n",
      "train:  (11694, 3)\n",
      "test:  (2924, 3)\n"
     ]
    }
   ],
   "source": [
    "excluded = [\"zyad\"]\n",
    "train_split = 0.8\n",
    "LAG = [128]\n",
    "\n",
    "for lag in LAG:\n",
    "    data = get_dataset(data_dir, lag, excluded)\n",
    "    print(data.shape)\n",
    "    data = remove_missing_value(data)\n",
    "    print(data.shape)\n",
    "    train_data, test_data = get_train_test(data, train_split)\n",
    "    print(\"train: \", train_data.shape)\n",
    "    print(\"test: \", test_data.shape)\n",
    "    train_batch = get_batch(train_data)\n",
    "    test_batch = get_batch(test_data)\n",
    "    tf.data.Dataset.save(train_batch, f\"{train_dir}_{lag}\")\n",
    "    tf.data.Dataset.save(test_batch, f\"{test_dir}_{lag}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load(\"datasets/features/rwb/segment_1 seconds/normal_128/amer/Amer_segment_1.csv_bispectrum.npy\")\n",
    "\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(96,)))\n",
    "    model.add(layers.Reshape((96, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [128]\n",
    "# folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000\n",
    "log_dirs = [f\"train_logs/logs7/RWB_ANN_512_256_RMSprop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 1.9635 - acc: 0.6151\n",
      "Epoch 1: val_acc improved from -inf to 0.67294, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 3s 7ms/step - loss: 1.9339 - acc: 0.6149 - val_loss: 0.6540 - val_acc: 0.6729\n",
      "Epoch 2/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6958 - acc: 0.6664\n",
      "Epoch 2: val_acc improved from 0.67294 to 0.67807, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6958 - acc: 0.6664 - val_loss: 0.6191 - val_acc: 0.6781\n",
      "Epoch 3/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6749 - acc: 0.6708\n",
      "Epoch 3: val_acc improved from 0.67807 to 0.69047, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6746 - acc: 0.6711 - val_loss: 0.6392 - val_acc: 0.6905\n",
      "Epoch 4/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.6820\n",
      "Epoch 4: val_acc improved from 0.69047 to 0.70073, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6641 - acc: 0.6823 - val_loss: 0.6308 - val_acc: 0.7007\n",
      "Epoch 5/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.6895\n",
      "Epoch 5: val_acc improved from 0.70073 to 0.70457, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6566 - acc: 0.6894 - val_loss: 0.6424 - val_acc: 0.7046\n",
      "Epoch 6/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6403 - acc: 0.6915\n",
      "Epoch 6: val_acc did not improve from 0.70457\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6397 - acc: 0.6914 - val_loss: 0.6064 - val_acc: 0.6999\n",
      "Epoch 7/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6222 - acc: 0.7030\n",
      "Epoch 7: val_acc improved from 0.70457 to 0.70543, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6225 - acc: 0.7024 - val_loss: 0.6010 - val_acc: 0.7054\n",
      "Epoch 8/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6185 - acc: 0.7026\n",
      "Epoch 8: val_acc improved from 0.70543 to 0.71227, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6192 - acc: 0.7025 - val_loss: 0.6322 - val_acc: 0.7123\n",
      "Epoch 9/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6076 - acc: 0.7062\n",
      "Epoch 9: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6075 - acc: 0.7064 - val_loss: 0.6014 - val_acc: 0.7080\n",
      "Epoch 10/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6035 - acc: 0.7118\n",
      "Epoch 10: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6038 - acc: 0.7113 - val_loss: 0.5942 - val_acc: 0.7080\n",
      "Epoch 11/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5960 - acc: 0.7117\n",
      "Epoch 11: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5961 - acc: 0.7115 - val_loss: 0.6048 - val_acc: 0.7114\n",
      "Epoch 12/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5908 - acc: 0.7138\n",
      "Epoch 12: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5900 - acc: 0.7134 - val_loss: 0.5956 - val_acc: 0.7067\n",
      "Epoch 13/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5834 - acc: 0.7155\n",
      "Epoch 13: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5837 - acc: 0.7152 - val_loss: 0.5985 - val_acc: 0.7067\n",
      "Epoch 14/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.7182\n",
      "Epoch 14: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5853 - acc: 0.7178 - val_loss: 0.5950 - val_acc: 0.7097\n",
      "Epoch 15/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5802 - acc: 0.7198\n",
      "Epoch 15: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5802 - acc: 0.7197 - val_loss: 0.5896 - val_acc: 0.7114\n",
      "Epoch 16/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.7194\n",
      "Epoch 16: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5793 - acc: 0.7194 - val_loss: 0.6005 - val_acc: 0.7080\n",
      "Epoch 17/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.7200\n",
      "Epoch 17: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5797 - acc: 0.7200 - val_loss: 0.5847 - val_acc: 0.7076\n",
      "Epoch 18/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.7217\n",
      "Epoch 18: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5823 - acc: 0.7218 - val_loss: 0.5848 - val_acc: 0.7080\n",
      "Epoch 19/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5758 - acc: 0.7192\n",
      "Epoch 19: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5778 - acc: 0.7185 - val_loss: 0.6015 - val_acc: 0.7101\n",
      "Epoch 20/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5690 - acc: 0.7232\n",
      "Epoch 20: val_acc improved from 0.71227 to 0.71355, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5691 - acc: 0.7226 - val_loss: 0.5923 - val_acc: 0.7136\n",
      "Epoch 21/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.7215\n",
      "Epoch 21: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5755 - acc: 0.7207 - val_loss: 0.5871 - val_acc: 0.7110\n",
      "Epoch 22/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5656 - acc: 0.7224\n",
      "Epoch 22: val_acc did not improve from 0.71355\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5659 - acc: 0.7221 - val_loss: 0.5915 - val_acc: 0.7114\n",
      "Epoch 23/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5721 - acc: 0.7259\n",
      "Epoch 23: val_acc improved from 0.71355 to 0.71441, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5725 - acc: 0.7255 - val_loss: 0.5873 - val_acc: 0.7144\n",
      "Epoch 24/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.7270\n",
      "Epoch 24: val_acc did not improve from 0.71441\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5707 - acc: 0.7271 - val_loss: 0.5865 - val_acc: 0.7144\n",
      "Epoch 25/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5708 - acc: 0.7276\n",
      "Epoch 25: val_acc improved from 0.71441 to 0.71868, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5704 - acc: 0.7280 - val_loss: 0.5922 - val_acc: 0.7187\n",
      "Epoch 26/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5728 - acc: 0.7241\n",
      "Epoch 26: val_acc did not improve from 0.71868\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5726 - acc: 0.7238 - val_loss: 0.5972 - val_acc: 0.7136\n",
      "Epoch 27/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.7260\n",
      "Epoch 27: val_acc did not improve from 0.71868\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5679 - acc: 0.7260 - val_loss: 0.5907 - val_acc: 0.7170\n",
      "Epoch 28/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5699 - acc: 0.7272\n",
      "Epoch 28: val_acc improved from 0.71868 to 0.71997, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5699 - acc: 0.7268 - val_loss: 0.5946 - val_acc: 0.7200\n",
      "Epoch 29/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.7264\n",
      "Epoch 29: val_acc did not improve from 0.71997\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5705 - acc: 0.7265 - val_loss: 0.6004 - val_acc: 0.7178\n",
      "Epoch 30/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5647 - acc: 0.7339\n",
      "Epoch 30: val_acc did not improve from 0.71997\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5646 - acc: 0.7333 - val_loss: 0.5862 - val_acc: 0.7140\n",
      "Epoch 31/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5604 - acc: 0.7296\n",
      "Epoch 31: val_acc did not improve from 0.71997\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5602 - acc: 0.7297 - val_loss: 0.6019 - val_acc: 0.7165\n",
      "Epoch 32/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.7315\n",
      "Epoch 32: val_acc did not improve from 0.71997\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5711 - acc: 0.7311 - val_loss: 0.5956 - val_acc: 0.7165\n",
      "Epoch 33/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5564 - acc: 0.7323\n",
      "Epoch 33: val_acc did not improve from 0.71997\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5573 - acc: 0.7318 - val_loss: 0.5910 - val_acc: 0.7183\n",
      "Epoch 34/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.7297\n",
      "Epoch 34: val_acc did not improve from 0.71997\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5629 - acc: 0.7298 - val_loss: 0.5936 - val_acc: 0.7157\n",
      "Epoch 35/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.7290\n",
      "Epoch 35: val_acc did not improve from 0.71997\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5646 - acc: 0.7287 - val_loss: 0.6000 - val_acc: 0.7144\n",
      "Epoch 36/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.7351\n",
      "Epoch 36: val_acc improved from 0.71997 to 0.72381, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5668 - acc: 0.7350 - val_loss: 0.6061 - val_acc: 0.7238\n",
      "Epoch 37/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.7323\n",
      "Epoch 37: val_acc did not improve from 0.72381\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5620 - acc: 0.7321 - val_loss: 0.5998 - val_acc: 0.7123\n",
      "Epoch 38/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.7329\n",
      "Epoch 38: val_acc improved from 0.72381 to 0.72510, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5638 - acc: 0.7323 - val_loss: 0.5829 - val_acc: 0.7251\n",
      "Epoch 39/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.7337\n",
      "Epoch 39: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5596 - acc: 0.7335 - val_loss: 0.6075 - val_acc: 0.7208\n",
      "Epoch 40/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.7312\n",
      "Epoch 40: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5595 - acc: 0.7313 - val_loss: 0.5973 - val_acc: 0.7200\n",
      "Epoch 41/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.7327\n",
      "Epoch 41: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5608 - acc: 0.7327 - val_loss: 0.5931 - val_acc: 0.7217\n",
      "Epoch 42/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5610 - acc: 0.7330\n",
      "Epoch 42: val_acc did not improve from 0.72510\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5610 - acc: 0.7330 - val_loss: 0.5950 - val_acc: 0.7153\n",
      "Epoch 43/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7335\n",
      "Epoch 43: val_acc improved from 0.72510 to 0.72852, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5541 - acc: 0.7336 - val_loss: 0.6028 - val_acc: 0.7285\n",
      "Epoch 44/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7364\n",
      "Epoch 44: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5574 - acc: 0.7362 - val_loss: 0.5917 - val_acc: 0.7191\n",
      "Epoch 45/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5630 - acc: 0.7394\n",
      "Epoch 45: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5626 - acc: 0.7396 - val_loss: 0.6001 - val_acc: 0.7221\n",
      "Epoch 46/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.7341\n",
      "Epoch 46: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5580 - acc: 0.7340 - val_loss: 0.6066 - val_acc: 0.7230\n",
      "Epoch 47/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7346\n",
      "Epoch 47: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5539 - acc: 0.7345 - val_loss: 0.6178 - val_acc: 0.7260\n",
      "Epoch 48/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.7345\n",
      "Epoch 48: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5543 - acc: 0.7346 - val_loss: 0.5952 - val_acc: 0.7217\n",
      "Epoch 49/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.7380\n",
      "Epoch 49: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5539 - acc: 0.7369 - val_loss: 0.6204 - val_acc: 0.7230\n",
      "Epoch 50/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7412\n",
      "Epoch 50: val_acc did not improve from 0.72852\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5555 - acc: 0.7412 - val_loss: 0.5987 - val_acc: 0.7161\n",
      "Epoch 51/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.7370\n",
      "Epoch 51: val_acc improved from 0.72852 to 0.73023, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5564 - acc: 0.7367 - val_loss: 0.5917 - val_acc: 0.7302\n",
      "Epoch 52/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.7373\n",
      "Epoch 52: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5591 - acc: 0.7374 - val_loss: 0.6209 - val_acc: 0.7221\n",
      "Epoch 53/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.7390\n",
      "Epoch 53: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5557 - acc: 0.7386 - val_loss: 0.6049 - val_acc: 0.7281\n",
      "Epoch 54/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.7400\n",
      "Epoch 54: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5564 - acc: 0.7396 - val_loss: 0.6092 - val_acc: 0.7289\n",
      "Epoch 55/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5727 - acc: 0.7384\n",
      "Epoch 55: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5727 - acc: 0.7384 - val_loss: 0.5955 - val_acc: 0.7238\n",
      "Epoch 56/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7389\n",
      "Epoch 56: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5504 - acc: 0.7386 - val_loss: 0.5995 - val_acc: 0.7230\n",
      "Epoch 57/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.7430\n",
      "Epoch 57: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5448 - acc: 0.7425 - val_loss: 0.5836 - val_acc: 0.7277\n",
      "Epoch 58/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.7441\n",
      "Epoch 58: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5542 - acc: 0.7439 - val_loss: 0.6009 - val_acc: 0.7294\n",
      "Epoch 59/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.7407\n",
      "Epoch 59: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5561 - acc: 0.7400 - val_loss: 0.5956 - val_acc: 0.7272\n",
      "Epoch 60/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7399\n",
      "Epoch 60: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5513 - acc: 0.7401 - val_loss: 0.5981 - val_acc: 0.7302\n",
      "Epoch 61/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.7430\n",
      "Epoch 61: val_acc improved from 0.73023 to 0.73236, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5447 - acc: 0.7428 - val_loss: 0.5973 - val_acc: 0.7324\n",
      "Epoch 62/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5509 - acc: 0.7422\n",
      "Epoch 62: val_acc improved from 0.73236 to 0.73365, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5509 - acc: 0.7422 - val_loss: 0.5957 - val_acc: 0.7336\n",
      "Epoch 63/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7441\n",
      "Epoch 63: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5523 - acc: 0.7439 - val_loss: 0.5984 - val_acc: 0.7328\n",
      "Epoch 64/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7451\n",
      "Epoch 64: val_acc did not improve from 0.73365\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5394 - acc: 0.7452 - val_loss: 0.5973 - val_acc: 0.7294\n",
      "Epoch 65/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7435\n",
      "Epoch 65: val_acc improved from 0.73365 to 0.73835, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5495 - acc: 0.7437 - val_loss: 0.6302 - val_acc: 0.7383\n",
      "Epoch 66/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.7437\n",
      "Epoch 66: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5507 - acc: 0.7438 - val_loss: 0.6269 - val_acc: 0.7354\n",
      "Epoch 67/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.7449\n",
      "Epoch 67: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5659 - acc: 0.7452 - val_loss: 0.6005 - val_acc: 0.7260\n",
      "Epoch 68/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.7437\n",
      "Epoch 68: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5462 - acc: 0.7435 - val_loss: 0.6232 - val_acc: 0.7341\n",
      "Epoch 69/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.7461\n",
      "Epoch 69: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5437 - acc: 0.7459 - val_loss: 0.6127 - val_acc: 0.7383\n",
      "Epoch 70/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.7445\n",
      "Epoch 70: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5413 - acc: 0.7448 - val_loss: 0.6047 - val_acc: 0.7332\n",
      "Epoch 71/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.7460\n",
      "Epoch 71: val_acc improved from 0.73835 to 0.74904, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5499 - acc: 0.7466 - val_loss: 0.6123 - val_acc: 0.7490\n",
      "Epoch 72/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7487\n",
      "Epoch 72: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5422 - acc: 0.7488 - val_loss: 0.6336 - val_acc: 0.7375\n",
      "Epoch 73/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7444\n",
      "Epoch 73: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5481 - acc: 0.7442 - val_loss: 0.6134 - val_acc: 0.7260\n",
      "Epoch 74/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7460\n",
      "Epoch 74: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5415 - acc: 0.7459 - val_loss: 0.6308 - val_acc: 0.7422\n",
      "Epoch 75/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7501\n",
      "Epoch 75: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5383 - acc: 0.7503 - val_loss: 0.6266 - val_acc: 0.7431\n",
      "Epoch 76/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5480 - acc: 0.7472\n",
      "Epoch 76: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5480 - acc: 0.7472 - val_loss: 0.6204 - val_acc: 0.7396\n",
      "Epoch 77/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.7462\n",
      "Epoch 77: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5452 - acc: 0.7462 - val_loss: 0.6151 - val_acc: 0.7319\n",
      "Epoch 78/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.7487\n",
      "Epoch 78: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5431 - acc: 0.7489 - val_loss: 0.6009 - val_acc: 0.7413\n",
      "Epoch 79/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.7517\n",
      "Epoch 79: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5366 - acc: 0.7514 - val_loss: 0.6458 - val_acc: 0.7431\n",
      "Epoch 80/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7532\n",
      "Epoch 80: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5428 - acc: 0.7523 - val_loss: 0.6434 - val_acc: 0.7349\n",
      "Epoch 81/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7541\n",
      "Epoch 81: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5461 - acc: 0.7540 - val_loss: 0.6177 - val_acc: 0.7439\n",
      "Epoch 82/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7508\n",
      "Epoch 82: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5466 - acc: 0.7492 - val_loss: 0.6207 - val_acc: 0.7272\n",
      "Epoch 83/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7550\n",
      "Epoch 83: val_acc did not improve from 0.74904\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5403 - acc: 0.7554 - val_loss: 0.5916 - val_acc: 0.7486\n",
      "Epoch 84/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7486\n",
      "Epoch 84: val_acc improved from 0.74904 to 0.75075, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5366 - acc: 0.7482 - val_loss: 0.6314 - val_acc: 0.7507\n",
      "Epoch 85/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7512\n",
      "Epoch 85: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5348 - acc: 0.7510 - val_loss: 0.6107 - val_acc: 0.7383\n",
      "Epoch 86/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7529\n",
      "Epoch 86: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5325 - acc: 0.7530 - val_loss: 0.5983 - val_acc: 0.7401\n",
      "Epoch 87/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.7478\n",
      "Epoch 87: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5336 - acc: 0.7477 - val_loss: 0.6127 - val_acc: 0.7507\n",
      "Epoch 88/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.7525\n",
      "Epoch 88: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5556 - acc: 0.7517 - val_loss: 0.6260 - val_acc: 0.7443\n",
      "Epoch 89/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7558\n",
      "Epoch 89: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5430 - acc: 0.7550 - val_loss: 0.5928 - val_acc: 0.7460\n",
      "Epoch 90/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.7520\n",
      "Epoch 90: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5395 - acc: 0.7520 - val_loss: 0.6074 - val_acc: 0.7345\n",
      "Epoch 91/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7467\n",
      "Epoch 91: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5433 - acc: 0.7468 - val_loss: 0.6046 - val_acc: 0.7456\n",
      "Epoch 92/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7541\n",
      "Epoch 92: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5374 - acc: 0.7539 - val_loss: 0.6147 - val_acc: 0.7392\n",
      "Epoch 93/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7557\n",
      "Epoch 93: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5374 - acc: 0.7540 - val_loss: 0.6189 - val_acc: 0.7486\n",
      "Epoch 94/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7530\n",
      "Epoch 94: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5420 - acc: 0.7525 - val_loss: 0.6316 - val_acc: 0.7349\n",
      "Epoch 95/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7524\n",
      "Epoch 95: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5339 - acc: 0.7524 - val_loss: 0.6384 - val_acc: 0.7478\n",
      "Epoch 96/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5456 - acc: 0.7524\n",
      "Epoch 96: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5456 - acc: 0.7524 - val_loss: 0.5986 - val_acc: 0.7405\n",
      "Epoch 97/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7544\n",
      "Epoch 97: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5398 - acc: 0.7539 - val_loss: 0.6439 - val_acc: 0.7298\n",
      "Epoch 98/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7558\n",
      "Epoch 98: val_acc improved from 0.75075 to 0.75118, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5299 - acc: 0.7556 - val_loss: 0.6289 - val_acc: 0.7512\n",
      "Epoch 99/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7590\n",
      "Epoch 99: val_acc did not improve from 0.75118\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5294 - acc: 0.7585 - val_loss: 0.6487 - val_acc: 0.7473\n",
      "Epoch 100/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.7515\n",
      "Epoch 100: val_acc did not improve from 0.75118\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5417 - acc: 0.7506 - val_loss: 0.6460 - val_acc: 0.7375\n",
      "Epoch 101/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7531\n",
      "Epoch 101: val_acc did not improve from 0.75118\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5494 - acc: 0.7530 - val_loss: 0.6620 - val_acc: 0.7469\n",
      "Epoch 102/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5354 - acc: 0.7554\n",
      "Epoch 102: val_acc improved from 0.75118 to 0.75289, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5354 - acc: 0.7554 - val_loss: 0.6179 - val_acc: 0.7529\n",
      "Epoch 103/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7534\n",
      "Epoch 103: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5325 - acc: 0.7534 - val_loss: 0.6385 - val_acc: 0.7473\n",
      "Epoch 104/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7618\n",
      "Epoch 104: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5399 - acc: 0.7617 - val_loss: 0.6560 - val_acc: 0.7388\n",
      "Epoch 105/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7586\n",
      "Epoch 105: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5374 - acc: 0.7588 - val_loss: 0.6198 - val_acc: 0.7405\n",
      "Epoch 106/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7548\n",
      "Epoch 106: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5350 - acc: 0.7552 - val_loss: 0.6173 - val_acc: 0.7413\n",
      "Epoch 107/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5399 - acc: 0.7577\n",
      "Epoch 107: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5399 - acc: 0.7577 - val_loss: 0.6194 - val_acc: 0.7413\n",
      "Epoch 108/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7555\n",
      "Epoch 108: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5349 - acc: 0.7557 - val_loss: 0.6154 - val_acc: 0.7499\n",
      "Epoch 109/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7593\n",
      "Epoch 109: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5374 - acc: 0.7594 - val_loss: 0.5963 - val_acc: 0.7520\n",
      "Epoch 110/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.7542\n",
      "Epoch 110: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5337 - acc: 0.7538 - val_loss: 0.6059 - val_acc: 0.7473\n",
      "Epoch 111/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7575\n",
      "Epoch 111: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5359 - acc: 0.7576 - val_loss: 0.6587 - val_acc: 0.7413\n",
      "Epoch 112/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7572\n",
      "Epoch 112: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5359 - acc: 0.7572 - val_loss: 0.6216 - val_acc: 0.7503\n",
      "Epoch 113/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5326 - acc: 0.7624\n",
      "Epoch 113: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5326 - acc: 0.7624 - val_loss: 0.6091 - val_acc: 0.7431\n",
      "Epoch 114/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7583\n",
      "Epoch 114: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5355 - acc: 0.7578 - val_loss: 0.6401 - val_acc: 0.7499\n",
      "Epoch 115/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7581\n",
      "Epoch 115: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5468 - acc: 0.7579 - val_loss: 0.6084 - val_acc: 0.7456\n",
      "Epoch 116/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7616\n",
      "Epoch 116: val_acc improved from 0.75289 to 0.75716, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5322 - acc: 0.7614 - val_loss: 0.6166 - val_acc: 0.7572\n",
      "Epoch 117/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.7558\n",
      "Epoch 117: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5430 - acc: 0.7562 - val_loss: 0.6377 - val_acc: 0.7507\n",
      "Epoch 118/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7625\n",
      "Epoch 118: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5348 - acc: 0.7626 - val_loss: 0.6247 - val_acc: 0.7516\n",
      "Epoch 119/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7573\n",
      "Epoch 119: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5378 - acc: 0.7571 - val_loss: 0.7198 - val_acc: 0.7294\n",
      "Epoch 120/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5372 - acc: 0.7645\n",
      "Epoch 120: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5372 - acc: 0.7645 - val_loss: 0.6504 - val_acc: 0.7413\n",
      "Epoch 121/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.7638\n",
      "Epoch 121: val_acc did not improve from 0.75716\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5311 - acc: 0.7632 - val_loss: 0.6548 - val_acc: 0.7555\n",
      "Epoch 122/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7615\n",
      "Epoch 122: val_acc improved from 0.75716 to 0.75759, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5299 - acc: 0.7617 - val_loss: 0.6155 - val_acc: 0.7576\n",
      "Epoch 123/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7639\n",
      "Epoch 123: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5275 - acc: 0.7641 - val_loss: 0.6432 - val_acc: 0.7469\n",
      "Epoch 124/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.7623\n",
      "Epoch 124: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5396 - acc: 0.7622 - val_loss: 0.6431 - val_acc: 0.7567\n",
      "Epoch 125/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.7697\n",
      "Epoch 125: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5263 - acc: 0.7697 - val_loss: 0.6909 - val_acc: 0.7520\n",
      "Epoch 126/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7628\n",
      "Epoch 126: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5329 - acc: 0.7623 - val_loss: 0.6225 - val_acc: 0.7525\n",
      "Epoch 127/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7611\n",
      "Epoch 127: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5342 - acc: 0.7613 - val_loss: 0.6796 - val_acc: 0.7503\n",
      "Epoch 128/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7624\n",
      "Epoch 128: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5403 - acc: 0.7624 - val_loss: 0.6213 - val_acc: 0.7516\n",
      "Epoch 129/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7642\n",
      "Epoch 129: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5243 - acc: 0.7641 - val_loss: 0.6534 - val_acc: 0.7422\n",
      "Epoch 130/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7638\n",
      "Epoch 130: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5321 - acc: 0.7640 - val_loss: 0.6409 - val_acc: 0.7512\n",
      "Epoch 131/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7648\n",
      "Epoch 131: val_acc did not improve from 0.75759\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5371 - acc: 0.7649 - val_loss: 0.6572 - val_acc: 0.7448\n",
      "Epoch 132/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7642\n",
      "Epoch 132: val_acc improved from 0.75759 to 0.76229, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5323 - acc: 0.7647 - val_loss: 0.6804 - val_acc: 0.7623\n",
      "Epoch 133/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7613\n",
      "Epoch 133: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5417 - acc: 0.7616 - val_loss: 0.6064 - val_acc: 0.7546\n",
      "Epoch 134/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5290 - acc: 0.7629\n",
      "Epoch 134: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5290 - acc: 0.7629 - val_loss: 0.6673 - val_acc: 0.7584\n",
      "Epoch 135/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7668\n",
      "Epoch 135: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5263 - acc: 0.7665 - val_loss: 0.6675 - val_acc: 0.7563\n",
      "Epoch 136/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7686\n",
      "Epoch 136: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5403 - acc: 0.7676 - val_loss: 0.6860 - val_acc: 0.7503\n",
      "Epoch 137/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.7583\n",
      "Epoch 137: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5247 - acc: 0.7584 - val_loss: 0.6855 - val_acc: 0.7460\n",
      "Epoch 138/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7657\n",
      "Epoch 138: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5390 - acc: 0.7658 - val_loss: 0.6496 - val_acc: 0.7516\n",
      "Epoch 139/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7611\n",
      "Epoch 139: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5384 - acc: 0.7609 - val_loss: 0.6415 - val_acc: 0.7465\n",
      "Epoch 140/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7660\n",
      "Epoch 140: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5321 - acc: 0.7660 - val_loss: 0.6886 - val_acc: 0.7486\n",
      "Epoch 141/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7631\n",
      "Epoch 141: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5450 - acc: 0.7628 - val_loss: 0.6646 - val_acc: 0.7542\n",
      "Epoch 142/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.7621\n",
      "Epoch 142: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5216 - acc: 0.7622 - val_loss: 0.6850 - val_acc: 0.7452\n",
      "Epoch 143/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7640\n",
      "Epoch 143: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5434 - acc: 0.7638 - val_loss: 0.6173 - val_acc: 0.7606\n",
      "Epoch 144/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7657\n",
      "Epoch 144: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5277 - acc: 0.7663 - val_loss: 0.6594 - val_acc: 0.7593\n",
      "Epoch 145/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.7611\n",
      "Epoch 145: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5270 - acc: 0.7616 - val_loss: 0.7213 - val_acc: 0.7602\n",
      "Epoch 146/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7677\n",
      "Epoch 146: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5330 - acc: 0.7674 - val_loss: 0.6627 - val_acc: 0.7448\n",
      "Epoch 147/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.7641\n",
      "Epoch 147: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5270 - acc: 0.7635 - val_loss: 0.6846 - val_acc: 0.7516\n",
      "Epoch 148/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7691\n",
      "Epoch 148: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5232 - acc: 0.7693 - val_loss: 0.7002 - val_acc: 0.7567\n",
      "Epoch 149/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7628\n",
      "Epoch 149: val_acc improved from 0.76229 to 0.76400, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5443 - acc: 0.7626 - val_loss: 0.6389 - val_acc: 0.7640\n",
      "Epoch 150/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7638\n",
      "Epoch 150: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5269 - acc: 0.7641 - val_loss: 0.6621 - val_acc: 0.7439\n",
      "Epoch 151/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7599\n",
      "Epoch 151: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5399 - acc: 0.7598 - val_loss: 0.6601 - val_acc: 0.7443\n",
      "Epoch 152/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7636\n",
      "Epoch 152: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5394 - acc: 0.7635 - val_loss: 0.6550 - val_acc: 0.7495\n",
      "Epoch 153/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7646\n",
      "Epoch 153: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5170 - acc: 0.7647 - val_loss: 0.7613 - val_acc: 0.7580\n",
      "Epoch 154/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7702\n",
      "Epoch 154: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5252 - acc: 0.7696 - val_loss: 0.7012 - val_acc: 0.7610\n",
      "Epoch 155/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7658\n",
      "Epoch 155: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5307 - acc: 0.7655 - val_loss: 0.7340 - val_acc: 0.7490\n",
      "Epoch 156/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7627\n",
      "Epoch 156: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5414 - acc: 0.7617 - val_loss: 0.7168 - val_acc: 0.7627\n",
      "Epoch 157/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5404 - acc: 0.7654\n",
      "Epoch 157: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5404 - acc: 0.7654 - val_loss: 0.6350 - val_acc: 0.7550\n",
      "Epoch 158/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7662\n",
      "Epoch 158: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5369 - acc: 0.7652 - val_loss: 0.6357 - val_acc: 0.7478\n",
      "Epoch 159/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5227 - acc: 0.7682\n",
      "Epoch 159: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5227 - acc: 0.7679 - val_loss: 0.7516 - val_acc: 0.7567\n",
      "Epoch 160/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.7644\n",
      "Epoch 160: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5456 - acc: 0.7643 - val_loss: 0.6720 - val_acc: 0.7550\n",
      "Epoch 161/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7632\n",
      "Epoch 161: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5320 - acc: 0.7622 - val_loss: 0.6489 - val_acc: 0.7572\n",
      "Epoch 162/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7627\n",
      "Epoch 162: val_acc improved from 0.76400 to 0.76443, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5335 - acc: 0.7629 - val_loss: 0.6639 - val_acc: 0.7644\n",
      "Epoch 163/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7661\n",
      "Epoch 163: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5328 - acc: 0.7653 - val_loss: 0.6732 - val_acc: 0.7572\n",
      "Epoch 164/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7700\n",
      "Epoch 164: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5508 - acc: 0.7703 - val_loss: 0.6506 - val_acc: 0.7593\n",
      "Epoch 165/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7698\n",
      "Epoch 165: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5288 - acc: 0.7699 - val_loss: 0.7512 - val_acc: 0.7619\n",
      "Epoch 166/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.7677\n",
      "Epoch 166: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5280 - acc: 0.7666 - val_loss: 0.6895 - val_acc: 0.7469\n",
      "Epoch 167/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.7657\n",
      "Epoch 167: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5324 - acc: 0.7658 - val_loss: 0.6143 - val_acc: 0.7644\n",
      "Epoch 168/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7693\n",
      "Epoch 168: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5179 - acc: 0.7691 - val_loss: 0.6679 - val_acc: 0.7529\n",
      "Epoch 169/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5225 - acc: 0.7599\n",
      "Epoch 169: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5225 - acc: 0.7599 - val_loss: 0.6410 - val_acc: 0.7525\n",
      "Epoch 170/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5393 - acc: 0.7678\n",
      "Epoch 170: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5393 - acc: 0.7678 - val_loss: 0.6796 - val_acc: 0.7435\n",
      "Epoch 171/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7657\n",
      "Epoch 171: val_acc improved from 0.76443 to 0.76699, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5439 - acc: 0.7650 - val_loss: 0.6778 - val_acc: 0.7670\n",
      "Epoch 172/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.7666\n",
      "Epoch 172: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5237 - acc: 0.7665 - val_loss: 0.7157 - val_acc: 0.7602\n",
      "Epoch 173/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.7702\n",
      "Epoch 173: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5589 - acc: 0.7704 - val_loss: 0.6808 - val_acc: 0.7520\n",
      "Epoch 174/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7697\n",
      "Epoch 174: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5460 - acc: 0.7695 - val_loss: 0.7685 - val_acc: 0.7478\n",
      "Epoch 175/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7743\n",
      "Epoch 175: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5348 - acc: 0.7748 - val_loss: 0.6737 - val_acc: 0.7572\n",
      "Epoch 176/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5356 - acc: 0.7612\n",
      "Epoch 176: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5356 - acc: 0.7612 - val_loss: 0.7101 - val_acc: 0.7542\n",
      "Epoch 177/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.7689\n",
      "Epoch 177: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5375 - acc: 0.7673 - val_loss: 0.7248 - val_acc: 0.7345\n",
      "Epoch 178/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5453 - acc: 0.7703\n",
      "Epoch 178: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5453 - acc: 0.7703 - val_loss: 0.6941 - val_acc: 0.7640\n",
      "Epoch 179/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.7746\n",
      "Epoch 179: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5097 - acc: 0.7750 - val_loss: 0.7439 - val_acc: 0.7610\n",
      "Epoch 180/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7730\n",
      "Epoch 180: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5480 - acc: 0.7715 - val_loss: 0.6179 - val_acc: 0.7563\n",
      "Epoch 181/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.7672\n",
      "Epoch 181: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5330 - acc: 0.7674 - val_loss: 0.6746 - val_acc: 0.7589\n",
      "Epoch 182/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7709\n",
      "Epoch 182: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5298 - acc: 0.7706 - val_loss: 0.6400 - val_acc: 0.7563\n",
      "Epoch 183/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7731\n",
      "Epoch 183: val_acc did not improve from 0.76699\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5223 - acc: 0.7728 - val_loss: 0.6816 - val_acc: 0.7602\n",
      "Epoch 184/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7748\n",
      "Epoch 184: val_acc improved from 0.76699 to 0.77170, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5189 - acc: 0.7751 - val_loss: 0.6936 - val_acc: 0.7717\n",
      "Epoch 185/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7684\n",
      "Epoch 185: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5447 - acc: 0.7681 - val_loss: 0.6623 - val_acc: 0.7546\n",
      "Epoch 186/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7723\n",
      "Epoch 186: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5203 - acc: 0.7715 - val_loss: 0.7093 - val_acc: 0.7606\n",
      "Epoch 187/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.7695\n",
      "Epoch 187: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5175 - acc: 0.7690 - val_loss: 0.7872 - val_acc: 0.7537\n",
      "Epoch 188/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7692\n",
      "Epoch 188: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5450 - acc: 0.7693 - val_loss: 0.7066 - val_acc: 0.7657\n",
      "Epoch 189/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7671\n",
      "Epoch 189: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5437 - acc: 0.7676 - val_loss: 0.7030 - val_acc: 0.7661\n",
      "Epoch 190/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.7694\n",
      "Epoch 190: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5148 - acc: 0.7695 - val_loss: 0.7561 - val_acc: 0.7525\n",
      "Epoch 191/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.7756\n",
      "Epoch 191: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5260 - acc: 0.7756 - val_loss: 0.6694 - val_acc: 0.7520\n",
      "Epoch 192/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7705\n",
      "Epoch 192: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5398 - acc: 0.7705 - val_loss: 0.6825 - val_acc: 0.7619\n",
      "Epoch 193/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.7742\n",
      "Epoch 193: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5095 - acc: 0.7735 - val_loss: 0.6736 - val_acc: 0.7627\n",
      "Epoch 194/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.7750\n",
      "Epoch 194: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5131 - acc: 0.7752 - val_loss: 0.7294 - val_acc: 0.7644\n",
      "Epoch 195/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.7792\n",
      "Epoch 195: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5675 - acc: 0.7792 - val_loss: 0.7148 - val_acc: 0.7610\n",
      "Epoch 196/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7751\n",
      "Epoch 196: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5357 - acc: 0.7746 - val_loss: 0.7146 - val_acc: 0.7580\n",
      "Epoch 197/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5427 - acc: 0.7747\n",
      "Epoch 197: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5427 - acc: 0.7747 - val_loss: 0.7401 - val_acc: 0.7644\n",
      "Epoch 198/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.7716\n",
      "Epoch 198: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5406 - acc: 0.7718 - val_loss: 0.7116 - val_acc: 0.7572\n",
      "Epoch 199/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7762\n",
      "Epoch 199: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5158 - acc: 0.7761 - val_loss: 0.6733 - val_acc: 0.7559\n",
      "Epoch 200/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7724\n",
      "Epoch 200: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5284 - acc: 0.7716 - val_loss: 0.6635 - val_acc: 0.7657\n",
      "Epoch 201/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.7809\n",
      "Epoch 201: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5173 - acc: 0.7811 - val_loss: 0.6760 - val_acc: 0.7661\n",
      "Epoch 202/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5125 - acc: 0.7731\n",
      "Epoch 202: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5126 - acc: 0.7723 - val_loss: 0.6290 - val_acc: 0.7610\n",
      "Epoch 203/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7682\n",
      "Epoch 203: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5187 - acc: 0.7674 - val_loss: 0.6521 - val_acc: 0.7619\n",
      "Epoch 204/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.7735\n",
      "Epoch 204: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5169 - acc: 0.7737 - val_loss: 0.6701 - val_acc: 0.7614\n",
      "Epoch 205/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7698\n",
      "Epoch 205: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5394 - acc: 0.7699 - val_loss: 0.6847 - val_acc: 0.7572\n",
      "Epoch 206/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.7755\n",
      "Epoch 206: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5083 - acc: 0.7755 - val_loss: 0.6998 - val_acc: 0.7593\n",
      "Epoch 207/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.7735\n",
      "Epoch 207: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5198 - acc: 0.7736 - val_loss: 0.7420 - val_acc: 0.7644\n",
      "Epoch 208/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.7726\n",
      "Epoch 208: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5181 - acc: 0.7724 - val_loss: 0.7411 - val_acc: 0.7443\n",
      "Epoch 209/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7718\n",
      "Epoch 209: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5333 - acc: 0.7707 - val_loss: 0.6818 - val_acc: 0.7589\n",
      "Epoch 210/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7776\n",
      "Epoch 210: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5286 - acc: 0.7773 - val_loss: 0.7465 - val_acc: 0.7610\n",
      "Epoch 211/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.7720\n",
      "Epoch 211: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5316 - acc: 0.7715 - val_loss: 0.6838 - val_acc: 0.7572\n",
      "Epoch 212/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7759\n",
      "Epoch 212: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5204 - acc: 0.7751 - val_loss: 0.6397 - val_acc: 0.7610\n",
      "Epoch 213/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7763\n",
      "Epoch 213: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5283 - acc: 0.7763 - val_loss: 0.7010 - val_acc: 0.7593\n",
      "Epoch 214/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7747\n",
      "Epoch 214: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5240 - acc: 0.7748 - val_loss: 0.6831 - val_acc: 0.7576\n",
      "Epoch 215/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7761\n",
      "Epoch 215: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5206 - acc: 0.7762 - val_loss: 0.6773 - val_acc: 0.7640\n",
      "Epoch 216/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5092 - acc: 0.7747\n",
      "Epoch 216: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5092 - acc: 0.7747 - val_loss: 0.7159 - val_acc: 0.7563\n",
      "Epoch 217/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7726\n",
      "Epoch 217: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5144 - acc: 0.7726 - val_loss: 0.7252 - val_acc: 0.7649\n",
      "Epoch 218/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.7740\n",
      "Epoch 218: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5272 - acc: 0.7734 - val_loss: 0.7429 - val_acc: 0.7529\n",
      "Epoch 219/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7792\n",
      "Epoch 219: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5282 - acc: 0.7785 - val_loss: 0.7042 - val_acc: 0.7597\n",
      "Epoch 220/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.7733\n",
      "Epoch 220: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5421 - acc: 0.7735 - val_loss: 0.7088 - val_acc: 0.7597\n",
      "Epoch 221/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.7727\n",
      "Epoch 221: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5170 - acc: 0.7728 - val_loss: 0.7253 - val_acc: 0.7683\n",
      "Epoch 222/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7709\n",
      "Epoch 222: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5314 - acc: 0.7706 - val_loss: 0.7638 - val_acc: 0.7559\n",
      "Epoch 223/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5630 - acc: 0.7701\n",
      "Epoch 223: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5616 - acc: 0.7704 - val_loss: 0.6997 - val_acc: 0.7576\n",
      "Epoch 224/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5329 - acc: 0.7743\n",
      "Epoch 224: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5329 - acc: 0.7743 - val_loss: 0.7350 - val_acc: 0.7610\n",
      "Epoch 225/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.7794\n",
      "Epoch 225: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5214 - acc: 0.7795 - val_loss: 0.7530 - val_acc: 0.7687\n",
      "Epoch 226/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7702\n",
      "Epoch 226: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5405 - acc: 0.7697 - val_loss: 0.6748 - val_acc: 0.7589\n",
      "Epoch 227/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7744\n",
      "Epoch 227: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5296 - acc: 0.7746 - val_loss: 0.7312 - val_acc: 0.7691\n",
      "Epoch 228/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7795\n",
      "Epoch 228: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5244 - acc: 0.7796 - val_loss: 0.6876 - val_acc: 0.7666\n",
      "Epoch 229/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.7694\n",
      "Epoch 229: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5272 - acc: 0.7700 - val_loss: 0.7290 - val_acc: 0.7619\n",
      "Epoch 230/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7765\n",
      "Epoch 230: val_acc improved from 0.77170 to 0.77255, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5357 - acc: 0.7766 - val_loss: 0.6733 - val_acc: 0.7726\n",
      "Epoch 231/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7761\n",
      "Epoch 231: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5179 - acc: 0.7763 - val_loss: 0.7045 - val_acc: 0.7631\n",
      "Epoch 232/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7690\n",
      "Epoch 232: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5416 - acc: 0.7688 - val_loss: 0.6632 - val_acc: 0.7431\n",
      "Epoch 233/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.7744\n",
      "Epoch 233: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5150 - acc: 0.7737 - val_loss: 0.7098 - val_acc: 0.7649\n",
      "Epoch 234/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7734\n",
      "Epoch 234: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5265 - acc: 0.7735 - val_loss: 0.7034 - val_acc: 0.7670\n",
      "Epoch 235/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.7770\n",
      "Epoch 235: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5195 - acc: 0.7768 - val_loss: 0.7968 - val_acc: 0.7525\n",
      "Epoch 236/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.7777\n",
      "Epoch 236: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5403 - acc: 0.7774 - val_loss: 0.7310 - val_acc: 0.7670\n",
      "Epoch 237/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.7759\n",
      "Epoch 237: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5567 - acc: 0.7756 - val_loss: 0.7332 - val_acc: 0.7661\n",
      "Epoch 238/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7709\n",
      "Epoch 238: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5272 - acc: 0.7714 - val_loss: 0.7957 - val_acc: 0.7670\n",
      "Epoch 239/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5414 - acc: 0.7784\n",
      "Epoch 239: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5424 - acc: 0.7784 - val_loss: 0.6982 - val_acc: 0.7627\n",
      "Epoch 240/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.7747\n",
      "Epoch 240: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5264 - acc: 0.7748 - val_loss: 0.7792 - val_acc: 0.7482\n",
      "Epoch 241/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7751\n",
      "Epoch 241: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5176 - acc: 0.7753 - val_loss: 0.7161 - val_acc: 0.7653\n",
      "Epoch 242/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7764\n",
      "Epoch 242: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5237 - acc: 0.7762 - val_loss: 0.7330 - val_acc: 0.7661\n",
      "Epoch 243/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7723\n",
      "Epoch 243: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5434 - acc: 0.7712 - val_loss: 0.7207 - val_acc: 0.7520\n",
      "Epoch 244/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7745\n",
      "Epoch 244: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5172 - acc: 0.7754 - val_loss: 0.7266 - val_acc: 0.7649\n",
      "Epoch 245/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.7793\n",
      "Epoch 245: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5253 - acc: 0.7798 - val_loss: 0.7488 - val_acc: 0.7713\n",
      "Epoch 246/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5359 - acc: 0.7711\n",
      "Epoch 246: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5359 - acc: 0.7711 - val_loss: 0.7696 - val_acc: 0.7666\n",
      "Epoch 247/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.7804\n",
      "Epoch 247: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5350 - acc: 0.7802 - val_loss: 0.6707 - val_acc: 0.7602\n",
      "Epoch 248/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7762\n",
      "Epoch 248: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5187 - acc: 0.7761 - val_loss: 0.7426 - val_acc: 0.7666\n",
      "Epoch 249/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7826\n",
      "Epoch 249: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5494 - acc: 0.7815 - val_loss: 0.7012 - val_acc: 0.7593\n",
      "Epoch 250/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7724\n",
      "Epoch 250: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5269 - acc: 0.7727 - val_loss: 0.7407 - val_acc: 0.7619\n",
      "Epoch 251/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.7785\n",
      "Epoch 251: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5214 - acc: 0.7779 - val_loss: 0.6912 - val_acc: 0.7456\n",
      "Epoch 252/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.7780\n",
      "Epoch 252: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4984 - acc: 0.7782 - val_loss: 0.7772 - val_acc: 0.7674\n",
      "Epoch 253/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7764\n",
      "Epoch 253: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5320 - acc: 0.7764 - val_loss: 0.7242 - val_acc: 0.7640\n",
      "Epoch 254/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.7753\n",
      "Epoch 254: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5140 - acc: 0.7748 - val_loss: 0.7159 - val_acc: 0.7649\n",
      "Epoch 255/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.7766\n",
      "Epoch 255: val_acc improved from 0.77255 to 0.77512, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5276 - acc: 0.7764 - val_loss: 0.6933 - val_acc: 0.7751\n",
      "Epoch 256/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7780\n",
      "Epoch 256: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5129 - acc: 0.7772 - val_loss: 0.7322 - val_acc: 0.7546\n",
      "Epoch 257/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.7775\n",
      "Epoch 257: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5005 - acc: 0.7772 - val_loss: 0.6883 - val_acc: 0.7589\n",
      "Epoch 258/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.7780\n",
      "Epoch 258: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5233 - acc: 0.7769 - val_loss: 0.6832 - val_acc: 0.7606\n",
      "Epoch 259/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5280 - acc: 0.7752\n",
      "Epoch 259: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5280 - acc: 0.7752 - val_loss: 0.8491 - val_acc: 0.7717\n",
      "Epoch 260/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5322 - acc: 0.7759\n",
      "Epoch 260: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5322 - acc: 0.7759 - val_loss: 0.7133 - val_acc: 0.7708\n",
      "Epoch 261/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7767\n",
      "Epoch 261: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5451 - acc: 0.7768 - val_loss: 0.7454 - val_acc: 0.7691\n",
      "Epoch 262/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.7761\n",
      "Epoch 262: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5202 - acc: 0.7753 - val_loss: 0.6688 - val_acc: 0.7576\n",
      "Epoch 263/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7803\n",
      "Epoch 263: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5159 - acc: 0.7809 - val_loss: 0.7287 - val_acc: 0.7653\n",
      "Epoch 264/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.7777\n",
      "Epoch 264: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5442 - acc: 0.7766 - val_loss: 0.7425 - val_acc: 0.7691\n",
      "Epoch 265/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.7791\n",
      "Epoch 265: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5543 - acc: 0.7789 - val_loss: 0.7464 - val_acc: 0.7593\n",
      "Epoch 266/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.7735\n",
      "Epoch 266: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5335 - acc: 0.7743 - val_loss: 0.7174 - val_acc: 0.7593\n",
      "Epoch 267/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7720\n",
      "Epoch 267: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5162 - acc: 0.7726 - val_loss: 0.9524 - val_acc: 0.7636\n",
      "Epoch 268/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.7775\n",
      "Epoch 268: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5351 - acc: 0.7776 - val_loss: 0.6984 - val_acc: 0.7606\n",
      "Epoch 269/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5135 - acc: 0.7791\n",
      "Epoch 269: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5133 - acc: 0.7793 - val_loss: 0.7106 - val_acc: 0.7584\n",
      "Epoch 270/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5141 - acc: 0.7765\n",
      "Epoch 270: val_acc improved from 0.77512 to 0.77597, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5141 - acc: 0.7765 - val_loss: 0.7181 - val_acc: 0.7760\n",
      "Epoch 271/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.7733\n",
      "Epoch 271: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5334 - acc: 0.7732 - val_loss: 0.7324 - val_acc: 0.7478\n",
      "Epoch 272/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7750\n",
      "Epoch 272: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5273 - acc: 0.7748 - val_loss: 0.7717 - val_acc: 0.7580\n",
      "Epoch 273/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7836\n",
      "Epoch 273: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5172 - acc: 0.7836 - val_loss: 0.8261 - val_acc: 0.7619\n",
      "Epoch 274/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5237 - acc: 0.7774\n",
      "Epoch 274: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5237 - acc: 0.7774 - val_loss: 0.8130 - val_acc: 0.7602\n",
      "Epoch 275/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5572 - acc: 0.7787\n",
      "Epoch 275: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5572 - acc: 0.7787 - val_loss: 0.6739 - val_acc: 0.7653\n",
      "Epoch 276/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.7755\n",
      "Epoch 276: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5086 - acc: 0.7751 - val_loss: 0.7349 - val_acc: 0.7606\n",
      "Epoch 277/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7710\n",
      "Epoch 277: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5354 - acc: 0.7712 - val_loss: 0.7232 - val_acc: 0.7640\n",
      "Epoch 278/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.7808\n",
      "Epoch 278: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5433 - acc: 0.7811 - val_loss: 0.7267 - val_acc: 0.7610\n",
      "Epoch 279/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7755\n",
      "Epoch 279: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5400 - acc: 0.7753 - val_loss: 0.7387 - val_acc: 0.7606\n",
      "Epoch 280/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.7799\n",
      "Epoch 280: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5057 - acc: 0.7788 - val_loss: 0.7034 - val_acc: 0.7529\n",
      "Epoch 281/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.7776\n",
      "Epoch 281: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5328 - acc: 0.7765 - val_loss: 0.7303 - val_acc: 0.7721\n",
      "Epoch 282/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7765\n",
      "Epoch 282: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5277 - acc: 0.7758 - val_loss: 0.7335 - val_acc: 0.7653\n",
      "Epoch 283/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.7815\n",
      "Epoch 283: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5134 - acc: 0.7805 - val_loss: 0.7589 - val_acc: 0.7713\n",
      "Epoch 284/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7768\n",
      "Epoch 284: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5293 - acc: 0.7768 - val_loss: 0.6944 - val_acc: 0.7593\n",
      "Epoch 285/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5369 - acc: 0.7784\n",
      "Epoch 285: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5369 - acc: 0.7784 - val_loss: 0.7844 - val_acc: 0.7657\n",
      "Epoch 286/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.7806\n",
      "Epoch 286: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5119 - acc: 0.7798 - val_loss: 0.7670 - val_acc: 0.7644\n",
      "Epoch 287/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7760\n",
      "Epoch 287: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5372 - acc: 0.7761 - val_loss: 0.8051 - val_acc: 0.7619\n",
      "Epoch 288/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7832\n",
      "Epoch 288: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5286 - acc: 0.7824 - val_loss: 0.8832 - val_acc: 0.7713\n",
      "Epoch 289/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.7782\n",
      "Epoch 289: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5738 - acc: 0.7776 - val_loss: 0.7997 - val_acc: 0.7606\n",
      "Epoch 290/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7804\n",
      "Epoch 290: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5338 - acc: 0.7804 - val_loss: 0.8225 - val_acc: 0.7606\n",
      "Epoch 291/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7809\n",
      "Epoch 291: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5481 - acc: 0.7798 - val_loss: 0.6990 - val_acc: 0.7666\n",
      "Epoch 292/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.7779\n",
      "Epoch 292: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5293 - acc: 0.7778 - val_loss: 0.8298 - val_acc: 0.7691\n",
      "Epoch 293/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7820\n",
      "Epoch 293: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5320 - acc: 0.7823 - val_loss: 0.6939 - val_acc: 0.7525\n",
      "Epoch 294/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7792\n",
      "Epoch 294: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5299 - acc: 0.7789 - val_loss: 0.8109 - val_acc: 0.7606\n",
      "Epoch 295/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5260 - acc: 0.7750\n",
      "Epoch 295: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5260 - acc: 0.7750 - val_loss: 0.8015 - val_acc: 0.7691\n",
      "Epoch 296/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.7787\n",
      "Epoch 296: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5255 - acc: 0.7784 - val_loss: 0.7617 - val_acc: 0.7670\n",
      "Epoch 297/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7824\n",
      "Epoch 297: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5170 - acc: 0.7817 - val_loss: 0.6795 - val_acc: 0.7631\n",
      "Epoch 298/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.7806\n",
      "Epoch 298: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5075 - acc: 0.7804 - val_loss: 0.7768 - val_acc: 0.7713\n",
      "Epoch 299/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7805\n",
      "Epoch 299: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5288 - acc: 0.7804 - val_loss: 0.7695 - val_acc: 0.7730\n",
      "Epoch 300/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7755\n",
      "Epoch 300: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5290 - acc: 0.7757 - val_loss: 0.7743 - val_acc: 0.7683\n",
      "Epoch 301/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7786\n",
      "Epoch 301: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5372 - acc: 0.7787 - val_loss: 0.7467 - val_acc: 0.7640\n",
      "Epoch 302/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5194 - acc: 0.7748\n",
      "Epoch 302: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5194 - acc: 0.7748 - val_loss: 0.8127 - val_acc: 0.7708\n",
      "Epoch 303/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7825\n",
      "Epoch 303: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5210 - acc: 0.7818 - val_loss: 0.7405 - val_acc: 0.7507\n",
      "Epoch 304/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5306 - acc: 0.7765\n",
      "Epoch 304: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5306 - acc: 0.7765 - val_loss: 0.7406 - val_acc: 0.7460\n",
      "Epoch 305/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7845\n",
      "Epoch 305: val_acc improved from 0.77597 to 0.77683, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5338 - acc: 0.7841 - val_loss: 0.7487 - val_acc: 0.7768\n",
      "Epoch 306/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7793\n",
      "Epoch 306: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5237 - acc: 0.7790 - val_loss: 0.7810 - val_acc: 0.7602\n",
      "Epoch 307/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.7848\n",
      "Epoch 307: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5116 - acc: 0.7836 - val_loss: 0.7327 - val_acc: 0.7666\n",
      "Epoch 308/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.7842\n",
      "Epoch 308: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5196 - acc: 0.7841 - val_loss: 0.7279 - val_acc: 0.7657\n",
      "Epoch 309/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5142 - acc: 0.7847\n",
      "Epoch 309: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5141 - acc: 0.7843 - val_loss: 0.7377 - val_acc: 0.7653\n",
      "Epoch 310/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7810\n",
      "Epoch 310: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5287 - acc: 0.7814 - val_loss: 0.7122 - val_acc: 0.7700\n",
      "Epoch 311/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.7763\n",
      "Epoch 311: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5213 - acc: 0.7761 - val_loss: 0.7434 - val_acc: 0.7734\n",
      "Epoch 312/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.7781\n",
      "Epoch 312: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5163 - acc: 0.7782 - val_loss: 0.7498 - val_acc: 0.7653\n",
      "Epoch 313/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5220 - acc: 0.7771\n",
      "Epoch 313: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5220 - acc: 0.7771 - val_loss: 0.7411 - val_acc: 0.7525\n",
      "Epoch 314/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7735\n",
      "Epoch 314: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5388 - acc: 0.7738 - val_loss: 0.7936 - val_acc: 0.7584\n",
      "Epoch 315/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.7810\n",
      "Epoch 315: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5163 - acc: 0.7810 - val_loss: 0.7379 - val_acc: 0.7730\n",
      "Epoch 316/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.7796\n",
      "Epoch 316: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5254 - acc: 0.7795 - val_loss: 0.7987 - val_acc: 0.7704\n",
      "Epoch 317/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7810\n",
      "Epoch 317: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5183 - acc: 0.7804 - val_loss: 0.8072 - val_acc: 0.7520\n",
      "Epoch 318/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7788\n",
      "Epoch 318: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5283 - acc: 0.7788 - val_loss: 0.8073 - val_acc: 0.7661\n",
      "Epoch 319/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5186 - acc: 0.7802\n",
      "Epoch 319: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5186 - acc: 0.7802 - val_loss: 0.7286 - val_acc: 0.7619\n",
      "Epoch 320/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.7812\n",
      "Epoch 320: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5233 - acc: 0.7808 - val_loss: 0.7149 - val_acc: 0.7700\n",
      "Epoch 321/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7774\n",
      "Epoch 321: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5191 - acc: 0.7777 - val_loss: 0.8053 - val_acc: 0.7649\n",
      "Epoch 322/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7800\n",
      "Epoch 322: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5295 - acc: 0.7800 - val_loss: 0.8173 - val_acc: 0.7610\n",
      "Epoch 323/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.7780\n",
      "Epoch 323: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5291 - acc: 0.7780 - val_loss: 0.6649 - val_acc: 0.7717\n",
      "Epoch 324/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5118 - acc: 0.7787\n",
      "Epoch 324: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5118 - acc: 0.7787 - val_loss: 0.7588 - val_acc: 0.7717\n",
      "Epoch 325/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7781\n",
      "Epoch 325: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5289 - acc: 0.7783 - val_loss: 0.7259 - val_acc: 0.7563\n",
      "Epoch 326/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7763\n",
      "Epoch 326: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5289 - acc: 0.7764 - val_loss: 0.7481 - val_acc: 0.7661\n",
      "Epoch 327/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.7779\n",
      "Epoch 327: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5542 - acc: 0.7782 - val_loss: 0.8752 - val_acc: 0.7734\n",
      "Epoch 328/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7824\n",
      "Epoch 328: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5287 - acc: 0.7823 - val_loss: 0.7806 - val_acc: 0.7687\n",
      "Epoch 329/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.7841\n",
      "Epoch 329: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5314 - acc: 0.7840 - val_loss: 0.8533 - val_acc: 0.7649\n",
      "Epoch 330/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.7768\n",
      "Epoch 330: val_acc improved from 0.77683 to 0.77982, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5158 - acc: 0.7770 - val_loss: 0.7867 - val_acc: 0.7798\n",
      "Epoch 331/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7815\n",
      "Epoch 331: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5374 - acc: 0.7817 - val_loss: 0.7681 - val_acc: 0.7687\n",
      "Epoch 332/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.7806\n",
      "Epoch 332: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5255 - acc: 0.7809 - val_loss: 0.8388 - val_acc: 0.7657\n",
      "Epoch 333/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7879\n",
      "Epoch 333: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5394 - acc: 0.7886 - val_loss: 0.7021 - val_acc: 0.7721\n",
      "Epoch 334/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.7871\n",
      "Epoch 334: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4970 - acc: 0.7872 - val_loss: 0.8243 - val_acc: 0.7726\n",
      "Epoch 335/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7803\n",
      "Epoch 335: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5289 - acc: 0.7787 - val_loss: 0.7351 - val_acc: 0.7576\n",
      "Epoch 336/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.7860\n",
      "Epoch 336: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5217 - acc: 0.7857 - val_loss: 0.7398 - val_acc: 0.7631\n",
      "Epoch 337/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7816\n",
      "Epoch 337: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5331 - acc: 0.7816 - val_loss: 0.7077 - val_acc: 0.7435\n",
      "Epoch 338/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.7819\n",
      "Epoch 338: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5179 - acc: 0.7820 - val_loss: 0.7589 - val_acc: 0.7678\n",
      "Epoch 339/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.7786\n",
      "Epoch 339: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5074 - acc: 0.7788 - val_loss: 0.7146 - val_acc: 0.7687\n",
      "Epoch 340/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7793\n",
      "Epoch 340: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5356 - acc: 0.7793 - val_loss: 0.7171 - val_acc: 0.7623\n",
      "Epoch 341/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7790\n",
      "Epoch 341: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5285 - acc: 0.7789 - val_loss: 0.8741 - val_acc: 0.7755\n",
      "Epoch 342/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7797\n",
      "Epoch 342: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5286 - acc: 0.7789 - val_loss: 0.7798 - val_acc: 0.7687\n",
      "Epoch 343/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5160 - acc: 0.7798\n",
      "Epoch 343: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5160 - acc: 0.7798 - val_loss: 0.7667 - val_acc: 0.7777\n",
      "Epoch 344/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7806\n",
      "Epoch 344: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5390 - acc: 0.7803 - val_loss: 0.7026 - val_acc: 0.7576\n",
      "Epoch 345/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7805\n",
      "Epoch 345: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5470 - acc: 0.7795 - val_loss: 0.7240 - val_acc: 0.7738\n",
      "Epoch 346/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.7774\n",
      "Epoch 346: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5093 - acc: 0.7773 - val_loss: 0.7663 - val_acc: 0.7507\n",
      "Epoch 347/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7796\n",
      "Epoch 347: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5202 - acc: 0.7800 - val_loss: 0.7086 - val_acc: 0.7696\n",
      "Epoch 348/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5240 - acc: 0.7810\n",
      "Epoch 348: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5240 - acc: 0.7810 - val_loss: 0.7410 - val_acc: 0.7704\n",
      "Epoch 349/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7846\n",
      "Epoch 349: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5290 - acc: 0.7847 - val_loss: 0.7681 - val_acc: 0.7760\n",
      "Epoch 350/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.7733\n",
      "Epoch 350: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5414 - acc: 0.7736 - val_loss: 0.7498 - val_acc: 0.7700\n",
      "Epoch 351/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.7829\n",
      "Epoch 351: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5156 - acc: 0.7824 - val_loss: 0.8040 - val_acc: 0.7640\n",
      "Epoch 352/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.7788\n",
      "Epoch 352: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5196 - acc: 0.7789 - val_loss: 0.7646 - val_acc: 0.7696\n",
      "Epoch 353/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7878\n",
      "Epoch 353: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5282 - acc: 0.7882 - val_loss: 0.9235 - val_acc: 0.7785\n",
      "Epoch 354/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7842\n",
      "Epoch 354: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5325 - acc: 0.7835 - val_loss: 0.7898 - val_acc: 0.7674\n",
      "Epoch 355/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.7839\n",
      "Epoch 355: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5244 - acc: 0.7836 - val_loss: 0.7839 - val_acc: 0.7743\n",
      "Epoch 356/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.7817\n",
      "Epoch 356: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5288 - acc: 0.7812 - val_loss: 0.8206 - val_acc: 0.7644\n",
      "Epoch 357/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.7830\n",
      "Epoch 357: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5552 - acc: 0.7830 - val_loss: 0.8051 - val_acc: 0.7631\n",
      "Epoch 358/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5646 - acc: 0.7772\n",
      "Epoch 358: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5634 - acc: 0.7767 - val_loss: 0.7228 - val_acc: 0.7738\n",
      "Epoch 359/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.7908\n",
      "Epoch 359: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5115 - acc: 0.7914 - val_loss: 0.7606 - val_acc: 0.7768\n",
      "Epoch 360/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.7877\n",
      "Epoch 360: val_acc did not improve from 0.77982\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5253 - acc: 0.7880 - val_loss: 0.7630 - val_acc: 0.7683\n",
      "Epoch 361/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.7796\n",
      "Epoch 361: val_acc improved from 0.77982 to 0.78025, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/1/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5109 - acc: 0.7797 - val_loss: 0.7675 - val_acc: 0.7802\n",
      "Epoch 362/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.7917\n",
      "Epoch 362: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5534 - acc: 0.7914 - val_loss: 0.7393 - val_acc: 0.7657\n",
      "Epoch 363/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.7863\n",
      "Epoch 363: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5104 - acc: 0.7867 - val_loss: 0.7980 - val_acc: 0.7704\n",
      "Epoch 364/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5154 - acc: 0.7854\n",
      "Epoch 364: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5154 - acc: 0.7854 - val_loss: 0.7632 - val_acc: 0.7738\n",
      "Epoch 365/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7844\n",
      "Epoch 365: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5179 - acc: 0.7846 - val_loss: 0.7628 - val_acc: 0.7768\n",
      "Epoch 366/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5177 - acc: 0.7843\n",
      "Epoch 366: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5177 - acc: 0.7843 - val_loss: 0.8132 - val_acc: 0.7721\n",
      "Epoch 367/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5105 - acc: 0.7887\n",
      "Epoch 367: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5105 - acc: 0.7887 - val_loss: 0.7748 - val_acc: 0.7636\n",
      "Epoch 368/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.7804\n",
      "Epoch 368: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5244 - acc: 0.7799 - val_loss: 0.7874 - val_acc: 0.7563\n",
      "Epoch 369/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7856\n",
      "Epoch 369: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5497 - acc: 0.7858 - val_loss: 0.7456 - val_acc: 0.7696\n",
      "Epoch 370/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5259 - acc: 0.7874\n",
      "Epoch 370: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5259 - acc: 0.7874 - val_loss: 0.9270 - val_acc: 0.7717\n",
      "Epoch 371/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5436 - acc: 0.7860\n",
      "Epoch 371: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5436 - acc: 0.7860 - val_loss: 0.7423 - val_acc: 0.7747\n",
      "Epoch 372/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.7837\n",
      "Epoch 372: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4978 - acc: 0.7841 - val_loss: 0.8466 - val_acc: 0.7683\n",
      "Epoch 373/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7846\n",
      "Epoch 373: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5182 - acc: 0.7846 - val_loss: 0.7480 - val_acc: 0.7683\n",
      "Epoch 374/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7873\n",
      "Epoch 374: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5115 - acc: 0.7867 - val_loss: 0.7220 - val_acc: 0.7708\n",
      "Epoch 375/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7871\n",
      "Epoch 375: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5246 - acc: 0.7874 - val_loss: 0.7210 - val_acc: 0.7696\n",
      "Epoch 376/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7847\n",
      "Epoch 376: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5246 - acc: 0.7848 - val_loss: 0.7736 - val_acc: 0.7700\n",
      "Epoch 377/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7843\n",
      "Epoch 377: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5336 - acc: 0.7831 - val_loss: 0.7526 - val_acc: 0.7559\n",
      "Epoch 378/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.7827\n",
      "Epoch 378: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5336 - acc: 0.7828 - val_loss: 0.7551 - val_acc: 0.7452\n",
      "Epoch 379/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.7822\n",
      "Epoch 379: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5193 - acc: 0.7823 - val_loss: 0.7792 - val_acc: 0.7773\n",
      "Epoch 380/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7817\n",
      "Epoch 380: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5242 - acc: 0.7811 - val_loss: 0.7279 - val_acc: 0.7606\n",
      "Epoch 381/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7797\n",
      "Epoch 381: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5261 - acc: 0.7787 - val_loss: 0.8107 - val_acc: 0.7691\n",
      "Epoch 382/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7814\n",
      "Epoch 382: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5389 - acc: 0.7805 - val_loss: 0.7720 - val_acc: 0.7730\n",
      "Epoch 383/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7817\n",
      "Epoch 383: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5255 - acc: 0.7824 - val_loss: 0.8089 - val_acc: 0.7696\n",
      "Epoch 384/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.7887\n",
      "Epoch 384: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5051 - acc: 0.7878 - val_loss: 0.7212 - val_acc: 0.7777\n",
      "Epoch 385/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.7824\n",
      "Epoch 385: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5228 - acc: 0.7825 - val_loss: 0.8466 - val_acc: 0.7738\n",
      "Epoch 386/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5348 - acc: 0.7831\n",
      "Epoch 386: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5348 - acc: 0.7831 - val_loss: 0.7447 - val_acc: 0.7559\n",
      "Epoch 387/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.7821\n",
      "Epoch 387: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5141 - acc: 0.7827 - val_loss: 0.8425 - val_acc: 0.7602\n",
      "Epoch 388/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7865\n",
      "Epoch 388: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5206 - acc: 0.7866 - val_loss: 0.9207 - val_acc: 0.7666\n",
      "Epoch 389/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7796\n",
      "Epoch 389: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5315 - acc: 0.7797 - val_loss: 0.8732 - val_acc: 0.7730\n",
      "Epoch 390/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5683 - acc: 0.7819\n",
      "Epoch 390: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5677 - acc: 0.7820 - val_loss: 0.8030 - val_acc: 0.7631\n",
      "Epoch 391/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7840\n",
      "Epoch 391: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5374 - acc: 0.7841 - val_loss: 0.7960 - val_acc: 0.7696\n",
      "Epoch 392/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.7829\n",
      "Epoch 392: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5421 - acc: 0.7825 - val_loss: 0.7958 - val_acc: 0.7627\n",
      "Epoch 393/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.7840\n",
      "Epoch 393: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5253 - acc: 0.7834 - val_loss: 0.9006 - val_acc: 0.7721\n",
      "Epoch 394/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7801\n",
      "Epoch 394: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5358 - acc: 0.7800 - val_loss: 0.7607 - val_acc: 0.7678\n",
      "Epoch 395/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7800\n",
      "Epoch 395: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5342 - acc: 0.7797 - val_loss: 0.7880 - val_acc: 0.7653\n",
      "Epoch 396/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7837\n",
      "Epoch 396: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5465 - acc: 0.7830 - val_loss: 0.7553 - val_acc: 0.7623\n",
      "Epoch 397/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7838\n",
      "Epoch 397: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5074 - acc: 0.7842 - val_loss: 0.9947 - val_acc: 0.7726\n",
      "Epoch 398/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.7904\n",
      "Epoch 398: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5475 - acc: 0.7902 - val_loss: 0.7965 - val_acc: 0.7674\n",
      "Epoch 399/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7849\n",
      "Epoch 399: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5095 - acc: 0.7851 - val_loss: 0.7597 - val_acc: 0.7738\n",
      "Epoch 400/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.7820\n",
      "Epoch 400: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5139 - acc: 0.7809 - val_loss: 0.8313 - val_acc: 0.7580\n",
      "Epoch 401/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.7803\n",
      "Epoch 401: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5234 - acc: 0.7803 - val_loss: 0.9249 - val_acc: 0.7713\n",
      "Epoch 402/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.7845\n",
      "Epoch 402: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5401 - acc: 0.7849 - val_loss: 0.8455 - val_acc: 0.7687\n",
      "Epoch 403/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7784\n",
      "Epoch 403: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5365 - acc: 0.7784 - val_loss: 0.8048 - val_acc: 0.7785\n",
      "Epoch 404/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7841\n",
      "Epoch 404: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5372 - acc: 0.7840 - val_loss: 0.7507 - val_acc: 0.7619\n",
      "Epoch 405/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7875\n",
      "Epoch 405: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5186 - acc: 0.7874 - val_loss: 0.7858 - val_acc: 0.7661\n",
      "Epoch 406/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7846\n",
      "Epoch 406: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5346 - acc: 0.7844 - val_loss: 0.7833 - val_acc: 0.7559\n",
      "Epoch 407/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7846\n",
      "Epoch 407: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5483 - acc: 0.7851 - val_loss: 0.8864 - val_acc: 0.7781\n",
      "Epoch 408/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5395 - acc: 0.7885\n",
      "Epoch 408: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5395 - acc: 0.7885 - val_loss: 0.8029 - val_acc: 0.7700\n",
      "Epoch 409/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.7811\n",
      "Epoch 409: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5256 - acc: 0.7803 - val_loss: 0.7827 - val_acc: 0.7691\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 1.9058 - acc: 0.6087\n",
      "Epoch 1: val_acc improved from -inf to 0.71270, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 3s 7ms/step - loss: 1.8832 - acc: 0.6098 - val_loss: 0.6385 - val_acc: 0.7127\n",
      "Epoch 2/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6954 - acc: 0.6578\n",
      "Epoch 2: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6958 - acc: 0.6577 - val_loss: 0.6379 - val_acc: 0.7118\n",
      "Epoch 3/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6689 - acc: 0.6756\n",
      "Epoch 3: val_acc did not improve from 0.71270\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6694 - acc: 0.6748 - val_loss: 0.6275 - val_acc: 0.7127\n",
      "Epoch 4/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6520 - acc: 0.6835\n",
      "Epoch 4: val_acc improved from 0.71270 to 0.71697, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6522 - acc: 0.6828 - val_loss: 0.6179 - val_acc: 0.7170\n",
      "Epoch 5/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.6896\n",
      "Epoch 5: val_acc improved from 0.71697 to 0.72168, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6489 - acc: 0.6896 - val_loss: 0.6150 - val_acc: 0.7217\n",
      "Epoch 6/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.6928\n",
      "Epoch 6: val_acc improved from 0.72168 to 0.72510, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6352 - acc: 0.6928 - val_loss: 0.6231 - val_acc: 0.7251\n",
      "Epoch 7/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6261 - acc: 0.7012\n",
      "Epoch 7: val_acc improved from 0.72510 to 0.73194, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6267 - acc: 0.7003 - val_loss: 0.6257 - val_acc: 0.7319\n",
      "Epoch 8/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.7022\n",
      "Epoch 8: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6189 - acc: 0.7022 - val_loss: 0.6269 - val_acc: 0.7307\n",
      "Epoch 9/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6131 - acc: 0.7031\n",
      "Epoch 9: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6131 - acc: 0.7028 - val_loss: 0.5839 - val_acc: 0.7289\n",
      "Epoch 10/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6146 - acc: 0.7019\n",
      "Epoch 10: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6141 - acc: 0.7020 - val_loss: 0.5926 - val_acc: 0.7264\n",
      "Epoch 11/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.7070\n",
      "Epoch 11: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6077 - acc: 0.7064 - val_loss: 0.5792 - val_acc: 0.7311\n",
      "Epoch 12/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.7073\n",
      "Epoch 12: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5970 - acc: 0.7067 - val_loss: 0.5784 - val_acc: 0.7277\n",
      "Epoch 13/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5917 - acc: 0.7112\n",
      "Epoch 13: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5917 - acc: 0.7112 - val_loss: 0.5766 - val_acc: 0.7281\n",
      "Epoch 14/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.7127\n",
      "Epoch 14: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5891 - acc: 0.7129 - val_loss: 0.5736 - val_acc: 0.7302\n",
      "Epoch 15/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.7125\n",
      "Epoch 15: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5937 - acc: 0.7115 - val_loss: 0.5846 - val_acc: 0.7319\n",
      "Epoch 16/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5834 - acc: 0.7136\n",
      "Epoch 16: val_acc improved from 0.73194 to 0.73707, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5833 - acc: 0.7136 - val_loss: 0.5601 - val_acc: 0.7371\n",
      "Epoch 17/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5835 - acc: 0.7156\n",
      "Epoch 17: val_acc did not improve from 0.73707\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5838 - acc: 0.7158 - val_loss: 0.5552 - val_acc: 0.7336\n",
      "Epoch 18/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.7113\n",
      "Epoch 18: val_acc did not improve from 0.73707\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5850 - acc: 0.7113 - val_loss: 0.5591 - val_acc: 0.7328\n",
      "Epoch 19/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.7158\n",
      "Epoch 19: val_acc did not improve from 0.73707\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5825 - acc: 0.7148 - val_loss: 0.5610 - val_acc: 0.7328\n",
      "Epoch 20/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.7162\n",
      "Epoch 20: val_acc did not improve from 0.73707\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5800 - acc: 0.7162 - val_loss: 0.5562 - val_acc: 0.7315\n",
      "Epoch 21/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.7180\n",
      "Epoch 21: val_acc improved from 0.73707 to 0.73792, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5842 - acc: 0.7170 - val_loss: 0.5645 - val_acc: 0.7379\n",
      "Epoch 22/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.7216\n",
      "Epoch 22: val_acc did not improve from 0.73792\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5813 - acc: 0.7205 - val_loss: 0.5720 - val_acc: 0.7366\n",
      "Epoch 23/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.7215\n",
      "Epoch 23: val_acc improved from 0.73792 to 0.73878, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5802 - acc: 0.7213 - val_loss: 0.5654 - val_acc: 0.7388\n",
      "Epoch 24/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.7238\n",
      "Epoch 24: val_acc improved from 0.73878 to 0.73920, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5720 - acc: 0.7226 - val_loss: 0.5413 - val_acc: 0.7392\n",
      "Epoch 25/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5719 - acc: 0.7208\n",
      "Epoch 25: val_acc improved from 0.73920 to 0.74348, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5721 - acc: 0.7209 - val_loss: 0.5719 - val_acc: 0.7435\n",
      "Epoch 26/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.7234\n",
      "Epoch 26: val_acc did not improve from 0.74348\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5731 - acc: 0.7226 - val_loss: 0.5454 - val_acc: 0.7409\n",
      "Epoch 27/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5728 - acc: 0.7225\n",
      "Epoch 27: val_acc improved from 0.74348 to 0.74947, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5728 - acc: 0.7225 - val_loss: 0.5392 - val_acc: 0.7495\n",
      "Epoch 28/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5708 - acc: 0.7202\n",
      "Epoch 28: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5711 - acc: 0.7200 - val_loss: 0.5623 - val_acc: 0.7371\n",
      "Epoch 29/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.7213\n",
      "Epoch 29: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5711 - acc: 0.7213 - val_loss: 0.5625 - val_acc: 0.7388\n",
      "Epoch 30/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7237\n",
      "Epoch 30: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5662 - acc: 0.7239 - val_loss: 0.5509 - val_acc: 0.7401\n",
      "Epoch 31/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.7269\n",
      "Epoch 31: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5690 - acc: 0.7268 - val_loss: 0.5504 - val_acc: 0.7443\n",
      "Epoch 32/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.7280\n",
      "Epoch 32: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5679 - acc: 0.7271 - val_loss: 0.5445 - val_acc: 0.7401\n",
      "Epoch 33/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.7284\n",
      "Epoch 33: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5669 - acc: 0.7274 - val_loss: 0.5495 - val_acc: 0.7379\n",
      "Epoch 34/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.7263\n",
      "Epoch 34: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5674 - acc: 0.7252 - val_loss: 0.5436 - val_acc: 0.7379\n",
      "Epoch 35/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.7245\n",
      "Epoch 35: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5702 - acc: 0.7245 - val_loss: 0.5346 - val_acc: 0.7405\n",
      "Epoch 36/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.7298\n",
      "Epoch 36: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5677 - acc: 0.7283 - val_loss: 0.5368 - val_acc: 0.7431\n",
      "Epoch 37/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5694 - acc: 0.7311\n",
      "Epoch 37: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5694 - acc: 0.7311 - val_loss: 0.5273 - val_acc: 0.7418\n",
      "Epoch 38/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.7327\n",
      "Epoch 38: val_acc improved from 0.74947 to 0.75160, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5555 - acc: 0.7327 - val_loss: 0.5447 - val_acc: 0.7516\n",
      "Epoch 39/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7279\n",
      "Epoch 39: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5643 - acc: 0.7267 - val_loss: 0.5439 - val_acc: 0.7469\n",
      "Epoch 40/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.7293\n",
      "Epoch 40: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5721 - acc: 0.7293 - val_loss: 0.5555 - val_acc: 0.7431\n",
      "Epoch 41/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7334\n",
      "Epoch 41: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5616 - acc: 0.7323 - val_loss: 0.5414 - val_acc: 0.7401\n",
      "Epoch 42/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.7343\n",
      "Epoch 42: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5610 - acc: 0.7336 - val_loss: 0.5278 - val_acc: 0.7499\n",
      "Epoch 43/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7332\n",
      "Epoch 43: val_acc improved from 0.75160 to 0.75887, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5585 - acc: 0.7332 - val_loss: 0.5638 - val_acc: 0.7589\n",
      "Epoch 44/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5668 - acc: 0.7328\n",
      "Epoch 44: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5668 - acc: 0.7328 - val_loss: 0.5540 - val_acc: 0.7559\n",
      "Epoch 45/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7378\n",
      "Epoch 45: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5516 - acc: 0.7359 - val_loss: 0.5440 - val_acc: 0.7448\n",
      "Epoch 46/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5586 - acc: 0.7306\n",
      "Epoch 46: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5583 - acc: 0.7307 - val_loss: 0.5378 - val_acc: 0.7473\n",
      "Epoch 47/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5614 - acc: 0.7346\n",
      "Epoch 47: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5610 - acc: 0.7347 - val_loss: 0.5326 - val_acc: 0.7456\n",
      "Epoch 48/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.7322\n",
      "Epoch 48: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5612 - acc: 0.7316 - val_loss: 0.5459 - val_acc: 0.7533\n",
      "Epoch 49/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.7359\n",
      "Epoch 49: val_acc did not improve from 0.75887\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5669 - acc: 0.7360 - val_loss: 0.5445 - val_acc: 0.7388\n",
      "Epoch 50/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.7323\n",
      "Epoch 50: val_acc improved from 0.75887 to 0.76101, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5675 - acc: 0.7319 - val_loss: 0.5454 - val_acc: 0.7610\n",
      "Epoch 51/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7363\n",
      "Epoch 51: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5501 - acc: 0.7364 - val_loss: 0.5286 - val_acc: 0.7422\n",
      "Epoch 52/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.7368\n",
      "Epoch 52: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5553 - acc: 0.7358 - val_loss: 0.5462 - val_acc: 0.7443\n",
      "Epoch 53/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.7337\n",
      "Epoch 53: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5531 - acc: 0.7336 - val_loss: 0.5354 - val_acc: 0.7542\n",
      "Epoch 54/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7363\n",
      "Epoch 54: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5445 - acc: 0.7355 - val_loss: 0.5428 - val_acc: 0.7490\n",
      "Epoch 55/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7407\n",
      "Epoch 55: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5510 - acc: 0.7398 - val_loss: 0.5334 - val_acc: 0.7516\n",
      "Epoch 56/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5673 - acc: 0.7385\n",
      "Epoch 56: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5677 - acc: 0.7371 - val_loss: 0.5321 - val_acc: 0.7567\n",
      "Epoch 57/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.7352\n",
      "Epoch 57: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5544 - acc: 0.7338 - val_loss: 0.5419 - val_acc: 0.7576\n",
      "Epoch 58/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7394\n",
      "Epoch 58: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5519 - acc: 0.7385 - val_loss: 0.5449 - val_acc: 0.7448\n",
      "Epoch 59/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.7377\n",
      "Epoch 59: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5438 - acc: 0.7373 - val_loss: 0.5433 - val_acc: 0.7533\n",
      "Epoch 60/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.7471\n",
      "Epoch 60: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5638 - acc: 0.7456 - val_loss: 0.5391 - val_acc: 0.7597\n",
      "Epoch 61/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.7359\n",
      "Epoch 61: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5569 - acc: 0.7361 - val_loss: 0.5347 - val_acc: 0.7469\n",
      "Epoch 62/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.7389\n",
      "Epoch 62: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5606 - acc: 0.7389 - val_loss: 0.5258 - val_acc: 0.7520\n",
      "Epoch 63/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5689 - acc: 0.7426\n",
      "Epoch 63: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5690 - acc: 0.7420 - val_loss: 0.5485 - val_acc: 0.7525\n",
      "Epoch 64/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.7396\n",
      "Epoch 64: val_acc did not improve from 0.76101\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5560 - acc: 0.7394 - val_loss: 0.5247 - val_acc: 0.7520\n",
      "Epoch 65/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.7445\n",
      "Epoch 65: val_acc improved from 0.76101 to 0.76486, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5546 - acc: 0.7436 - val_loss: 0.5352 - val_acc: 0.7649\n",
      "Epoch 66/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.7434\n",
      "Epoch 66: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5550 - acc: 0.7440 - val_loss: 0.5524 - val_acc: 0.7619\n",
      "Epoch 67/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.7437\n",
      "Epoch 67: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5568 - acc: 0.7431 - val_loss: 0.5428 - val_acc: 0.7572\n",
      "Epoch 68/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7428\n",
      "Epoch 68: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5513 - acc: 0.7429 - val_loss: 0.5318 - val_acc: 0.7550\n",
      "Epoch 69/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.7442\n",
      "Epoch 69: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5533 - acc: 0.7431 - val_loss: 0.5331 - val_acc: 0.7559\n",
      "Epoch 70/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.7427\n",
      "Epoch 70: val_acc improved from 0.76486 to 0.76528, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5548 - acc: 0.7427 - val_loss: 0.5228 - val_acc: 0.7653\n",
      "Epoch 71/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7419\n",
      "Epoch 71: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5660 - acc: 0.7409 - val_loss: 0.5367 - val_acc: 0.7533\n",
      "Epoch 72/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.7407\n",
      "Epoch 72: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5631 - acc: 0.7407 - val_loss: 0.5363 - val_acc: 0.7589\n",
      "Epoch 73/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5498 - acc: 0.7433\n",
      "Epoch 73: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5498 - acc: 0.7433 - val_loss: 0.5366 - val_acc: 0.7520\n",
      "Epoch 74/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5463 - acc: 0.7399\n",
      "Epoch 74: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5463 - acc: 0.7399 - val_loss: 0.5328 - val_acc: 0.7631\n",
      "Epoch 75/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5566 - acc: 0.7397\n",
      "Epoch 75: val_acc improved from 0.76528 to 0.76742, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5566 - acc: 0.7397 - val_loss: 0.5350 - val_acc: 0.7674\n",
      "Epoch 76/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.7428\n",
      "Epoch 76: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5557 - acc: 0.7428 - val_loss: 0.5380 - val_acc: 0.7597\n",
      "Epoch 77/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.7419\n",
      "Epoch 77: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5524 - acc: 0.7422 - val_loss: 0.5425 - val_acc: 0.7661\n",
      "Epoch 78/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7453\n",
      "Epoch 78: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5515 - acc: 0.7442 - val_loss: 0.5294 - val_acc: 0.7640\n",
      "Epoch 79/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.7452\n",
      "Epoch 79: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5627 - acc: 0.7445 - val_loss: 0.5593 - val_acc: 0.7636\n",
      "Epoch 80/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7446\n",
      "Epoch 80: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5544 - acc: 0.7438 - val_loss: 0.5464 - val_acc: 0.7623\n",
      "Epoch 81/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7465\n",
      "Epoch 81: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5518 - acc: 0.7454 - val_loss: 0.5561 - val_acc: 0.7606\n",
      "Epoch 82/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.7503\n",
      "Epoch 82: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5601 - acc: 0.7491 - val_loss: 0.5354 - val_acc: 0.7559\n",
      "Epoch 83/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5612 - acc: 0.7471\n",
      "Epoch 83: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5611 - acc: 0.7471 - val_loss: 0.5278 - val_acc: 0.7533\n",
      "Epoch 84/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7470\n",
      "Epoch 84: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5440 - acc: 0.7457 - val_loss: 0.5281 - val_acc: 0.7576\n",
      "Epoch 85/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7472\n",
      "Epoch 85: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5545 - acc: 0.7467 - val_loss: 0.5495 - val_acc: 0.7619\n",
      "Epoch 86/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7470\n",
      "Epoch 86: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5457 - acc: 0.7458 - val_loss: 0.5382 - val_acc: 0.7559\n",
      "Epoch 87/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7424\n",
      "Epoch 87: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5493 - acc: 0.7424 - val_loss: 0.5376 - val_acc: 0.7636\n",
      "Epoch 88/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7483\n",
      "Epoch 88: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5493 - acc: 0.7484 - val_loss: 0.5504 - val_acc: 0.7661\n",
      "Epoch 89/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7531\n",
      "Epoch 89: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5383 - acc: 0.7530 - val_loss: 0.5386 - val_acc: 0.7584\n",
      "Epoch 90/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7546\n",
      "Epoch 90: val_acc did not improve from 0.76742\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5508 - acc: 0.7538 - val_loss: 0.5193 - val_acc: 0.7627\n",
      "Epoch 91/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.7494\n",
      "Epoch 91: val_acc improved from 0.76742 to 0.77212, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5364 - acc: 0.7495 - val_loss: 0.5313 - val_acc: 0.7721\n",
      "Epoch 92/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5571 - acc: 0.7484\n",
      "Epoch 92: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5571 - acc: 0.7484 - val_loss: 0.5345 - val_acc: 0.7687\n",
      "Epoch 93/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7496\n",
      "Epoch 93: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5518 - acc: 0.7492 - val_loss: 0.5235 - val_acc: 0.7619\n",
      "Epoch 94/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7469\n",
      "Epoch 94: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5422 - acc: 0.7474 - val_loss: 0.5496 - val_acc: 0.7640\n",
      "Epoch 95/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.7462\n",
      "Epoch 95: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5639 - acc: 0.7460 - val_loss: 0.5280 - val_acc: 0.7700\n",
      "Epoch 96/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7522\n",
      "Epoch 96: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5503 - acc: 0.7524 - val_loss: 0.5538 - val_acc: 0.7678\n",
      "Epoch 97/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7544\n",
      "Epoch 97: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5336 - acc: 0.7544 - val_loss: 0.5681 - val_acc: 0.7614\n",
      "Epoch 98/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.7459\n",
      "Epoch 98: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5494 - acc: 0.7451 - val_loss: 0.5418 - val_acc: 0.7597\n",
      "Epoch 99/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.7496\n",
      "Epoch 99: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5555 - acc: 0.7497 - val_loss: 0.5413 - val_acc: 0.7649\n",
      "Epoch 100/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.7497\n",
      "Epoch 100: val_acc did not improve from 0.77212\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5392 - acc: 0.7497 - val_loss: 0.5427 - val_acc: 0.7580\n",
      "Epoch 101/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7510\n",
      "Epoch 101: val_acc improved from 0.77212 to 0.77255, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5420 - acc: 0.7509 - val_loss: 0.5459 - val_acc: 0.7726\n",
      "Epoch 102/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7514\n",
      "Epoch 102: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5462 - acc: 0.7516 - val_loss: 0.5533 - val_acc: 0.7653\n",
      "Epoch 103/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.7539\n",
      "Epoch 103: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5473 - acc: 0.7536 - val_loss: 0.5603 - val_acc: 0.7606\n",
      "Epoch 104/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.7496\n",
      "Epoch 104: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5543 - acc: 0.7498 - val_loss: 0.5391 - val_acc: 0.7691\n",
      "Epoch 105/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5468 - acc: 0.7459\n",
      "Epoch 105: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5468 - acc: 0.7459 - val_loss: 0.5423 - val_acc: 0.7606\n",
      "Epoch 106/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.7502\n",
      "Epoch 106: val_acc improved from 0.77255 to 0.77298, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5395 - acc: 0.7498 - val_loss: 0.5409 - val_acc: 0.7730\n",
      "Epoch 107/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7536\n",
      "Epoch 107: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5339 - acc: 0.7536 - val_loss: 0.5482 - val_acc: 0.7606\n",
      "Epoch 108/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.7596\n",
      "Epoch 108: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5310 - acc: 0.7585 - val_loss: 0.5535 - val_acc: 0.7683\n",
      "Epoch 109/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7488\n",
      "Epoch 109: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5461 - acc: 0.7487 - val_loss: 0.5334 - val_acc: 0.7708\n",
      "Epoch 110/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7533\n",
      "Epoch 110: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5569 - acc: 0.7522 - val_loss: 0.5422 - val_acc: 0.7593\n",
      "Epoch 111/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7589\n",
      "Epoch 111: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5316 - acc: 0.7580 - val_loss: 0.5281 - val_acc: 0.7678\n",
      "Epoch 112/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7557\n",
      "Epoch 112: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5385 - acc: 0.7559 - val_loss: 0.5439 - val_acc: 0.7683\n",
      "Epoch 113/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7558\n",
      "Epoch 113: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5499 - acc: 0.7552 - val_loss: 0.5510 - val_acc: 0.7696\n",
      "Epoch 114/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7500\n",
      "Epoch 114: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5357 - acc: 0.7500 - val_loss: 0.5629 - val_acc: 0.7576\n",
      "Epoch 115/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7576\n",
      "Epoch 115: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5391 - acc: 0.7568 - val_loss: 0.5338 - val_acc: 0.7580\n",
      "Epoch 116/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5343 - acc: 0.7610\n",
      "Epoch 116: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5343 - acc: 0.7610 - val_loss: 0.5437 - val_acc: 0.7597\n",
      "Epoch 117/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7519\n",
      "Epoch 117: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5485 - acc: 0.7511 - val_loss: 0.5328 - val_acc: 0.7661\n",
      "Epoch 118/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7623\n",
      "Epoch 118: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5362 - acc: 0.7624 - val_loss: 0.5488 - val_acc: 0.7674\n",
      "Epoch 119/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7553\n",
      "Epoch 119: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5459 - acc: 0.7549 - val_loss: 0.5472 - val_acc: 0.7666\n",
      "Epoch 120/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7571\n",
      "Epoch 120: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5388 - acc: 0.7563 - val_loss: 0.5621 - val_acc: 0.7666\n",
      "Epoch 121/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7536\n",
      "Epoch 121: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5434 - acc: 0.7533 - val_loss: 0.5345 - val_acc: 0.7653\n",
      "Epoch 122/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.7568\n",
      "Epoch 122: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5463 - acc: 0.7566 - val_loss: 0.5323 - val_acc: 0.7661\n",
      "Epoch 123/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.7573\n",
      "Epoch 123: val_acc did not improve from 0.77298\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5467 - acc: 0.7575 - val_loss: 0.5711 - val_acc: 0.7713\n",
      "Epoch 124/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7629\n",
      "Epoch 124: val_acc improved from 0.77298 to 0.77426, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5446 - acc: 0.7627 - val_loss: 0.5566 - val_acc: 0.7743\n",
      "Epoch 125/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7592\n",
      "Epoch 125: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7596 - val_loss: 0.5601 - val_acc: 0.7713\n",
      "Epoch 126/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7574\n",
      "Epoch 126: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5451 - acc: 0.7573 - val_loss: 0.5485 - val_acc: 0.7717\n",
      "Epoch 127/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7587\n",
      "Epoch 127: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5361 - acc: 0.7582 - val_loss: 0.5572 - val_acc: 0.7700\n",
      "Epoch 128/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.7545\n",
      "Epoch 128: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5526 - acc: 0.7544 - val_loss: 0.5645 - val_acc: 0.7623\n",
      "Epoch 129/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.7568\n",
      "Epoch 129: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5466 - acc: 0.7565 - val_loss: 0.5691 - val_acc: 0.7640\n",
      "Epoch 130/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7526\n",
      "Epoch 130: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5388 - acc: 0.7529 - val_loss: 0.5748 - val_acc: 0.7687\n",
      "Epoch 131/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.7591\n",
      "Epoch 131: val_acc improved from 0.77426 to 0.77726, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5567 - acc: 0.7580 - val_loss: 0.5497 - val_acc: 0.7773\n",
      "Epoch 132/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7589\n",
      "Epoch 132: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5366 - acc: 0.7585 - val_loss: 0.5534 - val_acc: 0.7631\n",
      "Epoch 133/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.7598\n",
      "Epoch 133: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5590 - acc: 0.7596 - val_loss: 0.5175 - val_acc: 0.7713\n",
      "Epoch 134/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7559\n",
      "Epoch 134: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5387 - acc: 0.7559 - val_loss: 0.5580 - val_acc: 0.7640\n",
      "Epoch 135/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7534\n",
      "Epoch 135: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5315 - acc: 0.7528 - val_loss: 0.5674 - val_acc: 0.7696\n",
      "Epoch 136/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7601\n",
      "Epoch 136: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5320 - acc: 0.7594 - val_loss: 0.5791 - val_acc: 0.7708\n",
      "Epoch 137/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7588\n",
      "Epoch 137: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5384 - acc: 0.7587 - val_loss: 0.5774 - val_acc: 0.7708\n",
      "Epoch 138/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7553\n",
      "Epoch 138: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5396 - acc: 0.7548 - val_loss: 0.5598 - val_acc: 0.7627\n",
      "Epoch 139/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.7569\n",
      "Epoch 139: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5543 - acc: 0.7568 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 140/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7631\n",
      "Epoch 140: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5364 - acc: 0.7631 - val_loss: 0.5593 - val_acc: 0.7640\n",
      "Epoch 141/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7601\n",
      "Epoch 141: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5340 - acc: 0.7598 - val_loss: 0.5914 - val_acc: 0.7670\n",
      "Epoch 142/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5683 - acc: 0.7521\n",
      "Epoch 142: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5677 - acc: 0.7516 - val_loss: 0.5607 - val_acc: 0.7661\n",
      "Epoch 143/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7598\n",
      "Epoch 143: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5387 - acc: 0.7586 - val_loss: 0.5699 - val_acc: 0.7661\n",
      "Epoch 144/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5605 - acc: 0.7550\n",
      "Epoch 144: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5599 - acc: 0.7546 - val_loss: 0.5562 - val_acc: 0.7636\n",
      "Epoch 145/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.7651\n",
      "Epoch 145: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5359 - acc: 0.7641 - val_loss: 0.5819 - val_acc: 0.7678\n",
      "Epoch 146/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.7549\n",
      "Epoch 146: val_acc did not improve from 0.77726\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5567 - acc: 0.7549 - val_loss: 0.5659 - val_acc: 0.7678\n",
      "Epoch 147/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7629\n",
      "Epoch 147: val_acc improved from 0.77726 to 0.77768, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5316 - acc: 0.7624 - val_loss: 0.5648 - val_acc: 0.7777\n",
      "Epoch 148/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.7609\n",
      "Epoch 148: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5406 - acc: 0.7606 - val_loss: 0.5651 - val_acc: 0.7687\n",
      "Epoch 149/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5310 - acc: 0.7610\n",
      "Epoch 149: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5309 - acc: 0.7611 - val_loss: 0.5954 - val_acc: 0.7717\n",
      "Epoch 150/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.7611\n",
      "Epoch 150: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5464 - acc: 0.7611 - val_loss: 0.5650 - val_acc: 0.7640\n",
      "Epoch 151/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.7621\n",
      "Epoch 151: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5487 - acc: 0.7608 - val_loss: 0.5413 - val_acc: 0.7666\n",
      "Epoch 152/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7663\n",
      "Epoch 152: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5204 - acc: 0.7663 - val_loss: 0.5590 - val_acc: 0.7683\n",
      "Epoch 153/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.7641\n",
      "Epoch 153: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5376 - acc: 0.7631 - val_loss: 0.5648 - val_acc: 0.7674\n",
      "Epoch 154/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7678\n",
      "Epoch 154: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5487 - acc: 0.7666 - val_loss: 0.5518 - val_acc: 0.7747\n",
      "Epoch 155/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7652\n",
      "Epoch 155: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5397 - acc: 0.7656 - val_loss: 0.5959 - val_acc: 0.7666\n",
      "Epoch 156/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.7622\n",
      "Epoch 156: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5561 - acc: 0.7606 - val_loss: 0.5549 - val_acc: 0.7691\n",
      "Epoch 157/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5243 - acc: 0.7615\n",
      "Epoch 157: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5243 - acc: 0.7615 - val_loss: 0.5614 - val_acc: 0.7700\n",
      "Epoch 158/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.7639\n",
      "Epoch 158: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5244 - acc: 0.7637 - val_loss: 0.5764 - val_acc: 0.7661\n",
      "Epoch 159/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7627\n",
      "Epoch 159: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5427 - acc: 0.7627 - val_loss: 0.5681 - val_acc: 0.7657\n",
      "Epoch 160/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7656\n",
      "Epoch 160: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5338 - acc: 0.7653 - val_loss: 0.5934 - val_acc: 0.7760\n",
      "Epoch 161/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.7661\n",
      "Epoch 161: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5345 - acc: 0.7654 - val_loss: 0.6037 - val_acc: 0.7747\n",
      "Epoch 162/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5601 - acc: 0.7595\n",
      "Epoch 162: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5601 - acc: 0.7595 - val_loss: 0.5681 - val_acc: 0.7644\n",
      "Epoch 163/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7557\n",
      "Epoch 163: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5489 - acc: 0.7555 - val_loss: 0.5766 - val_acc: 0.7687\n",
      "Epoch 164/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7661\n",
      "Epoch 164: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5385 - acc: 0.7652 - val_loss: 0.5675 - val_acc: 0.7670\n",
      "Epoch 165/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5296 - acc: 0.7655\n",
      "Epoch 165: val_acc improved from 0.77768 to 0.77854, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5293 - acc: 0.7646 - val_loss: 0.5370 - val_acc: 0.7785\n",
      "Epoch 166/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.7623\n",
      "Epoch 166: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5311 - acc: 0.7623 - val_loss: 0.5507 - val_acc: 0.7614\n",
      "Epoch 167/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7598\n",
      "Epoch 167: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5332 - acc: 0.7598 - val_loss: 0.5833 - val_acc: 0.7687\n",
      "Epoch 168/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.7623\n",
      "Epoch 168: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5511 - acc: 0.7621 - val_loss: 0.5573 - val_acc: 0.7691\n",
      "Epoch 169/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7649\n",
      "Epoch 169: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5283 - acc: 0.7652 - val_loss: 0.5658 - val_acc: 0.7721\n",
      "Epoch 170/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7617\n",
      "Epoch 170: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5490 - acc: 0.7618 - val_loss: 0.5848 - val_acc: 0.7674\n",
      "Epoch 171/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.7632\n",
      "Epoch 171: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5535 - acc: 0.7624 - val_loss: 0.5610 - val_acc: 0.7704\n",
      "Epoch 172/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5505 - acc: 0.7632\n",
      "Epoch 172: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5505 - acc: 0.7632 - val_loss: 0.5762 - val_acc: 0.7610\n",
      "Epoch 173/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7621\n",
      "Epoch 173: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5462 - acc: 0.7619 - val_loss: 0.5681 - val_acc: 0.7696\n",
      "Epoch 174/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7654\n",
      "Epoch 174: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5350 - acc: 0.7655 - val_loss: 0.6522 - val_acc: 0.7696\n",
      "Epoch 175/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5427 - acc: 0.7612\n",
      "Epoch 175: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5427 - acc: 0.7612 - val_loss: 0.5944 - val_acc: 0.7691\n",
      "Epoch 176/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.7628\n",
      "Epoch 176: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5634 - acc: 0.7629 - val_loss: 0.5667 - val_acc: 0.7751\n",
      "Epoch 177/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7639\n",
      "Epoch 177: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5300 - acc: 0.7631 - val_loss: 0.6288 - val_acc: 0.7683\n",
      "Epoch 178/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7673\n",
      "Epoch 178: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5238 - acc: 0.7670 - val_loss: 0.6061 - val_acc: 0.7627\n",
      "Epoch 179/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.7647\n",
      "Epoch 179: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5445 - acc: 0.7649 - val_loss: 0.5643 - val_acc: 0.7726\n",
      "Epoch 180/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.7682\n",
      "Epoch 180: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5412 - acc: 0.7672 - val_loss: 0.5607 - val_acc: 0.7619\n",
      "Epoch 181/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5306 - acc: 0.7645\n",
      "Epoch 181: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5306 - acc: 0.7645 - val_loss: 0.5994 - val_acc: 0.7734\n",
      "Epoch 182/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7640\n",
      "Epoch 182: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5461 - acc: 0.7631 - val_loss: 0.5865 - val_acc: 0.7717\n",
      "Epoch 183/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7750\n",
      "Epoch 183: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5360 - acc: 0.7749 - val_loss: 0.5884 - val_acc: 0.7674\n",
      "Epoch 184/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.7703\n",
      "Epoch 184: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5310 - acc: 0.7705 - val_loss: 0.6507 - val_acc: 0.7721\n",
      "Epoch 185/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7599\n",
      "Epoch 185: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.7602 - val_loss: 0.5672 - val_acc: 0.7704\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7680\n",
      "Epoch 186: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5323 - acc: 0.7679 - val_loss: 0.5738 - val_acc: 0.7734\n",
      "Epoch 187/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5352 - acc: 0.7657\n",
      "Epoch 187: val_acc did not improve from 0.77854\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5352 - acc: 0.7657 - val_loss: 0.5602 - val_acc: 0.7649\n",
      "Epoch 188/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.7749\n",
      "Epoch 188: val_acc improved from 0.77854 to 0.78025, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5165 - acc: 0.7750 - val_loss: 0.6133 - val_acc: 0.7802\n",
      "Epoch 189/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5304 - acc: 0.7690\n",
      "Epoch 189: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.7690 - val_loss: 0.5965 - val_acc: 0.7743\n",
      "Epoch 190/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.7659\n",
      "Epoch 190: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5656 - acc: 0.7663 - val_loss: 0.5703 - val_acc: 0.7687\n",
      "Epoch 191/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5296 - acc: 0.7671\n",
      "Epoch 191: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5298 - acc: 0.7677 - val_loss: 0.6350 - val_acc: 0.7649\n",
      "Epoch 192/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.7659\n",
      "Epoch 192: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5413 - acc: 0.7661 - val_loss: 0.5783 - val_acc: 0.7721\n",
      "Epoch 193/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7666\n",
      "Epoch 193: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5307 - acc: 0.7665 - val_loss: 0.6483 - val_acc: 0.7730\n",
      "Epoch 194/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7711\n",
      "Epoch 194: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5596 - acc: 0.7710 - val_loss: 0.5833 - val_acc: 0.7691\n",
      "Epoch 195/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.7667\n",
      "Epoch 195: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5436 - acc: 0.7666 - val_loss: 0.5986 - val_acc: 0.7802\n",
      "Epoch 196/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.7753\n",
      "Epoch 196: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5472 - acc: 0.7742 - val_loss: 0.5785 - val_acc: 0.7670\n",
      "Epoch 197/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.7737\n",
      "Epoch 197: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5192 - acc: 0.7736 - val_loss: 0.5739 - val_acc: 0.7700\n",
      "Epoch 198/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7739\n",
      "Epoch 198: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5299 - acc: 0.7734 - val_loss: 0.5906 - val_acc: 0.7755\n",
      "Epoch 199/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7673\n",
      "Epoch 199: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5214 - acc: 0.7674 - val_loss: 0.5863 - val_acc: 0.7687\n",
      "Epoch 200/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5257 - acc: 0.7700\n",
      "Epoch 200: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5257 - acc: 0.7700 - val_loss: 0.5841 - val_acc: 0.7721\n",
      "Epoch 201/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7695\n",
      "Epoch 201: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5239 - acc: 0.7694 - val_loss: 0.6025 - val_acc: 0.7644\n",
      "Epoch 202/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7615\n",
      "Epoch 202: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5470 - acc: 0.7617 - val_loss: 0.6187 - val_acc: 0.7738\n",
      "Epoch 203/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7689\n",
      "Epoch 203: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5255 - acc: 0.7687 - val_loss: 0.5954 - val_acc: 0.7755\n",
      "Epoch 204/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7680\n",
      "Epoch 204: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5335 - acc: 0.7679 - val_loss: 0.6233 - val_acc: 0.7619\n",
      "Epoch 205/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7725\n",
      "Epoch 205: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5303 - acc: 0.7724 - val_loss: 0.6153 - val_acc: 0.7721\n",
      "Epoch 206/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7664\n",
      "Epoch 206: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5381 - acc: 0.7663 - val_loss: 0.6019 - val_acc: 0.7661\n",
      "Epoch 207/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7696\n",
      "Epoch 207: val_acc improved from 0.78025 to 0.78281, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5344 - acc: 0.7697 - val_loss: 0.6750 - val_acc: 0.7828\n",
      "Epoch 208/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7726\n",
      "Epoch 208: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5264 - acc: 0.7728 - val_loss: 0.6310 - val_acc: 0.7717\n",
      "Epoch 209/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.7720\n",
      "Epoch 209: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5219 - acc: 0.7718 - val_loss: 0.6280 - val_acc: 0.7683\n",
      "Epoch 210/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7732\n",
      "Epoch 210: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5264 - acc: 0.7726 - val_loss: 0.6029 - val_acc: 0.7764\n",
      "Epoch 211/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7613\n",
      "Epoch 211: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5178 - acc: 0.7614 - val_loss: 0.6217 - val_acc: 0.7606\n",
      "Epoch 212/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.7695\n",
      "Epoch 212: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5390 - acc: 0.7688 - val_loss: 0.6406 - val_acc: 0.7747\n",
      "Epoch 213/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7662\n",
      "Epoch 213: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5356 - acc: 0.7662 - val_loss: 0.6269 - val_acc: 0.7755\n",
      "Epoch 214/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7693\n",
      "Epoch 214: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5290 - acc: 0.7692 - val_loss: 0.6296 - val_acc: 0.7631\n",
      "Epoch 215/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7711\n",
      "Epoch 215: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7703 - val_loss: 0.6277 - val_acc: 0.7704\n",
      "Epoch 216/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.7677\n",
      "Epoch 216: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5468 - acc: 0.7669 - val_loss: 0.6095 - val_acc: 0.7623\n",
      "Epoch 217/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.7712\n",
      "Epoch 217: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5500 - acc: 0.7711 - val_loss: 0.6111 - val_acc: 0.7768\n",
      "Epoch 218/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7719\n",
      "Epoch 218: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5321 - acc: 0.7720 - val_loss: 0.6282 - val_acc: 0.7785\n",
      "Epoch 219/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7684\n",
      "Epoch 219: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5384 - acc: 0.7681 - val_loss: 0.5961 - val_acc: 0.7755\n",
      "Epoch 220/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7723\n",
      "Epoch 220: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5346 - acc: 0.7719 - val_loss: 0.6171 - val_acc: 0.7751\n",
      "Epoch 221/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7734\n",
      "Epoch 221: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5394 - acc: 0.7733 - val_loss: 0.6593 - val_acc: 0.7738\n",
      "Epoch 222/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7716\n",
      "Epoch 222: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5241 - acc: 0.7715 - val_loss: 0.6174 - val_acc: 0.7631\n",
      "Epoch 223/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7674\n",
      "Epoch 223: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5191 - acc: 0.7673 - val_loss: 0.6323 - val_acc: 0.7751\n",
      "Epoch 224/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5377 - acc: 0.7753\n",
      "Epoch 224: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5377 - acc: 0.7753 - val_loss: 0.6846 - val_acc: 0.7777\n",
      "Epoch 225/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.7679\n",
      "Epoch 225: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5565 - acc: 0.7680 - val_loss: 0.6125 - val_acc: 0.7700\n",
      "Epoch 226/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7669\n",
      "Epoch 226: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5415 - acc: 0.7672 - val_loss: 0.6136 - val_acc: 0.7785\n",
      "Epoch 227/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7667\n",
      "Epoch 227: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5400 - acc: 0.7668 - val_loss: 0.6279 - val_acc: 0.7653\n",
      "Epoch 228/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7703\n",
      "Epoch 228: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5395 - acc: 0.7707 - val_loss: 0.6187 - val_acc: 0.7730\n",
      "Epoch 229/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5570 - acc: 0.7725\n",
      "Epoch 229: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5570 - acc: 0.7725 - val_loss: 0.6404 - val_acc: 0.7653\n",
      "Epoch 230/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.7690\n",
      "Epoch 230: val_acc improved from 0.78281 to 0.78495, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5306 - acc: 0.7692 - val_loss: 0.6523 - val_acc: 0.7850\n",
      "Epoch 231/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.7764\n",
      "Epoch 231: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5271 - acc: 0.7764 - val_loss: 0.5962 - val_acc: 0.7755\n",
      "Epoch 232/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7741\n",
      "Epoch 232: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5486 - acc: 0.7736 - val_loss: 0.6136 - val_acc: 0.7743\n",
      "Epoch 233/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7739\n",
      "Epoch 233: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5228 - acc: 0.7737 - val_loss: 0.6417 - val_acc: 0.7717\n",
      "Epoch 234/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7702\n",
      "Epoch 234: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5501 - acc: 0.7702 - val_loss: 0.5856 - val_acc: 0.7798\n",
      "Epoch 235/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.7761\n",
      "Epoch 235: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5188 - acc: 0.7758 - val_loss: 0.6361 - val_acc: 0.7790\n",
      "Epoch 236/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5430 - acc: 0.7684\n",
      "Epoch 236: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5430 - acc: 0.7684 - val_loss: 0.5975 - val_acc: 0.7755\n",
      "Epoch 237/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.7712\n",
      "Epoch 237: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5381 - acc: 0.7719 - val_loss: 0.6395 - val_acc: 0.7777\n",
      "Epoch 238/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.7760\n",
      "Epoch 238: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5217 - acc: 0.7763 - val_loss: 0.6531 - val_acc: 0.7760\n",
      "Epoch 239/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.7718\n",
      "Epoch 239: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5217 - acc: 0.7712 - val_loss: 0.6427 - val_acc: 0.7785\n",
      "Epoch 240/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7749\n",
      "Epoch 240: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5301 - acc: 0.7748 - val_loss: 0.6178 - val_acc: 0.7760\n",
      "Epoch 241/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7707\n",
      "Epoch 241: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5287 - acc: 0.7696 - val_loss: 0.6506 - val_acc: 0.7726\n",
      "Epoch 242/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7686\n",
      "Epoch 242: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5489 - acc: 0.7691 - val_loss: 0.6404 - val_acc: 0.7820\n",
      "Epoch 243/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7685\n",
      "Epoch 243: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5353 - acc: 0.7685 - val_loss: 0.6177 - val_acc: 0.7811\n",
      "Epoch 244/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7798\n",
      "Epoch 244: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7792 - val_loss: 0.6039 - val_acc: 0.7640\n",
      "Epoch 245/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7785\n",
      "Epoch 245: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5158 - acc: 0.7784 - val_loss: 0.6118 - val_acc: 0.7768\n",
      "Epoch 246/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7745\n",
      "Epoch 246: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5262 - acc: 0.7740 - val_loss: 0.6784 - val_acc: 0.7730\n",
      "Epoch 247/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.7688\n",
      "Epoch 247: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5472 - acc: 0.7687 - val_loss: 0.6131 - val_acc: 0.7726\n",
      "Epoch 248/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7787\n",
      "Epoch 248: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5315 - acc: 0.7782 - val_loss: 0.6643 - val_acc: 0.7678\n",
      "Epoch 249/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7787\n",
      "Epoch 249: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5323 - acc: 0.7782 - val_loss: 0.6860 - val_acc: 0.7670\n",
      "Epoch 250/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7767\n",
      "Epoch 250: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5363 - acc: 0.7770 - val_loss: 0.6798 - val_acc: 0.7777\n",
      "Epoch 251/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7758\n",
      "Epoch 251: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5321 - acc: 0.7761 - val_loss: 0.6945 - val_acc: 0.7653\n",
      "Epoch 252/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.7675\n",
      "Epoch 252: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5376 - acc: 0.7672 - val_loss: 0.6853 - val_acc: 0.7811\n",
      "Epoch 253/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7715\n",
      "Epoch 253: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5387 - acc: 0.7717 - val_loss: 0.6936 - val_acc: 0.7717\n",
      "Epoch 254/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5626 - acc: 0.7717\n",
      "Epoch 254: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5626 - acc: 0.7717 - val_loss: 0.6579 - val_acc: 0.7721\n",
      "Epoch 255/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7737\n",
      "Epoch 255: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5253 - acc: 0.7737 - val_loss: 0.7095 - val_acc: 0.7623\n",
      "Epoch 256/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7703\n",
      "Epoch 256: val_acc did not improve from 0.78495\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5328 - acc: 0.7707 - val_loss: 0.6933 - val_acc: 0.7768\n",
      "Epoch 257/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7744\n",
      "Epoch 257: val_acc improved from 0.78495 to 0.78538, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5228 - acc: 0.7743 - val_loss: 0.6606 - val_acc: 0.7854\n",
      "Epoch 258/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.7714\n",
      "Epoch 258: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5371 - acc: 0.7704 - val_loss: 0.6051 - val_acc: 0.7614\n",
      "Epoch 259/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7727\n",
      "Epoch 259: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5329 - acc: 0.7725 - val_loss: 0.6266 - val_acc: 0.7670\n",
      "Epoch 260/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5484 - acc: 0.7704\n",
      "Epoch 260: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5484 - acc: 0.7704 - val_loss: 0.6183 - val_acc: 0.7713\n",
      "Epoch 261/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7818\n",
      "Epoch 261: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5361 - acc: 0.7817 - val_loss: 0.6834 - val_acc: 0.7798\n",
      "Epoch 262/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5464 - acc: 0.7677\n",
      "Epoch 262: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5464 - acc: 0.7677 - val_loss: 0.6598 - val_acc: 0.7730\n",
      "Epoch 263/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.7740\n",
      "Epoch 263: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5346 - acc: 0.7731 - val_loss: 0.6046 - val_acc: 0.7730\n",
      "Epoch 264/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.7756\n",
      "Epoch 264: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5311 - acc: 0.7752 - val_loss: 0.6827 - val_acc: 0.7708\n",
      "Epoch 265/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.7753\n",
      "Epoch 265: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5476 - acc: 0.7736 - val_loss: 0.6185 - val_acc: 0.7726\n",
      "Epoch 266/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7761\n",
      "Epoch 266: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5275 - acc: 0.7763 - val_loss: 0.6044 - val_acc: 0.7777\n",
      "Epoch 267/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.7749\n",
      "Epoch 267: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5227 - acc: 0.7749 - val_loss: 0.6159 - val_acc: 0.7751\n",
      "Epoch 268/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7764\n",
      "Epoch 268: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5254 - acc: 0.7765 - val_loss: 0.6494 - val_acc: 0.7751\n",
      "Epoch 269/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7802\n",
      "Epoch 269: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5386 - acc: 0.7801 - val_loss: 0.6397 - val_acc: 0.7781\n",
      "Epoch 270/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7771\n",
      "Epoch 270: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5335 - acc: 0.7765 - val_loss: 0.5793 - val_acc: 0.7798\n",
      "Epoch 271/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7712\n",
      "Epoch 271: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5317 - acc: 0.7710 - val_loss: 0.6403 - val_acc: 0.7738\n",
      "Epoch 272/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7778\n",
      "Epoch 272: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5270 - acc: 0.7780 - val_loss: 0.6993 - val_acc: 0.7747\n",
      "Epoch 273/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7729\n",
      "Epoch 273: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5460 - acc: 0.7730 - val_loss: 0.5602 - val_acc: 0.7619\n",
      "Epoch 274/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7796\n",
      "Epoch 274: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5191 - acc: 0.7793 - val_loss: 0.5932 - val_acc: 0.7726\n",
      "Epoch 275/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7786\n",
      "Epoch 275: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5289 - acc: 0.7778 - val_loss: 0.6220 - val_acc: 0.7751\n",
      "Epoch 276/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7733\n",
      "Epoch 276: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5493 - acc: 0.7733 - val_loss: 0.6042 - val_acc: 0.7781\n",
      "Epoch 277/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7772\n",
      "Epoch 277: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5488 - acc: 0.7764 - val_loss: 0.6522 - val_acc: 0.7815\n",
      "Epoch 278/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7717\n",
      "Epoch 278: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5323 - acc: 0.7717 - val_loss: 0.6775 - val_acc: 0.7755\n",
      "Epoch 279/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5287 - acc: 0.7786\n",
      "Epoch 279: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5287 - acc: 0.7786 - val_loss: 0.6924 - val_acc: 0.7850\n",
      "Epoch 280/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7781\n",
      "Epoch 280: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5343 - acc: 0.7781 - val_loss: 0.6401 - val_acc: 0.7773\n",
      "Epoch 281/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.7757\n",
      "Epoch 281: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5150 - acc: 0.7757 - val_loss: 0.7194 - val_acc: 0.7815\n",
      "Epoch 282/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7809\n",
      "Epoch 282: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5411 - acc: 0.7803 - val_loss: 0.6632 - val_acc: 0.7845\n",
      "Epoch 283/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7763\n",
      "Epoch 283: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5293 - acc: 0.7756 - val_loss: 0.6525 - val_acc: 0.7743\n",
      "Epoch 284/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5475 - acc: 0.7809\n",
      "Epoch 284: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5475 - acc: 0.7809 - val_loss: 0.7639 - val_acc: 0.7828\n",
      "Epoch 285/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7759\n",
      "Epoch 285: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5236 - acc: 0.7762 - val_loss: 0.7276 - val_acc: 0.7832\n",
      "Epoch 286/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.7809\n",
      "Epoch 286: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5130 - acc: 0.7810 - val_loss: 0.6669 - val_acc: 0.7794\n",
      "Epoch 287/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7782\n",
      "Epoch 287: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5492 - acc: 0.7781 - val_loss: 0.6755 - val_acc: 0.7794\n",
      "Epoch 288/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.7744\n",
      "Epoch 288: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5525 - acc: 0.7736 - val_loss: 0.6268 - val_acc: 0.7747\n",
      "Epoch 289/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7765\n",
      "Epoch 289: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5188 - acc: 0.7769 - val_loss: 0.6927 - val_acc: 0.7785\n",
      "Epoch 290/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5454 - acc: 0.7787\n",
      "Epoch 290: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5454 - acc: 0.7787 - val_loss: 0.6777 - val_acc: 0.7777\n",
      "Epoch 291/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7789\n",
      "Epoch 291: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5329 - acc: 0.7787 - val_loss: 0.6728 - val_acc: 0.7730\n",
      "Epoch 292/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5346 - acc: 0.7778\n",
      "Epoch 292: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5346 - acc: 0.7778 - val_loss: 0.6188 - val_acc: 0.7653\n",
      "Epoch 293/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.7797\n",
      "Epoch 293: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5213 - acc: 0.7786 - val_loss: 0.6654 - val_acc: 0.7683\n",
      "Epoch 294/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7749\n",
      "Epoch 294: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5269 - acc: 0.7743 - val_loss: 0.6593 - val_acc: 0.7773\n",
      "Epoch 295/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.7778\n",
      "Epoch 295: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5270 - acc: 0.7772 - val_loss: 0.7586 - val_acc: 0.7802\n",
      "Epoch 296/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7801\n",
      "Epoch 296: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5360 - acc: 0.7804 - val_loss: 0.6750 - val_acc: 0.7777\n",
      "Epoch 297/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7817\n",
      "Epoch 297: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5275 - acc: 0.7817 - val_loss: 0.6722 - val_acc: 0.7845\n",
      "Epoch 298/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.7801\n",
      "Epoch 298: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5249 - acc: 0.7802 - val_loss: 0.6756 - val_acc: 0.7743\n",
      "Epoch 299/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7733\n",
      "Epoch 299: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7724 - val_loss: 0.6836 - val_acc: 0.7828\n",
      "Epoch 300/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7826\n",
      "Epoch 300: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5293 - acc: 0.7829 - val_loss: 0.7637 - val_acc: 0.7854\n",
      "Epoch 301/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.7800\n",
      "Epoch 301: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5231 - acc: 0.7795 - val_loss: 0.6975 - val_acc: 0.7850\n",
      "Epoch 302/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7775\n",
      "Epoch 302: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5242 - acc: 0.7774 - val_loss: 0.6422 - val_acc: 0.7798\n",
      "Epoch 303/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7829\n",
      "Epoch 303: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5211 - acc: 0.7829 - val_loss: 0.7169 - val_acc: 0.7790\n",
      "Epoch 304/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7785\n",
      "Epoch 304: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5231 - acc: 0.7776 - val_loss: 0.6868 - val_acc: 0.7678\n",
      "Epoch 305/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5412 - acc: 0.7746\n",
      "Epoch 305: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.7746 - val_loss: 0.6497 - val_acc: 0.7734\n",
      "Epoch 306/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5469 - acc: 0.7736\n",
      "Epoch 306: val_acc did not improve from 0.78538\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5469 - acc: 0.7736 - val_loss: 0.6525 - val_acc: 0.7700\n",
      "Epoch 307/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.7766\n",
      "Epoch 307: val_acc improved from 0.78538 to 0.78837, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5322 - acc: 0.7751 - val_loss: 0.7101 - val_acc: 0.7884\n",
      "Epoch 308/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7772\n",
      "Epoch 308: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7773 - val_loss: 0.7214 - val_acc: 0.7777\n",
      "Epoch 309/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7775\n",
      "Epoch 309: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5249 - acc: 0.7773 - val_loss: 0.7533 - val_acc: 0.7820\n",
      "Epoch 310/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7795\n",
      "Epoch 310: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5241 - acc: 0.7795 - val_loss: 0.6827 - val_acc: 0.7738\n",
      "Epoch 311/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7828\n",
      "Epoch 311: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5210 - acc: 0.7818 - val_loss: 0.6628 - val_acc: 0.7666\n",
      "Epoch 312/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.7820\n",
      "Epoch 312: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5151 - acc: 0.7818 - val_loss: 0.6694 - val_acc: 0.7687\n",
      "Epoch 313/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.7748\n",
      "Epoch 313: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5524 - acc: 0.7746 - val_loss: 0.7725 - val_acc: 0.7747\n",
      "Epoch 314/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.7734\n",
      "Epoch 314: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5336 - acc: 0.7737 - val_loss: 0.6784 - val_acc: 0.7785\n",
      "Epoch 315/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7802\n",
      "Epoch 315: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5255 - acc: 0.7792 - val_loss: 0.6764 - val_acc: 0.7747\n",
      "Epoch 316/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.7803\n",
      "Epoch 316: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5405 - acc: 0.7797 - val_loss: 0.7158 - val_acc: 0.7824\n",
      "Epoch 317/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7841\n",
      "Epoch 317: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5472 - acc: 0.7841 - val_loss: 0.6157 - val_acc: 0.7704\n",
      "Epoch 318/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5136 - acc: 0.7816\n",
      "Epoch 318: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5136 - acc: 0.7816 - val_loss: 0.7142 - val_acc: 0.7794\n",
      "Epoch 319/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.7841\n",
      "Epoch 319: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5117 - acc: 0.7843 - val_loss: 0.7155 - val_acc: 0.7790\n",
      "Epoch 320/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7783\n",
      "Epoch 320: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5466 - acc: 0.7779 - val_loss: 0.6453 - val_acc: 0.7781\n",
      "Epoch 321/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7862\n",
      "Epoch 321: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5139 - acc: 0.7860 - val_loss: 0.7168 - val_acc: 0.7828\n",
      "Epoch 322/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.7828\n",
      "Epoch 322: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5277 - acc: 0.7826 - val_loss: 0.6386 - val_acc: 0.7832\n",
      "Epoch 323/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7825\n",
      "Epoch 323: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5168 - acc: 0.7826 - val_loss: 0.6980 - val_acc: 0.7773\n",
      "Epoch 324/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7794\n",
      "Epoch 324: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5328 - acc: 0.7797 - val_loss: 0.7094 - val_acc: 0.7755\n",
      "Epoch 325/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.7720\n",
      "Epoch 325: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5334 - acc: 0.7717 - val_loss: 0.7169 - val_acc: 0.7730\n",
      "Epoch 326/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.7759\n",
      "Epoch 326: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5287 - acc: 0.7766 - val_loss: 0.7823 - val_acc: 0.7704\n",
      "Epoch 327/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5367 - acc: 0.7790\n",
      "Epoch 327: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5357 - acc: 0.7795 - val_loss: 0.6620 - val_acc: 0.7696\n",
      "Epoch 328/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.7855\n",
      "Epoch 328: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5418 - acc: 0.7855 - val_loss: 0.6380 - val_acc: 0.7700\n",
      "Epoch 329/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7794\n",
      "Epoch 329: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5449 - acc: 0.7779 - val_loss: 0.6777 - val_acc: 0.7781\n",
      "Epoch 330/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7727\n",
      "Epoch 330: val_acc improved from 0.78837 to 0.79008, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5452 - acc: 0.7725 - val_loss: 0.7569 - val_acc: 0.7901\n",
      "Epoch 331/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.7810\n",
      "Epoch 331: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5217 - acc: 0.7809 - val_loss: 0.7147 - val_acc: 0.7837\n",
      "Epoch 332/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7808\n",
      "Epoch 332: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5295 - acc: 0.7809 - val_loss: 0.6519 - val_acc: 0.7486\n",
      "Epoch 333/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5227 - acc: 0.7764\n",
      "Epoch 333: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5219 - acc: 0.7756 - val_loss: 0.7732 - val_acc: 0.7820\n",
      "Epoch 334/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7821\n",
      "Epoch 334: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5353 - acc: 0.7824 - val_loss: 0.6793 - val_acc: 0.7850\n",
      "Epoch 335/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.7861\n",
      "Epoch 335: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5271 - acc: 0.7854 - val_loss: 0.6958 - val_acc: 0.7734\n",
      "Epoch 336/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.7754\n",
      "Epoch 336: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5271 - acc: 0.7743 - val_loss: 0.7364 - val_acc: 0.7777\n",
      "Epoch 337/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7816\n",
      "Epoch 337: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5234 - acc: 0.7804 - val_loss: 0.6740 - val_acc: 0.7824\n",
      "Epoch 338/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.7869\n",
      "Epoch 338: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5034 - acc: 0.7873 - val_loss: 0.7085 - val_acc: 0.7879\n",
      "Epoch 339/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5604 - acc: 0.7784\n",
      "Epoch 339: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5598 - acc: 0.7785 - val_loss: 0.6770 - val_acc: 0.7743\n",
      "Epoch 340/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.7827\n",
      "Epoch 340: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5301 - acc: 0.7827 - val_loss: 0.7019 - val_acc: 0.7790\n",
      "Epoch 341/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7786\n",
      "Epoch 341: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5397 - acc: 0.7780 - val_loss: 0.6560 - val_acc: 0.7636\n",
      "Epoch 342/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7841\n",
      "Epoch 342: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5183 - acc: 0.7841 - val_loss: 0.7088 - val_acc: 0.7649\n",
      "Epoch 343/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.7836\n",
      "Epoch 343: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5114 - acc: 0.7835 - val_loss: 0.7890 - val_acc: 0.7850\n",
      "Epoch 344/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7777\n",
      "Epoch 344: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5378 - acc: 0.7779 - val_loss: 0.6696 - val_acc: 0.7704\n",
      "Epoch 345/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5306 - acc: 0.7816\n",
      "Epoch 345: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5306 - acc: 0.7816 - val_loss: 0.7240 - val_acc: 0.7850\n",
      "Epoch 346/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7852\n",
      "Epoch 346: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.7846 - val_loss: 0.6570 - val_acc: 0.7841\n",
      "Epoch 347/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.7862\n",
      "Epoch 347: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5057 - acc: 0.7850 - val_loss: 0.6743 - val_acc: 0.7738\n",
      "Epoch 348/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5484 - acc: 0.7746\n",
      "Epoch 348: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5474 - acc: 0.7740 - val_loss: 0.6893 - val_acc: 0.7708\n",
      "Epoch 349/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5129 - acc: 0.7793\n",
      "Epoch 349: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5129 - acc: 0.7793 - val_loss: 0.7770 - val_acc: 0.7760\n",
      "Epoch 350/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7755\n",
      "Epoch 350: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5329 - acc: 0.7750 - val_loss: 0.7890 - val_acc: 0.7798\n",
      "Epoch 351/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5415 - acc: 0.7747\n",
      "Epoch 351: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5415 - acc: 0.7747 - val_loss: 0.6918 - val_acc: 0.7657\n",
      "Epoch 352/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7818\n",
      "Epoch 352: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5271 - acc: 0.7813 - val_loss: 0.6462 - val_acc: 0.7730\n",
      "Epoch 353/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7823\n",
      "Epoch 353: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5313 - acc: 0.7810 - val_loss: 0.6883 - val_acc: 0.7717\n",
      "Epoch 354/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5258 - acc: 0.7832\n",
      "Epoch 354: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5258 - acc: 0.7832 - val_loss: 0.6830 - val_acc: 0.7764\n",
      "Epoch 355/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7773\n",
      "Epoch 355: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5256 - acc: 0.7759 - val_loss: 0.7313 - val_acc: 0.7704\n",
      "Epoch 356/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7739\n",
      "Epoch 356: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5360 - acc: 0.7745 - val_loss: 0.8422 - val_acc: 0.7713\n",
      "Epoch 357/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7816\n",
      "Epoch 357: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5360 - acc: 0.7803 - val_loss: 0.7634 - val_acc: 0.7790\n",
      "Epoch 358/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.7784\n",
      "Epoch 358: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5425 - acc: 0.7776 - val_loss: 0.7075 - val_acc: 0.7794\n",
      "Epoch 359/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.7828\n",
      "Epoch 359: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5267 - acc: 0.7831 - val_loss: 0.7402 - val_acc: 0.7790\n",
      "Epoch 360/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7862\n",
      "Epoch 360: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5369 - acc: 0.7856 - val_loss: 0.7398 - val_acc: 0.7824\n",
      "Epoch 361/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7812\n",
      "Epoch 361: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5370 - acc: 0.7808 - val_loss: 0.7756 - val_acc: 0.7700\n",
      "Epoch 362/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7830\n",
      "Epoch 362: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5332 - acc: 0.7816 - val_loss: 0.7439 - val_acc: 0.7845\n",
      "Epoch 363/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7821\n",
      "Epoch 363: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5396 - acc: 0.7815 - val_loss: 0.7108 - val_acc: 0.7802\n",
      "Epoch 364/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.7853\n",
      "Epoch 364: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5159 - acc: 0.7847 - val_loss: 0.7420 - val_acc: 0.7717\n",
      "Epoch 365/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5025 - acc: 0.7844\n",
      "Epoch 365: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5023 - acc: 0.7842 - val_loss: 0.7429 - val_acc: 0.7892\n",
      "Epoch 366/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.7841\n",
      "Epoch 366: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5155 - acc: 0.7835 - val_loss: 0.7533 - val_acc: 0.7708\n",
      "Epoch 367/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.7838\n",
      "Epoch 367: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5234 - acc: 0.7839 - val_loss: 0.6671 - val_acc: 0.7850\n",
      "Epoch 368/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.7837\n",
      "Epoch 368: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5161 - acc: 0.7838 - val_loss: 0.6406 - val_acc: 0.7738\n",
      "Epoch 369/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.7820\n",
      "Epoch 369: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5077 - acc: 0.7817 - val_loss: 0.7180 - val_acc: 0.7708\n",
      "Epoch 370/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7826\n",
      "Epoch 370: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5236 - acc: 0.7814 - val_loss: 0.7641 - val_acc: 0.7832\n",
      "Epoch 371/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.7830\n",
      "Epoch 371: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5316 - acc: 0.7821 - val_loss: 0.6518 - val_acc: 0.7743\n",
      "Epoch 372/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7770\n",
      "Epoch 372: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5355 - acc: 0.7767 - val_loss: 0.7009 - val_acc: 0.7807\n",
      "Epoch 373/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7802\n",
      "Epoch 373: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5282 - acc: 0.7807 - val_loss: 0.7010 - val_acc: 0.7862\n",
      "Epoch 374/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.7789\n",
      "Epoch 374: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5468 - acc: 0.7788 - val_loss: 0.6689 - val_acc: 0.7773\n",
      "Epoch 375/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.7815\n",
      "Epoch 375: val_acc improved from 0.79008 to 0.79393, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/2/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5177 - acc: 0.7815 - val_loss: 0.6953 - val_acc: 0.7939\n",
      "Epoch 376/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7850\n",
      "Epoch 376: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5377 - acc: 0.7848 - val_loss: 0.6824 - val_acc: 0.7837\n",
      "Epoch 377/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.7832\n",
      "Epoch 377: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5195 - acc: 0.7828 - val_loss: 0.7021 - val_acc: 0.7790\n",
      "Epoch 378/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7801\n",
      "Epoch 378: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5271 - acc: 0.7802 - val_loss: 0.6552 - val_acc: 0.7636\n",
      "Epoch 379/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.7871\n",
      "Epoch 379: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5064 - acc: 0.7869 - val_loss: 0.7247 - val_acc: 0.7666\n",
      "Epoch 380/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.7792\n",
      "Epoch 380: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5307 - acc: 0.7788 - val_loss: 0.6959 - val_acc: 0.7820\n",
      "Epoch 381/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5467 - acc: 0.7863\n",
      "Epoch 381: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5467 - acc: 0.7863 - val_loss: 0.6694 - val_acc: 0.7888\n",
      "Epoch 382/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7832\n",
      "Epoch 382: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5268 - acc: 0.7832 - val_loss: 0.6796 - val_acc: 0.7790\n",
      "Epoch 383/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.7815\n",
      "Epoch 383: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5176 - acc: 0.7813 - val_loss: 0.7239 - val_acc: 0.7854\n",
      "Epoch 384/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7817\n",
      "Epoch 384: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5277 - acc: 0.7817 - val_loss: 0.8156 - val_acc: 0.7678\n",
      "Epoch 385/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.7824\n",
      "Epoch 385: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5464 - acc: 0.7823 - val_loss: 0.6723 - val_acc: 0.7726\n",
      "Epoch 386/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7829\n",
      "Epoch 386: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5261 - acc: 0.7820 - val_loss: 0.6552 - val_acc: 0.7713\n",
      "Epoch 387/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.7812\n",
      "Epoch 387: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5372 - acc: 0.7816 - val_loss: 0.7157 - val_acc: 0.7619\n",
      "Epoch 388/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7793\n",
      "Epoch 388: val_acc did not improve from 0.79393\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5062 - acc: 0.7796 - val_loss: 0.8797 - val_acc: 0.7914\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 1.7546 - acc: 0.6182\n",
      "Epoch 1: val_acc improved from -inf to 0.70329, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 3s 8ms/step - loss: 1.7508 - acc: 0.6177 - val_loss: 0.6576 - val_acc: 0.7033\n",
      "Epoch 2/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.6576\n",
      "Epoch 2: val_acc did not improve from 0.70329\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6906 - acc: 0.6577 - val_loss: 0.6658 - val_acc: 0.6580\n",
      "Epoch 3/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6652 - acc: 0.6747\n",
      "Epoch 3: val_acc did not improve from 0.70329\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6647 - acc: 0.6747 - val_loss: 0.6586 - val_acc: 0.6601\n",
      "Epoch 4/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6523 - acc: 0.6849\n",
      "Epoch 4: val_acc improved from 0.70329 to 0.70928, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6530 - acc: 0.6841 - val_loss: 0.6416 - val_acc: 0.7093\n",
      "Epoch 5/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.6963\n",
      "Epoch 5: val_acc did not improve from 0.70928\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6365 - acc: 0.6960 - val_loss: 0.6441 - val_acc: 0.6571\n",
      "Epoch 6/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6232 - acc: 0.6999\n",
      "Epoch 6: val_acc did not improve from 0.70928\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6234 - acc: 0.6992 - val_loss: 0.6448 - val_acc: 0.6994\n",
      "Epoch 7/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6225 - acc: 0.7044\n",
      "Epoch 7: val_acc improved from 0.70928 to 0.71227, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6226 - acc: 0.7036 - val_loss: 0.6070 - val_acc: 0.7123\n",
      "Epoch 8/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6078 - acc: 0.7081\n",
      "Epoch 8: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6086 - acc: 0.7075 - val_loss: 0.6377 - val_acc: 0.7123\n",
      "Epoch 9/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6080 - acc: 0.7062\n",
      "Epoch 9: val_acc did not improve from 0.71227\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6081 - acc: 0.7061 - val_loss: 0.6050 - val_acc: 0.7114\n",
      "Epoch 10/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6051 - acc: 0.7078\n",
      "Epoch 10: val_acc improved from 0.71227 to 0.71783, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6053 - acc: 0.7071 - val_loss: 0.5984 - val_acc: 0.7178\n",
      "Epoch 11/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.7110\n",
      "Epoch 11: val_acc did not improve from 0.71783\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5965 - acc: 0.7106 - val_loss: 0.5979 - val_acc: 0.7170\n",
      "Epoch 12/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.7165\n",
      "Epoch 12: val_acc did not improve from 0.71783\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5916 - acc: 0.7157 - val_loss: 0.6112 - val_acc: 0.7144\n",
      "Epoch 13/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5927 - acc: 0.7146\n",
      "Epoch 13: val_acc improved from 0.71783 to 0.72039, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5934 - acc: 0.7135 - val_loss: 0.5846 - val_acc: 0.7204\n",
      "Epoch 14/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5874 - acc: 0.7145\n",
      "Epoch 14: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5877 - acc: 0.7138 - val_loss: 0.6084 - val_acc: 0.7170\n",
      "Epoch 15/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.7170\n",
      "Epoch 15: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5962 - acc: 0.7162 - val_loss: 0.6084 - val_acc: 0.7174\n",
      "Epoch 16/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5936 - acc: 0.7141\n",
      "Epoch 16: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5936 - acc: 0.7141 - val_loss: 0.6029 - val_acc: 0.7191\n",
      "Epoch 17/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5828 - acc: 0.7194\n",
      "Epoch 17: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5828 - acc: 0.7194 - val_loss: 0.5964 - val_acc: 0.7153\n",
      "Epoch 18/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.7172\n",
      "Epoch 18: val_acc improved from 0.72039 to 0.72082, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5868 - acc: 0.7162 - val_loss: 0.6217 - val_acc: 0.7208\n",
      "Epoch 19/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5833 - acc: 0.7191\n",
      "Epoch 19: val_acc did not improve from 0.72082\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5846 - acc: 0.7178 - val_loss: 0.6116 - val_acc: 0.7191\n",
      "Epoch 20/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.7174\n",
      "Epoch 20: val_acc did not improve from 0.72082\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5797 - acc: 0.7168 - val_loss: 0.6428 - val_acc: 0.7131\n",
      "Epoch 21/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5809 - acc: 0.7220\n",
      "Epoch 21: val_acc improved from 0.72082 to 0.72253, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5817 - acc: 0.7211 - val_loss: 0.5989 - val_acc: 0.7225\n",
      "Epoch 22/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.7207\n",
      "Epoch 22: val_acc improved from 0.72253 to 0.72339, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5801 - acc: 0.7200 - val_loss: 0.6234 - val_acc: 0.7234\n",
      "Epoch 23/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.7215\n",
      "Epoch 23: val_acc did not improve from 0.72339\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5839 - acc: 0.7208 - val_loss: 0.5896 - val_acc: 0.7234\n",
      "Epoch 24/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.7209\n",
      "Epoch 24: val_acc did not improve from 0.72339\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5718 - acc: 0.7206 - val_loss: 0.6103 - val_acc: 0.7204\n",
      "Epoch 25/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5793 - acc: 0.7264\n",
      "Epoch 25: val_acc improved from 0.72339 to 0.72980, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5795 - acc: 0.7262 - val_loss: 0.5862 - val_acc: 0.7298\n",
      "Epoch 26/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5686 - acc: 0.7229\n",
      "Epoch 26: val_acc did not improve from 0.72980\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5686 - acc: 0.7229 - val_loss: 0.6025 - val_acc: 0.7260\n",
      "Epoch 27/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5777 - acc: 0.7246\n",
      "Epoch 27: val_acc did not improve from 0.72980\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5780 - acc: 0.7241 - val_loss: 0.6081 - val_acc: 0.7148\n",
      "Epoch 28/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.7265\n",
      "Epoch 28: val_acc improved from 0.72980 to 0.73023, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5743 - acc: 0.7258 - val_loss: 0.6275 - val_acc: 0.7302\n",
      "Epoch 29/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5805 - acc: 0.7252\n",
      "Epoch 29: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5805 - acc: 0.7252 - val_loss: 0.5794 - val_acc: 0.7221\n",
      "Epoch 30/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.7275\n",
      "Epoch 30: val_acc did not improve from 0.73023\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5678 - acc: 0.7270 - val_loss: 0.6141 - val_acc: 0.7242\n",
      "Epoch 31/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5793 - acc: 0.7248\n",
      "Epoch 31: val_acc improved from 0.73023 to 0.73151, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5796 - acc: 0.7235 - val_loss: 0.6082 - val_acc: 0.7315\n",
      "Epoch 32/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5721 - acc: 0.7270\n",
      "Epoch 32: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5733 - acc: 0.7267 - val_loss: 0.6185 - val_acc: 0.7242\n",
      "Epoch 33/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.7297\n",
      "Epoch 33: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5734 - acc: 0.7296 - val_loss: 0.6025 - val_acc: 0.7302\n",
      "Epoch 34/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.7277\n",
      "Epoch 34: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5720 - acc: 0.7275 - val_loss: 0.5945 - val_acc: 0.7311\n",
      "Epoch 35/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.7266\n",
      "Epoch 35: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5719 - acc: 0.7259 - val_loss: 0.5917 - val_acc: 0.7272\n",
      "Epoch 36/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7286\n",
      "Epoch 36: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5575 - acc: 0.7280 - val_loss: 0.6103 - val_acc: 0.7294\n",
      "Epoch 37/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.7339\n",
      "Epoch 37: val_acc improved from 0.73151 to 0.73493, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5651 - acc: 0.7334 - val_loss: 0.6044 - val_acc: 0.7349\n",
      "Epoch 38/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.7328\n",
      "Epoch 38: val_acc improved from 0.73493 to 0.73835, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5694 - acc: 0.7319 - val_loss: 0.6033 - val_acc: 0.7383\n",
      "Epoch 39/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.7282\n",
      "Epoch 39: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5599 - acc: 0.7278 - val_loss: 0.5737 - val_acc: 0.7264\n",
      "Epoch 40/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5609 - acc: 0.7337\n",
      "Epoch 40: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5611 - acc: 0.7333 - val_loss: 0.5947 - val_acc: 0.7302\n",
      "Epoch 41/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.7305\n",
      "Epoch 41: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5703 - acc: 0.7313 - val_loss: 0.5926 - val_acc: 0.7285\n",
      "Epoch 42/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7314\n",
      "Epoch 42: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5744 - acc: 0.7305 - val_loss: 0.5898 - val_acc: 0.7315\n",
      "Epoch 43/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5641 - acc: 0.7319\n",
      "Epoch 43: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5641 - acc: 0.7319 - val_loss: 0.6093 - val_acc: 0.7277\n",
      "Epoch 44/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5653 - acc: 0.7337\n",
      "Epoch 44: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5654 - acc: 0.7330 - val_loss: 0.5910 - val_acc: 0.7281\n",
      "Epoch 45/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.7327\n",
      "Epoch 45: val_acc did not improve from 0.73835\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5638 - acc: 0.7322 - val_loss: 0.5834 - val_acc: 0.7336\n",
      "Epoch 46/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7329\n",
      "Epoch 46: val_acc improved from 0.73835 to 0.74134, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5633 - acc: 0.7325 - val_loss: 0.5978 - val_acc: 0.7413\n",
      "Epoch 47/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5707 - acc: 0.7352\n",
      "Epoch 47: val_acc did not improve from 0.74134\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5707 - acc: 0.7352 - val_loss: 0.5876 - val_acc: 0.7298\n",
      "Epoch 48/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.7308\n",
      "Epoch 48: val_acc did not improve from 0.74134\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5613 - acc: 0.7301 - val_loss: 0.6102 - val_acc: 0.7289\n",
      "Epoch 49/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.7363\n",
      "Epoch 49: val_acc did not improve from 0.74134\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5639 - acc: 0.7354 - val_loss: 0.6190 - val_acc: 0.7401\n",
      "Epoch 50/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.7398\n",
      "Epoch 50: val_acc did not improve from 0.74134\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5574 - acc: 0.7400 - val_loss: 0.6792 - val_acc: 0.7260\n",
      "Epoch 51/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.7381\n",
      "Epoch 51: val_acc improved from 0.74134 to 0.74391, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5611 - acc: 0.7365 - val_loss: 0.6334 - val_acc: 0.7439\n",
      "Epoch 52/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.7361\n",
      "Epoch 52: val_acc did not improve from 0.74391\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5595 - acc: 0.7360 - val_loss: 0.6211 - val_acc: 0.7422\n",
      "Epoch 53/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5605 - acc: 0.7330\n",
      "Epoch 53: val_acc did not improve from 0.74391\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5606 - acc: 0.7325 - val_loss: 0.5994 - val_acc: 0.7345\n",
      "Epoch 54/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.7361\n",
      "Epoch 54: val_acc did not improve from 0.74391\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5704 - acc: 0.7352 - val_loss: 0.6135 - val_acc: 0.7307\n",
      "Epoch 55/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.7380\n",
      "Epoch 55: val_acc did not improve from 0.74391\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5629 - acc: 0.7374 - val_loss: 0.5968 - val_acc: 0.7371\n",
      "Epoch 56/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.7383\n",
      "Epoch 56: val_acc did not improve from 0.74391\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5527 - acc: 0.7377 - val_loss: 0.6039 - val_acc: 0.7401\n",
      "Epoch 57/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7361\n",
      "Epoch 57: val_acc improved from 0.74391 to 0.74476, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5487 - acc: 0.7359 - val_loss: 0.5984 - val_acc: 0.7448\n",
      "Epoch 58/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5632 - acc: 0.7349\n",
      "Epoch 58: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5632 - acc: 0.7349 - val_loss: 0.5974 - val_acc: 0.7375\n",
      "Epoch 59/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.7392\n",
      "Epoch 59: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5528 - acc: 0.7390 - val_loss: 0.6531 - val_acc: 0.7396\n",
      "Epoch 60/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.7404\n",
      "Epoch 60: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5690 - acc: 0.7394 - val_loss: 0.6493 - val_acc: 0.7349\n",
      "Epoch 61/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.7399\n",
      "Epoch 61: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5661 - acc: 0.7398 - val_loss: 0.6224 - val_acc: 0.7431\n",
      "Epoch 62/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.7410\n",
      "Epoch 62: val_acc improved from 0.74476 to 0.74605, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5573 - acc: 0.7409 - val_loss: 0.6277 - val_acc: 0.7460\n",
      "Epoch 63/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.7435\n",
      "Epoch 63: val_acc improved from 0.74605 to 0.74947, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5581 - acc: 0.7435 - val_loss: 0.6201 - val_acc: 0.7495\n",
      "Epoch 64/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.7421\n",
      "Epoch 64: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5483 - acc: 0.7417 - val_loss: 0.6055 - val_acc: 0.7482\n",
      "Epoch 65/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5502 - acc: 0.7401\n",
      "Epoch 65: val_acc improved from 0.74947 to 0.75417, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5502 - acc: 0.7401 - val_loss: 0.6272 - val_acc: 0.7542\n",
      "Epoch 66/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.7434\n",
      "Epoch 66: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5651 - acc: 0.7432 - val_loss: 0.5983 - val_acc: 0.7482\n",
      "Epoch 67/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5586 - acc: 0.7418\n",
      "Epoch 67: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5583 - acc: 0.7420 - val_loss: 0.6140 - val_acc: 0.7529\n",
      "Epoch 68/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5619 - acc: 0.7464\n",
      "Epoch 68: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5619 - acc: 0.7464 - val_loss: 0.6069 - val_acc: 0.7525\n",
      "Epoch 69/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.7427\n",
      "Epoch 69: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.7426 - val_loss: 0.6287 - val_acc: 0.7520\n",
      "Epoch 70/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5607 - acc: 0.7422\n",
      "Epoch 70: val_acc did not improve from 0.75417\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5607 - acc: 0.7422 - val_loss: 0.6210 - val_acc: 0.7392\n",
      "Epoch 71/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.7471\n",
      "Epoch 71: val_acc improved from 0.75417 to 0.75460, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5575 - acc: 0.7464 - val_loss: 0.6285 - val_acc: 0.7546\n",
      "Epoch 72/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.7476\n",
      "Epoch 72: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5524 - acc: 0.7476 - val_loss: 0.6534 - val_acc: 0.7486\n",
      "Epoch 73/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7464\n",
      "Epoch 73: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.7457 - val_loss: 0.6182 - val_acc: 0.7396\n",
      "Epoch 74/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5559 - acc: 0.7431\n",
      "Epoch 74: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5559 - acc: 0.7431 - val_loss: 0.6840 - val_acc: 0.7490\n",
      "Epoch 75/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7445\n",
      "Epoch 75: val_acc improved from 0.75460 to 0.75545, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5513 - acc: 0.7444 - val_loss: 0.6472 - val_acc: 0.7555\n",
      "Epoch 76/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7458\n",
      "Epoch 76: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5363 - acc: 0.7451 - val_loss: 0.6313 - val_acc: 0.7383\n",
      "Epoch 77/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5442 - acc: 0.7454\n",
      "Epoch 77: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5442 - acc: 0.7454 - val_loss: 0.6547 - val_acc: 0.7516\n",
      "Epoch 78/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7460\n",
      "Epoch 78: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5495 - acc: 0.7459 - val_loss: 0.6578 - val_acc: 0.7503\n",
      "Epoch 79/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5587 - acc: 0.7446\n",
      "Epoch 79: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5588 - acc: 0.7439 - val_loss: 0.6063 - val_acc: 0.7465\n",
      "Epoch 80/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7490\n",
      "Epoch 80: val_acc did not improve from 0.75545\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5406 - acc: 0.7483 - val_loss: 0.6706 - val_acc: 0.7465\n",
      "Epoch 81/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5505 - acc: 0.7507\n",
      "Epoch 81: val_acc improved from 0.75545 to 0.76058, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5505 - acc: 0.7507 - val_loss: 0.6011 - val_acc: 0.7606\n",
      "Epoch 82/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7520\n",
      "Epoch 82: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5496 - acc: 0.7519 - val_loss: 0.6455 - val_acc: 0.7563\n",
      "Epoch 83/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5609 - acc: 0.7488\n",
      "Epoch 83: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5600 - acc: 0.7484 - val_loss: 0.6300 - val_acc: 0.7584\n",
      "Epoch 84/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.7439\n",
      "Epoch 84: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5462 - acc: 0.7432 - val_loss: 0.6419 - val_acc: 0.7495\n",
      "Epoch 85/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7469\n",
      "Epoch 85: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5409 - acc: 0.7466 - val_loss: 0.6642 - val_acc: 0.7460\n",
      "Epoch 86/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.7480\n",
      "Epoch 86: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5527 - acc: 0.7475 - val_loss: 0.6254 - val_acc: 0.7593\n",
      "Epoch 87/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7502\n",
      "Epoch 87: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5495 - acc: 0.7486 - val_loss: 0.6310 - val_acc: 0.7439\n",
      "Epoch 88/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.7489\n",
      "Epoch 88: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5550 - acc: 0.7482 - val_loss: 0.6186 - val_acc: 0.7507\n",
      "Epoch 89/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7461\n",
      "Epoch 89: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5398 - acc: 0.7462 - val_loss: 0.6871 - val_acc: 0.7469\n",
      "Epoch 90/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5445 - acc: 0.7493\n",
      "Epoch 90: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5445 - acc: 0.7493 - val_loss: 0.6438 - val_acc: 0.7452\n",
      "Epoch 91/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.7498\n",
      "Epoch 91: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5531 - acc: 0.7493 - val_loss: 0.6439 - val_acc: 0.7473\n",
      "Epoch 92/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5605 - acc: 0.7473\n",
      "Epoch 92: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5605 - acc: 0.7473 - val_loss: 0.6446 - val_acc: 0.7580\n",
      "Epoch 93/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.7523\n",
      "Epoch 93: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5522 - acc: 0.7517 - val_loss: 0.6299 - val_acc: 0.7593\n",
      "Epoch 94/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7535\n",
      "Epoch 94: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5416 - acc: 0.7532 - val_loss: 0.6500 - val_acc: 0.7597\n",
      "Epoch 95/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.7533\n",
      "Epoch 95: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5480 - acc: 0.7525 - val_loss: 0.6453 - val_acc: 0.7589\n",
      "Epoch 96/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.7546\n",
      "Epoch 96: val_acc improved from 0.76058 to 0.76486, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5409 - acc: 0.7541 - val_loss: 0.6965 - val_acc: 0.7649\n",
      "Epoch 97/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.7517\n",
      "Epoch 97: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5424 - acc: 0.7514 - val_loss: 0.6432 - val_acc: 0.7555\n",
      "Epoch 98/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7517\n",
      "Epoch 98: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5388 - acc: 0.7513 - val_loss: 0.6324 - val_acc: 0.7593\n",
      "Epoch 99/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.7548\n",
      "Epoch 99: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5445 - acc: 0.7546 - val_loss: 0.6196 - val_acc: 0.7593\n",
      "Epoch 100/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7559\n",
      "Epoch 100: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5358 - acc: 0.7553 - val_loss: 0.5903 - val_acc: 0.7499\n",
      "Epoch 101/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7585\n",
      "Epoch 101: val_acc did not improve from 0.76486\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5397 - acc: 0.7579 - val_loss: 0.6115 - val_acc: 0.7576\n",
      "Epoch 102/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.7613\n",
      "Epoch 102: val_acc improved from 0.76486 to 0.76528, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5350 - acc: 0.7607 - val_loss: 0.6395 - val_acc: 0.7653\n",
      "Epoch 103/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.7526\n",
      "Epoch 103: val_acc improved from 0.76528 to 0.76956, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5500 - acc: 0.7531 - val_loss: 0.6057 - val_acc: 0.7696\n",
      "Epoch 104/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7529\n",
      "Epoch 104: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5297 - acc: 0.7529 - val_loss: 0.6359 - val_acc: 0.7678\n",
      "Epoch 105/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.7547\n",
      "Epoch 105: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5467 - acc: 0.7539 - val_loss: 0.6409 - val_acc: 0.7631\n",
      "Epoch 106/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.7521\n",
      "Epoch 106: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5589 - acc: 0.7517 - val_loss: 0.5940 - val_acc: 0.7610\n",
      "Epoch 107/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.7542\n",
      "Epoch 107: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5468 - acc: 0.7534 - val_loss: 0.6306 - val_acc: 0.7631\n",
      "Epoch 108/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.7528\n",
      "Epoch 108: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5526 - acc: 0.7524 - val_loss: 0.6048 - val_acc: 0.7623\n",
      "Epoch 109/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5428 - acc: 0.7519\n",
      "Epoch 109: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5428 - acc: 0.7519 - val_loss: 0.6147 - val_acc: 0.7593\n",
      "Epoch 110/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5340 - acc: 0.7552\n",
      "Epoch 110: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5340 - acc: 0.7552 - val_loss: 0.6685 - val_acc: 0.7563\n",
      "Epoch 111/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.7583\n",
      "Epoch 111: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5499 - acc: 0.7576 - val_loss: 0.6353 - val_acc: 0.7584\n",
      "Epoch 112/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7577\n",
      "Epoch 112: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5489 - acc: 0.7569 - val_loss: 0.5899 - val_acc: 0.7606\n",
      "Epoch 113/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7577\n",
      "Epoch 113: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5483 - acc: 0.7576 - val_loss: 0.6438 - val_acc: 0.7589\n",
      "Epoch 114/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7581\n",
      "Epoch 114: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5395 - acc: 0.7580 - val_loss: 0.6805 - val_acc: 0.7683\n",
      "Epoch 115/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7565\n",
      "Epoch 115: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5690 - acc: 0.7556 - val_loss: 0.6189 - val_acc: 0.7567\n",
      "Epoch 116/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7549\n",
      "Epoch 116: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5352 - acc: 0.7549 - val_loss: 0.6231 - val_acc: 0.7533\n",
      "Epoch 117/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7519\n",
      "Epoch 117: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5507 - acc: 0.7518 - val_loss: 0.6330 - val_acc: 0.7589\n",
      "Epoch 118/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7610\n",
      "Epoch 118: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5492 - acc: 0.7604 - val_loss: 0.6652 - val_acc: 0.7631\n",
      "Epoch 119/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.7577\n",
      "Epoch 119: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5478 - acc: 0.7573 - val_loss: 0.6303 - val_acc: 0.7623\n",
      "Epoch 120/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.7589\n",
      "Epoch 120: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5426 - acc: 0.7581 - val_loss: 0.6534 - val_acc: 0.7537\n",
      "Epoch 121/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7578\n",
      "Epoch 121: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5453 - acc: 0.7575 - val_loss: 0.6419 - val_acc: 0.7653\n",
      "Epoch 122/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.7661\n",
      "Epoch 122: val_acc did not improve from 0.76956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5392 - acc: 0.7656 - val_loss: 0.6375 - val_acc: 0.7657\n",
      "Epoch 123/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.7586\n",
      "Epoch 123: val_acc improved from 0.76956 to 0.77341, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5473 - acc: 0.7582 - val_loss: 0.6489 - val_acc: 0.7734\n",
      "Epoch 124/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7597\n",
      "Epoch 124: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5366 - acc: 0.7582 - val_loss: 0.6438 - val_acc: 0.7610\n",
      "Epoch 125/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.7603\n",
      "Epoch 125: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5571 - acc: 0.7598 - val_loss: 0.6637 - val_acc: 0.7721\n",
      "Epoch 126/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.7595\n",
      "Epoch 126: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5405 - acc: 0.7593 - val_loss: 0.6334 - val_acc: 0.7700\n",
      "Epoch 127/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7576\n",
      "Epoch 127: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5362 - acc: 0.7566 - val_loss: 0.6458 - val_acc: 0.7674\n",
      "Epoch 128/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.7617\n",
      "Epoch 128: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5476 - acc: 0.7610 - val_loss: 0.5910 - val_acc: 0.7452\n",
      "Epoch 129/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7598\n",
      "Epoch 129: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5487 - acc: 0.7599 - val_loss: 0.6298 - val_acc: 0.7614\n",
      "Epoch 130/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7631\n",
      "Epoch 130: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5368 - acc: 0.7629 - val_loss: 0.6212 - val_acc: 0.7610\n",
      "Epoch 131/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7625\n",
      "Epoch 131: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.7615 - val_loss: 0.6306 - val_acc: 0.7674\n",
      "Epoch 132/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.7581\n",
      "Epoch 132: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5508 - acc: 0.7576 - val_loss: 0.5796 - val_acc: 0.7520\n",
      "Epoch 133/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.7620\n",
      "Epoch 133: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5283 - acc: 0.7613 - val_loss: 0.6198 - val_acc: 0.7696\n",
      "Epoch 134/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7637\n",
      "Epoch 134: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5432 - acc: 0.7627 - val_loss: 0.6237 - val_acc: 0.7657\n",
      "Epoch 135/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7617\n",
      "Epoch 135: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5387 - acc: 0.7613 - val_loss: 0.6404 - val_acc: 0.7507\n",
      "Epoch 136/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.7606\n",
      "Epoch 136: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5341 - acc: 0.7599 - val_loss: 0.6302 - val_acc: 0.7610\n",
      "Epoch 137/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7609\n",
      "Epoch 137: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5409 - acc: 0.7610 - val_loss: 0.6371 - val_acc: 0.7640\n",
      "Epoch 138/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7548\n",
      "Epoch 138: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5400 - acc: 0.7546 - val_loss: 0.7047 - val_acc: 0.7593\n",
      "Epoch 139/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.7564\n",
      "Epoch 139: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5690 - acc: 0.7561 - val_loss: 0.6584 - val_acc: 0.7537\n",
      "Epoch 140/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5414 - acc: 0.7542\n",
      "Epoch 140: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5413 - acc: 0.7530 - val_loss: 0.6321 - val_acc: 0.7550\n",
      "Epoch 141/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7594\n",
      "Epoch 141: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5479 - acc: 0.7590 - val_loss: 0.6217 - val_acc: 0.7704\n",
      "Epoch 142/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7668\n",
      "Epoch 142: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5276 - acc: 0.7660 - val_loss: 0.6391 - val_acc: 0.7542\n",
      "Epoch 143/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7593\n",
      "Epoch 143: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5495 - acc: 0.7588 - val_loss: 0.6228 - val_acc: 0.7661\n",
      "Epoch 144/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.7621\n",
      "Epoch 144: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5505 - acc: 0.7615 - val_loss: 0.6259 - val_acc: 0.7623\n",
      "Epoch 145/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7607\n",
      "Epoch 145: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5371 - acc: 0.7604 - val_loss: 0.6761 - val_acc: 0.7696\n",
      "Epoch 146/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.7564\n",
      "Epoch 146: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5454 - acc: 0.7557 - val_loss: 0.6168 - val_acc: 0.7636\n",
      "Epoch 147/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.7637\n",
      "Epoch 147: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5553 - acc: 0.7639 - val_loss: 0.7008 - val_acc: 0.7678\n",
      "Epoch 148/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7645\n",
      "Epoch 148: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5441 - acc: 0.7642 - val_loss: 0.6455 - val_acc: 0.7683\n",
      "Epoch 149/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7615\n",
      "Epoch 149: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5464 - acc: 0.7603 - val_loss: 0.6479 - val_acc: 0.7401\n",
      "Epoch 150/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7598\n",
      "Epoch 150: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5421 - acc: 0.7598 - val_loss: 0.6834 - val_acc: 0.7627\n",
      "Epoch 151/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7618\n",
      "Epoch 151: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5398 - acc: 0.7613 - val_loss: 0.6484 - val_acc: 0.7593\n",
      "Epoch 152/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.7641\n",
      "Epoch 152: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5429 - acc: 0.7635 - val_loss: 0.6030 - val_acc: 0.7674\n",
      "Epoch 153/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.7645\n",
      "Epoch 153: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5398 - acc: 0.7642 - val_loss: 0.6763 - val_acc: 0.7649\n",
      "Epoch 154/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5516 - acc: 0.7603\n",
      "Epoch 154: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5516 - acc: 0.7603 - val_loss: 0.6313 - val_acc: 0.7499\n",
      "Epoch 155/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.7614\n",
      "Epoch 155: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5407 - acc: 0.7614 - val_loss: 0.6407 - val_acc: 0.7670\n",
      "Epoch 156/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7623\n",
      "Epoch 156: val_acc did not improve from 0.77341\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5447 - acc: 0.7613 - val_loss: 0.6054 - val_acc: 0.7529\n",
      "Epoch 157/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.7636\n",
      "Epoch 157: val_acc improved from 0.77341 to 0.77426, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7632 - val_loss: 0.6339 - val_acc: 0.7743\n",
      "Epoch 158/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5396 - acc: 0.7623\n",
      "Epoch 158: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5396 - acc: 0.7623 - val_loss: 0.6408 - val_acc: 0.7631\n",
      "Epoch 159/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7626\n",
      "Epoch 159: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5432 - acc: 0.7616 - val_loss: 0.6352 - val_acc: 0.7482\n",
      "Epoch 160/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7641\n",
      "Epoch 160: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5312 - acc: 0.7631 - val_loss: 0.6140 - val_acc: 0.7678\n",
      "Epoch 161/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7607\n",
      "Epoch 161: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.7607 - val_loss: 0.7134 - val_acc: 0.7542\n",
      "Epoch 162/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5362 - acc: 0.7650\n",
      "Epoch 162: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5362 - acc: 0.7650 - val_loss: 0.6069 - val_acc: 0.7691\n",
      "Epoch 163/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7651\n",
      "Epoch 163: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5264 - acc: 0.7641 - val_loss: 0.6535 - val_acc: 0.7520\n",
      "Epoch 164/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5484 - acc: 0.7582\n",
      "Epoch 164: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5479 - acc: 0.7581 - val_loss: 0.6297 - val_acc: 0.7743\n",
      "Epoch 165/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7648\n",
      "Epoch 165: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7648 - val_loss: 0.6156 - val_acc: 0.7713\n",
      "Epoch 166/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.7639\n",
      "Epoch 166: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5317 - acc: 0.7642 - val_loss: 0.6779 - val_acc: 0.7670\n",
      "Epoch 167/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7654\n",
      "Epoch 167: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5364 - acc: 0.7645 - val_loss: 0.6294 - val_acc: 0.7525\n",
      "Epoch 168/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7595\n",
      "Epoch 168: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5405 - acc: 0.7588 - val_loss: 0.6167 - val_acc: 0.7614\n",
      "Epoch 169/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7645\n",
      "Epoch 169: val_acc improved from 0.77426 to 0.77768, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5389 - acc: 0.7639 - val_loss: 0.6167 - val_acc: 0.7777\n",
      "Epoch 170/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5521 - acc: 0.7636\n",
      "Epoch 170: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5507 - acc: 0.7631 - val_loss: 0.6327 - val_acc: 0.7721\n",
      "Epoch 171/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.7654\n",
      "Epoch 171: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5413 - acc: 0.7645 - val_loss: 0.6148 - val_acc: 0.7619\n",
      "Epoch 172/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7613\n",
      "Epoch 172: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5456 - acc: 0.7612 - val_loss: 0.6049 - val_acc: 0.7520\n",
      "Epoch 173/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7714\n",
      "Epoch 173: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5498 - acc: 0.7694 - val_loss: 0.6056 - val_acc: 0.7623\n",
      "Epoch 174/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7698\n",
      "Epoch 174: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5335 - acc: 0.7689 - val_loss: 0.5599 - val_acc: 0.7717\n",
      "Epoch 175/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7700\n",
      "Epoch 175: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5380 - acc: 0.7688 - val_loss: 0.6508 - val_acc: 0.7610\n",
      "Epoch 176/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.7654\n",
      "Epoch 176: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5459 - acc: 0.7654 - val_loss: 0.6278 - val_acc: 0.7649\n",
      "Epoch 177/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.7675\n",
      "Epoch 177: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5215 - acc: 0.7668 - val_loss: 0.6330 - val_acc: 0.7627\n",
      "Epoch 178/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5273 - acc: 0.7637\n",
      "Epoch 178: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5273 - acc: 0.7637 - val_loss: 0.6824 - val_acc: 0.7683\n",
      "Epoch 179/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7702\n",
      "Epoch 179: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5321 - acc: 0.7705 - val_loss: 0.6045 - val_acc: 0.7657\n",
      "Epoch 180/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7640\n",
      "Epoch 180: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5488 - acc: 0.7637 - val_loss: 0.6676 - val_acc: 0.7674\n",
      "Epoch 181/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7640\n",
      "Epoch 181: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5439 - acc: 0.7640 - val_loss: 0.6255 - val_acc: 0.7751\n",
      "Epoch 182/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.7639\n",
      "Epoch 182: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5525 - acc: 0.7638 - val_loss: 0.6786 - val_acc: 0.7713\n",
      "Epoch 183/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.7641\n",
      "Epoch 183: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5523 - acc: 0.7640 - val_loss: 0.6641 - val_acc: 0.7546\n",
      "Epoch 184/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5360 - acc: 0.7693\n",
      "Epoch 184: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5360 - acc: 0.7693 - val_loss: 0.6175 - val_acc: 0.7700\n",
      "Epoch 185/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7607\n",
      "Epoch 185: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5249 - acc: 0.7601 - val_loss: 0.6504 - val_acc: 0.7555\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7687\n",
      "Epoch 186: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5508 - acc: 0.7687 - val_loss: 0.6638 - val_acc: 0.7768\n",
      "Epoch 187/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7676\n",
      "Epoch 187: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5423 - acc: 0.7674 - val_loss: 0.6730 - val_acc: 0.7636\n",
      "Epoch 188/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.7669\n",
      "Epoch 188: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5479 - acc: 0.7668 - val_loss: 0.6629 - val_acc: 0.7717\n",
      "Epoch 189/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.7641\n",
      "Epoch 189: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5380 - acc: 0.7647 - val_loss: 0.6725 - val_acc: 0.7623\n",
      "Epoch 190/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7664\n",
      "Epoch 190: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7663 - val_loss: 0.6483 - val_acc: 0.7743\n",
      "Epoch 191/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.7714\n",
      "Epoch 191: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5213 - acc: 0.7708 - val_loss: 0.6832 - val_acc: 0.7734\n",
      "Epoch 192/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.7656\n",
      "Epoch 192: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5344 - acc: 0.7653 - val_loss: 0.7353 - val_acc: 0.7751\n",
      "Epoch 193/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7602\n",
      "Epoch 193: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5510 - acc: 0.7600 - val_loss: 0.6593 - val_acc: 0.7627\n",
      "Epoch 194/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.7711\n",
      "Epoch 194: val_acc did not improve from 0.77768\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.7703 - val_loss: 0.6794 - val_acc: 0.7537\n",
      "Epoch 195/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7649\n",
      "Epoch 195: val_acc improved from 0.77768 to 0.78153, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5429 - acc: 0.7641 - val_loss: 0.7177 - val_acc: 0.7815\n",
      "Epoch 196/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.7610\n",
      "Epoch 196: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5649 - acc: 0.7609 - val_loss: 0.6384 - val_acc: 0.7550\n",
      "Epoch 197/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7621\n",
      "Epoch 197: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5404 - acc: 0.7618 - val_loss: 0.7371 - val_acc: 0.7751\n",
      "Epoch 198/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.7621\n",
      "Epoch 198: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5568 - acc: 0.7609 - val_loss: 0.6512 - val_acc: 0.7495\n",
      "Epoch 199/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.7636\n",
      "Epoch 199: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5382 - acc: 0.7628 - val_loss: 0.7188 - val_acc: 0.7687\n",
      "Epoch 200/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7675\n",
      "Epoch 200: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5486 - acc: 0.7663 - val_loss: 0.7011 - val_acc: 0.7456\n",
      "Epoch 201/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5539 - acc: 0.7666\n",
      "Epoch 201: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5541 - acc: 0.7661 - val_loss: 0.6687 - val_acc: 0.7670\n",
      "Epoch 202/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5493 - acc: 0.7671\n",
      "Epoch 202: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5493 - acc: 0.7671 - val_loss: 0.6867 - val_acc: 0.7768\n",
      "Epoch 203/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7673\n",
      "Epoch 203: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5521 - acc: 0.7672 - val_loss: 0.6933 - val_acc: 0.7773\n",
      "Epoch 204/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7663\n",
      "Epoch 204: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5373 - acc: 0.7664 - val_loss: 0.6311 - val_acc: 0.7794\n",
      "Epoch 205/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.7648\n",
      "Epoch 205: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5469 - acc: 0.7635 - val_loss: 0.6338 - val_acc: 0.7640\n",
      "Epoch 206/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.7685\n",
      "Epoch 206: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5238 - acc: 0.7688 - val_loss: 0.6796 - val_acc: 0.7687\n",
      "Epoch 207/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.7705\n",
      "Epoch 207: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5312 - acc: 0.7699 - val_loss: 0.6248 - val_acc: 0.7640\n",
      "Epoch 208/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7675\n",
      "Epoch 208: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5468 - acc: 0.7671 - val_loss: 0.6489 - val_acc: 0.7678\n",
      "Epoch 209/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.7718\n",
      "Epoch 209: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5271 - acc: 0.7712 - val_loss: 0.6481 - val_acc: 0.7798\n",
      "Epoch 210/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7604\n",
      "Epoch 210: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5466 - acc: 0.7599 - val_loss: 0.7063 - val_acc: 0.7717\n",
      "Epoch 211/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.7631\n",
      "Epoch 211: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5557 - acc: 0.7624 - val_loss: 0.7119 - val_acc: 0.7700\n",
      "Epoch 212/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7707\n",
      "Epoch 212: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5466 - acc: 0.7695 - val_loss: 0.6522 - val_acc: 0.7644\n",
      "Epoch 213/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7689\n",
      "Epoch 213: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5289 - acc: 0.7688 - val_loss: 0.6135 - val_acc: 0.7747\n",
      "Epoch 214/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7700\n",
      "Epoch 214: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5302 - acc: 0.7700 - val_loss: 0.7384 - val_acc: 0.7815\n",
      "Epoch 215/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7682\n",
      "Epoch 215: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5442 - acc: 0.7680 - val_loss: 0.7432 - val_acc: 0.7614\n",
      "Epoch 216/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.7666\n",
      "Epoch 216: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5343 - acc: 0.7661 - val_loss: 0.7337 - val_acc: 0.7542\n",
      "Epoch 217/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.7687\n",
      "Epoch 217: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5430 - acc: 0.7686 - val_loss: 0.6256 - val_acc: 0.7687\n",
      "Epoch 218/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.7698\n",
      "Epoch 218: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5459 - acc: 0.7694 - val_loss: 0.6240 - val_acc: 0.7794\n",
      "Epoch 219/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5539 - acc: 0.7694\n",
      "Epoch 219: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5539 - acc: 0.7692 - val_loss: 0.6454 - val_acc: 0.7717\n",
      "Epoch 220/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5223 - acc: 0.7740\n",
      "Epoch 220: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5223 - acc: 0.7740 - val_loss: 0.6936 - val_acc: 0.7747\n",
      "Epoch 221/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7698\n",
      "Epoch 221: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.7687 - val_loss: 0.6656 - val_acc: 0.7614\n",
      "Epoch 222/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5599 - acc: 0.7736\n",
      "Epoch 222: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5603 - acc: 0.7730 - val_loss: 0.6474 - val_acc: 0.7597\n",
      "Epoch 223/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7689\n",
      "Epoch 223: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5259 - acc: 0.7680 - val_loss: 0.6663 - val_acc: 0.7738\n",
      "Epoch 224/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7646\n",
      "Epoch 224: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5356 - acc: 0.7643 - val_loss: 0.6673 - val_acc: 0.7559\n",
      "Epoch 225/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7698\n",
      "Epoch 225: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5503 - acc: 0.7692 - val_loss: 0.7048 - val_acc: 0.7743\n",
      "Epoch 226/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7653\n",
      "Epoch 226: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7653 - val_loss: 0.6919 - val_acc: 0.7602\n",
      "Epoch 227/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5480 - acc: 0.7659\n",
      "Epoch 227: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5480 - acc: 0.7659 - val_loss: 0.6450 - val_acc: 0.7576\n",
      "Epoch 228/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.7702\n",
      "Epoch 228: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5129 - acc: 0.7703 - val_loss: 0.7373 - val_acc: 0.7747\n",
      "Epoch 229/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7678\n",
      "Epoch 229: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5639 - acc: 0.7677 - val_loss: 0.6814 - val_acc: 0.7751\n",
      "Epoch 230/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7737\n",
      "Epoch 230: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5456 - acc: 0.7725 - val_loss: 0.6821 - val_acc: 0.7738\n",
      "Epoch 231/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7693\n",
      "Epoch 231: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5195 - acc: 0.7685 - val_loss: 0.6767 - val_acc: 0.7661\n",
      "Epoch 232/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7715\n",
      "Epoch 232: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7716 - val_loss: 0.6924 - val_acc: 0.7713\n",
      "Epoch 233/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7710\n",
      "Epoch 233: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5509 - acc: 0.7706 - val_loss: 0.6092 - val_acc: 0.7691\n",
      "Epoch 234/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.7727\n",
      "Epoch 234: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5419 - acc: 0.7720 - val_loss: 0.6100 - val_acc: 0.7678\n",
      "Epoch 235/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.7705\n",
      "Epoch 235: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5353 - acc: 0.7705 - val_loss: 0.6957 - val_acc: 0.7674\n",
      "Epoch 236/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.7714\n",
      "Epoch 236: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5266 - acc: 0.7705 - val_loss: 0.6806 - val_acc: 0.7708\n",
      "Epoch 237/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.7687\n",
      "Epoch 237: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5284 - acc: 0.7687 - val_loss: 0.7091 - val_acc: 0.7764\n",
      "Epoch 238/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.7686\n",
      "Epoch 238: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5475 - acc: 0.7677 - val_loss: 0.6754 - val_acc: 0.7734\n",
      "Epoch 239/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.7688\n",
      "Epoch 239: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5572 - acc: 0.7690 - val_loss: 0.6781 - val_acc: 0.7751\n",
      "Epoch 240/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7698\n",
      "Epoch 240: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5515 - acc: 0.7695 - val_loss: 0.6729 - val_acc: 0.7602\n",
      "Epoch 241/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7690\n",
      "Epoch 241: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5476 - acc: 0.7687 - val_loss: 0.6299 - val_acc: 0.7734\n",
      "Epoch 242/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7726\n",
      "Epoch 242: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5306 - acc: 0.7720 - val_loss: 0.6781 - val_acc: 0.7666\n",
      "Epoch 243/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7679\n",
      "Epoch 243: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7680 - val_loss: 0.7441 - val_acc: 0.7760\n",
      "Epoch 244/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7700\n",
      "Epoch 244: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5356 - acc: 0.7693 - val_loss: 0.6751 - val_acc: 0.7777\n",
      "Epoch 245/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7686\n",
      "Epoch 245: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5294 - acc: 0.7683 - val_loss: 0.6215 - val_acc: 0.7734\n",
      "Epoch 246/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7670\n",
      "Epoch 246: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5423 - acc: 0.7669 - val_loss: 0.6766 - val_acc: 0.7768\n",
      "Epoch 247/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7695\n",
      "Epoch 247: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5274 - acc: 0.7694 - val_loss: 0.6776 - val_acc: 0.7726\n",
      "Epoch 248/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.7701\n",
      "Epoch 248: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5185 - acc: 0.7701 - val_loss: 0.7679 - val_acc: 0.7768\n",
      "Epoch 249/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.7741\n",
      "Epoch 249: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5300 - acc: 0.7731 - val_loss: 0.6914 - val_acc: 0.7678\n",
      "Epoch 250/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5423 - acc: 0.7679\n",
      "Epoch 250: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5423 - acc: 0.7679 - val_loss: 0.6403 - val_acc: 0.7606\n",
      "Epoch 251/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7710\n",
      "Epoch 251: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5357 - acc: 0.7712 - val_loss: 0.6671 - val_acc: 0.7623\n",
      "Epoch 252/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7724\n",
      "Epoch 252: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5353 - acc: 0.7724 - val_loss: 0.6732 - val_acc: 0.7576\n",
      "Epoch 253/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7759\n",
      "Epoch 253: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5270 - acc: 0.7758 - val_loss: 0.6537 - val_acc: 0.7747\n",
      "Epoch 254/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7746\n",
      "Epoch 254: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5373 - acc: 0.7742 - val_loss: 0.7243 - val_acc: 0.7755\n",
      "Epoch 255/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5436 - acc: 0.7751\n",
      "Epoch 255: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5436 - acc: 0.7751 - val_loss: 0.6568 - val_acc: 0.7815\n",
      "Epoch 256/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7730\n",
      "Epoch 256: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5413 - acc: 0.7721 - val_loss: 0.6862 - val_acc: 0.7623\n",
      "Epoch 257/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.7694\n",
      "Epoch 257: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5499 - acc: 0.7688 - val_loss: 0.6483 - val_acc: 0.7747\n",
      "Epoch 258/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5410 - acc: 0.7699\n",
      "Epoch 258: val_acc improved from 0.78153 to 0.78666, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5410 - acc: 0.7699 - val_loss: 0.7401 - val_acc: 0.7867\n",
      "Epoch 259/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7720\n",
      "Epoch 259: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5356 - acc: 0.7720 - val_loss: 0.7706 - val_acc: 0.7717\n",
      "Epoch 260/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.7741\n",
      "Epoch 260: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5543 - acc: 0.7736 - val_loss: 0.7946 - val_acc: 0.7704\n",
      "Epoch 261/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5484 - acc: 0.7709\n",
      "Epoch 261: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5486 - acc: 0.7703 - val_loss: 0.8372 - val_acc: 0.7708\n",
      "Epoch 262/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.7717\n",
      "Epoch 262: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5396 - acc: 0.7712 - val_loss: 0.7836 - val_acc: 0.7593\n",
      "Epoch 263/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7746\n",
      "Epoch 263: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5355 - acc: 0.7742 - val_loss: 0.7429 - val_acc: 0.7751\n",
      "Epoch 264/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7733\n",
      "Epoch 264: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5410 - acc: 0.7719 - val_loss: 0.7505 - val_acc: 0.7674\n",
      "Epoch 265/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5409 - acc: 0.7761\n",
      "Epoch 265: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5409 - acc: 0.7761 - val_loss: 0.8017 - val_acc: 0.7691\n",
      "Epoch 266/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7685\n",
      "Epoch 266: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5449 - acc: 0.7679 - val_loss: 0.7619 - val_acc: 0.7640\n",
      "Epoch 267/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7708\n",
      "Epoch 267: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5551 - acc: 0.7706 - val_loss: 0.6933 - val_acc: 0.7832\n",
      "Epoch 268/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7713\n",
      "Epoch 268: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5387 - acc: 0.7710 - val_loss: 0.6977 - val_acc: 0.7704\n",
      "Epoch 269/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7741\n",
      "Epoch 269: val_acc did not improve from 0.78666\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7737 - val_loss: 0.6811 - val_acc: 0.7764\n",
      "Epoch 270/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.7745\n",
      "Epoch 270: val_acc improved from 0.78666 to 0.78923, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5343 - acc: 0.7743 - val_loss: 0.7376 - val_acc: 0.7892\n",
      "Epoch 271/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7744\n",
      "Epoch 271: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5407 - acc: 0.7735 - val_loss: 0.7321 - val_acc: 0.7572\n",
      "Epoch 272/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.7738\n",
      "Epoch 272: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5726 - acc: 0.7726 - val_loss: 0.7362 - val_acc: 0.7794\n",
      "Epoch 273/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5518 - acc: 0.7733\n",
      "Epoch 273: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5518 - acc: 0.7733 - val_loss: 0.7622 - val_acc: 0.7768\n",
      "Epoch 274/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.7690\n",
      "Epoch 274: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5204 - acc: 0.7687 - val_loss: 0.7119 - val_acc: 0.7760\n",
      "Epoch 275/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5312 - acc: 0.7758\n",
      "Epoch 275: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5312 - acc: 0.7758 - val_loss: 0.6970 - val_acc: 0.7649\n",
      "Epoch 276/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7685\n",
      "Epoch 276: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5485 - acc: 0.7683 - val_loss: 0.7244 - val_acc: 0.7841\n",
      "Epoch 277/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.7760\n",
      "Epoch 277: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5565 - acc: 0.7758 - val_loss: 0.6933 - val_acc: 0.7734\n",
      "Epoch 278/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7694\n",
      "Epoch 278: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5492 - acc: 0.7695 - val_loss: 0.7084 - val_acc: 0.7867\n",
      "Epoch 279/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7748\n",
      "Epoch 279: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5283 - acc: 0.7745 - val_loss: 0.7617 - val_acc: 0.7696\n",
      "Epoch 280/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5648 - acc: 0.7732\n",
      "Epoch 280: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5660 - acc: 0.7730 - val_loss: 0.7072 - val_acc: 0.7678\n",
      "Epoch 281/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7748\n",
      "Epoch 281: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5258 - acc: 0.7747 - val_loss: 0.7350 - val_acc: 0.7802\n",
      "Epoch 282/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7781\n",
      "Epoch 282: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5222 - acc: 0.7776 - val_loss: 0.7853 - val_acc: 0.7734\n",
      "Epoch 283/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.7717\n",
      "Epoch 283: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5382 - acc: 0.7719 - val_loss: 0.6872 - val_acc: 0.7790\n",
      "Epoch 284/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.7698\n",
      "Epoch 284: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5679 - acc: 0.7692 - val_loss: 0.6934 - val_acc: 0.7815\n",
      "Epoch 285/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.7734\n",
      "Epoch 285: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5517 - acc: 0.7720 - val_loss: 0.7309 - val_acc: 0.7862\n",
      "Epoch 286/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.7687\n",
      "Epoch 286: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5636 - acc: 0.7684 - val_loss: 0.7810 - val_acc: 0.7636\n",
      "Epoch 287/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5633 - acc: 0.7708\n",
      "Epoch 287: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5633 - acc: 0.7708 - val_loss: 0.8613 - val_acc: 0.7807\n",
      "Epoch 288/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.7745\n",
      "Epoch 288: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5747 - acc: 0.7739 - val_loss: 0.6498 - val_acc: 0.7550\n",
      "Epoch 289/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7741\n",
      "Epoch 289: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5265 - acc: 0.7736 - val_loss: 0.7204 - val_acc: 0.7700\n",
      "Epoch 290/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.7737\n",
      "Epoch 290: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5652 - acc: 0.7735 - val_loss: 0.7650 - val_acc: 0.7773\n",
      "Epoch 291/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7752\n",
      "Epoch 291: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5411 - acc: 0.7737 - val_loss: 0.6932 - val_acc: 0.7657\n",
      "Epoch 292/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7770\n",
      "Epoch 292: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5513 - acc: 0.7765 - val_loss: 0.6747 - val_acc: 0.7755\n",
      "Epoch 293/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7711\n",
      "Epoch 293: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5446 - acc: 0.7706 - val_loss: 0.7380 - val_acc: 0.7794\n",
      "Epoch 294/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.7730\n",
      "Epoch 294: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5551 - acc: 0.7728 - val_loss: 0.7718 - val_acc: 0.7850\n",
      "Epoch 295/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.7765\n",
      "Epoch 295: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5471 - acc: 0.7761 - val_loss: 0.7022 - val_acc: 0.7854\n",
      "Epoch 296/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.7729\n",
      "Epoch 296: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5485 - acc: 0.7728 - val_loss: 0.6691 - val_acc: 0.7802\n",
      "Epoch 297/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7731\n",
      "Epoch 297: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7736 - val_loss: 0.7192 - val_acc: 0.7708\n",
      "Epoch 298/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7748\n",
      "Epoch 298: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5423 - acc: 0.7747 - val_loss: 0.6827 - val_acc: 0.7619\n",
      "Epoch 299/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7715\n",
      "Epoch 299: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5405 - acc: 0.7710 - val_loss: 0.6468 - val_acc: 0.7666\n",
      "Epoch 300/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.7704\n",
      "Epoch 300: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5559 - acc: 0.7705 - val_loss: 0.6922 - val_acc: 0.7884\n",
      "Epoch 301/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7722\n",
      "Epoch 301: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5140 - acc: 0.7717 - val_loss: 0.7394 - val_acc: 0.7777\n",
      "Epoch 302/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7732\n",
      "Epoch 302: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7727 - val_loss: 0.7825 - val_acc: 0.7781\n",
      "Epoch 303/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.7718\n",
      "Epoch 303: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5602 - acc: 0.7715 - val_loss: 0.7333 - val_acc: 0.7533\n",
      "Epoch 304/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.7723\n",
      "Epoch 304: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5441 - acc: 0.7718 - val_loss: 0.7313 - val_acc: 0.7794\n",
      "Epoch 305/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7770\n",
      "Epoch 305: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5375 - acc: 0.7767 - val_loss: 0.6679 - val_acc: 0.7858\n",
      "Epoch 306/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7724\n",
      "Epoch 306: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5319 - acc: 0.7720 - val_loss: 0.6686 - val_acc: 0.7653\n",
      "Epoch 307/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.7683\n",
      "Epoch 307: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5551 - acc: 0.7674 - val_loss: 0.6839 - val_acc: 0.7691\n",
      "Epoch 308/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.7708\n",
      "Epoch 308: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5432 - acc: 0.7703 - val_loss: 0.6760 - val_acc: 0.7768\n",
      "Epoch 309/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.7708\n",
      "Epoch 309: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5545 - acc: 0.7699 - val_loss: 0.7197 - val_acc: 0.7670\n",
      "Epoch 310/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7763\n",
      "Epoch 310: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5307 - acc: 0.7759 - val_loss: 0.7389 - val_acc: 0.7696\n",
      "Epoch 311/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7759\n",
      "Epoch 311: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5298 - acc: 0.7757 - val_loss: 0.7309 - val_acc: 0.7691\n",
      "Epoch 312/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.7775\n",
      "Epoch 312: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5194 - acc: 0.7772 - val_loss: 0.6771 - val_acc: 0.7696\n",
      "Epoch 313/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.7762\n",
      "Epoch 313: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5333 - acc: 0.7753 - val_loss: 0.6757 - val_acc: 0.7661\n",
      "Epoch 314/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7727\n",
      "Epoch 314: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5233 - acc: 0.7732 - val_loss: 0.8274 - val_acc: 0.7811\n",
      "Epoch 315/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.7802\n",
      "Epoch 315: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5474 - acc: 0.7800 - val_loss: 0.7752 - val_acc: 0.7867\n",
      "Epoch 316/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.7741\n",
      "Epoch 316: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5441 - acc: 0.7736 - val_loss: 0.6419 - val_acc: 0.7674\n",
      "Epoch 317/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7719\n",
      "Epoch 317: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7716 - val_loss: 0.7733 - val_acc: 0.7837\n",
      "Epoch 318/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7747\n",
      "Epoch 318: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5454 - acc: 0.7746 - val_loss: 0.7314 - val_acc: 0.7815\n",
      "Epoch 319/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7684\n",
      "Epoch 319: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5544 - acc: 0.7684 - val_loss: 0.7430 - val_acc: 0.7721\n",
      "Epoch 320/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7702\n",
      "Epoch 320: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5430 - acc: 0.7702 - val_loss: 0.7061 - val_acc: 0.7785\n",
      "Epoch 321/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5532 - acc: 0.7711\n",
      "Epoch 321: val_acc improved from 0.78923 to 0.79179, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/3/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5532 - acc: 0.7711 - val_loss: 0.7118 - val_acc: 0.7918\n",
      "Epoch 322/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.7786\n",
      "Epoch 322: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5484 - acc: 0.7781 - val_loss: 0.6698 - val_acc: 0.7726\n",
      "Epoch 323/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7737\n",
      "Epoch 323: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5296 - acc: 0.7735 - val_loss: 0.7685 - val_acc: 0.7811\n",
      "Epoch 324/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5505 - acc: 0.7732\n",
      "Epoch 324: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5505 - acc: 0.7732 - val_loss: 0.7025 - val_acc: 0.7751\n",
      "Epoch 325/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7776\n",
      "Epoch 325: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5411 - acc: 0.7769 - val_loss: 0.6697 - val_acc: 0.7721\n",
      "Epoch 326/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.7794\n",
      "Epoch 326: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5328 - acc: 0.7792 - val_loss: 0.7283 - val_acc: 0.7572\n",
      "Epoch 327/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.7733\n",
      "Epoch 327: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5340 - acc: 0.7732 - val_loss: 0.6911 - val_acc: 0.7713\n",
      "Epoch 328/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7774\n",
      "Epoch 328: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.7772 - val_loss: 0.7044 - val_acc: 0.7606\n",
      "Epoch 329/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7752\n",
      "Epoch 329: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5306 - acc: 0.7748 - val_loss: 0.7451 - val_acc: 0.7674\n",
      "Epoch 330/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.7730\n",
      "Epoch 330: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5730 - acc: 0.7723 - val_loss: 0.6708 - val_acc: 0.7661\n",
      "Epoch 331/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7791\n",
      "Epoch 331: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5264 - acc: 0.7785 - val_loss: 0.7084 - val_acc: 0.7708\n",
      "Epoch 332/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.7723\n",
      "Epoch 332: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5592 - acc: 0.7717 - val_loss: 0.7260 - val_acc: 0.7820\n",
      "Epoch 333/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7754\n",
      "Epoch 333: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5165 - acc: 0.7750 - val_loss: 0.6680 - val_acc: 0.7726\n",
      "Epoch 334/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7760\n",
      "Epoch 334: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5312 - acc: 0.7762 - val_loss: 0.8026 - val_acc: 0.7734\n",
      "Epoch 335/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.7716\n",
      "Epoch 335: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5469 - acc: 0.7712 - val_loss: 0.7779 - val_acc: 0.7747\n",
      "Epoch 336/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.7779\n",
      "Epoch 336: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5334 - acc: 0.7770 - val_loss: 0.6687 - val_acc: 0.7644\n",
      "Epoch 337/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7843\n",
      "Epoch 337: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5230 - acc: 0.7840 - val_loss: 0.7687 - val_acc: 0.7674\n",
      "Epoch 338/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.7775\n",
      "Epoch 338: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5314 - acc: 0.7770 - val_loss: 0.7851 - val_acc: 0.7687\n",
      "Epoch 339/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5371 - acc: 0.7766\n",
      "Epoch 339: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5371 - acc: 0.7766 - val_loss: 0.6961 - val_acc: 0.7794\n",
      "Epoch 340/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.7737\n",
      "Epoch 340: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5593 - acc: 0.7735 - val_loss: 0.6996 - val_acc: 0.7644\n",
      "Epoch 341/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7737\n",
      "Epoch 341: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5294 - acc: 0.7737 - val_loss: 0.7074 - val_acc: 0.7824\n",
      "Epoch 342/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5464 - acc: 0.7717\n",
      "Epoch 342: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5462 - acc: 0.7714 - val_loss: 0.8333 - val_acc: 0.7734\n",
      "Epoch 343/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.7750\n",
      "Epoch 343: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5524 - acc: 0.7749 - val_loss: 0.7456 - val_acc: 0.7674\n",
      "Epoch 344/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.7759\n",
      "Epoch 344: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5568 - acc: 0.7745 - val_loss: 0.6627 - val_acc: 0.7730\n",
      "Epoch 345/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7755\n",
      "Epoch 345: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5218 - acc: 0.7755 - val_loss: 0.6876 - val_acc: 0.7781\n",
      "Epoch 346/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7753\n",
      "Epoch 346: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5284 - acc: 0.7750 - val_loss: 0.7150 - val_acc: 0.7807\n",
      "Epoch 347/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.7812\n",
      "Epoch 347: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5246 - acc: 0.7807 - val_loss: 0.6763 - val_acc: 0.7867\n",
      "Epoch 348/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7710\n",
      "Epoch 348: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5494 - acc: 0.7706 - val_loss: 0.7407 - val_acc: 0.7768\n",
      "Epoch 349/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5693 - acc: 0.7774\n",
      "Epoch 349: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5677 - acc: 0.7778 - val_loss: 0.7329 - val_acc: 0.7781\n",
      "Epoch 350/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7744\n",
      "Epoch 350: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5433 - acc: 0.7739 - val_loss: 0.8967 - val_acc: 0.7708\n",
      "Epoch 351/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5536 - acc: 0.7787\n",
      "Epoch 351: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5532 - acc: 0.7783 - val_loss: 0.7667 - val_acc: 0.7743\n",
      "Epoch 352/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5358 - acc: 0.7755\n",
      "Epoch 352: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5358 - acc: 0.7755 - val_loss: 0.7283 - val_acc: 0.7726\n",
      "Epoch 353/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7775\n",
      "Epoch 353: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5407 - acc: 0.7762 - val_loss: 0.6895 - val_acc: 0.7602\n",
      "Epoch 354/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.7707\n",
      "Epoch 354: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5544 - acc: 0.7701 - val_loss: 0.6774 - val_acc: 0.7708\n",
      "Epoch 355/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7778\n",
      "Epoch 355: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5324 - acc: 0.7767 - val_loss: 0.6580 - val_acc: 0.7696\n",
      "Epoch 356/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7761\n",
      "Epoch 356: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5301 - acc: 0.7758 - val_loss: 0.6671 - val_acc: 0.7785\n",
      "Epoch 357/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.7797\n",
      "Epoch 357: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5529 - acc: 0.7785 - val_loss: 0.8135 - val_acc: 0.7764\n",
      "Epoch 358/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7759\n",
      "Epoch 358: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5397 - acc: 0.7754 - val_loss: 0.8019 - val_acc: 0.7717\n",
      "Epoch 359/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.7756\n",
      "Epoch 359: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5657 - acc: 0.7755 - val_loss: 0.7965 - val_acc: 0.7768\n",
      "Epoch 360/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7744\n",
      "Epoch 360: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5388 - acc: 0.7735 - val_loss: 0.8357 - val_acc: 0.7644\n",
      "Epoch 361/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.7728\n",
      "Epoch 361: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5306 - acc: 0.7733 - val_loss: 0.8084 - val_acc: 0.7785\n",
      "Epoch 362/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7801\n",
      "Epoch 362: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5374 - acc: 0.7794 - val_loss: 0.8294 - val_acc: 0.7708\n",
      "Epoch 363/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6006 - acc: 0.7803\n",
      "Epoch 363: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5970 - acc: 0.7801 - val_loss: 0.7766 - val_acc: 0.7738\n",
      "Epoch 364/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7799\n",
      "Epoch 364: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5295 - acc: 0.7788 - val_loss: 0.6181 - val_acc: 0.7743\n",
      "Epoch 365/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.7794\n",
      "Epoch 365: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5206 - acc: 0.7793 - val_loss: 0.8336 - val_acc: 0.7781\n",
      "Epoch 366/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5319 - acc: 0.7772\n",
      "Epoch 366: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5319 - acc: 0.7772 - val_loss: 0.7225 - val_acc: 0.7563\n",
      "Epoch 367/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.7762\n",
      "Epoch 367: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.7758 - val_loss: 0.9917 - val_acc: 0.7841\n",
      "Epoch 368/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6041 - acc: 0.7776\n",
      "Epoch 368: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6039 - acc: 0.7776 - val_loss: 0.7892 - val_acc: 0.7760\n",
      "Epoch 369/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7766\n",
      "Epoch 369: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5314 - acc: 0.7761 - val_loss: 0.7865 - val_acc: 0.7815\n",
      "Epoch 370/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.7765\n",
      "Epoch 370: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5383 - acc: 0.7757 - val_loss: 0.7535 - val_acc: 0.7743\n",
      "Epoch 371/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7784\n",
      "Epoch 371: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5394 - acc: 0.7781 - val_loss: 0.9023 - val_acc: 0.7743\n",
      "Epoch 372/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7748\n",
      "Epoch 372: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5436 - acc: 0.7743 - val_loss: 0.8185 - val_acc: 0.7760\n",
      "Epoch 373/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5545 - acc: 0.7723\n",
      "Epoch 373: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5545 - acc: 0.7723 - val_loss: 0.7201 - val_acc: 0.7713\n",
      "Epoch 374/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.7735\n",
      "Epoch 374: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5351 - acc: 0.7734 - val_loss: 0.9237 - val_acc: 0.7807\n",
      "Epoch 375/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5816 - acc: 0.7759\n",
      "Epoch 375: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5804 - acc: 0.7752 - val_loss: 0.6463 - val_acc: 0.7627\n",
      "Epoch 376/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5188 - acc: 0.7777\n",
      "Epoch 376: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5188 - acc: 0.7777 - val_loss: 0.7470 - val_acc: 0.7619\n",
      "Epoch 377/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.7735\n",
      "Epoch 377: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5354 - acc: 0.7739 - val_loss: 0.9113 - val_acc: 0.7726\n",
      "Epoch 378/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.7743\n",
      "Epoch 378: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5616 - acc: 0.7742 - val_loss: 0.7793 - val_acc: 0.7802\n",
      "Epoch 379/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5225 - acc: 0.7756\n",
      "Epoch 379: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5225 - acc: 0.7756 - val_loss: 0.8028 - val_acc: 0.7751\n",
      "Epoch 380/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.7773\n",
      "Epoch 380: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5576 - acc: 0.7764 - val_loss: 0.7827 - val_acc: 0.7854\n",
      "Epoch 381/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.7810\n",
      "Epoch 381: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5312 - acc: 0.7796 - val_loss: 0.6928 - val_acc: 0.7747\n",
      "Epoch 382/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.7741\n",
      "Epoch 382: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5584 - acc: 0.7736 - val_loss: 0.8085 - val_acc: 0.7790\n",
      "Epoch 383/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7750\n",
      "Epoch 383: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5427 - acc: 0.7753 - val_loss: 0.7234 - val_acc: 0.7820\n",
      "Epoch 384/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.7736\n",
      "Epoch 384: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5424 - acc: 0.7732 - val_loss: 0.6536 - val_acc: 0.7764\n",
      "Epoch 385/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.7742\n",
      "Epoch 385: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5513 - acc: 0.7737 - val_loss: 0.7308 - val_acc: 0.7837\n",
      "Epoch 386/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7790\n",
      "Epoch 386: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5455 - acc: 0.7789 - val_loss: 0.7376 - val_acc: 0.7760\n",
      "Epoch 387/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7791\n",
      "Epoch 387: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5430 - acc: 0.7787 - val_loss: 0.8262 - val_acc: 0.7832\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 1.8060 - acc: 0.6109\n",
      "Epoch 1: val_acc improved from -inf to 0.69816, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 3s 7ms/step - loss: 1.7793 - acc: 0.6106 - val_loss: 0.6520 - val_acc: 0.6982\n",
      "Epoch 2/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6991 - acc: 0.6579\n",
      "Epoch 2: val_acc improved from 0.69816 to 0.71184, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6986 - acc: 0.6579 - val_loss: 0.6375 - val_acc: 0.7118\n",
      "Epoch 3/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6668 - acc: 0.6775\n",
      "Epoch 3: val_acc improved from 0.71184 to 0.71441, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6664 - acc: 0.6777 - val_loss: 0.6214 - val_acc: 0.7144\n",
      "Epoch 4/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6529 - acc: 0.6917\n",
      "Epoch 4: val_acc improved from 0.71441 to 0.72039, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6530 - acc: 0.6911 - val_loss: 0.5958 - val_acc: 0.7204\n",
      "Epoch 5/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.6980\n",
      "Epoch 5: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6408 - acc: 0.6965 - val_loss: 0.6078 - val_acc: 0.7165\n",
      "Epoch 6/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6316 - acc: 0.6994\n",
      "Epoch 6: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6329 - acc: 0.6989 - val_loss: 0.6084 - val_acc: 0.7187\n",
      "Epoch 7/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6158 - acc: 0.7044\n",
      "Epoch 7: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6157 - acc: 0.7044 - val_loss: 0.6099 - val_acc: 0.7204\n",
      "Epoch 8/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.7011\n",
      "Epoch 8: val_acc did not improve from 0.72039\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6114 - acc: 0.7010 - val_loss: 0.5904 - val_acc: 0.7191\n",
      "Epoch 9/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.7090\n",
      "Epoch 9: val_acc improved from 0.72039 to 0.72381, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6076 - acc: 0.7084 - val_loss: 0.5821 - val_acc: 0.7238\n",
      "Epoch 10/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.7071\n",
      "Epoch 10: val_acc improved from 0.72381 to 0.72467, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6009 - acc: 0.7064 - val_loss: 0.6108 - val_acc: 0.7247\n",
      "Epoch 11/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.7106\n",
      "Epoch 11: val_acc did not improve from 0.72467\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5977 - acc: 0.7102 - val_loss: 0.6052 - val_acc: 0.7200\n",
      "Epoch 12/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5955 - acc: 0.7135\n",
      "Epoch 12: val_acc improved from 0.72467 to 0.73108, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5969 - acc: 0.7127 - val_loss: 0.5878 - val_acc: 0.7311\n",
      "Epoch 13/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5897 - acc: 0.7135\n",
      "Epoch 13: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5896 - acc: 0.7134 - val_loss: 0.6012 - val_acc: 0.7221\n",
      "Epoch 14/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5916 - acc: 0.7106\n",
      "Epoch 14: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5918 - acc: 0.7103 - val_loss: 0.5671 - val_acc: 0.7238\n",
      "Epoch 15/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5870 - acc: 0.7107\n",
      "Epoch 15: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5870 - acc: 0.7107 - val_loss: 0.5798 - val_acc: 0.7161\n",
      "Epoch 16/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5898 - acc: 0.7153\n",
      "Epoch 16: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5898 - acc: 0.7153 - val_loss: 0.5600 - val_acc: 0.7260\n",
      "Epoch 17/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5860 - acc: 0.7131\n",
      "Epoch 17: val_acc improved from 0.73108 to 0.73279, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5860 - acc: 0.7132 - val_loss: 0.5543 - val_acc: 0.7328\n",
      "Epoch 18/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.7205\n",
      "Epoch 18: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5897 - acc: 0.7200 - val_loss: 0.5818 - val_acc: 0.7302\n",
      "Epoch 19/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5818 - acc: 0.7206\n",
      "Epoch 19: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5816 - acc: 0.7201 - val_loss: 0.5488 - val_acc: 0.7281\n",
      "Epoch 20/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.7174\n",
      "Epoch 20: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5788 - acc: 0.7174 - val_loss: 0.5549 - val_acc: 0.7277\n",
      "Epoch 21/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5763 - acc: 0.7175\n",
      "Epoch 21: val_acc did not improve from 0.73279\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5775 - acc: 0.7166 - val_loss: 0.5737 - val_acc: 0.7264\n",
      "Epoch 22/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5766 - acc: 0.7144\n",
      "Epoch 22: val_acc improved from 0.73279 to 0.73493, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5764 - acc: 0.7144 - val_loss: 0.5492 - val_acc: 0.7349\n",
      "Epoch 23/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.7180\n",
      "Epoch 23: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5829 - acc: 0.7175 - val_loss: 0.5675 - val_acc: 0.7345\n",
      "Epoch 24/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.7190\n",
      "Epoch 24: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5824 - acc: 0.7184 - val_loss: 0.5831 - val_acc: 0.7315\n",
      "Epoch 25/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5777 - acc: 0.7191\n",
      "Epoch 25: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5775 - acc: 0.7188 - val_loss: 0.5734 - val_acc: 0.7328\n",
      "Epoch 26/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5840 - acc: 0.7204\n",
      "Epoch 26: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5841 - acc: 0.7193 - val_loss: 0.5709 - val_acc: 0.7341\n",
      "Epoch 27/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5851 - acc: 0.7219\n",
      "Epoch 27: val_acc improved from 0.73493 to 0.73578, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5851 - acc: 0.7212 - val_loss: 0.5340 - val_acc: 0.7358\n",
      "Epoch 28/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.7242\n",
      "Epoch 28: val_acc did not improve from 0.73578\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5772 - acc: 0.7240 - val_loss: 0.5580 - val_acc: 0.7328\n",
      "Epoch 29/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5719 - acc: 0.7243\n",
      "Epoch 29: val_acc improved from 0.73578 to 0.74177, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5718 - acc: 0.7240 - val_loss: 0.5541 - val_acc: 0.7418\n",
      "Epoch 30/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.7234\n",
      "Epoch 30: val_acc did not improve from 0.74177\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5787 - acc: 0.7228 - val_loss: 0.5636 - val_acc: 0.7332\n",
      "Epoch 31/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.7225\n",
      "Epoch 31: val_acc improved from 0.74177 to 0.74476, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5742 - acc: 0.7225 - val_loss: 0.5589 - val_acc: 0.7448\n",
      "Epoch 32/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5766 - acc: 0.7208\n",
      "Epoch 32: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5764 - acc: 0.7206 - val_loss: 0.5813 - val_acc: 0.7328\n",
      "Epoch 33/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.7247\n",
      "Epoch 33: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5820 - acc: 0.7246 - val_loss: 0.5427 - val_acc: 0.7302\n",
      "Epoch 34/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.7248\n",
      "Epoch 34: val_acc improved from 0.74476 to 0.74647, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5719 - acc: 0.7252 - val_loss: 0.5415 - val_acc: 0.7465\n",
      "Epoch 35/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.7250\n",
      "Epoch 35: val_acc improved from 0.74647 to 0.74989, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5673 - acc: 0.7244 - val_loss: 0.5441 - val_acc: 0.7499\n",
      "Epoch 36/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.7258\n",
      "Epoch 36: val_acc did not improve from 0.74989\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5707 - acc: 0.7259 - val_loss: 0.5445 - val_acc: 0.7482\n",
      "Epoch 37/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7227\n",
      "Epoch 37: val_acc did not improve from 0.74989\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5700 - acc: 0.7226 - val_loss: 0.5614 - val_acc: 0.7371\n",
      "Epoch 38/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.7268\n",
      "Epoch 38: val_acc did not improve from 0.74989\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5704 - acc: 0.7268 - val_loss: 0.5406 - val_acc: 0.7452\n",
      "Epoch 39/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.7251\n",
      "Epoch 39: val_acc did not improve from 0.74989\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5848 - acc: 0.7251 - val_loss: 0.5284 - val_acc: 0.7439\n",
      "Epoch 40/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.7242\n",
      "Epoch 40: val_acc did not improve from 0.74989\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5689 - acc: 0.7242 - val_loss: 0.5385 - val_acc: 0.7482\n",
      "Epoch 41/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.7240\n",
      "Epoch 41: val_acc did not improve from 0.74989\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5750 - acc: 0.7227 - val_loss: 0.5572 - val_acc: 0.7465\n",
      "Epoch 42/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5728 - acc: 0.7284\n",
      "Epoch 42: val_acc improved from 0.74989 to 0.75075, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5728 - acc: 0.7287 - val_loss: 0.5491 - val_acc: 0.7507\n",
      "Epoch 43/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5729 - acc: 0.7285\n",
      "Epoch 43: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5742 - acc: 0.7281 - val_loss: 0.5702 - val_acc: 0.7478\n",
      "Epoch 44/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.7298\n",
      "Epoch 44: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5612 - acc: 0.7293 - val_loss: 0.5672 - val_acc: 0.7443\n",
      "Epoch 45/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5717 - acc: 0.7272\n",
      "Epoch 45: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5712 - acc: 0.7268 - val_loss: 0.5510 - val_acc: 0.7486\n",
      "Epoch 46/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.7313\n",
      "Epoch 46: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5734 - acc: 0.7313 - val_loss: 0.5288 - val_acc: 0.7486\n",
      "Epoch 47/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.7327\n",
      "Epoch 47: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5637 - acc: 0.7327 - val_loss: 0.5375 - val_acc: 0.7495\n",
      "Epoch 48/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.7344\n",
      "Epoch 48: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5644 - acc: 0.7342 - val_loss: 0.5571 - val_acc: 0.7439\n",
      "Epoch 49/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5750 - acc: 0.7320\n",
      "Epoch 49: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5754 - acc: 0.7315 - val_loss: 0.5554 - val_acc: 0.7486\n",
      "Epoch 50/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.7298\n",
      "Epoch 50: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5611 - acc: 0.7301 - val_loss: 0.5272 - val_acc: 0.7452\n",
      "Epoch 51/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5721 - acc: 0.7284\n",
      "Epoch 51: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5718 - acc: 0.7276 - val_loss: 0.5374 - val_acc: 0.7409\n",
      "Epoch 52/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5588 - acc: 0.7311\n",
      "Epoch 52: val_acc improved from 0.75075 to 0.75203, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5588 - acc: 0.7311 - val_loss: 0.5483 - val_acc: 0.7520\n",
      "Epoch 53/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5668 - acc: 0.7368\n",
      "Epoch 53: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5679 - acc: 0.7364 - val_loss: 0.5396 - val_acc: 0.7486\n",
      "Epoch 54/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.7338\n",
      "Epoch 54: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5674 - acc: 0.7339 - val_loss: 0.5337 - val_acc: 0.7499\n",
      "Epoch 55/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.7333\n",
      "Epoch 55: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5701 - acc: 0.7330 - val_loss: 0.5440 - val_acc: 0.7490\n",
      "Epoch 56/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5669 - acc: 0.7327\n",
      "Epoch 56: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5673 - acc: 0.7319 - val_loss: 0.5343 - val_acc: 0.7443\n",
      "Epoch 57/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.7355\n",
      "Epoch 57: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5557 - acc: 0.7349 - val_loss: 0.5499 - val_acc: 0.7469\n",
      "Epoch 58/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7330\n",
      "Epoch 58: val_acc improved from 0.75203 to 0.76144, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5660 - acc: 0.7331 - val_loss: 0.5561 - val_acc: 0.7614\n",
      "Epoch 59/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.7390\n",
      "Epoch 59: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5587 - acc: 0.7386 - val_loss: 0.5496 - val_acc: 0.7567\n",
      "Epoch 60/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.7363\n",
      "Epoch 60: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5595 - acc: 0.7354 - val_loss: 0.5438 - val_acc: 0.7537\n",
      "Epoch 61/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.7368\n",
      "Epoch 61: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5555 - acc: 0.7367 - val_loss: 0.5352 - val_acc: 0.7567\n",
      "Epoch 62/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7439\n",
      "Epoch 62: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5496 - acc: 0.7433 - val_loss: 0.5364 - val_acc: 0.7529\n",
      "Epoch 63/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5686 - acc: 0.7382\n",
      "Epoch 63: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5686 - acc: 0.7382 - val_loss: 0.5476 - val_acc: 0.7537\n",
      "Epoch 64/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7400\n",
      "Epoch 64: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5509 - acc: 0.7395 - val_loss: 0.5362 - val_acc: 0.7499\n",
      "Epoch 65/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.7404\n",
      "Epoch 65: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5640 - acc: 0.7397 - val_loss: 0.5612 - val_acc: 0.7431\n",
      "Epoch 66/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.7407\n",
      "Epoch 66: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5575 - acc: 0.7398 - val_loss: 0.5330 - val_acc: 0.7520\n",
      "Epoch 67/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5683 - acc: 0.7357\n",
      "Epoch 67: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5690 - acc: 0.7346 - val_loss: 0.5394 - val_acc: 0.7482\n",
      "Epoch 68/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5613 - acc: 0.7408\n",
      "Epoch 68: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5610 - acc: 0.7406 - val_loss: 0.5461 - val_acc: 0.7580\n",
      "Epoch 69/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.7406\n",
      "Epoch 69: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.7407 - val_loss: 0.5555 - val_acc: 0.7537\n",
      "Epoch 70/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.7386\n",
      "Epoch 70: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5529 - acc: 0.7381 - val_loss: 0.5450 - val_acc: 0.7520\n",
      "Epoch 71/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.7381\n",
      "Epoch 71: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5547 - acc: 0.7379 - val_loss: 0.5341 - val_acc: 0.7572\n",
      "Epoch 72/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5728 - acc: 0.7342\n",
      "Epoch 72: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5731 - acc: 0.7338 - val_loss: 0.5348 - val_acc: 0.7473\n",
      "Epoch 73/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.7435\n",
      "Epoch 73: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5616 - acc: 0.7424 - val_loss: 0.5265 - val_acc: 0.7580\n",
      "Epoch 74/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.7388\n",
      "Epoch 74: val_acc improved from 0.76144 to 0.76186, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.7386 - val_loss: 0.5395 - val_acc: 0.7619\n",
      "Epoch 75/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.7449\n",
      "Epoch 75: val_acc did not improve from 0.76186\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5495 - acc: 0.7451 - val_loss: 0.5623 - val_acc: 0.7584\n",
      "Epoch 76/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.7424\n",
      "Epoch 76: val_acc did not improve from 0.76186\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5563 - acc: 0.7425 - val_loss: 0.5482 - val_acc: 0.7563\n",
      "Epoch 77/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7465\n",
      "Epoch 77: val_acc improved from 0.76186 to 0.76443, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5454 - acc: 0.7458 - val_loss: 0.5287 - val_acc: 0.7644\n",
      "Epoch 78/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.7444\n",
      "Epoch 78: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5550 - acc: 0.7443 - val_loss: 0.5564 - val_acc: 0.7550\n",
      "Epoch 79/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7459\n",
      "Epoch 79: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5513 - acc: 0.7458 - val_loss: 0.5483 - val_acc: 0.7589\n",
      "Epoch 80/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.7404\n",
      "Epoch 80: val_acc did not improve from 0.76443\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5581 - acc: 0.7406 - val_loss: 0.5703 - val_acc: 0.7644\n",
      "Epoch 81/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.7385\n",
      "Epoch 81: val_acc improved from 0.76443 to 0.76785, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5608 - acc: 0.7383 - val_loss: 0.5740 - val_acc: 0.7678\n",
      "Epoch 82/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7504\n",
      "Epoch 82: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5504 - acc: 0.7497 - val_loss: 0.5440 - val_acc: 0.7649\n",
      "Epoch 83/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5630 - acc: 0.7474\n",
      "Epoch 83: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5628 - acc: 0.7474 - val_loss: 0.5345 - val_acc: 0.7533\n",
      "Epoch 84/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.7445\n",
      "Epoch 84: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5582 - acc: 0.7438 - val_loss: 0.5309 - val_acc: 0.7572\n",
      "Epoch 85/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5394 - acc: 0.7477\n",
      "Epoch 85: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5394 - acc: 0.7477 - val_loss: 0.5512 - val_acc: 0.7525\n",
      "Epoch 86/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.7442\n",
      "Epoch 86: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5538 - acc: 0.7436 - val_loss: 0.5277 - val_acc: 0.7576\n",
      "Epoch 87/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.7454\n",
      "Epoch 87: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5559 - acc: 0.7452 - val_loss: 0.5400 - val_acc: 0.7559\n",
      "Epoch 88/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.7466\n",
      "Epoch 88: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5544 - acc: 0.7466 - val_loss: 0.5395 - val_acc: 0.7610\n",
      "Epoch 89/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.7441\n",
      "Epoch 89: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5477 - acc: 0.7441 - val_loss: 0.5429 - val_acc: 0.7512\n",
      "Epoch 90/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.7473\n",
      "Epoch 90: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5461 - acc: 0.7464 - val_loss: 0.5491 - val_acc: 0.7525\n",
      "Epoch 91/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7514\n",
      "Epoch 91: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5382 - acc: 0.7514 - val_loss: 0.5392 - val_acc: 0.7636\n",
      "Epoch 92/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.7452\n",
      "Epoch 92: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5635 - acc: 0.7441 - val_loss: 0.5385 - val_acc: 0.7584\n",
      "Epoch 93/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7458\n",
      "Epoch 93: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5496 - acc: 0.7455 - val_loss: 0.5402 - val_acc: 0.7602\n",
      "Epoch 94/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5574 - acc: 0.7432\n",
      "Epoch 94: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5571 - acc: 0.7435 - val_loss: 0.5322 - val_acc: 0.7542\n",
      "Epoch 95/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7460\n",
      "Epoch 95: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5408 - acc: 0.7460 - val_loss: 0.5516 - val_acc: 0.7627\n",
      "Epoch 96/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.7459\n",
      "Epoch 96: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5471 - acc: 0.7458 - val_loss: 0.5400 - val_acc: 0.7589\n",
      "Epoch 97/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7522\n",
      "Epoch 97: val_acc did not improve from 0.76785\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5466 - acc: 0.7523 - val_loss: 0.5440 - val_acc: 0.7670\n",
      "Epoch 98/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5492 - acc: 0.7482\n",
      "Epoch 98: val_acc improved from 0.76785 to 0.77383, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5492 - acc: 0.7482 - val_loss: 0.5430 - val_acc: 0.7738\n",
      "Epoch 99/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.7465\n",
      "Epoch 99: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5587 - acc: 0.7461 - val_loss: 0.5376 - val_acc: 0.7610\n",
      "Epoch 100/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.7541\n",
      "Epoch 100: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5625 - acc: 0.7533 - val_loss: 0.5250 - val_acc: 0.7597\n",
      "Epoch 101/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.7508\n",
      "Epoch 101: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5464 - acc: 0.7506 - val_loss: 0.5337 - val_acc: 0.7683\n",
      "Epoch 102/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.7515\n",
      "Epoch 102: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5771 - acc: 0.7514 - val_loss: 0.5460 - val_acc: 0.7546\n",
      "Epoch 103/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5613 - acc: 0.7481\n",
      "Epoch 103: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5605 - acc: 0.7485 - val_loss: 0.5326 - val_acc: 0.7627\n",
      "Epoch 104/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.7501\n",
      "Epoch 104: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5572 - acc: 0.7501 - val_loss: 0.5304 - val_acc: 0.7614\n",
      "Epoch 105/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7465\n",
      "Epoch 105: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5477 - acc: 0.7466 - val_loss: 0.5817 - val_acc: 0.7631\n",
      "Epoch 106/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.7530\n",
      "Epoch 106: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5476 - acc: 0.7528 - val_loss: 0.5402 - val_acc: 0.7674\n",
      "Epoch 107/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7482\n",
      "Epoch 107: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5465 - acc: 0.7472 - val_loss: 0.5304 - val_acc: 0.7546\n",
      "Epoch 108/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5490 - acc: 0.7548\n",
      "Epoch 108: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5490 - acc: 0.7548 - val_loss: 0.5483 - val_acc: 0.7653\n",
      "Epoch 109/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.7583\n",
      "Epoch 109: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5643 - acc: 0.7572 - val_loss: 0.5368 - val_acc: 0.7678\n",
      "Epoch 110/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.7517\n",
      "Epoch 110: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5463 - acc: 0.7514 - val_loss: 0.5371 - val_acc: 0.7623\n",
      "Epoch 111/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7462\n",
      "Epoch 111: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5507 - acc: 0.7459 - val_loss: 0.5547 - val_acc: 0.7525\n",
      "Epoch 112/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7505\n",
      "Epoch 112: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5493 - acc: 0.7500 - val_loss: 0.5650 - val_acc: 0.7542\n",
      "Epoch 113/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5672 - acc: 0.7472\n",
      "Epoch 113: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5672 - acc: 0.7472 - val_loss: 0.5287 - val_acc: 0.7721\n",
      "Epoch 114/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7570\n",
      "Epoch 114: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5438 - acc: 0.7559 - val_loss: 0.5299 - val_acc: 0.7623\n",
      "Epoch 115/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.7547\n",
      "Epoch 115: val_acc did not improve from 0.77383\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5529 - acc: 0.7548 - val_loss: 0.5261 - val_acc: 0.7670\n",
      "Epoch 116/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.7507\n",
      "Epoch 116: val_acc improved from 0.77383 to 0.77555, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5494 - acc: 0.7507 - val_loss: 0.5613 - val_acc: 0.7755\n",
      "Epoch 117/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.7463\n",
      "Epoch 117: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5542 - acc: 0.7462 - val_loss: 0.5376 - val_acc: 0.7602\n",
      "Epoch 118/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5539 - acc: 0.7542\n",
      "Epoch 118: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5526 - acc: 0.7538 - val_loss: 0.5601 - val_acc: 0.7696\n",
      "Epoch 119/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7527\n",
      "Epoch 119: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5485 - acc: 0.7531 - val_loss: 0.5510 - val_acc: 0.7683\n",
      "Epoch 120/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.7514\n",
      "Epoch 120: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5659 - acc: 0.7514 - val_loss: 0.5457 - val_acc: 0.7726\n",
      "Epoch 121/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7510\n",
      "Epoch 121: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5507 - acc: 0.7501 - val_loss: 0.5385 - val_acc: 0.7640\n",
      "Epoch 122/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7548\n",
      "Epoch 122: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5399 - acc: 0.7538 - val_loss: 0.5309 - val_acc: 0.7666\n",
      "Epoch 123/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.7504\n",
      "Epoch 123: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5582 - acc: 0.7504 - val_loss: 0.5379 - val_acc: 0.7589\n",
      "Epoch 124/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5604 - acc: 0.7521\n",
      "Epoch 124: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5584 - acc: 0.7515 - val_loss: 0.5339 - val_acc: 0.7747\n",
      "Epoch 125/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.7513\n",
      "Epoch 125: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5639 - acc: 0.7514 - val_loss: 0.5380 - val_acc: 0.7636\n",
      "Epoch 126/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7541\n",
      "Epoch 126: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5602 - acc: 0.7540 - val_loss: 0.5424 - val_acc: 0.7738\n",
      "Epoch 127/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7542\n",
      "Epoch 127: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5387 - acc: 0.7540 - val_loss: 0.5475 - val_acc: 0.7691\n",
      "Epoch 128/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.7557\n",
      "Epoch 128: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5553 - acc: 0.7554 - val_loss: 0.5380 - val_acc: 0.7755\n",
      "Epoch 129/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7596\n",
      "Epoch 129: val_acc did not improve from 0.77555\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.7599 - val_loss: 0.5479 - val_acc: 0.7636\n",
      "Epoch 130/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.7490\n",
      "Epoch 130: val_acc improved from 0.77555 to 0.77683, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5481 - acc: 0.7489 - val_loss: 0.5357 - val_acc: 0.7768\n",
      "Epoch 131/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7529\n",
      "Epoch 131: val_acc improved from 0.77683 to 0.78281, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5419 - acc: 0.7529 - val_loss: 0.5358 - val_acc: 0.7828\n",
      "Epoch 132/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.7536\n",
      "Epoch 132: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5480 - acc: 0.7534 - val_loss: 0.5224 - val_acc: 0.7751\n",
      "Epoch 133/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7561\n",
      "Epoch 133: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5438 - acc: 0.7560 - val_loss: 0.5507 - val_acc: 0.7640\n",
      "Epoch 134/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.7546\n",
      "Epoch 134: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5663 - acc: 0.7540 - val_loss: 0.5316 - val_acc: 0.7597\n",
      "Epoch 135/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7558\n",
      "Epoch 135: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5418 - acc: 0.7559 - val_loss: 0.5453 - val_acc: 0.7777\n",
      "Epoch 136/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.7533\n",
      "Epoch 136: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5523 - acc: 0.7531 - val_loss: 0.5366 - val_acc: 0.7555\n",
      "Epoch 137/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.7561\n",
      "Epoch 137: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5576 - acc: 0.7560 - val_loss: 0.5412 - val_acc: 0.7623\n",
      "Epoch 138/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7556\n",
      "Epoch 138: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5403 - acc: 0.7560 - val_loss: 0.5513 - val_acc: 0.7807\n",
      "Epoch 139/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.7584\n",
      "Epoch 139: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5410 - acc: 0.7580 - val_loss: 0.5477 - val_acc: 0.7704\n",
      "Epoch 140/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.7552\n",
      "Epoch 140: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5431 - acc: 0.7551 - val_loss: 0.5759 - val_acc: 0.7614\n",
      "Epoch 141/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7536\n",
      "Epoch 141: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5434 - acc: 0.7537 - val_loss: 0.5556 - val_acc: 0.7683\n",
      "Epoch 142/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.7538\n",
      "Epoch 142: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5510 - acc: 0.7535 - val_loss: 0.5573 - val_acc: 0.7721\n",
      "Epoch 143/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.7546\n",
      "Epoch 143: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5476 - acc: 0.7546 - val_loss: 0.5533 - val_acc: 0.7649\n",
      "Epoch 144/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.7539\n",
      "Epoch 144: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5544 - acc: 0.7539 - val_loss: 0.5401 - val_acc: 0.7747\n",
      "Epoch 145/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7553\n",
      "Epoch 145: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5437 - acc: 0.7548 - val_loss: 0.5579 - val_acc: 0.7751\n",
      "Epoch 146/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.7607\n",
      "Epoch 146: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5458 - acc: 0.7604 - val_loss: 0.5328 - val_acc: 0.7760\n",
      "Epoch 147/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5367 - acc: 0.7531\n",
      "Epoch 147: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5367 - acc: 0.7531 - val_loss: 0.5396 - val_acc: 0.7678\n",
      "Epoch 148/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7624\n",
      "Epoch 148: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5363 - acc: 0.7617 - val_loss: 0.5432 - val_acc: 0.7751\n",
      "Epoch 149/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.7540\n",
      "Epoch 149: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5561 - acc: 0.7532 - val_loss: 0.5659 - val_acc: 0.7563\n",
      "Epoch 150/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7540\n",
      "Epoch 150: val_acc did not improve from 0.78281\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5503 - acc: 0.7538 - val_loss: 0.5252 - val_acc: 0.7567\n",
      "Epoch 151/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7603\n",
      "Epoch 151: val_acc improved from 0.78281 to 0.78923, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5382 - acc: 0.7608 - val_loss: 0.5430 - val_acc: 0.7892\n",
      "Epoch 152/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5634 - acc: 0.7634\n",
      "Epoch 152: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5634 - acc: 0.7634 - val_loss: 0.5446 - val_acc: 0.7764\n",
      "Epoch 153/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.7590\n",
      "Epoch 153: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5365 - acc: 0.7582 - val_loss: 0.5638 - val_acc: 0.7691\n",
      "Epoch 154/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.7574\n",
      "Epoch 154: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5648 - acc: 0.7575 - val_loss: 0.5392 - val_acc: 0.7738\n",
      "Epoch 155/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7550\n",
      "Epoch 155: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5514 - acc: 0.7548 - val_loss: 0.5570 - val_acc: 0.7734\n",
      "Epoch 156/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.7577\n",
      "Epoch 156: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5434 - acc: 0.7579 - val_loss: 0.5357 - val_acc: 0.7755\n",
      "Epoch 157/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7542\n",
      "Epoch 157: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5392 - acc: 0.7538 - val_loss: 0.5628 - val_acc: 0.7614\n",
      "Epoch 158/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.7543\n",
      "Epoch 158: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5487 - acc: 0.7545 - val_loss: 0.5691 - val_acc: 0.7691\n",
      "Epoch 159/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.7561\n",
      "Epoch 159: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5765 - acc: 0.7564 - val_loss: 0.5418 - val_acc: 0.7627\n",
      "Epoch 160/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.7630\n",
      "Epoch 160: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5309 - acc: 0.7629 - val_loss: 0.5304 - val_acc: 0.7751\n",
      "Epoch 161/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.7586\n",
      "Epoch 161: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5406 - acc: 0.7585 - val_loss: 0.5363 - val_acc: 0.7614\n",
      "Epoch 162/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5627 - acc: 0.7602\n",
      "Epoch 162: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5622 - acc: 0.7597 - val_loss: 0.5408 - val_acc: 0.7661\n",
      "Epoch 163/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.7545\n",
      "Epoch 163: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5473 - acc: 0.7546 - val_loss: 0.5573 - val_acc: 0.7777\n",
      "Epoch 164/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.7548\n",
      "Epoch 164: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5485 - acc: 0.7546 - val_loss: 0.5350 - val_acc: 0.7760\n",
      "Epoch 165/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.7574\n",
      "Epoch 165: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.7577 - val_loss: 0.5435 - val_acc: 0.7597\n",
      "Epoch 166/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7576\n",
      "Epoch 166: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5488 - acc: 0.7570 - val_loss: 0.5579 - val_acc: 0.7696\n",
      "Epoch 167/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.7556\n",
      "Epoch 167: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5510 - acc: 0.7560 - val_loss: 0.5372 - val_acc: 0.7764\n",
      "Epoch 168/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.7556\n",
      "Epoch 168: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5439 - acc: 0.7557 - val_loss: 0.5517 - val_acc: 0.7696\n",
      "Epoch 169/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7593\n",
      "Epoch 169: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5396 - acc: 0.7590 - val_loss: 0.5433 - val_acc: 0.7794\n",
      "Epoch 170/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.7563\n",
      "Epoch 170: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5405 - acc: 0.7565 - val_loss: 0.5727 - val_acc: 0.7704\n",
      "Epoch 171/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5414 - acc: 0.7576\n",
      "Epoch 171: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5414 - acc: 0.7576 - val_loss: 0.5369 - val_acc: 0.7815\n",
      "Epoch 172/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7572\n",
      "Epoch 172: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5651 - acc: 0.7569 - val_loss: 0.5214 - val_acc: 0.7657\n",
      "Epoch 173/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7562\n",
      "Epoch 173: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5454 - acc: 0.7563 - val_loss: 0.5303 - val_acc: 0.7820\n",
      "Epoch 174/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.7595\n",
      "Epoch 174: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5590 - acc: 0.7595 - val_loss: 0.5375 - val_acc: 0.7820\n",
      "Epoch 175/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7606\n",
      "Epoch 175: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5274 - acc: 0.7606 - val_loss: 0.5307 - val_acc: 0.7751\n",
      "Epoch 176/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7609\n",
      "Epoch 176: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5494 - acc: 0.7607 - val_loss: 0.5431 - val_acc: 0.7790\n",
      "Epoch 177/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.7626\n",
      "Epoch 177: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5278 - acc: 0.7618 - val_loss: 0.5482 - val_acc: 0.7751\n",
      "Epoch 178/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5681 - acc: 0.7584\n",
      "Epoch 178: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5662 - acc: 0.7583 - val_loss: 0.5625 - val_acc: 0.7751\n",
      "Epoch 179/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7642\n",
      "Epoch 179: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5276 - acc: 0.7642 - val_loss: 0.5667 - val_acc: 0.7832\n",
      "Epoch 180/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.7568\n",
      "Epoch 180: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5558 - acc: 0.7569 - val_loss: 0.6007 - val_acc: 0.7764\n",
      "Epoch 181/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7592\n",
      "Epoch 181: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5419 - acc: 0.7595 - val_loss: 0.5549 - val_acc: 0.7730\n",
      "Epoch 182/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7622\n",
      "Epoch 182: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5313 - acc: 0.7608 - val_loss: 0.5404 - val_acc: 0.7721\n",
      "Epoch 183/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7616\n",
      "Epoch 183: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5347 - acc: 0.7615 - val_loss: 0.5750 - val_acc: 0.7747\n",
      "Epoch 184/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7621\n",
      "Epoch 184: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5326 - acc: 0.7616 - val_loss: 0.5758 - val_acc: 0.7747\n",
      "Epoch 185/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7552\n",
      "Epoch 185: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5704 - acc: 0.7549 - val_loss: 0.5200 - val_acc: 0.7867\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7592\n",
      "Epoch 186: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.7592 - val_loss: 0.5207 - val_acc: 0.7755\n",
      "Epoch 187/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7599\n",
      "Epoch 187: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5369 - acc: 0.7591 - val_loss: 0.5119 - val_acc: 0.7820\n",
      "Epoch 188/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7634\n",
      "Epoch 188: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5272 - acc: 0.7633 - val_loss: 0.5411 - val_acc: 0.7653\n",
      "Epoch 189/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.7631\n",
      "Epoch 189: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5514 - acc: 0.7625 - val_loss: 0.5726 - val_acc: 0.7798\n",
      "Epoch 190/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7603\n",
      "Epoch 190: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5415 - acc: 0.7599 - val_loss: 0.5538 - val_acc: 0.7751\n",
      "Epoch 191/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.7636\n",
      "Epoch 191: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5559 - acc: 0.7635 - val_loss: 0.5449 - val_acc: 0.7717\n",
      "Epoch 192/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.7563\n",
      "Epoch 192: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5603 - acc: 0.7563 - val_loss: 0.5294 - val_acc: 0.7721\n",
      "Epoch 193/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.7660\n",
      "Epoch 193: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5188 - acc: 0.7654 - val_loss: 0.5438 - val_acc: 0.7773\n",
      "Epoch 194/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5539 - acc: 0.7626\n",
      "Epoch 194: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5547 - acc: 0.7622 - val_loss: 0.5602 - val_acc: 0.7700\n",
      "Epoch 195/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5614 - acc: 0.7579\n",
      "Epoch 195: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5614 - acc: 0.7579 - val_loss: 0.5449 - val_acc: 0.7537\n",
      "Epoch 196/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7664\n",
      "Epoch 196: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5435 - acc: 0.7662 - val_loss: 0.5423 - val_acc: 0.7832\n",
      "Epoch 197/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7612\n",
      "Epoch 197: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5517 - acc: 0.7611 - val_loss: 0.5780 - val_acc: 0.7850\n",
      "Epoch 198/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.7629\n",
      "Epoch 198: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5369 - acc: 0.7624 - val_loss: 0.5944 - val_acc: 0.7824\n",
      "Epoch 199/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7586\n",
      "Epoch 199: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5343 - acc: 0.7588 - val_loss: 0.5672 - val_acc: 0.7798\n",
      "Epoch 200/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7627\n",
      "Epoch 200: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5507 - acc: 0.7627 - val_loss: 0.5632 - val_acc: 0.7743\n",
      "Epoch 201/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5648 - acc: 0.7618\n",
      "Epoch 201: val_acc improved from 0.78923 to 0.79179, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5648 - acc: 0.7618 - val_loss: 0.5428 - val_acc: 0.7918\n",
      "Epoch 202/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.7668\n",
      "Epoch 202: val_acc improved from 0.79179 to 0.79265, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5512 - acc: 0.7665 - val_loss: 0.5511 - val_acc: 0.7926\n",
      "Epoch 203/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.7610\n",
      "Epoch 203: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5564 - acc: 0.7610 - val_loss: 0.5718 - val_acc: 0.7828\n",
      "Epoch 204/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7640\n",
      "Epoch 204: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5350 - acc: 0.7638 - val_loss: 0.5487 - val_acc: 0.7657\n",
      "Epoch 205/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7589\n",
      "Epoch 205: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5661 - acc: 0.7588 - val_loss: 0.5490 - val_acc: 0.7533\n",
      "Epoch 206/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.7569\n",
      "Epoch 206: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5397 - acc: 0.7566 - val_loss: 0.5243 - val_acc: 0.7905\n",
      "Epoch 207/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7696\n",
      "Epoch 207: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5397 - acc: 0.7693 - val_loss: 0.5378 - val_acc: 0.7845\n",
      "Epoch 208/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.7615\n",
      "Epoch 208: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5466 - acc: 0.7614 - val_loss: 0.5663 - val_acc: 0.7837\n",
      "Epoch 209/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7585\n",
      "Epoch 209: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5500 - acc: 0.7586 - val_loss: 0.5661 - val_acc: 0.7892\n",
      "Epoch 210/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.7587\n",
      "Epoch 210: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5567 - acc: 0.7581 - val_loss: 0.5564 - val_acc: 0.7798\n",
      "Epoch 211/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7650\n",
      "Epoch 211: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5466 - acc: 0.7642 - val_loss: 0.5825 - val_acc: 0.7850\n",
      "Epoch 212/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7603\n",
      "Epoch 212: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5449 - acc: 0.7602 - val_loss: 0.5617 - val_acc: 0.7837\n",
      "Epoch 213/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.7753\n",
      "Epoch 213: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5243 - acc: 0.7753 - val_loss: 0.5816 - val_acc: 0.7837\n",
      "Epoch 214/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.7680\n",
      "Epoch 214: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5644 - acc: 0.7675 - val_loss: 0.5634 - val_acc: 0.7743\n",
      "Epoch 215/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7592\n",
      "Epoch 215: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5502 - acc: 0.7590 - val_loss: 0.5482 - val_acc: 0.7815\n",
      "Epoch 216/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5541 - acc: 0.7655\n",
      "Epoch 216: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5541 - acc: 0.7655 - val_loss: 0.6331 - val_acc: 0.7755\n",
      "Epoch 217/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.7679\n",
      "Epoch 217: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5398 - acc: 0.7666 - val_loss: 0.5370 - val_acc: 0.7777\n",
      "Epoch 218/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.7635\n",
      "Epoch 218: val_acc did not improve from 0.79265\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5334 - acc: 0.7630 - val_loss: 0.5573 - val_acc: 0.7824\n",
      "Epoch 219/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5666 - acc: 0.7655\n",
      "Epoch 219: val_acc improved from 0.79265 to 0.79350, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5666 - acc: 0.7655 - val_loss: 0.5494 - val_acc: 0.7935\n",
      "Epoch 220/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.7624\n",
      "Epoch 220: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5518 - acc: 0.7625 - val_loss: 0.5748 - val_acc: 0.7631\n",
      "Epoch 221/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7651\n",
      "Epoch 221: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5360 - acc: 0.7650 - val_loss: 0.5389 - val_acc: 0.7721\n",
      "Epoch 222/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7667\n",
      "Epoch 222: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.7669 - val_loss: 0.5479 - val_acc: 0.7837\n",
      "Epoch 223/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7609\n",
      "Epoch 223: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5567 - acc: 0.7608 - val_loss: 0.5640 - val_acc: 0.7794\n",
      "Epoch 224/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7616\n",
      "Epoch 224: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5524 - acc: 0.7613 - val_loss: 0.5438 - val_acc: 0.7914\n",
      "Epoch 225/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5615 - acc: 0.7639\n",
      "Epoch 225: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5615 - acc: 0.7639 - val_loss: 0.5537 - val_acc: 0.7666\n",
      "Epoch 226/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7629\n",
      "Epoch 226: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5455 - acc: 0.7619 - val_loss: 0.5357 - val_acc: 0.7815\n",
      "Epoch 227/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.7652\n",
      "Epoch 227: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5453 - acc: 0.7661 - val_loss: 0.5320 - val_acc: 0.7858\n",
      "Epoch 228/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.7678\n",
      "Epoch 228: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5401 - acc: 0.7677 - val_loss: 0.5673 - val_acc: 0.7790\n",
      "Epoch 229/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7664\n",
      "Epoch 229: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5446 - acc: 0.7664 - val_loss: 0.5726 - val_acc: 0.7854\n",
      "Epoch 230/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5380 - acc: 0.7680\n",
      "Epoch 230: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5380 - acc: 0.7680 - val_loss: 0.5852 - val_acc: 0.7824\n",
      "Epoch 231/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.7666\n",
      "Epoch 231: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5585 - acc: 0.7666 - val_loss: 0.5573 - val_acc: 0.7730\n",
      "Epoch 232/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5716 - acc: 0.7663\n",
      "Epoch 232: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5698 - acc: 0.7660 - val_loss: 0.5919 - val_acc: 0.7807\n",
      "Epoch 233/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.7603\n",
      "Epoch 233: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5473 - acc: 0.7598 - val_loss: 0.5587 - val_acc: 0.7730\n",
      "Epoch 234/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7640\n",
      "Epoch 234: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5512 - acc: 0.7642 - val_loss: 0.5633 - val_acc: 0.7888\n",
      "Epoch 235/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7646\n",
      "Epoch 235: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5269 - acc: 0.7649 - val_loss: 0.5378 - val_acc: 0.7815\n",
      "Epoch 236/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7641\n",
      "Epoch 236: val_acc did not improve from 0.79350\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5358 - acc: 0.7640 - val_loss: 0.5420 - val_acc: 0.7726\n",
      "Epoch 237/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.7694\n",
      "Epoch 237: val_acc improved from 0.79350 to 0.79564, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/4/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5605 - acc: 0.7693 - val_loss: 0.5681 - val_acc: 0.7956\n",
      "Epoch 238/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5550 - acc: 0.7654\n",
      "Epoch 238: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5550 - acc: 0.7654 - val_loss: 0.5553 - val_acc: 0.7897\n",
      "Epoch 239/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5647 - acc: 0.7598\n",
      "Epoch 239: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5639 - acc: 0.7599 - val_loss: 0.5433 - val_acc: 0.7820\n",
      "Epoch 240/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7662\n",
      "Epoch 240: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5407 - acc: 0.7664 - val_loss: 0.5503 - val_acc: 0.7909\n",
      "Epoch 241/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5399 - acc: 0.7644\n",
      "Epoch 241: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5399 - acc: 0.7644 - val_loss: 0.5674 - val_acc: 0.7730\n",
      "Epoch 242/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7648\n",
      "Epoch 242: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5349 - acc: 0.7650 - val_loss: 0.5813 - val_acc: 0.7670\n",
      "Epoch 243/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7669\n",
      "Epoch 243: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5506 - acc: 0.7664 - val_loss: 0.5391 - val_acc: 0.7781\n",
      "Epoch 244/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5354 - acc: 0.7702\n",
      "Epoch 244: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5354 - acc: 0.7702 - val_loss: 0.5752 - val_acc: 0.7892\n",
      "Epoch 245/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7675\n",
      "Epoch 245: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5271 - acc: 0.7672 - val_loss: 0.5723 - val_acc: 0.7785\n",
      "Epoch 246/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7692\n",
      "Epoch 246: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5330 - acc: 0.7691 - val_loss: 0.5547 - val_acc: 0.7640\n",
      "Epoch 247/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.7707\n",
      "Epoch 247: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5398 - acc: 0.7710 - val_loss: 0.5645 - val_acc: 0.7897\n",
      "Epoch 248/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5248 - acc: 0.7690\n",
      "Epoch 248: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5248 - acc: 0.7690 - val_loss: 0.5588 - val_acc: 0.7820\n",
      "Epoch 249/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7717\n",
      "Epoch 249: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5290 - acc: 0.7714 - val_loss: 0.5849 - val_acc: 0.7845\n",
      "Epoch 250/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.7678\n",
      "Epoch 250: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5600 - acc: 0.7670 - val_loss: 0.5478 - val_acc: 0.7820\n",
      "Epoch 251/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7677\n",
      "Epoch 251: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7676 - val_loss: 0.5670 - val_acc: 0.7828\n",
      "Epoch 252/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.7671\n",
      "Epoch 252: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5405 - acc: 0.7670 - val_loss: 0.5509 - val_acc: 0.7798\n",
      "Epoch 253/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7639\n",
      "Epoch 253: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5193 - acc: 0.7638 - val_loss: 0.5699 - val_acc: 0.7828\n",
      "Epoch 254/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.7658\n",
      "Epoch 254: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5592 - acc: 0.7658 - val_loss: 0.5837 - val_acc: 0.7862\n",
      "Epoch 255/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7672\n",
      "Epoch 255: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5503 - acc: 0.7677 - val_loss: 0.5750 - val_acc: 0.7944\n",
      "Epoch 256/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.7696\n",
      "Epoch 256: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5594 - acc: 0.7689 - val_loss: 0.6036 - val_acc: 0.7781\n",
      "Epoch 257/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7727\n",
      "Epoch 257: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5228 - acc: 0.7722 - val_loss: 0.5630 - val_acc: 0.7854\n",
      "Epoch 258/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7629\n",
      "Epoch 258: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5360 - acc: 0.7632 - val_loss: 0.5791 - val_acc: 0.7867\n",
      "Epoch 259/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5925 - acc: 0.7689\n",
      "Epoch 259: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5924 - acc: 0.7689 - val_loss: 0.5699 - val_acc: 0.7764\n",
      "Epoch 260/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.7696\n",
      "Epoch 260: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5273 - acc: 0.7697 - val_loss: 0.5732 - val_acc: 0.7807\n",
      "Epoch 261/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7644\n",
      "Epoch 261: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5461 - acc: 0.7643 - val_loss: 0.5737 - val_acc: 0.7738\n",
      "Epoch 262/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5521 - acc: 0.7607\n",
      "Epoch 262: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5518 - acc: 0.7607 - val_loss: 0.5633 - val_acc: 0.7901\n",
      "Epoch 263/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.7620\n",
      "Epoch 263: val_acc did not improve from 0.79564\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5553 - acc: 0.7626 - val_loss: 0.5893 - val_acc: 0.7790\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 1.9146 - acc: 0.6173\n",
      "Epoch 1: val_acc improved from -inf to 0.69418, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 3s 7ms/step - loss: 1.8924 - acc: 0.6180 - val_loss: 0.6220 - val_acc: 0.6942\n",
      "Epoch 2/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.6678\n",
      "Epoch 2: val_acc improved from 0.69418 to 0.70488, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6890 - acc: 0.6673 - val_loss: 0.6311 - val_acc: 0.7049\n",
      "Epoch 3/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6630 - acc: 0.6767\n",
      "Epoch 3: val_acc did not improve from 0.70488\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6632 - acc: 0.6767 - val_loss: 0.6200 - val_acc: 0.7010\n",
      "Epoch 4/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.6897\n",
      "Epoch 4: val_acc improved from 0.70488 to 0.71129, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6508 - acc: 0.6896 - val_loss: 0.6164 - val_acc: 0.7113\n",
      "Epoch 5/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6385 - acc: 0.6943\n",
      "Epoch 5: val_acc improved from 0.71129 to 0.71771, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6383 - acc: 0.6941 - val_loss: 0.6158 - val_acc: 0.7177\n",
      "Epoch 6/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6260 - acc: 0.6985\n",
      "Epoch 6: val_acc did not improve from 0.71771\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6260 - acc: 0.6983 - val_loss: 0.6044 - val_acc: 0.7173\n",
      "Epoch 7/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6211 - acc: 0.7051\n",
      "Epoch 7: val_acc improved from 0.71771 to 0.71814, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6209 - acc: 0.7045 - val_loss: 0.6041 - val_acc: 0.7181\n",
      "Epoch 8/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6101 - acc: 0.7073\n",
      "Epoch 8: val_acc improved from 0.71814 to 0.72284, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6102 - acc: 0.7069 - val_loss: 0.5940 - val_acc: 0.7228\n",
      "Epoch 9/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.7074\n",
      "Epoch 9: val_acc did not improve from 0.72284\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6143 - acc: 0.7075 - val_loss: 0.5678 - val_acc: 0.7147\n",
      "Epoch 10/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.7086\n",
      "Epoch 10: val_acc did not improve from 0.72284\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5968 - acc: 0.7077 - val_loss: 0.5984 - val_acc: 0.7139\n",
      "Epoch 11/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.7120\n",
      "Epoch 11: val_acc did not improve from 0.72284\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5939 - acc: 0.7118 - val_loss: 0.5942 - val_acc: 0.7181\n",
      "Epoch 12/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5977 - acc: 0.7124\n",
      "Epoch 12: val_acc did not improve from 0.72284\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5976 - acc: 0.7125 - val_loss: 0.5795 - val_acc: 0.7207\n",
      "Epoch 13/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5940 - acc: 0.7139\n",
      "Epoch 13: val_acc improved from 0.72284 to 0.72669, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5942 - acc: 0.7138 - val_loss: 0.5626 - val_acc: 0.7267\n",
      "Epoch 14/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5885 - acc: 0.7162\n",
      "Epoch 14: val_acc did not improve from 0.72669\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5891 - acc: 0.7160 - val_loss: 0.5763 - val_acc: 0.7228\n",
      "Epoch 15/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.7158\n",
      "Epoch 15: val_acc improved from 0.72669 to 0.72712, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5819 - acc: 0.7157 - val_loss: 0.5838 - val_acc: 0.7271\n",
      "Epoch 16/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.7198\n",
      "Epoch 16: val_acc did not improve from 0.72712\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5894 - acc: 0.7192 - val_loss: 0.5532 - val_acc: 0.7267\n",
      "Epoch 17/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.7169\n",
      "Epoch 17: val_acc improved from 0.72712 to 0.72754, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5863 - acc: 0.7161 - val_loss: 0.5669 - val_acc: 0.7275\n",
      "Epoch 18/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.7156\n",
      "Epoch 18: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5880 - acc: 0.7150 - val_loss: 0.5701 - val_acc: 0.7241\n",
      "Epoch 19/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5811 - acc: 0.7166\n",
      "Epoch 19: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5820 - acc: 0.7163 - val_loss: 0.5833 - val_acc: 0.7220\n",
      "Epoch 20/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.7218\n",
      "Epoch 20: val_acc improved from 0.72754 to 0.72883, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5860 - acc: 0.7206 - val_loss: 0.5742 - val_acc: 0.7288\n",
      "Epoch 21/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5828 - acc: 0.7179\n",
      "Epoch 21: val_acc did not improve from 0.72883\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5828 - acc: 0.7179 - val_loss: 0.5607 - val_acc: 0.7194\n",
      "Epoch 22/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5808 - acc: 0.7189\n",
      "Epoch 22: val_acc did not improve from 0.72883\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5808 - acc: 0.7189 - val_loss: 0.5617 - val_acc: 0.7271\n",
      "Epoch 23/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5803 - acc: 0.7199\n",
      "Epoch 23: val_acc did not improve from 0.72883\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5822 - acc: 0.7193 - val_loss: 0.5700 - val_acc: 0.7216\n",
      "Epoch 24/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.7226\n",
      "Epoch 24: val_acc improved from 0.72883 to 0.73567, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5796 - acc: 0.7220 - val_loss: 0.5594 - val_acc: 0.7357\n",
      "Epoch 25/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7253\n",
      "Epoch 25: val_acc did not improve from 0.73567\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5701 - acc: 0.7243 - val_loss: 0.5438 - val_acc: 0.7224\n",
      "Epoch 26/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.7234\n",
      "Epoch 26: val_acc did not improve from 0.73567\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5746 - acc: 0.7232 - val_loss: 0.5528 - val_acc: 0.7322\n",
      "Epoch 27/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.7261\n",
      "Epoch 27: val_acc improved from 0.73567 to 0.73952, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5790 - acc: 0.7250 - val_loss: 0.5512 - val_acc: 0.7395\n",
      "Epoch 28/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.7236\n",
      "Epoch 28: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5767 - acc: 0.7224 - val_loss: 0.5555 - val_acc: 0.7211\n",
      "Epoch 29/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.7245\n",
      "Epoch 29: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5775 - acc: 0.7241 - val_loss: 0.5462 - val_acc: 0.7335\n",
      "Epoch 30/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.7249\n",
      "Epoch 30: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5727 - acc: 0.7247 - val_loss: 0.5510 - val_acc: 0.7370\n",
      "Epoch 31/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.7273\n",
      "Epoch 31: val_acc did not improve from 0.73952\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5736 - acc: 0.7272 - val_loss: 0.5458 - val_acc: 0.7246\n",
      "Epoch 32/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.7297\n",
      "Epoch 32: val_acc improved from 0.73952 to 0.74850, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5647 - acc: 0.7293 - val_loss: 0.5611 - val_acc: 0.7485\n",
      "Epoch 33/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.7296\n",
      "Epoch 33: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5709 - acc: 0.7296 - val_loss: 0.5557 - val_acc: 0.7271\n",
      "Epoch 34/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5753 - acc: 0.7291\n",
      "Epoch 34: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5757 - acc: 0.7280 - val_loss: 0.5486 - val_acc: 0.7241\n",
      "Epoch 35/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.7283\n",
      "Epoch 35: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5699 - acc: 0.7282 - val_loss: 0.5437 - val_acc: 0.7387\n",
      "Epoch 36/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.7308\n",
      "Epoch 36: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5578 - acc: 0.7301 - val_loss: 0.5607 - val_acc: 0.7374\n",
      "Epoch 37/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5699 - acc: 0.7268\n",
      "Epoch 37: val_acc did not improve from 0.74850\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5705 - acc: 0.7263 - val_loss: 0.5666 - val_acc: 0.7417\n",
      "Epoch 38/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.7255\n",
      "Epoch 38: val_acc improved from 0.74850 to 0.75235, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5660 - acc: 0.7253 - val_loss: 0.5526 - val_acc: 0.7524\n",
      "Epoch 39/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.7306\n",
      "Epoch 39: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5667 - acc: 0.7303 - val_loss: 0.5436 - val_acc: 0.7322\n",
      "Epoch 40/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.7330\n",
      "Epoch 40: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5635 - acc: 0.7320 - val_loss: 0.5493 - val_acc: 0.7442\n",
      "Epoch 41/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.7358\n",
      "Epoch 41: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5578 - acc: 0.7353 - val_loss: 0.5533 - val_acc: 0.7447\n",
      "Epoch 42/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.7339\n",
      "Epoch 42: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5628 - acc: 0.7329 - val_loss: 0.5524 - val_acc: 0.7438\n",
      "Epoch 43/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.7331\n",
      "Epoch 43: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5637 - acc: 0.7331 - val_loss: 0.5424 - val_acc: 0.7519\n",
      "Epoch 44/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.7340\n",
      "Epoch 44: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5583 - acc: 0.7328 - val_loss: 0.5552 - val_acc: 0.7335\n",
      "Epoch 45/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.7358\n",
      "Epoch 45: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5556 - acc: 0.7346 - val_loss: 0.5562 - val_acc: 0.7335\n",
      "Epoch 46/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.7330\n",
      "Epoch 46: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5651 - acc: 0.7320 - val_loss: 0.5484 - val_acc: 0.7288\n",
      "Epoch 47/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.7364\n",
      "Epoch 47: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5681 - acc: 0.7361 - val_loss: 0.5470 - val_acc: 0.7361\n",
      "Epoch 48/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.7409\n",
      "Epoch 48: val_acc did not improve from 0.75235\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5546 - acc: 0.7395 - val_loss: 0.5422 - val_acc: 0.7370\n",
      "Epoch 49/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.7359\n",
      "Epoch 49: val_acc improved from 0.75235 to 0.76518, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5563 - acc: 0.7360 - val_loss: 0.5507 - val_acc: 0.7652\n",
      "Epoch 50/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.7382\n",
      "Epoch 50: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5563 - acc: 0.7373 - val_loss: 0.5545 - val_acc: 0.7451\n",
      "Epoch 51/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5711 - acc: 0.7349\n",
      "Epoch 51: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5710 - acc: 0.7349 - val_loss: 0.5432 - val_acc: 0.7322\n",
      "Epoch 52/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7375\n",
      "Epoch 52: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5465 - acc: 0.7369 - val_loss: 0.5459 - val_acc: 0.7511\n",
      "Epoch 53/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5586 - acc: 0.7395\n",
      "Epoch 53: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5585 - acc: 0.7393 - val_loss: 0.5403 - val_acc: 0.7434\n",
      "Epoch 54/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7383\n",
      "Epoch 54: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5519 - acc: 0.7379 - val_loss: 0.5599 - val_acc: 0.7382\n",
      "Epoch 55/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.7377\n",
      "Epoch 55: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5559 - acc: 0.7375 - val_loss: 0.5514 - val_acc: 0.7511\n",
      "Epoch 56/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5516 - acc: 0.7394\n",
      "Epoch 56: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5516 - acc: 0.7394 - val_loss: 0.5468 - val_acc: 0.7498\n",
      "Epoch 57/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5539 - acc: 0.7408\n",
      "Epoch 57: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5539 - acc: 0.7408 - val_loss: 0.5409 - val_acc: 0.7429\n",
      "Epoch 58/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7402\n",
      "Epoch 58: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5550 - acc: 0.7402 - val_loss: 0.5404 - val_acc: 0.7498\n",
      "Epoch 59/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.7379\n",
      "Epoch 59: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5555 - acc: 0.7378 - val_loss: 0.5433 - val_acc: 0.7425\n",
      "Epoch 60/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7449\n",
      "Epoch 60: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5519 - acc: 0.7437 - val_loss: 0.5543 - val_acc: 0.7455\n",
      "Epoch 61/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.7463\n",
      "Epoch 61: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5547 - acc: 0.7453 - val_loss: 0.5443 - val_acc: 0.7476\n",
      "Epoch 62/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5544 - acc: 0.7455\n",
      "Epoch 62: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5544 - acc: 0.7455 - val_loss: 0.5400 - val_acc: 0.7451\n",
      "Epoch 63/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.7472\n",
      "Epoch 63: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5454 - acc: 0.7463 - val_loss: 0.5421 - val_acc: 0.7575\n",
      "Epoch 64/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7425\n",
      "Epoch 64: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5508 - acc: 0.7419 - val_loss: 0.5399 - val_acc: 0.7613\n",
      "Epoch 65/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.7435\n",
      "Epoch 65: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5639 - acc: 0.7425 - val_loss: 0.5506 - val_acc: 0.7476\n",
      "Epoch 66/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7450\n",
      "Epoch 66: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5444 - acc: 0.7448 - val_loss: 0.5606 - val_acc: 0.7541\n",
      "Epoch 67/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.7450\n",
      "Epoch 67: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5578 - acc: 0.7437 - val_loss: 0.5537 - val_acc: 0.7583\n",
      "Epoch 68/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5500 - acc: 0.7463\n",
      "Epoch 68: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5500 - acc: 0.7463 - val_loss: 0.5584 - val_acc: 0.7429\n",
      "Epoch 69/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7512\n",
      "Epoch 69: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5424 - acc: 0.7509 - val_loss: 0.5534 - val_acc: 0.7652\n",
      "Epoch 70/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.7479\n",
      "Epoch 70: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5471 - acc: 0.7479 - val_loss: 0.5571 - val_acc: 0.7417\n",
      "Epoch 71/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.7476\n",
      "Epoch 71: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5490 - acc: 0.7473 - val_loss: 0.5499 - val_acc: 0.7489\n",
      "Epoch 72/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.7481\n",
      "Epoch 72: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5484 - acc: 0.7471 - val_loss: 0.5474 - val_acc: 0.7558\n",
      "Epoch 73/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.7478\n",
      "Epoch 73: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5537 - acc: 0.7468 - val_loss: 0.5422 - val_acc: 0.7579\n",
      "Epoch 74/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7486\n",
      "Epoch 74: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5391 - acc: 0.7479 - val_loss: 0.5454 - val_acc: 0.7468\n",
      "Epoch 75/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.7503\n",
      "Epoch 75: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5450 - acc: 0.7497 - val_loss: 0.5454 - val_acc: 0.7536\n",
      "Epoch 76/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7500\n",
      "Epoch 76: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5456 - acc: 0.7496 - val_loss: 0.5415 - val_acc: 0.7588\n",
      "Epoch 77/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.7481\n",
      "Epoch 77: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5493 - acc: 0.7472 - val_loss: 0.5442 - val_acc: 0.7468\n",
      "Epoch 78/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.7493\n",
      "Epoch 78: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5420 - acc: 0.7481 - val_loss: 0.5344 - val_acc: 0.7592\n",
      "Epoch 79/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7501\n",
      "Epoch 79: val_acc did not improve from 0.76518\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5374 - acc: 0.7488 - val_loss: 0.5458 - val_acc: 0.7639\n",
      "Epoch 80/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7488\n",
      "Epoch 80: val_acc improved from 0.76518 to 0.76732, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7488 - val_loss: 0.5543 - val_acc: 0.7673\n",
      "Epoch 81/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.7485\n",
      "Epoch 81: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5655 - acc: 0.7478 - val_loss: 0.5519 - val_acc: 0.7528\n",
      "Epoch 82/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.7459\n",
      "Epoch 82: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5551 - acc: 0.7455 - val_loss: 0.5553 - val_acc: 0.7476\n",
      "Epoch 83/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5367 - acc: 0.7490\n",
      "Epoch 83: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5367 - acc: 0.7490 - val_loss: 0.5551 - val_acc: 0.7613\n",
      "Epoch 84/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.7486\n",
      "Epoch 84: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5483 - acc: 0.7476 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 85/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7520\n",
      "Epoch 85: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5482 - acc: 0.7521 - val_loss: 0.5381 - val_acc: 0.7665\n",
      "Epoch 86/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.7481\n",
      "Epoch 86: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5601 - acc: 0.7481 - val_loss: 0.5375 - val_acc: 0.7609\n",
      "Epoch 87/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7566\n",
      "Epoch 87: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5370 - acc: 0.7551 - val_loss: 0.5551 - val_acc: 0.7566\n",
      "Epoch 88/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7558\n",
      "Epoch 88: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5323 - acc: 0.7545 - val_loss: 0.5556 - val_acc: 0.7571\n",
      "Epoch 89/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7572\n",
      "Epoch 89: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5368 - acc: 0.7560 - val_loss: 0.5414 - val_acc: 0.7583\n",
      "Epoch 90/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7554\n",
      "Epoch 90: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5440 - acc: 0.7544 - val_loss: 0.5572 - val_acc: 0.7553\n",
      "Epoch 91/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5499 - acc: 0.7529\n",
      "Epoch 91: val_acc did not improve from 0.76732\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5499 - acc: 0.7529 - val_loss: 0.5612 - val_acc: 0.7421\n",
      "Epoch 92/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.7510\n",
      "Epoch 92: val_acc improved from 0.76732 to 0.77203, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5414 - acc: 0.7505 - val_loss: 0.5459 - val_acc: 0.7720\n",
      "Epoch 93/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.7557\n",
      "Epoch 93: val_acc improved from 0.77203 to 0.77759, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5540 - acc: 0.7553 - val_loss: 0.5529 - val_acc: 0.7776\n",
      "Epoch 94/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7561\n",
      "Epoch 94: val_acc did not improve from 0.77759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5396 - acc: 0.7556 - val_loss: 0.5581 - val_acc: 0.7737\n",
      "Epoch 95/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.7518\n",
      "Epoch 95: val_acc did not improve from 0.77759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5468 - acc: 0.7515 - val_loss: 0.5649 - val_acc: 0.7703\n",
      "Epoch 96/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7585\n",
      "Epoch 96: val_acc did not improve from 0.77759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5395 - acc: 0.7578 - val_loss: 0.5592 - val_acc: 0.7776\n",
      "Epoch 97/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7481\n",
      "Epoch 97: val_acc did not improve from 0.77759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5454 - acc: 0.7479 - val_loss: 0.5495 - val_acc: 0.7613\n",
      "Epoch 98/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7526\n",
      "Epoch 98: val_acc did not improve from 0.77759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5505 - acc: 0.7516 - val_loss: 0.5694 - val_acc: 0.7613\n",
      "Epoch 99/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7581\n",
      "Epoch 99: val_acc did not improve from 0.77759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5316 - acc: 0.7573 - val_loss: 0.5555 - val_acc: 0.7707\n",
      "Epoch 100/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.7548\n",
      "Epoch 100: val_acc did not improve from 0.77759\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5440 - acc: 0.7535 - val_loss: 0.5514 - val_acc: 0.7528\n",
      "Epoch 101/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5477 - acc: 0.7567\n",
      "Epoch 101: val_acc improved from 0.77759 to 0.78058, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5477 - acc: 0.7567 - val_loss: 0.5493 - val_acc: 0.7806\n",
      "Epoch 102/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.7527\n",
      "Epoch 102: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5484 - acc: 0.7519 - val_loss: 0.5565 - val_acc: 0.7575\n",
      "Epoch 103/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5480 - acc: 0.7546\n",
      "Epoch 103: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5480 - acc: 0.7546 - val_loss: 0.5725 - val_acc: 0.7669\n",
      "Epoch 104/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7535\n",
      "Epoch 104: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5458 - acc: 0.7519 - val_loss: 0.5637 - val_acc: 0.7566\n",
      "Epoch 105/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.7504\n",
      "Epoch 105: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5481 - acc: 0.7505 - val_loss: 0.5862 - val_acc: 0.7690\n",
      "Epoch 106/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7515\n",
      "Epoch 106: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5395 - acc: 0.7504 - val_loss: 0.5534 - val_acc: 0.7673\n",
      "Epoch 107/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.7549\n",
      "Epoch 107: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5497 - acc: 0.7550 - val_loss: 0.5650 - val_acc: 0.7759\n",
      "Epoch 108/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5424 - acc: 0.7574\n",
      "Epoch 108: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5424 - acc: 0.7574 - val_loss: 0.5577 - val_acc: 0.7583\n",
      "Epoch 109/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7539\n",
      "Epoch 109: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5369 - acc: 0.7532 - val_loss: 0.5663 - val_acc: 0.7601\n",
      "Epoch 110/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.7558\n",
      "Epoch 110: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5620 - acc: 0.7551 - val_loss: 0.5788 - val_acc: 0.7408\n",
      "Epoch 111/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7520\n",
      "Epoch 111: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5450 - acc: 0.7520 - val_loss: 0.5600 - val_acc: 0.7485\n",
      "Epoch 112/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.7560\n",
      "Epoch 112: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5439 - acc: 0.7555 - val_loss: 0.5592 - val_acc: 0.7665\n",
      "Epoch 113/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7587\n",
      "Epoch 113: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5457 - acc: 0.7572 - val_loss: 0.5512 - val_acc: 0.7725\n",
      "Epoch 114/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7526\n",
      "Epoch 114: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5365 - acc: 0.7519 - val_loss: 0.5672 - val_acc: 0.7566\n",
      "Epoch 115/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7557\n",
      "Epoch 115: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5288 - acc: 0.7547 - val_loss: 0.5905 - val_acc: 0.7639\n",
      "Epoch 116/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7575\n",
      "Epoch 116: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5448 - acc: 0.7562 - val_loss: 0.5530 - val_acc: 0.7601\n",
      "Epoch 117/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.7565\n",
      "Epoch 117: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5545 - acc: 0.7564 - val_loss: 0.5570 - val_acc: 0.7549\n",
      "Epoch 118/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7598\n",
      "Epoch 118: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5418 - acc: 0.7579 - val_loss: 0.5471 - val_acc: 0.7566\n",
      "Epoch 119/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7564\n",
      "Epoch 119: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5429 - acc: 0.7557 - val_loss: 0.5587 - val_acc: 0.7716\n",
      "Epoch 120/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7607\n",
      "Epoch 120: val_acc did not improve from 0.78058\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5336 - acc: 0.7591 - val_loss: 0.5697 - val_acc: 0.7532\n",
      "Epoch 121/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7523\n",
      "Epoch 121: val_acc improved from 0.78058 to 0.78400, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5421 - acc: 0.7517 - val_loss: 0.5556 - val_acc: 0.7840\n",
      "Epoch 122/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7562\n",
      "Epoch 122: val_acc improved from 0.78400 to 0.78956, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5361 - acc: 0.7558 - val_loss: 0.5861 - val_acc: 0.7896\n",
      "Epoch 123/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.7590\n",
      "Epoch 123: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5505 - acc: 0.7584 - val_loss: 0.5728 - val_acc: 0.7690\n",
      "Epoch 124/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7618\n",
      "Epoch 124: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5289 - acc: 0.7618 - val_loss: 0.5701 - val_acc: 0.7742\n",
      "Epoch 125/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7611\n",
      "Epoch 125: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5404 - acc: 0.7602 - val_loss: 0.5462 - val_acc: 0.7819\n",
      "Epoch 126/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7598\n",
      "Epoch 126: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5310 - acc: 0.7583 - val_loss: 0.5827 - val_acc: 0.7447\n",
      "Epoch 127/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5758 - acc: 0.7543\n",
      "Epoch 127: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5760 - acc: 0.7531 - val_loss: 0.5951 - val_acc: 0.7571\n",
      "Epoch 128/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7594\n",
      "Epoch 128: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5433 - acc: 0.7583 - val_loss: 0.5751 - val_acc: 0.7592\n",
      "Epoch 129/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7589\n",
      "Epoch 129: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5341 - acc: 0.7589 - val_loss: 0.6003 - val_acc: 0.7866\n",
      "Epoch 130/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7578\n",
      "Epoch 130: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5518 - acc: 0.7567 - val_loss: 0.5484 - val_acc: 0.7643\n",
      "Epoch 131/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7561\n",
      "Epoch 131: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5369 - acc: 0.7561 - val_loss: 0.5718 - val_acc: 0.7879\n",
      "Epoch 132/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.7599\n",
      "Epoch 132: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5525 - acc: 0.7590 - val_loss: 0.5647 - val_acc: 0.7635\n",
      "Epoch 133/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7607\n",
      "Epoch 133: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5314 - acc: 0.7603 - val_loss: 0.5453 - val_acc: 0.7806\n",
      "Epoch 134/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7575\n",
      "Epoch 134: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5471 - acc: 0.7559 - val_loss: 0.5547 - val_acc: 0.7605\n",
      "Epoch 135/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7567\n",
      "Epoch 135: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5374 - acc: 0.7562 - val_loss: 0.5927 - val_acc: 0.7682\n",
      "Epoch 136/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.7539\n",
      "Epoch 136: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5508 - acc: 0.7527 - val_loss: 0.5891 - val_acc: 0.7588\n",
      "Epoch 137/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.7601\n",
      "Epoch 137: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5368 - acc: 0.7591 - val_loss: 0.5988 - val_acc: 0.7643\n",
      "Epoch 138/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.7588\n",
      "Epoch 138: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5484 - acc: 0.7584 - val_loss: 0.5965 - val_acc: 0.7763\n",
      "Epoch 139/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.7598\n",
      "Epoch 139: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5407 - acc: 0.7593 - val_loss: 0.5929 - val_acc: 0.7870\n",
      "Epoch 140/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5434 - acc: 0.7567\n",
      "Epoch 140: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5434 - acc: 0.7567 - val_loss: 0.5789 - val_acc: 0.7729\n",
      "Epoch 141/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7590\n",
      "Epoch 141: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5279 - acc: 0.7586 - val_loss: 0.6009 - val_acc: 0.7814\n",
      "Epoch 142/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7591\n",
      "Epoch 142: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5394 - acc: 0.7579 - val_loss: 0.5788 - val_acc: 0.7690\n",
      "Epoch 143/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7625\n",
      "Epoch 143: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5377 - acc: 0.7606 - val_loss: 0.5438 - val_acc: 0.7780\n",
      "Epoch 144/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7596\n",
      "Epoch 144: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5319 - acc: 0.7586 - val_loss: 0.5576 - val_acc: 0.7720\n",
      "Epoch 145/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7625\n",
      "Epoch 145: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5403 - acc: 0.7620 - val_loss: 0.5731 - val_acc: 0.7720\n",
      "Epoch 146/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7599\n",
      "Epoch 146: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5528 - acc: 0.7598 - val_loss: 0.5521 - val_acc: 0.7682\n",
      "Epoch 147/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7580\n",
      "Epoch 147: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5366 - acc: 0.7577 - val_loss: 0.5604 - val_acc: 0.7853\n",
      "Epoch 148/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7610\n",
      "Epoch 148: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5437 - acc: 0.7608 - val_loss: 0.5966 - val_acc: 0.7524\n",
      "Epoch 149/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7621\n",
      "Epoch 149: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5421 - acc: 0.7612 - val_loss: 0.5756 - val_acc: 0.7515\n",
      "Epoch 150/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7638\n",
      "Epoch 150: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7637 - val_loss: 0.5869 - val_acc: 0.7669\n",
      "Epoch 151/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.7636\n",
      "Epoch 151: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5389 - acc: 0.7623 - val_loss: 0.6071 - val_acc: 0.7699\n",
      "Epoch 152/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.7647\n",
      "Epoch 152: val_acc did not improve from 0.78956\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5392 - acc: 0.7643 - val_loss: 0.5930 - val_acc: 0.7802\n",
      "Epoch 153/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.7612\n",
      "Epoch 153: val_acc improved from 0.78956 to 0.78999, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5362 - acc: 0.7604 - val_loss: 0.6002 - val_acc: 0.7900\n",
      "Epoch 154/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7681\n",
      "Epoch 154: val_acc did not improve from 0.78999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5375 - acc: 0.7670 - val_loss: 0.5660 - val_acc: 0.7823\n",
      "Epoch 155/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.7634\n",
      "Epoch 155: val_acc did not improve from 0.78999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5248 - acc: 0.7634 - val_loss: 0.6055 - val_acc: 0.7669\n",
      "Epoch 156/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7654\n",
      "Epoch 156: val_acc did not improve from 0.78999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5424 - acc: 0.7654 - val_loss: 0.5792 - val_acc: 0.7601\n",
      "Epoch 157/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7644\n",
      "Epoch 157: val_acc did not improve from 0.78999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5442 - acc: 0.7635 - val_loss: 0.6188 - val_acc: 0.7802\n",
      "Epoch 158/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7613\n",
      "Epoch 158: val_acc did not improve from 0.78999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5276 - acc: 0.7602 - val_loss: 0.5723 - val_acc: 0.7802\n",
      "Epoch 159/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7639\n",
      "Epoch 159: val_acc did not improve from 0.78999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5422 - acc: 0.7635 - val_loss: 0.5689 - val_acc: 0.7836\n",
      "Epoch 160/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.7615\n",
      "Epoch 160: val_acc did not improve from 0.78999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5282 - acc: 0.7611 - val_loss: 0.5615 - val_acc: 0.7665\n",
      "Epoch 161/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7667\n",
      "Epoch 161: val_acc improved from 0.78999 to 0.79683, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5527 - acc: 0.7668 - val_loss: 0.6163 - val_acc: 0.7968\n",
      "Epoch 162/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7649\n",
      "Epoch 162: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5300 - acc: 0.7640 - val_loss: 0.6182 - val_acc: 0.7870\n",
      "Epoch 163/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7651\n",
      "Epoch 163: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5405 - acc: 0.7652 - val_loss: 0.6095 - val_acc: 0.7630\n",
      "Epoch 164/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7614\n",
      "Epoch 164: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5275 - acc: 0.7607 - val_loss: 0.5908 - val_acc: 0.7827\n",
      "Epoch 165/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7691\n",
      "Epoch 165: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5225 - acc: 0.7677 - val_loss: 0.5828 - val_acc: 0.7742\n",
      "Epoch 166/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7637\n",
      "Epoch 166: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5354 - acc: 0.7625 - val_loss: 0.5750 - val_acc: 0.7900\n",
      "Epoch 167/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7651\n",
      "Epoch 167: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5311 - acc: 0.7649 - val_loss: 0.6162 - val_acc: 0.7673\n",
      "Epoch 168/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7611\n",
      "Epoch 168: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5432 - acc: 0.7602 - val_loss: 0.5942 - val_acc: 0.7725\n",
      "Epoch 169/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7633\n",
      "Epoch 169: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5406 - acc: 0.7633 - val_loss: 0.6036 - val_acc: 0.7686\n",
      "Epoch 170/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.7685\n",
      "Epoch 170: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5464 - acc: 0.7677 - val_loss: 0.6246 - val_acc: 0.7857\n",
      "Epoch 171/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.7605\n",
      "Epoch 171: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5396 - acc: 0.7596 - val_loss: 0.5907 - val_acc: 0.7789\n",
      "Epoch 172/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7649\n",
      "Epoch 172: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5398 - acc: 0.7640 - val_loss: 0.5835 - val_acc: 0.7819\n",
      "Epoch 173/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.7678\n",
      "Epoch 173: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5182 - acc: 0.7666 - val_loss: 0.5980 - val_acc: 0.7699\n",
      "Epoch 174/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7668\n",
      "Epoch 174: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5282 - acc: 0.7660 - val_loss: 0.6030 - val_acc: 0.7797\n",
      "Epoch 175/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.7688\n",
      "Epoch 175: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5438 - acc: 0.7677 - val_loss: 0.6123 - val_acc: 0.7849\n",
      "Epoch 176/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.7662\n",
      "Epoch 176: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5315 - acc: 0.7650 - val_loss: 0.6183 - val_acc: 0.7780\n",
      "Epoch 177/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.7671\n",
      "Epoch 177: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5275 - acc: 0.7664 - val_loss: 0.6527 - val_acc: 0.7823\n",
      "Epoch 178/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5239 - acc: 0.7652\n",
      "Epoch 178: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5239 - acc: 0.7652 - val_loss: 0.6294 - val_acc: 0.7613\n",
      "Epoch 179/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5443 - acc: 0.7675\n",
      "Epoch 179: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5443 - acc: 0.7675 - val_loss: 0.5834 - val_acc: 0.7601\n",
      "Epoch 180/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7696\n",
      "Epoch 180: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5284 - acc: 0.7683 - val_loss: 0.6050 - val_acc: 0.7840\n",
      "Epoch 181/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7708\n",
      "Epoch 181: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5373 - acc: 0.7707 - val_loss: 0.6129 - val_acc: 0.7750\n",
      "Epoch 182/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.7563\n",
      "Epoch 182: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5564 - acc: 0.7557 - val_loss: 0.6207 - val_acc: 0.7558\n",
      "Epoch 183/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7622\n",
      "Epoch 183: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5428 - acc: 0.7620 - val_loss: 0.6217 - val_acc: 0.7840\n",
      "Epoch 184/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5411 - acc: 0.7652\n",
      "Epoch 184: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5411 - acc: 0.7652 - val_loss: 0.5908 - val_acc: 0.7840\n",
      "Epoch 185/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5390 - acc: 0.7680\n",
      "Epoch 185: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7680 - val_loss: 0.6090 - val_acc: 0.7857\n",
      "Epoch 186/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7658\n",
      "Epoch 186: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5431 - acc: 0.7655 - val_loss: 0.6177 - val_acc: 0.7712\n",
      "Epoch 187/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7603\n",
      "Epoch 187: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5516 - acc: 0.7600 - val_loss: 0.6374 - val_acc: 0.7947\n",
      "Epoch 188/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.7611\n",
      "Epoch 188: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5444 - acc: 0.7611 - val_loss: 0.5996 - val_acc: 0.7797\n",
      "Epoch 189/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.7709\n",
      "Epoch 189: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5358 - acc: 0.7701 - val_loss: 0.6489 - val_acc: 0.7695\n",
      "Epoch 190/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.7636\n",
      "Epoch 190: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5442 - acc: 0.7631 - val_loss: 0.5930 - val_acc: 0.7827\n",
      "Epoch 191/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7685\n",
      "Epoch 191: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5292 - acc: 0.7680 - val_loss: 0.6133 - val_acc: 0.7938\n",
      "Epoch 192/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7657\n",
      "Epoch 192: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5496 - acc: 0.7651 - val_loss: 0.5861 - val_acc: 0.7789\n",
      "Epoch 193/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7685\n",
      "Epoch 193: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5299 - acc: 0.7680 - val_loss: 0.6041 - val_acc: 0.7849\n",
      "Epoch 194/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7702\n",
      "Epoch 194: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5272 - acc: 0.7700 - val_loss: 0.6512 - val_acc: 0.7853\n",
      "Epoch 195/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.7707\n",
      "Epoch 195: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5356 - acc: 0.7702 - val_loss: 0.6403 - val_acc: 0.7630\n",
      "Epoch 196/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5495 - acc: 0.7669\n",
      "Epoch 196: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5495 - acc: 0.7669 - val_loss: 0.5954 - val_acc: 0.7883\n",
      "Epoch 197/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.7669\n",
      "Epoch 197: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5091 - acc: 0.7666 - val_loss: 0.5957 - val_acc: 0.7652\n",
      "Epoch 198/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5290 - acc: 0.7647\n",
      "Epoch 198: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5290 - acc: 0.7647 - val_loss: 0.6602 - val_acc: 0.7840\n",
      "Epoch 199/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5463 - acc: 0.7670\n",
      "Epoch 199: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5463 - acc: 0.7670 - val_loss: 0.6184 - val_acc: 0.7802\n",
      "Epoch 200/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7635\n",
      "Epoch 200: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5321 - acc: 0.7631 - val_loss: 0.5864 - val_acc: 0.7665\n",
      "Epoch 201/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7618\n",
      "Epoch 201: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5373 - acc: 0.7614 - val_loss: 0.6233 - val_acc: 0.7733\n",
      "Epoch 202/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.7658\n",
      "Epoch 202: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7646 - val_loss: 0.5952 - val_acc: 0.7913\n",
      "Epoch 203/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7688\n",
      "Epoch 203: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5246 - acc: 0.7678 - val_loss: 0.6541 - val_acc: 0.7772\n",
      "Epoch 204/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5497 - acc: 0.7689\n",
      "Epoch 204: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5497 - acc: 0.7689 - val_loss: 0.6052 - val_acc: 0.7780\n",
      "Epoch 205/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7707\n",
      "Epoch 205: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5361 - acc: 0.7689 - val_loss: 0.6268 - val_acc: 0.7562\n",
      "Epoch 206/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.7696\n",
      "Epoch 206: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5224 - acc: 0.7696 - val_loss: 0.5954 - val_acc: 0.7857\n",
      "Epoch 207/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.7703\n",
      "Epoch 207: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5272 - acc: 0.7697 - val_loss: 0.5812 - val_acc: 0.7802\n",
      "Epoch 208/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5186 - acc: 0.7676\n",
      "Epoch 208: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5186 - acc: 0.7676 - val_loss: 0.6381 - val_acc: 0.7840\n",
      "Epoch 209/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7681\n",
      "Epoch 209: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5388 - acc: 0.7670 - val_loss: 0.6313 - val_acc: 0.7827\n",
      "Epoch 210/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7691\n",
      "Epoch 210: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5334 - acc: 0.7678 - val_loss: 0.5678 - val_acc: 0.7806\n",
      "Epoch 211/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7743\n",
      "Epoch 211: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5322 - acc: 0.7727 - val_loss: 0.6099 - val_acc: 0.7630\n",
      "Epoch 212/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5389 - acc: 0.7681\n",
      "Epoch 212: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5389 - acc: 0.7681 - val_loss: 0.6605 - val_acc: 0.7596\n",
      "Epoch 213/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.7655\n",
      "Epoch 213: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5490 - acc: 0.7641 - val_loss: 0.6279 - val_acc: 0.7849\n",
      "Epoch 214/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.7718\n",
      "Epoch 214: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5330 - acc: 0.7712 - val_loss: 0.6248 - val_acc: 0.7703\n",
      "Epoch 215/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.7630\n",
      "Epoch 215: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5420 - acc: 0.7615 - val_loss: 0.6635 - val_acc: 0.7699\n",
      "Epoch 216/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7722\n",
      "Epoch 216: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5438 - acc: 0.7705 - val_loss: 0.6299 - val_acc: 0.7887\n",
      "Epoch 217/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7670\n",
      "Epoch 217: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5495 - acc: 0.7665 - val_loss: 0.6501 - val_acc: 0.7870\n",
      "Epoch 218/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.7699\n",
      "Epoch 218: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5423 - acc: 0.7698 - val_loss: 0.6151 - val_acc: 0.7908\n",
      "Epoch 219/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5414 - acc: 0.7680\n",
      "Epoch 219: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.7676 - val_loss: 0.6784 - val_acc: 0.7742\n",
      "Epoch 220/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.7699\n",
      "Epoch 220: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5429 - acc: 0.7686 - val_loss: 0.6122 - val_acc: 0.7630\n",
      "Epoch 221/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.7645\n",
      "Epoch 221: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5355 - acc: 0.7634 - val_loss: 0.5937 - val_acc: 0.7904\n",
      "Epoch 222/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.7684\n",
      "Epoch 222: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5340 - acc: 0.7678 - val_loss: 0.6467 - val_acc: 0.7754\n",
      "Epoch 223/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.7670\n",
      "Epoch 223: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5311 - acc: 0.7658 - val_loss: 0.6280 - val_acc: 0.7669\n",
      "Epoch 224/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7684\n",
      "Epoch 224: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5282 - acc: 0.7685 - val_loss: 0.6631 - val_acc: 0.7857\n",
      "Epoch 225/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7718\n",
      "Epoch 225: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5381 - acc: 0.7712 - val_loss: 0.6777 - val_acc: 0.7819\n",
      "Epoch 226/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.7713\n",
      "Epoch 226: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5555 - acc: 0.7711 - val_loss: 0.7022 - val_acc: 0.7789\n",
      "Epoch 227/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.7630\n",
      "Epoch 227: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5541 - acc: 0.7620 - val_loss: 0.6763 - val_acc: 0.7737\n",
      "Epoch 228/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.7687\n",
      "Epoch 228: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5535 - acc: 0.7675 - val_loss: 0.6565 - val_acc: 0.7729\n",
      "Epoch 229/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7700\n",
      "Epoch 229: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5450 - acc: 0.7688 - val_loss: 0.6334 - val_acc: 0.7926\n",
      "Epoch 230/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7728\n",
      "Epoch 230: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5388 - acc: 0.7721 - val_loss: 0.6371 - val_acc: 0.7836\n",
      "Epoch 231/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.7708\n",
      "Epoch 231: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5317 - acc: 0.7698 - val_loss: 0.6141 - val_acc: 0.7849\n",
      "Epoch 232/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7728\n",
      "Epoch 232: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5272 - acc: 0.7721 - val_loss: 0.6131 - val_acc: 0.7938\n",
      "Epoch 233/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7689\n",
      "Epoch 233: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5509 - acc: 0.7685 - val_loss: 0.6363 - val_acc: 0.7797\n",
      "Epoch 234/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7738\n",
      "Epoch 234: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5333 - acc: 0.7721 - val_loss: 0.6545 - val_acc: 0.7793\n",
      "Epoch 235/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7705\n",
      "Epoch 235: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5343 - acc: 0.7699 - val_loss: 0.6682 - val_acc: 0.7806\n",
      "Epoch 236/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.7702\n",
      "Epoch 236: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7692 - val_loss: 0.6619 - val_acc: 0.7750\n",
      "Epoch 237/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7708\n",
      "Epoch 237: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5367 - acc: 0.7708 - val_loss: 0.6086 - val_acc: 0.7802\n",
      "Epoch 238/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.7680\n",
      "Epoch 238: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5515 - acc: 0.7678 - val_loss: 0.6241 - val_acc: 0.7840\n",
      "Epoch 239/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7721\n",
      "Epoch 239: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5518 - acc: 0.7708 - val_loss: 0.6040 - val_acc: 0.7643\n",
      "Epoch 240/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5173 - acc: 0.7778\n",
      "Epoch 240: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5184 - acc: 0.7763 - val_loss: 0.6480 - val_acc: 0.7887\n",
      "Epoch 241/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.7715\n",
      "Epoch 241: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5424 - acc: 0.7699 - val_loss: 0.6472 - val_acc: 0.7806\n",
      "Epoch 242/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7716\n",
      "Epoch 242: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5326 - acc: 0.7703 - val_loss: 0.6021 - val_acc: 0.7926\n",
      "Epoch 243/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7740\n",
      "Epoch 243: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5313 - acc: 0.7735 - val_loss: 0.6288 - val_acc: 0.7827\n",
      "Epoch 244/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5571 - acc: 0.7720\n",
      "Epoch 244: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5571 - acc: 0.7720 - val_loss: 0.6389 - val_acc: 0.7896\n",
      "Epoch 245/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7698\n",
      "Epoch 245: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5481 - acc: 0.7690 - val_loss: 0.6538 - val_acc: 0.7836\n",
      "Epoch 246/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7771\n",
      "Epoch 246: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5472 - acc: 0.7755 - val_loss: 0.6398 - val_acc: 0.7823\n",
      "Epoch 247/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7738\n",
      "Epoch 247: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5225 - acc: 0.7729 - val_loss: 0.6321 - val_acc: 0.7921\n",
      "Epoch 248/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7747\n",
      "Epoch 248: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5326 - acc: 0.7740 - val_loss: 0.6410 - val_acc: 0.7827\n",
      "Epoch 249/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7792\n",
      "Epoch 249: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5313 - acc: 0.7790 - val_loss: 0.6597 - val_acc: 0.7943\n",
      "Epoch 250/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7723\n",
      "Epoch 250: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5316 - acc: 0.7722 - val_loss: 0.6664 - val_acc: 0.7836\n",
      "Epoch 251/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5262 - acc: 0.7755\n",
      "Epoch 251: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5262 - acc: 0.7755 - val_loss: 0.6629 - val_acc: 0.7626\n",
      "Epoch 252/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.7714\n",
      "Epoch 252: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5638 - acc: 0.7701 - val_loss: 0.5990 - val_acc: 0.7926\n",
      "Epoch 253/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7738\n",
      "Epoch 253: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5261 - acc: 0.7727 - val_loss: 0.6356 - val_acc: 0.7891\n",
      "Epoch 254/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.7709\n",
      "Epoch 254: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5447 - acc: 0.7700 - val_loss: 0.6920 - val_acc: 0.7797\n",
      "Epoch 255/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7690\n",
      "Epoch 255: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5336 - acc: 0.7685 - val_loss: 0.6497 - val_acc: 0.7793\n",
      "Epoch 256/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5435 - acc: 0.7690\n",
      "Epoch 256: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5435 - acc: 0.7690 - val_loss: 0.5900 - val_acc: 0.7900\n",
      "Epoch 257/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.7726\n",
      "Epoch 257: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5215 - acc: 0.7722 - val_loss: 0.6213 - val_acc: 0.7866\n",
      "Epoch 258/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.7713\n",
      "Epoch 258: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5330 - acc: 0.7711 - val_loss: 0.6467 - val_acc: 0.7934\n",
      "Epoch 259/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7714\n",
      "Epoch 259: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7715 - val_loss: 0.6931 - val_acc: 0.7917\n",
      "Epoch 260/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7753\n",
      "Epoch 260: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5439 - acc: 0.7753 - val_loss: 0.6511 - val_acc: 0.7921\n",
      "Epoch 261/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5374 - acc: 0.7656\n",
      "Epoch 261: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5374 - acc: 0.7656 - val_loss: 0.6763 - val_acc: 0.7827\n",
      "Epoch 262/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7664\n",
      "Epoch 262: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5334 - acc: 0.7662 - val_loss: 0.6571 - val_acc: 0.7673\n",
      "Epoch 263/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7688\n",
      "Epoch 263: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7677 - val_loss: 0.6661 - val_acc: 0.7861\n",
      "Epoch 264/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.7728\n",
      "Epoch 264: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5482 - acc: 0.7724 - val_loss: 0.6556 - val_acc: 0.7883\n",
      "Epoch 265/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.7744\n",
      "Epoch 265: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5341 - acc: 0.7742 - val_loss: 0.6603 - val_acc: 0.7836\n",
      "Epoch 266/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7723\n",
      "Epoch 266: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5285 - acc: 0.7722 - val_loss: 0.6994 - val_acc: 0.7806\n",
      "Epoch 267/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7726\n",
      "Epoch 267: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5420 - acc: 0.7718 - val_loss: 0.7299 - val_acc: 0.7917\n",
      "Epoch 268/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.7747\n",
      "Epoch 268: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5414 - acc: 0.7740 - val_loss: 0.6604 - val_acc: 0.7947\n",
      "Epoch 269/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7756\n",
      "Epoch 269: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5551 - acc: 0.7743 - val_loss: 0.6567 - val_acc: 0.7763\n",
      "Epoch 270/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5291 - acc: 0.7733\n",
      "Epoch 270: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7733 - val_loss: 0.6236 - val_acc: 0.7823\n",
      "Epoch 271/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7750\n",
      "Epoch 271: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5450 - acc: 0.7737 - val_loss: 0.6916 - val_acc: 0.7579\n",
      "Epoch 272/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7732\n",
      "Epoch 272: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5368 - acc: 0.7716 - val_loss: 0.6456 - val_acc: 0.7772\n",
      "Epoch 273/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7705\n",
      "Epoch 273: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5455 - acc: 0.7695 - val_loss: 0.6290 - val_acc: 0.7819\n",
      "Epoch 274/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7736\n",
      "Epoch 274: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5324 - acc: 0.7722 - val_loss: 0.6467 - val_acc: 0.7921\n",
      "Epoch 275/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5693 - acc: 0.7666\n",
      "Epoch 275: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5695 - acc: 0.7661 - val_loss: 0.6131 - val_acc: 0.7853\n",
      "Epoch 276/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.7709\n",
      "Epoch 276: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5255 - acc: 0.7707 - val_loss: 0.6629 - val_acc: 0.7930\n",
      "Epoch 277/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5596 - acc: 0.7705\n",
      "Epoch 277: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5596 - acc: 0.7705 - val_loss: 0.6059 - val_acc: 0.7763\n",
      "Epoch 278/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7724\n",
      "Epoch 278: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5334 - acc: 0.7718 - val_loss: 0.6825 - val_acc: 0.7964\n",
      "Epoch 279/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.7746\n",
      "Epoch 279: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5439 - acc: 0.7732 - val_loss: 0.6561 - val_acc: 0.7772\n",
      "Epoch 280/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.7705\n",
      "Epoch 280: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5527 - acc: 0.7703 - val_loss: 0.6240 - val_acc: 0.7900\n",
      "Epoch 281/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7698\n",
      "Epoch 281: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5250 - acc: 0.7693 - val_loss: 0.6547 - val_acc: 0.7750\n",
      "Epoch 282/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5545 - acc: 0.7709\n",
      "Epoch 282: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5545 - acc: 0.7709 - val_loss: 0.6380 - val_acc: 0.7776\n",
      "Epoch 283/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7757\n",
      "Epoch 283: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5345 - acc: 0.7757 - val_loss: 0.6553 - val_acc: 0.7964\n",
      "Epoch 284/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7766\n",
      "Epoch 284: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5378 - acc: 0.7754 - val_loss: 0.6878 - val_acc: 0.7874\n",
      "Epoch 285/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.7716\n",
      "Epoch 285: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5431 - acc: 0.7714 - val_loss: 0.6847 - val_acc: 0.7926\n",
      "Epoch 286/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.7807\n",
      "Epoch 286: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5387 - acc: 0.7805 - val_loss: 0.6256 - val_acc: 0.7913\n",
      "Epoch 287/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7686\n",
      "Epoch 287: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5256 - acc: 0.7684 - val_loss: 0.6918 - val_acc: 0.7861\n",
      "Epoch 288/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.7724\n",
      "Epoch 288: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5472 - acc: 0.7712 - val_loss: 0.6683 - val_acc: 0.7921\n",
      "Epoch 289/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.7760\n",
      "Epoch 289: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5252 - acc: 0.7748 - val_loss: 0.6494 - val_acc: 0.7861\n",
      "Epoch 290/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7749\n",
      "Epoch 290: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5365 - acc: 0.7731 - val_loss: 0.6842 - val_acc: 0.7793\n",
      "Epoch 291/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7757\n",
      "Epoch 291: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5240 - acc: 0.7755 - val_loss: 0.6325 - val_acc: 0.7763\n",
      "Epoch 292/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7735\n",
      "Epoch 292: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7723 - val_loss: 0.6390 - val_acc: 0.7759\n",
      "Epoch 293/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5296 - acc: 0.7762\n",
      "Epoch 293: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5292 - acc: 0.7754 - val_loss: 0.7033 - val_acc: 0.7861\n",
      "Epoch 294/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7735\n",
      "Epoch 294: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5196 - acc: 0.7724 - val_loss: 0.6709 - val_acc: 0.7874\n",
      "Epoch 295/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.7730\n",
      "Epoch 295: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5411 - acc: 0.7726 - val_loss: 0.6874 - val_acc: 0.7754\n",
      "Epoch 296/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7771\n",
      "Epoch 296: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5146 - acc: 0.7758 - val_loss: 0.6695 - val_acc: 0.7861\n",
      "Epoch 297/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5399 - acc: 0.7736\n",
      "Epoch 297: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5399 - acc: 0.7736 - val_loss: 0.7112 - val_acc: 0.7926\n",
      "Epoch 298/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7762\n",
      "Epoch 298: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5242 - acc: 0.7753 - val_loss: 0.6542 - val_acc: 0.7908\n",
      "Epoch 299/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7742\n",
      "Epoch 299: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5453 - acc: 0.7743 - val_loss: 0.6795 - val_acc: 0.7870\n",
      "Epoch 300/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.7745\n",
      "Epoch 300: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5523 - acc: 0.7743 - val_loss: 0.6866 - val_acc: 0.7968\n",
      "Epoch 301/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7766\n",
      "Epoch 301: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5381 - acc: 0.7754 - val_loss: 0.6978 - val_acc: 0.7879\n",
      "Epoch 302/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.7762\n",
      "Epoch 302: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5257 - acc: 0.7760 - val_loss: 0.6894 - val_acc: 0.7951\n",
      "Epoch 303/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7748\n",
      "Epoch 303: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5479 - acc: 0.7749 - val_loss: 0.6719 - val_acc: 0.7810\n",
      "Epoch 304/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.7796\n",
      "Epoch 304: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5263 - acc: 0.7791 - val_loss: 0.6471 - val_acc: 0.7857\n",
      "Epoch 305/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7739\n",
      "Epoch 305: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5184 - acc: 0.7734 - val_loss: 0.7713 - val_acc: 0.7943\n",
      "Epoch 306/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7756\n",
      "Epoch 306: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5272 - acc: 0.7748 - val_loss: 0.7784 - val_acc: 0.7827\n",
      "Epoch 307/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.7753\n",
      "Epoch 307: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5768 - acc: 0.7748 - val_loss: 0.7138 - val_acc: 0.7814\n",
      "Epoch 308/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.7772\n",
      "Epoch 308: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5521 - acc: 0.7768 - val_loss: 0.6994 - val_acc: 0.7917\n",
      "Epoch 309/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7769\n",
      "Epoch 309: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7765 - val_loss: 0.7071 - val_acc: 0.7879\n",
      "Epoch 310/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.7738\n",
      "Epoch 310: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5329 - acc: 0.7731 - val_loss: 0.6261 - val_acc: 0.7866\n",
      "Epoch 311/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.7746\n",
      "Epoch 311: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5164 - acc: 0.7743 - val_loss: 0.6869 - val_acc: 0.7737\n",
      "Epoch 312/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.7766\n",
      "Epoch 312: val_acc did not improve from 0.79683\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5157 - acc: 0.7754 - val_loss: 0.6912 - val_acc: 0.7840\n",
      "Epoch 313/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.7795\n",
      "Epoch 313: val_acc improved from 0.79683 to 0.80453, saving model to train_logs/logs7/RWB_ANN_512_256_RMSprop/5/128/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5478 - acc: 0.7786 - val_loss: 0.6740 - val_acc: 0.8045\n",
      "Epoch 314/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5403 - acc: 0.7755\n",
      "Epoch 314: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5403 - acc: 0.7755 - val_loss: 0.7780 - val_acc: 0.7861\n",
      "Epoch 315/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5388 - acc: 0.7752\n",
      "Epoch 315: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5388 - acc: 0.7752 - val_loss: 0.7412 - val_acc: 0.7900\n",
      "Epoch 316/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7823\n",
      "Epoch 316: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5470 - acc: 0.7809 - val_loss: 0.6558 - val_acc: 0.7827\n",
      "Epoch 317/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5310 - acc: 0.7761\n",
      "Epoch 317: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7746 - val_loss: 0.6824 - val_acc: 0.7772\n",
      "Epoch 318/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7790\n",
      "Epoch 318: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5392 - acc: 0.7779 - val_loss: 0.6530 - val_acc: 0.7831\n",
      "Epoch 319/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.7751\n",
      "Epoch 319: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5230 - acc: 0.7745 - val_loss: 0.6628 - val_acc: 0.7930\n",
      "Epoch 320/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7781\n",
      "Epoch 320: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5219 - acc: 0.7778 - val_loss: 0.6721 - val_acc: 0.7930\n",
      "Epoch 321/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.7754\n",
      "Epoch 321: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5471 - acc: 0.7745 - val_loss: 0.6489 - val_acc: 0.7883\n",
      "Epoch 322/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.7823\n",
      "Epoch 322: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5101 - acc: 0.7822 - val_loss: 0.6248 - val_acc: 0.7849\n",
      "Epoch 323/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7798\n",
      "Epoch 323: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5408 - acc: 0.7792 - val_loss: 0.6803 - val_acc: 0.7896\n",
      "Epoch 324/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7734\n",
      "Epoch 324: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5416 - acc: 0.7723 - val_loss: 0.6944 - val_acc: 0.7896\n",
      "Epoch 325/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5310 - acc: 0.7737\n",
      "Epoch 325: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5315 - acc: 0.7735 - val_loss: 0.7080 - val_acc: 0.7938\n",
      "Epoch 326/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7808\n",
      "Epoch 326: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5203 - acc: 0.7809 - val_loss: 0.6613 - val_acc: 0.7956\n",
      "Epoch 327/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7731\n",
      "Epoch 327: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5400 - acc: 0.7732 - val_loss: 0.7770 - val_acc: 0.7870\n",
      "Epoch 328/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.7826\n",
      "Epoch 328: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5415 - acc: 0.7827 - val_loss: 0.7919 - val_acc: 0.7887\n",
      "Epoch 329/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7764\n",
      "Epoch 329: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5303 - acc: 0.7751 - val_loss: 0.6652 - val_acc: 0.7712\n",
      "Epoch 330/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7802\n",
      "Epoch 330: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5541 - acc: 0.7788 - val_loss: 0.6307 - val_acc: 0.7776\n",
      "Epoch 331/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.7781\n",
      "Epoch 331: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5103 - acc: 0.7774 - val_loss: 0.6893 - val_acc: 0.7921\n",
      "Epoch 332/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.7746\n",
      "Epoch 332: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5204 - acc: 0.7747 - val_loss: 0.6561 - val_acc: 0.7947\n",
      "Epoch 333/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.7751\n",
      "Epoch 333: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5263 - acc: 0.7751 - val_loss: 0.7411 - val_acc: 0.7699\n",
      "Epoch 334/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.7825\n",
      "Epoch 334: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5306 - acc: 0.7811 - val_loss: 0.7305 - val_acc: 0.7866\n",
      "Epoch 335/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7781\n",
      "Epoch 335: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5236 - acc: 0.7776 - val_loss: 0.6952 - val_acc: 0.7763\n",
      "Epoch 336/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.7817\n",
      "Epoch 336: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5107 - acc: 0.7814 - val_loss: 0.7287 - val_acc: 0.7789\n",
      "Epoch 337/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7767\n",
      "Epoch 337: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5247 - acc: 0.7753 - val_loss: 0.7144 - val_acc: 0.7951\n",
      "Epoch 338/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7778\n",
      "Epoch 338: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5428 - acc: 0.7770 - val_loss: 0.6884 - val_acc: 0.7776\n",
      "Epoch 339/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7751\n",
      "Epoch 339: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5360 - acc: 0.7735 - val_loss: 0.6154 - val_acc: 0.7844\n",
      "Epoch 340/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7731\n",
      "Epoch 340: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5401 - acc: 0.7723 - val_loss: 0.7643 - val_acc: 0.7938\n",
      "Epoch 341/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5344 - acc: 0.7733\n",
      "Epoch 341: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5344 - acc: 0.7733 - val_loss: 0.6895 - val_acc: 0.7579\n",
      "Epoch 342/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7723\n",
      "Epoch 342: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5211 - acc: 0.7723 - val_loss: 0.7661 - val_acc: 0.7857\n",
      "Epoch 343/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.7783\n",
      "Epoch 343: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5599 - acc: 0.7773 - val_loss: 0.6878 - val_acc: 0.7964\n",
      "Epoch 344/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5332 - acc: 0.7774\n",
      "Epoch 344: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5332 - acc: 0.7774 - val_loss: 0.6984 - val_acc: 0.7960\n",
      "Epoch 345/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7778\n",
      "Epoch 345: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5245 - acc: 0.7766 - val_loss: 0.7308 - val_acc: 0.7861\n",
      "Epoch 346/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7785\n",
      "Epoch 346: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5190 - acc: 0.7775 - val_loss: 0.6818 - val_acc: 0.7831\n",
      "Epoch 347/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.7791\n",
      "Epoch 347: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5443 - acc: 0.7776 - val_loss: 0.7259 - val_acc: 0.7729\n",
      "Epoch 348/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.7767\n",
      "Epoch 348: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5543 - acc: 0.7766 - val_loss: 0.7204 - val_acc: 0.7891\n",
      "Epoch 349/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7785\n",
      "Epoch 349: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5354 - acc: 0.7770 - val_loss: 0.7257 - val_acc: 0.7887\n",
      "Epoch 350/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5611 - acc: 0.7768\n",
      "Epoch 350: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5611 - acc: 0.7768 - val_loss: 0.6527 - val_acc: 0.7956\n",
      "Epoch 351/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7775\n",
      "Epoch 351: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7768 - val_loss: 0.7145 - val_acc: 0.7887\n",
      "Epoch 352/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.7762\n",
      "Epoch 352: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5316 - acc: 0.7761 - val_loss: 0.8135 - val_acc: 0.7874\n",
      "Epoch 353/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.7746\n",
      "Epoch 353: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5431 - acc: 0.7746 - val_loss: 0.8335 - val_acc: 0.7836\n",
      "Epoch 354/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7814\n",
      "Epoch 354: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5493 - acc: 0.7813 - val_loss: 0.8864 - val_acc: 0.7908\n",
      "Epoch 355/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7802\n",
      "Epoch 355: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5293 - acc: 0.7796 - val_loss: 0.6831 - val_acc: 0.7874\n",
      "Epoch 356/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.7785\n",
      "Epoch 356: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5479 - acc: 0.7775 - val_loss: 0.7845 - val_acc: 0.7870\n",
      "Epoch 357/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7780\n",
      "Epoch 357: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7777 - val_loss: 0.7609 - val_acc: 0.7750\n",
      "Epoch 358/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7759\n",
      "Epoch 358: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5431 - acc: 0.7749 - val_loss: 0.7019 - val_acc: 0.7908\n",
      "Epoch 359/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7761\n",
      "Epoch 359: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7757 - val_loss: 0.6833 - val_acc: 0.7913\n",
      "Epoch 360/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7772\n",
      "Epoch 360: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5263 - acc: 0.7766 - val_loss: 0.7177 - val_acc: 0.7891\n",
      "Epoch 361/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.7771\n",
      "Epoch 361: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5516 - acc: 0.7761 - val_loss: 0.6417 - val_acc: 0.7930\n",
      "Epoch 362/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.7801\n",
      "Epoch 362: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5348 - acc: 0.7799 - val_loss: 0.7189 - val_acc: 0.7870\n",
      "Epoch 363/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7767\n",
      "Epoch 363: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5276 - acc: 0.7759 - val_loss: 0.7456 - val_acc: 0.7921\n",
      "Epoch 364/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.7791\n",
      "Epoch 364: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5410 - acc: 0.7789 - val_loss: 0.7371 - val_acc: 0.7934\n",
      "Epoch 365/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.7836\n",
      "Epoch 365: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5467 - acc: 0.7823 - val_loss: 0.7487 - val_acc: 0.7844\n",
      "Epoch 366/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7782\n",
      "Epoch 366: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5423 - acc: 0.7776 - val_loss: 0.6680 - val_acc: 0.7938\n",
      "Epoch 367/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7781\n",
      "Epoch 367: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5281 - acc: 0.7779 - val_loss: 0.6363 - val_acc: 0.7926\n",
      "Epoch 368/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7880\n",
      "Epoch 368: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5182 - acc: 0.7867 - val_loss: 0.7705 - val_acc: 0.7857\n",
      "Epoch 369/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5314 - acc: 0.7781\n",
      "Epoch 369: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5314 - acc: 0.7781 - val_loss: 0.7077 - val_acc: 0.7930\n",
      "Epoch 370/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.7780\n",
      "Epoch 370: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5260 - acc: 0.7774 - val_loss: 0.7453 - val_acc: 0.7866\n",
      "Epoch 371/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.7806\n",
      "Epoch 371: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5286 - acc: 0.7797 - val_loss: 0.6741 - val_acc: 0.7917\n",
      "Epoch 372/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.7779\n",
      "Epoch 372: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5382 - acc: 0.7775 - val_loss: 0.7294 - val_acc: 0.7913\n",
      "Epoch 373/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7812\n",
      "Epoch 373: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5221 - acc: 0.7792 - val_loss: 0.6660 - val_acc: 0.7789\n",
      "Epoch 374/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7767\n",
      "Epoch 374: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5218 - acc: 0.7760 - val_loss: 0.7321 - val_acc: 0.7943\n",
      "Epoch 375/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.7800\n",
      "Epoch 375: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5644 - acc: 0.7794 - val_loss: 0.7053 - val_acc: 0.7874\n",
      "Epoch 376/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7772\n",
      "Epoch 376: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5463 - acc: 0.7757 - val_loss: 0.6962 - val_acc: 0.7879\n",
      "Epoch 377/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.7771\n",
      "Epoch 377: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5497 - acc: 0.7753 - val_loss: 0.7306 - val_acc: 0.7532\n",
      "Epoch 378/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7712\n",
      "Epoch 378: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5337 - acc: 0.7707 - val_loss: 0.7118 - val_acc: 0.7853\n",
      "Epoch 379/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7805\n",
      "Epoch 379: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5342 - acc: 0.7800 - val_loss: 0.7308 - val_acc: 0.7831\n",
      "Epoch 380/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5439 - acc: 0.7691\n",
      "Epoch 380: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5439 - acc: 0.7691 - val_loss: 0.7025 - val_acc: 0.7579\n",
      "Epoch 381/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.7731\n",
      "Epoch 381: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5381 - acc: 0.7733 - val_loss: 0.7078 - val_acc: 0.7908\n",
      "Epoch 382/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7754\n",
      "Epoch 382: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5439 - acc: 0.7743 - val_loss: 0.6632 - val_acc: 0.7951\n",
      "Epoch 383/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7781\n",
      "Epoch 383: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5347 - acc: 0.7781 - val_loss: 0.7126 - val_acc: 0.7921\n",
      "Epoch 384/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7748\n",
      "Epoch 384: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5499 - acc: 0.7744 - val_loss: 0.7154 - val_acc: 0.7896\n",
      "Epoch 385/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.7762\n",
      "Epoch 385: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5299 - acc: 0.7763 - val_loss: 0.7104 - val_acc: 0.7879\n",
      "Epoch 386/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7781\n",
      "Epoch 386: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5360 - acc: 0.7776 - val_loss: 0.7292 - val_acc: 0.7874\n",
      "Epoch 387/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5258 - acc: 0.7714\n",
      "Epoch 387: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5258 - acc: 0.7714 - val_loss: 0.6727 - val_acc: 0.7831\n",
      "Epoch 388/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7773\n",
      "Epoch 388: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5365 - acc: 0.7766 - val_loss: 0.7677 - val_acc: 0.7900\n",
      "Epoch 389/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7802\n",
      "Epoch 389: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5299 - acc: 0.7793 - val_loss: 0.7381 - val_acc: 0.7793\n",
      "Epoch 390/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5363 - acc: 0.7745\n",
      "Epoch 390: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5363 - acc: 0.7745 - val_loss: 0.7518 - val_acc: 0.7896\n",
      "Epoch 391/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5191 - acc: 0.7760\n",
      "Epoch 391: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5191 - acc: 0.7760 - val_loss: 0.7757 - val_acc: 0.7917\n",
      "Epoch 392/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.7761\n",
      "Epoch 392: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5905 - acc: 0.7744 - val_loss: 0.7003 - val_acc: 0.7639\n",
      "Epoch 393/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7817\n",
      "Epoch 393: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5321 - acc: 0.7813 - val_loss: 0.7321 - val_acc: 0.7720\n",
      "Epoch 394/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7779\n",
      "Epoch 394: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5536 - acc: 0.7767 - val_loss: 0.6367 - val_acc: 0.7857\n",
      "Epoch 395/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7759\n",
      "Epoch 395: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5342 - acc: 0.7758 - val_loss: 0.7404 - val_acc: 0.7951\n",
      "Epoch 396/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.7782\n",
      "Epoch 396: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5518 - acc: 0.7771 - val_loss: 0.7637 - val_acc: 0.7849\n",
      "Epoch 397/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5252 - acc: 0.7747\n",
      "Epoch 397: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5252 - acc: 0.7747 - val_loss: 0.7568 - val_acc: 0.7908\n",
      "Epoch 398/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.7769\n",
      "Epoch 398: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5556 - acc: 0.7766 - val_loss: 0.7575 - val_acc: 0.7981\n",
      "Epoch 399/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7778\n",
      "Epoch 399: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7768 - val_loss: 0.7143 - val_acc: 0.7951\n",
      "Epoch 400/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.7762\n",
      "Epoch 400: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5265 - acc: 0.7753 - val_loss: 0.7476 - val_acc: 0.7866\n",
      "Epoch 401/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.7789\n",
      "Epoch 401: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5634 - acc: 0.7776 - val_loss: 0.6921 - val_acc: 0.7844\n",
      "Epoch 402/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7761\n",
      "Epoch 402: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5196 - acc: 0.7762 - val_loss: 0.7247 - val_acc: 0.7879\n",
      "Epoch 403/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.7846\n",
      "Epoch 403: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5238 - acc: 0.7836 - val_loss: 0.6994 - val_acc: 0.7968\n",
      "Epoch 404/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.7804\n",
      "Epoch 404: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5295 - acc: 0.7800 - val_loss: 0.7206 - val_acc: 0.7887\n",
      "Epoch 405/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7784\n",
      "Epoch 405: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5417 - acc: 0.7781 - val_loss: 0.7711 - val_acc: 0.7733\n",
      "Epoch 406/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7785\n",
      "Epoch 406: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5251 - acc: 0.7777 - val_loss: 0.7930 - val_acc: 0.7977\n",
      "Epoch 407/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7781\n",
      "Epoch 407: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5349 - acc: 0.7777 - val_loss: 0.7594 - val_acc: 0.7926\n",
      "Epoch 408/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.7851\n",
      "Epoch 408: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5264 - acc: 0.7841 - val_loss: 0.7113 - val_acc: 0.7759\n",
      "Epoch 409/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5261 - acc: 0.7775\n",
      "Epoch 409: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5261 - acc: 0.7775 - val_loss: 0.7293 - val_acc: 0.7840\n",
      "Epoch 410/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7812\n",
      "Epoch 410: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5357 - acc: 0.7808 - val_loss: 0.6676 - val_acc: 0.7891\n",
      "Epoch 411/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7747\n",
      "Epoch 411: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5461 - acc: 0.7733 - val_loss: 0.7184 - val_acc: 0.7917\n",
      "Epoch 412/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7747\n",
      "Epoch 412: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5372 - acc: 0.7748 - val_loss: 0.7147 - val_acc: 0.7913\n",
      "Epoch 413/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7752\n",
      "Epoch 413: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5422 - acc: 0.7750 - val_loss: 0.7267 - val_acc: 0.7934\n",
      "Epoch 414/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7773\n",
      "Epoch 414: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5284 - acc: 0.7769 - val_loss: 0.7705 - val_acc: 0.7887\n",
      "Epoch 415/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.7781\n",
      "Epoch 415: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5453 - acc: 0.7782 - val_loss: 0.7339 - val_acc: 0.8015\n",
      "Epoch 416/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.7729\n",
      "Epoch 416: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5495 - acc: 0.7720 - val_loss: 0.6722 - val_acc: 0.7866\n",
      "Epoch 417/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.7790\n",
      "Epoch 417: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5345 - acc: 0.7790 - val_loss: 0.6736 - val_acc: 0.7793\n",
      "Epoch 418/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5395 - acc: 0.7768\n",
      "Epoch 418: val_acc did not improve from 0.80453\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5395 - acc: 0.7768 - val_loss: 0.7362 - val_acc: 0.7904\n",
      "92/92 [==============================] - 1s 3ms/step - loss: 0.8670 - acc: 0.7825\n"
     ]
    }
   ],
   "source": [
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=range(1, 6))\n",
    "    training_time = pd.DataFrame(index=lags, columns=[f'CPU_Time_{i}' for i in range(1, 6)] + [f'Wall_Time_{i}' for i in range(1, 6)])\n",
    "\n",
    "    for lag in lags:\n",
    "        train_temp_dir = train_dir + '_' + str(lag)\n",
    "        train = tf.data.Dataset.load(train_temp_dir)\n",
    "        flattened_train = train.unbatch()\n",
    "\n",
    "        train_data = list(flattened_train.as_numpy_iterator())\n",
    "        train_size = len(list(train_data))\n",
    "\n",
    "        kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_data), 1):\n",
    "            train_fold_data = ([train_data[i][0] for i in train_index], [train_data[i][1] for i in train_index])\n",
    "            val_fold_data = ([train_data[i][0] for i in val_index], [train_data[i][1] for i in val_index])\n",
    "\n",
    "            train_fold = tf.data.Dataset.from_tensor_slices(train_fold_data).batch(32)\n",
    "            val_fold = tf.data.Dataset.from_tensor_slices(val_fold_data).batch(32)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(momentum=0.01), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_fold, epochs=epochs, validation_data=val_fold, callbacks=myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "\n",
    "            training_time.loc[lag, f'CPU_Time_{fold}'] = cpu_time\n",
    "            training_time.loc[lag, f'Wall_Time_{fold}'] = wall_time\n",
    "\n",
    "            recap.loc[lag, fold] = history.history['acc'][-1]\n",
    "\n",
    "\n",
    "    # Evaluate on the test dataset after cross-validation\n",
    "    test_temp_dir = test_dir + '_' + str(lag)\n",
    "    test_ds = tf.data.Dataset.load(test_temp_dir)\n",
    "    results = model.evaluate(test_ds, callbacks=myCallbacks(log_path))\n",
    "\n",
    "    recap[f'test_{lag}'] = results[1]\n",
    "\n",
    "    log_recap_dir = os.path.join(log_dir, 'Recap')\n",
    "    if not os.path.exists(log_recap_dir):\n",
    "        os.makedirs(log_recap_dir)\n",
    "\n",
    "    recap.to_csv(os.path.join(log_recap_dir, 'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_recap_dir, 'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "LAG = 128\n",
    "\n",
    "test_dir = f\"datasets/tf_batch/rwb/segment_1 seconds/test_{LAG}\"\n",
    "test_ds = tf.data.Dataset.load(test_dir)\n",
    "model_dir = [f\"train_logs/logs7/RWB_ANN_512_256_RMSprop/{i}/{LAG}/best_model.h5\" for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_test = test_ds.unbatch()\n",
    "test_data = list(flattened_test.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_value = np.array([test_data[i][0] for i in range(len(test_data))])\n",
    "test_data_label = np.array([test_data[i][1] for i in range(len(test_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 96)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_value.reshape(test_data_value.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6600 - acc: 0.7869\n",
      "0.6599942445755005 0.7869356870651245\n",
      "92/92 [==============================] - 0s 3ms/step\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6716 - acc: 0.7849\n",
      "0.6716217398643494 0.7848837375640869\n",
      "92/92 [==============================] - 0s 3ms/step\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.6319 - acc: 0.7767\n",
      "0.6319417953491211 0.7766757607460022\n",
      "92/92 [==============================] - 0s 3ms/step\n",
      "92/92 [==============================] - 1s 4ms/step - loss: 0.6771 - acc: 0.7828\n",
      "0.6771185398101807 0.7828317284584045\n",
      "92/92 [==============================] - 0s 2ms/step\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.7957 - acc: 0.7869\n",
      "0.7957186102867126 0.7869356870651245\n",
      "92/92 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, model_path in enumerate(model_dir):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(loss, acc)\n",
    "    pred = model.predict(test_data_value.reshape(test_data_value.shape[0], -1))\n",
    "    results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_data_label.astype(bool), pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'autism'])\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Result: RWB ANN (512,256)(RMSprop) lag 128 for model in fold {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: RWB ANN (512,256)(RMSprop) lag 128 for model in fold 1\n",
      "Precision: 0.7902335456475584\n",
      "Recall: 0.9351758793969849\n",
      "Accuracy: 0.7869357045143639\n",
      "F1-score: 0.8566168009205984\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(RMSprop) lag 128 for model in fold 2\n",
      "Precision: 0.7926881720430108\n",
      "Recall: 0.9261306532663317\n",
      "Accuracy: 0.7848837209302325\n",
      "F1-score: 0.8542294322132098\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(RMSprop) lag 128 for model in fold 3\n",
      "Precision: 0.8087759815242495\n",
      "Recall: 0.8798994974874372\n",
      "Accuracy: 0.7766757865937073\n",
      "F1-score: 0.8428399518652226\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(RMSprop) lag 128 for model in fold 4\n",
      "Precision: 0.7896536981616076\n",
      "Recall: 0.9281407035175879\n",
      "Accuracy: 0.7828317373461012\n",
      "F1-score: 0.8533148533148533\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256)(RMSprop) lag 128 for model in fold 5\n",
      "Precision: 0.8066397487662629\n",
      "Recall: 0.9035175879396985\n",
      "Accuracy: 0.7869357045143639\n",
      "F1-score: 0.8523346764636169\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    test_data_label = test_data_label.astype(bool)  \n",
    "\n",
    "    print(f\"Result: RWB ANN (512,256)(RMSprop) lag 128 for model in fold {i}\")\n",
    "\n",
    "    TP = np.sum((test_data_label == True) & (pred == True))\n",
    "    FP = np.sum((test_data_label == False) & (pred == True))\n",
    "    TN = np.sum((test_data_label == False) & (pred == False))\n",
    "    FN = np.sum((test_data_label == True) & (pred == False))  \n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for log_dir in log_dirs:\n",
    "#     recap = pd.DataFrame(index=lags, columns=folds)\n",
    "#     training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "#     for fold in range(1,3):\n",
    "#         for lag in lags:\n",
    "#             if fold == 2:\n",
    "#                 train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "#             train_temp_dir = train_dir + '_' + str(lag)\n",
    "#             test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "#             train = get_batch(train_temp_dir)\n",
    "#             test_ds = get_batch(test_temp_dir)\n",
    "\n",
    "#             train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "#             train_ds = train.take(train_size)\n",
    "#             val_ds = train.skip(train_size)\n",
    "\n",
    "#             log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "#             model = create_model()\n",
    "#             model.summary()\n",
    "\n",
    "#             model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "#             cpu_start = time.process_time()\n",
    "#             wt_start = time.time()\n",
    "\n",
    "#             history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             wt_end = time.time()\n",
    "#             cpu_end = time.process_time()\n",
    "#             wall_time = wt_end - wt_start\n",
    "#             cpu_time = cpu_end - cpu_start\n",
    "#             training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "#             training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "#             results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "#             recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "#             recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "#     log_dir = os.path.join(log_dir,'Recap')\n",
    "#     if not os.path.exists(log_dir):\n",
    "#         os.makedirs(log_dir)\n",
    "#     recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "#     training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir c:/Users/farra/Documents/Pribadi/EEG/EEG-Autism-Classification --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farra\\Documents\\Pribadi\\EEG\\EEG-Autism-Classification\n"
     ]
    }
   ],
   "source": [
    "! cd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
