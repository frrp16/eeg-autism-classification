{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, lag, excluded_name=[]):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):        \n",
    "        folder = os.path.join(directory, foldername)\n",
    "        # print(folder)\n",
    "        if str(lag) in folder:\n",
    "            # print(os.listdir(folder))\n",
    "            for name in os.listdir(folder):\n",
    "                if name in excluded_name:\n",
    "                    # print(name)\n",
    "                    continue\n",
    "                filename = os.path.join(folder, name)\n",
    "                # print(filename)\n",
    "                for files in os.listdir(filename):\n",
    "                    rel_path = os.path.join(filename, files)\n",
    "                    # print(rel_path)\n",
    "                    temp_label = folder\n",
    "                    if \"autism\" in temp_label:\n",
    "                        label = 'autism'\n",
    "                    else:\n",
    "                        label = 'normal'\n",
    "\n",
    "                    temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                    rwb = np.load(rel_path)\n",
    "                    rwb.astype(np.float64).reshape(-1,1)\n",
    "                                    \n",
    "                    temp_data.loc[0, \"data\"] = rwb\n",
    "                    temp_data['label'] = label\n",
    "                    data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"autism\": 1, \"normal\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_value(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "    missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "    clean_data = data.drop(index=data.index[missing_indices])\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data, des_path):\n",
    "    if not os.path.exists(des_path):\n",
    "        os.makedirs(des_path)\n",
    "    data.save(des_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, train_split: float):\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "        data['data'],\n",
    "        data[['label', 'label_map']],\n",
    "        train_size=train_split,\n",
    "        stratify=data['label_map']\n",
    "    )\n",
    "\n",
    "    train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "    test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "    train_df[\"data\"] = train_x\n",
    "    train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "    test_df[\"data\"] = test_x\n",
    "    test_df[['label', 'label_map']] = test_y\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    # loading extracted feature & label\n",
    "    # x = get_dataset(path, lag, excluded_name)\n",
    "\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = data[\"label_map\"].values\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list))\n",
    "\n",
    "    # train_size = int(train_split * len(labels_list))  \n",
    "    # test_size = len(labels_list) - train_size  \n",
    "\n",
    "    # train_dataset = dataset.take(train_size)\n",
    "    # test_dataset = dataset.skip(train_size)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/features/rwb/segment_1 seconds\"\n",
    "\n",
    "train_dir = \"datasets/tf_batch/rwb/segment_1 seconds/train\"\n",
    "test_dir = \"datasets/tf_batch/rwb/segment_1 seconds/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15434, 3)\n",
      "(14618, 3)\n",
      "train:  (11694, 3)\n",
      "test:  (2924, 3)\n"
     ]
    }
   ],
   "source": [
    "excluded = [\"zyad\"]\n",
    "train_split = 0.8\n",
    "LAG = [256]\n",
    "\n",
    "for lag in LAG:\n",
    "    data = get_dataset(data_dir, lag, excluded)\n",
    "    print(data.shape)\n",
    "    data = remove_missing_value(data)\n",
    "    print(data.shape)\n",
    "    train_data, test_data = get_train_test(data, train_split)\n",
    "    print(\"train: \", train_data.shape)\n",
    "    print(\"test: \", test_data.shape)\n",
    "    train_batch = get_batch(train_data)\n",
    "    test_batch = get_batch(test_data)\n",
    "    tf.data.Dataset.save(train_batch, f\"{train_dir}_{lag}\")\n",
    "    tf.data.Dataset.save(test_batch, f\"{test_dir}_{lag}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load(\"datasets/features/rwb/segment_1 seconds/normal_256/amer/Amer_segment_1.csv_bispectrum.npy\")\n",
    "\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(96,)))\n",
    "    model.add(layers.Reshape((96, 1)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [256]\n",
    "# folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000\n",
    "log_dirs = [f\"train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               24832     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,521\n",
      "Trainable params: 107,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 1.5180 - acc: 0.5936\n",
      "Epoch 1: val_acc improved from -inf to 0.68063, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 3s 8ms/step - loss: 1.5120 - acc: 0.5939 - val_loss: 0.6665 - val_acc: 0.6806\n",
      "Epoch 2/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6695 - acc: 0.6496\n",
      "Epoch 2: val_acc did not improve from 0.68063\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6693 - acc: 0.6497 - val_loss: 0.6673 - val_acc: 0.6627\n",
      "Epoch 3/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6362 - acc: 0.6737\n",
      "Epoch 3: val_acc improved from 0.68063 to 0.70928, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6366 - acc: 0.6735 - val_loss: 0.6523 - val_acc: 0.7093\n",
      "Epoch 4/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6093 - acc: 0.6907\n",
      "Epoch 4: val_acc improved from 0.70928 to 0.72424, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6097 - acc: 0.6904 - val_loss: 0.6066 - val_acc: 0.7242\n",
      "Epoch 5/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.7075\n",
      "Epoch 5: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5879 - acc: 0.7059 - val_loss: 0.5821 - val_acc: 0.7212\n",
      "Epoch 6/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.7122\n",
      "Epoch 6: val_acc improved from 0.72424 to 0.73194, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5859 - acc: 0.7117 - val_loss: 0.5803 - val_acc: 0.7319\n",
      "Epoch 7/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5736 - acc: 0.7206\n",
      "Epoch 7: val_acc improved from 0.73194 to 0.73365, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5736 - acc: 0.7206 - val_loss: 0.5986 - val_acc: 0.7336\n",
      "Epoch 8/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.7278\n",
      "Epoch 8: val_acc improved from 0.73365 to 0.74690, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5625 - acc: 0.7278 - val_loss: 0.5688 - val_acc: 0.7469\n",
      "Epoch 9/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.7314\n",
      "Epoch 9: val_acc improved from 0.74690 to 0.74733, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5551 - acc: 0.7307 - val_loss: 0.5730 - val_acc: 0.7473\n",
      "Epoch 10/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7322\n",
      "Epoch 10: val_acc improved from 0.74733 to 0.74989, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5545 - acc: 0.7320 - val_loss: 0.5637 - val_acc: 0.7499\n",
      "Epoch 11/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7327\n",
      "Epoch 11: val_acc improved from 0.74989 to 0.75118, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5508 - acc: 0.7320 - val_loss: 0.5385 - val_acc: 0.7512\n",
      "Epoch 12/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.7401\n",
      "Epoch 12: val_acc improved from 0.75118 to 0.75289, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5472 - acc: 0.7399 - val_loss: 0.5555 - val_acc: 0.7529\n",
      "Epoch 13/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7440\n",
      "Epoch 13: val_acc did not improve from 0.75289\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5379 - acc: 0.7436 - val_loss: 0.5457 - val_acc: 0.7529\n",
      "Epoch 14/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7436\n",
      "Epoch 14: val_acc improved from 0.75289 to 0.75673, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5378 - acc: 0.7428 - val_loss: 0.5436 - val_acc: 0.7567\n",
      "Epoch 15/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7424\n",
      "Epoch 15: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5450 - acc: 0.7425 - val_loss: 0.5324 - val_acc: 0.7550\n",
      "Epoch 16/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7476\n",
      "Epoch 16: val_acc improved from 0.75673 to 0.76144, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5332 - acc: 0.7462 - val_loss: 0.5433 - val_acc: 0.7614\n",
      "Epoch 17/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7441\n",
      "Epoch 17: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5399 - acc: 0.7442 - val_loss: 0.5127 - val_acc: 0.7597\n",
      "Epoch 18/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7477\n",
      "Epoch 18: val_acc did not improve from 0.76144\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5397 - acc: 0.7469 - val_loss: 0.5241 - val_acc: 0.7610\n",
      "Epoch 19/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7528\n",
      "Epoch 19: val_acc improved from 0.76144 to 0.76400, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5286 - acc: 0.7520 - val_loss: 0.5235 - val_acc: 0.7640\n",
      "Epoch 20/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7534\n",
      "Epoch 20: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5238 - acc: 0.7521 - val_loss: 0.5415 - val_acc: 0.7584\n",
      "Epoch 21/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5310 - acc: 0.7518\n",
      "Epoch 21: val_acc did not improve from 0.76400\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.7517 - val_loss: 0.5538 - val_acc: 0.7542\n",
      "Epoch 22/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7545\n",
      "Epoch 22: val_acc improved from 0.76400 to 0.76528, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5311 - acc: 0.7547 - val_loss: 0.5390 - val_acc: 0.7653\n",
      "Epoch 23/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.7564\n",
      "Epoch 23: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5229 - acc: 0.7537 - val_loss: 0.5075 - val_acc: 0.7512\n",
      "Epoch 24/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7559\n",
      "Epoch 24: val_acc improved from 0.76528 to 0.76614, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5362 - acc: 0.7562 - val_loss: 0.5415 - val_acc: 0.7661\n",
      "Epoch 25/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.7582\n",
      "Epoch 25: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5205 - acc: 0.7583 - val_loss: 0.5353 - val_acc: 0.7631\n",
      "Epoch 26/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7563\n",
      "Epoch 26: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5170 - acc: 0.7563 - val_loss: 0.5134 - val_acc: 0.7653\n",
      "Epoch 27/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7586\n",
      "Epoch 27: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5233 - acc: 0.7587 - val_loss: 0.5402 - val_acc: 0.7631\n",
      "Epoch 28/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.7592\n",
      "Epoch 28: val_acc improved from 0.76614 to 0.76999, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5176 - acc: 0.7592 - val_loss: 0.5403 - val_acc: 0.7700\n",
      "Epoch 29/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7611\n",
      "Epoch 29: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5195 - acc: 0.7607 - val_loss: 0.5102 - val_acc: 0.7606\n",
      "Epoch 30/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7587\n",
      "Epoch 30: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5173 - acc: 0.7590 - val_loss: 0.5314 - val_acc: 0.7559\n",
      "Epoch 31/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5313 - acc: 0.7545\n",
      "Epoch 31: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5313 - acc: 0.7545 - val_loss: 0.5058 - val_acc: 0.7670\n",
      "Epoch 32/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7638\n",
      "Epoch 32: val_acc improved from 0.76999 to 0.77512, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5265 - acc: 0.7635 - val_loss: 0.5196 - val_acc: 0.7751\n",
      "Epoch 33/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.7621\n",
      "Epoch 33: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5252 - acc: 0.7626 - val_loss: 0.5067 - val_acc: 0.7606\n",
      "Epoch 34/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7617\n",
      "Epoch 34: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5212 - acc: 0.7617 - val_loss: 0.5179 - val_acc: 0.7589\n",
      "Epoch 35/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7571\n",
      "Epoch 35: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5160 - acc: 0.7571 - val_loss: 0.5288 - val_acc: 0.7674\n",
      "Epoch 36/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.7674\n",
      "Epoch 36: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5132 - acc: 0.7669 - val_loss: 0.5242 - val_acc: 0.7627\n",
      "Epoch 37/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5205 - acc: 0.7601\n",
      "Epoch 37: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5205 - acc: 0.7601 - val_loss: 0.5337 - val_acc: 0.7674\n",
      "Epoch 38/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5125 - acc: 0.7639\n",
      "Epoch 38: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5125 - acc: 0.7639 - val_loss: 0.5144 - val_acc: 0.7674\n",
      "Epoch 39/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.7610\n",
      "Epoch 39: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5150 - acc: 0.7606 - val_loss: 0.5203 - val_acc: 0.7589\n",
      "Epoch 40/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.7605\n",
      "Epoch 40: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.7603 - val_loss: 0.5100 - val_acc: 0.7627\n",
      "Epoch 41/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.7583\n",
      "Epoch 41: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5157 - acc: 0.7564 - val_loss: 0.5091 - val_acc: 0.7563\n",
      "Epoch 42/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.7660\n",
      "Epoch 42: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5207 - acc: 0.7645 - val_loss: 0.5248 - val_acc: 0.7751\n",
      "Epoch 43/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.7666\n",
      "Epoch 43: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5170 - acc: 0.7653 - val_loss: 0.5240 - val_acc: 0.7576\n",
      "Epoch 44/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.7674\n",
      "Epoch 44: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5222 - acc: 0.7673 - val_loss: 0.5258 - val_acc: 0.7708\n",
      "Epoch 45/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5255 - acc: 0.7652\n",
      "Epoch 45: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5255 - acc: 0.7652 - val_loss: 0.5216 - val_acc: 0.7597\n",
      "Epoch 46/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5102 - acc: 0.7660\n",
      "Epoch 46: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5102 - acc: 0.7660 - val_loss: 0.5320 - val_acc: 0.7610\n",
      "Epoch 47/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7652\n",
      "Epoch 47: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5233 - acc: 0.7655 - val_loss: 0.5140 - val_acc: 0.7623\n",
      "Epoch 48/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7654\n",
      "Epoch 48: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5206 - acc: 0.7655 - val_loss: 0.5127 - val_acc: 0.7717\n",
      "Epoch 49/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.7743\n",
      "Epoch 49: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5097 - acc: 0.7743 - val_loss: 0.5154 - val_acc: 0.7606\n",
      "Epoch 50/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.7713\n",
      "Epoch 50: val_acc improved from 0.77512 to 0.77597, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5213 - acc: 0.7699 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 51/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7696\n",
      "Epoch 51: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5363 - acc: 0.7692 - val_loss: 0.5126 - val_acc: 0.7593\n",
      "Epoch 52/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7707\n",
      "Epoch 52: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5250 - acc: 0.7691 - val_loss: 0.5137 - val_acc: 0.7602\n",
      "Epoch 53/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5138 - acc: 0.7699\n",
      "Epoch 53: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5138 - acc: 0.7699 - val_loss: 0.5242 - val_acc: 0.7533\n",
      "Epoch 54/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7700\n",
      "Epoch 54: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5164 - acc: 0.7693 - val_loss: 0.5126 - val_acc: 0.7691\n",
      "Epoch 55/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.7695\n",
      "Epoch 55: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5140 - acc: 0.7691 - val_loss: 0.5032 - val_acc: 0.7760\n",
      "Epoch 56/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.7658\n",
      "Epoch 56: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5153 - acc: 0.7647 - val_loss: 0.5091 - val_acc: 0.7619\n",
      "Epoch 57/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5173 - acc: 0.7683\n",
      "Epoch 57: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5170 - acc: 0.7684 - val_loss: 0.5120 - val_acc: 0.7636\n",
      "Epoch 58/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5170 - acc: 0.7649\n",
      "Epoch 58: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5170 - acc: 0.7649 - val_loss: 0.5266 - val_acc: 0.7640\n",
      "Epoch 59/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7706\n",
      "Epoch 59: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5185 - acc: 0.7699 - val_loss: 0.4990 - val_acc: 0.7619\n",
      "Epoch 60/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5233 - acc: 0.7705\n",
      "Epoch 60: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5233 - acc: 0.7705 - val_loss: 0.5171 - val_acc: 0.7542\n",
      "Epoch 61/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.7677\n",
      "Epoch 61: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5212 - acc: 0.7673 - val_loss: 0.5008 - val_acc: 0.7755\n",
      "Epoch 62/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.7719\n",
      "Epoch 62: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5114 - acc: 0.7709 - val_loss: 0.5221 - val_acc: 0.7623\n",
      "Epoch 63/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7787\n",
      "Epoch 63: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7787 - val_loss: 0.5054 - val_acc: 0.7700\n",
      "Epoch 64/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7699\n",
      "Epoch 64: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5355 - acc: 0.7703 - val_loss: 0.5225 - val_acc: 0.7584\n",
      "Epoch 65/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5201 - acc: 0.7732\n",
      "Epoch 65: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5201 - acc: 0.7732 - val_loss: 0.5127 - val_acc: 0.7700\n",
      "Epoch 66/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7767\n",
      "Epoch 66: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5259 - acc: 0.7761 - val_loss: 0.5218 - val_acc: 0.7704\n",
      "Epoch 67/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5513 - acc: 0.7678\n",
      "Epoch 67: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5513 - acc: 0.7678 - val_loss: 0.5144 - val_acc: 0.7661\n",
      "Epoch 68/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5087 - acc: 0.7740\n",
      "Epoch 68: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5087 - acc: 0.7740 - val_loss: 0.5144 - val_acc: 0.7649\n",
      "Epoch 69/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7747\n",
      "Epoch 69: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5285 - acc: 0.7747 - val_loss: 0.5002 - val_acc: 0.7704\n",
      "Epoch 70/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.7737\n",
      "Epoch 70: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5202 - acc: 0.7737 - val_loss: 0.5076 - val_acc: 0.7602\n",
      "Epoch 71/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5125 - acc: 0.7760\n",
      "Epoch 71: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5112 - acc: 0.7756 - val_loss: 0.5079 - val_acc: 0.7691\n",
      "Epoch 72/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.7750\n",
      "Epoch 72: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5286 - acc: 0.7755 - val_loss: 0.5141 - val_acc: 0.7661\n",
      "Epoch 73/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5254 - acc: 0.7709\n",
      "Epoch 73: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5254 - acc: 0.7709 - val_loss: 0.4937 - val_acc: 0.7730\n",
      "Epoch 74/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.7743\n",
      "Epoch 74: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5285 - acc: 0.7742 - val_loss: 0.5070 - val_acc: 0.7644\n",
      "Epoch 75/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.7823\n",
      "Epoch 75: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5040 - acc: 0.7823 - val_loss: 0.5035 - val_acc: 0.7584\n",
      "Epoch 76/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7725\n",
      "Epoch 76: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5224 - acc: 0.7725 - val_loss: 0.4936 - val_acc: 0.7691\n",
      "Epoch 77/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.7793\n",
      "Epoch 77: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5444 - acc: 0.7793 - val_loss: 0.5181 - val_acc: 0.7606\n",
      "Epoch 78/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5019 - acc: 0.7796\n",
      "Epoch 78: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5019 - acc: 0.7796 - val_loss: 0.5057 - val_acc: 0.7674\n",
      "Epoch 79/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5166 - acc: 0.7806\n",
      "Epoch 79: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5156 - acc: 0.7810 - val_loss: 0.5008 - val_acc: 0.7687\n",
      "Epoch 80/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5321 - acc: 0.7779\n",
      "Epoch 80: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5321 - acc: 0.7779 - val_loss: 0.5032 - val_acc: 0.7657\n",
      "Epoch 81/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.7709\n",
      "Epoch 81: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5458 - acc: 0.7711 - val_loss: 0.5053 - val_acc: 0.7661\n",
      "Epoch 82/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5225 - acc: 0.7738\n",
      "Epoch 82: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5225 - acc: 0.7738 - val_loss: 0.5114 - val_acc: 0.7614\n",
      "Epoch 83/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.7762\n",
      "Epoch 83: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5118 - acc: 0.7743 - val_loss: 0.5150 - val_acc: 0.7661\n",
      "Epoch 84/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7794\n",
      "Epoch 84: val_acc improved from 0.77597 to 0.78025, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5164 - acc: 0.7779 - val_loss: 0.5046 - val_acc: 0.7802\n",
      "Epoch 85/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7755\n",
      "Epoch 85: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5313 - acc: 0.7753 - val_loss: 0.5098 - val_acc: 0.7627\n",
      "Epoch 86/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5294 - acc: 0.7779\n",
      "Epoch 86: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5294 - acc: 0.7779 - val_loss: 0.5044 - val_acc: 0.7584\n",
      "Epoch 87/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7793\n",
      "Epoch 87: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5429 - acc: 0.7788 - val_loss: 0.5052 - val_acc: 0.7691\n",
      "Epoch 88/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7795\n",
      "Epoch 88: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5325 - acc: 0.7786 - val_loss: 0.5066 - val_acc: 0.7687\n",
      "Epoch 89/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7715\n",
      "Epoch 89: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5347 - acc: 0.7721 - val_loss: 0.4993 - val_acc: 0.7674\n",
      "Epoch 90/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.7826\n",
      "Epoch 90: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5097 - acc: 0.7827 - val_loss: 0.5167 - val_acc: 0.7760\n",
      "Epoch 91/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.7805\n",
      "Epoch 91: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5129 - acc: 0.7805 - val_loss: 0.5067 - val_acc: 0.7717\n",
      "Epoch 92/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5215 - acc: 0.7759\n",
      "Epoch 92: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5215 - acc: 0.7759 - val_loss: 0.5146 - val_acc: 0.7743\n",
      "Epoch 93/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.7774\n",
      "Epoch 93: val_acc did not improve from 0.78025\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5353 - acc: 0.7777 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 94/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7794\n",
      "Epoch 94: val_acc improved from 0.78025 to 0.79008, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5238 - acc: 0.7796 - val_loss: 0.4819 - val_acc: 0.7901\n",
      "Epoch 95/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7761\n",
      "Epoch 95: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5173 - acc: 0.7763 - val_loss: 0.4966 - val_acc: 0.7730\n",
      "Epoch 96/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7726\n",
      "Epoch 96: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5142 - acc: 0.7722 - val_loss: 0.4998 - val_acc: 0.7619\n",
      "Epoch 97/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5209 - acc: 0.7811\n",
      "Epoch 97: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5209 - acc: 0.7811 - val_loss: 0.4954 - val_acc: 0.7743\n",
      "Epoch 98/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5148 - acc: 0.7789\n",
      "Epoch 98: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5148 - acc: 0.7789 - val_loss: 0.4953 - val_acc: 0.7730\n",
      "Epoch 99/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5212 - acc: 0.7737\n",
      "Epoch 99: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5212 - acc: 0.7737 - val_loss: 0.5040 - val_acc: 0.7610\n",
      "Epoch 100/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.7841\n",
      "Epoch 100: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5152 - acc: 0.7828 - val_loss: 0.5039 - val_acc: 0.7751\n",
      "Epoch 101/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.7802\n",
      "Epoch 101: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5040 - acc: 0.7801 - val_loss: 0.5088 - val_acc: 0.7555\n",
      "Epoch 102/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7837\n",
      "Epoch 102: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5092 - acc: 0.7829 - val_loss: 0.5335 - val_acc: 0.7790\n",
      "Epoch 103/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5391 - acc: 0.7862\n",
      "Epoch 103: val_acc did not improve from 0.79008\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5391 - acc: 0.7862 - val_loss: 0.4999 - val_acc: 0.7687\n",
      "Epoch 104/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5391 - acc: 0.7888\n",
      "Epoch 104: val_acc improved from 0.79008 to 0.79478, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5391 - acc: 0.7888 - val_loss: 0.4870 - val_acc: 0.7948\n",
      "Epoch 105/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5522 - acc: 0.7794\n",
      "Epoch 105: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5522 - acc: 0.7794 - val_loss: 0.4883 - val_acc: 0.7811\n",
      "Epoch 106/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.7840\n",
      "Epoch 106: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5000 - acc: 0.7845 - val_loss: 0.5037 - val_acc: 0.7858\n",
      "Epoch 107/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.7835\n",
      "Epoch 107: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5137 - acc: 0.7835 - val_loss: 0.4783 - val_acc: 0.7798\n",
      "Epoch 108/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5030 - acc: 0.7811\n",
      "Epoch 108: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5018 - acc: 0.7801 - val_loss: 0.5031 - val_acc: 0.7755\n",
      "Epoch 109/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7877\n",
      "Epoch 109: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5167 - acc: 0.7879 - val_loss: 0.4965 - val_acc: 0.7768\n",
      "Epoch 110/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7930\n",
      "Epoch 110: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5353 - acc: 0.7917 - val_loss: 0.4906 - val_acc: 0.7674\n",
      "Epoch 111/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5196 - acc: 0.7879\n",
      "Epoch 111: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5196 - acc: 0.7879 - val_loss: 0.5024 - val_acc: 0.7905\n",
      "Epoch 112/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7853\n",
      "Epoch 112: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5077 - acc: 0.7854 - val_loss: 0.5136 - val_acc: 0.7597\n",
      "Epoch 113/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7844\n",
      "Epoch 113: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5101 - acc: 0.7845 - val_loss: 0.4898 - val_acc: 0.7760\n",
      "Epoch 114/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.7866\n",
      "Epoch 114: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5089 - acc: 0.7864 - val_loss: 0.5028 - val_acc: 0.7704\n",
      "Epoch 115/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5385 - acc: 0.7820\n",
      "Epoch 115: val_acc did not improve from 0.79478\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7820 - val_loss: 0.4921 - val_acc: 0.7858\n",
      "Epoch 116/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5030 - acc: 0.7845\n",
      "Epoch 116: val_acc improved from 0.79478 to 0.79521, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5026 - acc: 0.7847 - val_loss: 0.4920 - val_acc: 0.7952\n",
      "Epoch 117/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.7855\n",
      "Epoch 117: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5203 - acc: 0.7857 - val_loss: 0.4886 - val_acc: 0.7914\n",
      "Epoch 118/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4944 - acc: 0.7861\n",
      "Epoch 118: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4930 - acc: 0.7856 - val_loss: 0.4967 - val_acc: 0.7815\n",
      "Epoch 119/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7900\n",
      "Epoch 119: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5220 - acc: 0.7886 - val_loss: 0.5019 - val_acc: 0.7657\n",
      "Epoch 120/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5312 - acc: 0.7810\n",
      "Epoch 120: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5312 - acc: 0.7810 - val_loss: 0.4990 - val_acc: 0.7832\n",
      "Epoch 121/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5349 - acc: 0.7832\n",
      "Epoch 121: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5349 - acc: 0.7832 - val_loss: 0.5245 - val_acc: 0.7785\n",
      "Epoch 122/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5292 - acc: 0.7886\n",
      "Epoch 122: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5292 - acc: 0.7886 - val_loss: 0.4782 - val_acc: 0.7798\n",
      "Epoch 123/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7860\n",
      "Epoch 123: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5066 - acc: 0.7860 - val_loss: 0.4854 - val_acc: 0.7653\n",
      "Epoch 124/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.7875\n",
      "Epoch 124: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4944 - acc: 0.7874 - val_loss: 0.4941 - val_acc: 0.7841\n",
      "Epoch 125/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5071 - acc: 0.7894\n",
      "Epoch 125: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5071 - acc: 0.7894 - val_loss: 0.4985 - val_acc: 0.7828\n",
      "Epoch 126/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5038 - acc: 0.7903\n",
      "Epoch 126: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5029 - acc: 0.7905 - val_loss: 0.5024 - val_acc: 0.7892\n",
      "Epoch 127/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7854\n",
      "Epoch 127: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5344 - acc: 0.7856 - val_loss: 0.4997 - val_acc: 0.7914\n",
      "Epoch 128/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.7872\n",
      "Epoch 128: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5521 - acc: 0.7873 - val_loss: 0.4806 - val_acc: 0.7747\n",
      "Epoch 129/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7902\n",
      "Epoch 129: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5185 - acc: 0.7892 - val_loss: 0.4878 - val_acc: 0.7708\n",
      "Epoch 130/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7884\n",
      "Epoch 130: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5310 - acc: 0.7887 - val_loss: 0.4882 - val_acc: 0.7661\n",
      "Epoch 131/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5024 - acc: 0.7858\n",
      "Epoch 131: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5024 - acc: 0.7858 - val_loss: 0.4933 - val_acc: 0.7747\n",
      "Epoch 132/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7892\n",
      "Epoch 132: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5198 - acc: 0.7882 - val_loss: 0.4933 - val_acc: 0.7751\n",
      "Epoch 133/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.7853\n",
      "Epoch 133: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5127 - acc: 0.7856 - val_loss: 0.5068 - val_acc: 0.7730\n",
      "Epoch 134/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.7821\n",
      "Epoch 134: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5103 - acc: 0.7824 - val_loss: 0.4962 - val_acc: 0.7674\n",
      "Epoch 135/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5144 - acc: 0.7914\n",
      "Epoch 135: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5144 - acc: 0.7914 - val_loss: 0.5088 - val_acc: 0.7670\n",
      "Epoch 136/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.7878\n",
      "Epoch 136: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5097 - acc: 0.7863 - val_loss: 0.5088 - val_acc: 0.7631\n",
      "Epoch 137/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7924\n",
      "Epoch 137: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5301 - acc: 0.7923 - val_loss: 0.5049 - val_acc: 0.7815\n",
      "Epoch 138/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7862\n",
      "Epoch 138: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7856 - val_loss: 0.4970 - val_acc: 0.7721\n",
      "Epoch 139/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6026 - acc: 0.7798\n",
      "Epoch 139: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6003 - acc: 0.7799 - val_loss: 0.5135 - val_acc: 0.7708\n",
      "Epoch 140/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7891\n",
      "Epoch 140: val_acc improved from 0.79521 to 0.81445, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5425 - acc: 0.7894 - val_loss: 0.4833 - val_acc: 0.8145\n",
      "Epoch 141/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7844\n",
      "Epoch 141: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5210 - acc: 0.7845 - val_loss: 0.4820 - val_acc: 0.7764\n",
      "Epoch 142/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7916\n",
      "Epoch 142: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5181 - acc: 0.7901 - val_loss: 0.4986 - val_acc: 0.7614\n",
      "Epoch 143/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7866\n",
      "Epoch 143: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5238 - acc: 0.7865 - val_loss: 0.5046 - val_acc: 0.7696\n",
      "Epoch 144/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4973 - acc: 0.7869\n",
      "Epoch 144: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4973 - acc: 0.7869 - val_loss: 0.4993 - val_acc: 0.7738\n",
      "Epoch 145/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.7802\n",
      "Epoch 145: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5292 - acc: 0.7800 - val_loss: 0.4892 - val_acc: 0.7726\n",
      "Epoch 146/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7904\n",
      "Epoch 146: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5164 - acc: 0.7908 - val_loss: 0.4978 - val_acc: 0.7751\n",
      "Epoch 147/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7826\n",
      "Epoch 147: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5188 - acc: 0.7828 - val_loss: 0.5081 - val_acc: 0.7717\n",
      "Epoch 148/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.7957\n",
      "Epoch 148: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5487 - acc: 0.7957 - val_loss: 0.4917 - val_acc: 0.7696\n",
      "Epoch 149/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7887\n",
      "Epoch 149: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7888 - val_loss: 0.4831 - val_acc: 0.7760\n",
      "Epoch 150/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7883\n",
      "Epoch 150: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7885 - val_loss: 0.4866 - val_acc: 0.7708\n",
      "Epoch 151/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5120 - acc: 0.7917\n",
      "Epoch 151: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5120 - acc: 0.7917 - val_loss: 0.4940 - val_acc: 0.7713\n",
      "Epoch 152/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.7906\n",
      "Epoch 152: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5064 - acc: 0.7888 - val_loss: 0.4955 - val_acc: 0.7708\n",
      "Epoch 153/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.7944\n",
      "Epoch 153: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5055 - acc: 0.7923 - val_loss: 0.4992 - val_acc: 0.7785\n",
      "Epoch 154/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7967\n",
      "Epoch 154: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5149 - acc: 0.7966 - val_loss: 0.4863 - val_acc: 0.7986\n",
      "Epoch 155/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.7917\n",
      "Epoch 155: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5087 - acc: 0.7918 - val_loss: 0.4934 - val_acc: 0.7743\n",
      "Epoch 156/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.7934\n",
      "Epoch 156: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4938 - acc: 0.7931 - val_loss: 0.4762 - val_acc: 0.7952\n",
      "Epoch 157/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5108 - acc: 0.7893\n",
      "Epoch 157: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5093 - acc: 0.7896 - val_loss: 0.4955 - val_acc: 0.7879\n",
      "Epoch 158/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5343 - acc: 0.7927\n",
      "Epoch 158: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5343 - acc: 0.7927 - val_loss: 0.4988 - val_acc: 0.7777\n",
      "Epoch 159/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.7923\n",
      "Epoch 159: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5218 - acc: 0.7924 - val_loss: 0.4733 - val_acc: 0.7978\n",
      "Epoch 160/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.7932\n",
      "Epoch 160: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5024 - acc: 0.7929 - val_loss: 0.4938 - val_acc: 0.7790\n",
      "Epoch 161/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7892\n",
      "Epoch 161: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5364 - acc: 0.7882 - val_loss: 0.4837 - val_acc: 0.7683\n",
      "Epoch 162/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7936\n",
      "Epoch 162: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5232 - acc: 0.7926 - val_loss: 0.4678 - val_acc: 0.7931\n",
      "Epoch 163/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5199 - acc: 0.7923\n",
      "Epoch 163: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5199 - acc: 0.7923 - val_loss: 0.4871 - val_acc: 0.7854\n",
      "Epoch 164/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.7909\n",
      "Epoch 164: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5046 - acc: 0.7912 - val_loss: 0.4849 - val_acc: 0.7820\n",
      "Epoch 165/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5284 - acc: 0.7957\n",
      "Epoch 165: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5284 - acc: 0.7957 - val_loss: 0.4780 - val_acc: 0.7926\n",
      "Epoch 166/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4809 - acc: 0.7898\n",
      "Epoch 166: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4810 - acc: 0.7890 - val_loss: 0.4882 - val_acc: 0.7850\n",
      "Epoch 167/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.7941\n",
      "Epoch 167: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5116 - acc: 0.7940 - val_loss: 0.4931 - val_acc: 0.7751\n",
      "Epoch 168/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5193 - acc: 0.7964\n",
      "Epoch 168: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5193 - acc: 0.7964 - val_loss: 0.4908 - val_acc: 0.7841\n",
      "Epoch 169/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5151 - acc: 0.7911\n",
      "Epoch 169: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5151 - acc: 0.7911 - val_loss: 0.5097 - val_acc: 0.7619\n",
      "Epoch 170/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7979\n",
      "Epoch 170: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5489 - acc: 0.7978 - val_loss: 0.4747 - val_acc: 0.7914\n",
      "Epoch 171/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7924\n",
      "Epoch 171: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5296 - acc: 0.7929 - val_loss: 0.4689 - val_acc: 0.7875\n",
      "Epoch 172/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5169 - acc: 0.7913\n",
      "Epoch 172: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5169 - acc: 0.7913 - val_loss: 0.4663 - val_acc: 0.7909\n",
      "Epoch 173/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5406 - acc: 0.7963\n",
      "Epoch 173: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5406 - acc: 0.7963 - val_loss: 0.4735 - val_acc: 0.7879\n",
      "Epoch 174/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4996 - acc: 0.7990\n",
      "Epoch 174: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4986 - acc: 0.7993 - val_loss: 0.4757 - val_acc: 0.7790\n",
      "Epoch 175/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7942\n",
      "Epoch 175: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5125 - acc: 0.7943 - val_loss: 0.4762 - val_acc: 0.7935\n",
      "Epoch 176/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7954\n",
      "Epoch 176: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5247 - acc: 0.7955 - val_loss: 0.4723 - val_acc: 0.7909\n",
      "Epoch 177/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.7994\n",
      "Epoch 177: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5007 - acc: 0.7999 - val_loss: 0.4852 - val_acc: 0.8003\n",
      "Epoch 178/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.7964\n",
      "Epoch 178: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4954 - acc: 0.7968 - val_loss: 0.4728 - val_acc: 0.8063\n",
      "Epoch 179/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7924\n",
      "Epoch 179: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5177 - acc: 0.7926 - val_loss: 0.4880 - val_acc: 0.7931\n",
      "Epoch 180/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7949\n",
      "Epoch 180: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5364 - acc: 0.7949 - val_loss: 0.4816 - val_acc: 0.7815\n",
      "Epoch 181/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7920\n",
      "Epoch 181: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5251 - acc: 0.7921 - val_loss: 0.4799 - val_acc: 0.7820\n",
      "Epoch 182/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.7935\n",
      "Epoch 182: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4943 - acc: 0.7920 - val_loss: 0.4889 - val_acc: 0.7956\n",
      "Epoch 183/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5056 - acc: 0.7958\n",
      "Epoch 183: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5056 - acc: 0.7958 - val_loss: 0.4726 - val_acc: 0.7777\n",
      "Epoch 184/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4886 - acc: 0.7925\n",
      "Epoch 184: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4878 - acc: 0.7926 - val_loss: 0.4967 - val_acc: 0.7781\n",
      "Epoch 185/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7970\n",
      "Epoch 185: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5286 - acc: 0.7973 - val_loss: 0.4831 - val_acc: 0.7798\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.7933\n",
      "Epoch 186: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5315 - acc: 0.7934 - val_loss: 0.4845 - val_acc: 0.7888\n",
      "Epoch 187/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5352 - acc: 0.7944\n",
      "Epoch 187: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5352 - acc: 0.7944 - val_loss: 0.4791 - val_acc: 0.7687\n",
      "Epoch 188/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.7939\n",
      "Epoch 188: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4922 - acc: 0.7939 - val_loss: 0.4849 - val_acc: 0.7790\n",
      "Epoch 189/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7962\n",
      "Epoch 189: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5174 - acc: 0.7964 - val_loss: 0.4899 - val_acc: 0.7892\n",
      "Epoch 190/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5056 - acc: 0.7921\n",
      "Epoch 190: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5056 - acc: 0.7921 - val_loss: 0.4735 - val_acc: 0.7802\n",
      "Epoch 191/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7928\n",
      "Epoch 191: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5482 - acc: 0.7929 - val_loss: 0.4835 - val_acc: 0.7918\n",
      "Epoch 192/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.7962\n",
      "Epoch 192: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5033 - acc: 0.7959 - val_loss: 0.4744 - val_acc: 0.7969\n",
      "Epoch 193/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7962\n",
      "Epoch 193: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5160 - acc: 0.7953 - val_loss: 0.4803 - val_acc: 0.7875\n",
      "Epoch 194/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.8033\n",
      "Epoch 194: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.8034 - val_loss: 0.4811 - val_acc: 0.7845\n",
      "Epoch 195/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.7919\n",
      "Epoch 195: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5088 - acc: 0.7919 - val_loss: 0.4826 - val_acc: 0.7773\n",
      "Epoch 196/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.7990\n",
      "Epoch 196: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4898 - acc: 0.7989 - val_loss: 0.4708 - val_acc: 0.7897\n",
      "Epoch 197/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.7935\n",
      "Epoch 197: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5195 - acc: 0.7937 - val_loss: 0.4798 - val_acc: 0.7944\n",
      "Epoch 198/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.7955\n",
      "Epoch 198: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5083 - acc: 0.7954 - val_loss: 0.4591 - val_acc: 0.8033\n",
      "Epoch 199/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7969\n",
      "Epoch 199: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5358 - acc: 0.7965 - val_loss: 0.4860 - val_acc: 0.7704\n",
      "Epoch 200/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.7955\n",
      "Epoch 200: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5097 - acc: 0.7954 - val_loss: 0.4946 - val_acc: 0.7755\n",
      "Epoch 201/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7962\n",
      "Epoch 201: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5254 - acc: 0.7956 - val_loss: 0.4968 - val_acc: 0.7614\n",
      "Epoch 202/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7948\n",
      "Epoch 202: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5430 - acc: 0.7951 - val_loss: 0.4843 - val_acc: 0.7914\n",
      "Epoch 203/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7952\n",
      "Epoch 203: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5493 - acc: 0.7952 - val_loss: 0.4811 - val_acc: 0.7760\n",
      "Epoch 204/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5247 - acc: 0.7957\n",
      "Epoch 204: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5247 - acc: 0.7957 - val_loss: 0.4801 - val_acc: 0.7875\n",
      "Epoch 205/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.7956\n",
      "Epoch 205: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5004 - acc: 0.7957 - val_loss: 0.4827 - val_acc: 0.7841\n",
      "Epoch 206/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.7961\n",
      "Epoch 206: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4941 - acc: 0.7966 - val_loss: 0.4806 - val_acc: 0.8012\n",
      "Epoch 207/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.8038\n",
      "Epoch 207: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5368 - acc: 0.8038 - val_loss: 0.4790 - val_acc: 0.7832\n",
      "Epoch 208/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7971\n",
      "Epoch 208: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5178 - acc: 0.7971 - val_loss: 0.4708 - val_acc: 0.7785\n",
      "Epoch 209/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.7921\n",
      "Epoch 209: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5133 - acc: 0.7924 - val_loss: 0.4933 - val_acc: 0.7824\n",
      "Epoch 210/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5284 - acc: 0.7983\n",
      "Epoch 210: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5284 - acc: 0.7983 - val_loss: 0.4819 - val_acc: 0.7743\n",
      "Epoch 211/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.7991\n",
      "Epoch 211: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4843 - acc: 0.7974 - val_loss: 0.4702 - val_acc: 0.7755\n",
      "Epoch 212/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8001\n",
      "Epoch 212: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5226 - acc: 0.8000 - val_loss: 0.4707 - val_acc: 0.7832\n",
      "Epoch 213/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.8064\n",
      "Epoch 213: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4943 - acc: 0.8061 - val_loss: 0.4659 - val_acc: 0.7986\n",
      "Epoch 214/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8029\n",
      "Epoch 214: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5074 - acc: 0.8021 - val_loss: 0.4848 - val_acc: 0.7956\n",
      "Epoch 215/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5152 - acc: 0.8035\n",
      "Epoch 215: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5152 - acc: 0.8035 - val_loss: 0.4667 - val_acc: 0.7730\n",
      "Epoch 216/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4860 - acc: 0.7990\n",
      "Epoch 216: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4860 - acc: 0.7990 - val_loss: 0.4692 - val_acc: 0.7867\n",
      "Epoch 217/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8017\n",
      "Epoch 217: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4954 - acc: 0.8018 - val_loss: 0.4797 - val_acc: 0.7948\n",
      "Epoch 218/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5137 - acc: 0.8010\n",
      "Epoch 218: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5137 - acc: 0.8010 - val_loss: 0.4845 - val_acc: 0.8063\n",
      "Epoch 219/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5005 - acc: 0.8047\n",
      "Epoch 219: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4998 - acc: 0.8050 - val_loss: 0.4883 - val_acc: 0.7845\n",
      "Epoch 220/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8017\n",
      "Epoch 220: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5199 - acc: 0.8017 - val_loss: 0.4771 - val_acc: 0.7862\n",
      "Epoch 221/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.8014\n",
      "Epoch 221: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5298 - acc: 0.8006 - val_loss: 0.4855 - val_acc: 0.7802\n",
      "Epoch 222/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.7999\n",
      "Epoch 222: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5003 - acc: 0.7999 - val_loss: 0.4840 - val_acc: 0.7708\n",
      "Epoch 223/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8020\n",
      "Epoch 223: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5146 - acc: 0.8024 - val_loss: 0.4648 - val_acc: 0.7956\n",
      "Epoch 224/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8006\n",
      "Epoch 224: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5204 - acc: 0.8002 - val_loss: 0.4713 - val_acc: 0.7961\n",
      "Epoch 225/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.8031\n",
      "Epoch 225: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4731 - acc: 0.8031 - val_loss: 0.4750 - val_acc: 0.7837\n",
      "Epoch 226/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5845 - acc: 0.8041\n",
      "Epoch 226: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5845 - acc: 0.8041 - val_loss: 0.4750 - val_acc: 0.7871\n",
      "Epoch 227/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8012\n",
      "Epoch 227: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5046 - acc: 0.8012 - val_loss: 0.4765 - val_acc: 0.8021\n",
      "Epoch 228/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.8010\n",
      "Epoch 228: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5281 - acc: 0.8011 - val_loss: 0.4642 - val_acc: 0.7982\n",
      "Epoch 229/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.7995\n",
      "Epoch 229: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5118 - acc: 0.7995 - val_loss: 0.4751 - val_acc: 0.7884\n",
      "Epoch 230/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.8064\n",
      "Epoch 230: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5033 - acc: 0.8067 - val_loss: 0.4818 - val_acc: 0.7901\n",
      "Epoch 231/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8009\n",
      "Epoch 231: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4803 - acc: 0.8017 - val_loss: 0.4655 - val_acc: 0.7999\n",
      "Epoch 232/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8053\n",
      "Epoch 232: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5021 - acc: 0.8053 - val_loss: 0.4603 - val_acc: 0.7884\n",
      "Epoch 233/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.8023\n",
      "Epoch 233: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5321 - acc: 0.8018 - val_loss: 0.4653 - val_acc: 0.7926\n",
      "Epoch 234/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.8006\n",
      "Epoch 234: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5783 - acc: 0.8005 - val_loss: 0.4824 - val_acc: 0.7649\n",
      "Epoch 235/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8008\n",
      "Epoch 235: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4814 - acc: 0.8011 - val_loss: 0.4691 - val_acc: 0.8008\n",
      "Epoch 236/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7974\n",
      "Epoch 236: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5342 - acc: 0.7974 - val_loss: 0.4625 - val_acc: 0.7905\n",
      "Epoch 237/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8040\n",
      "Epoch 237: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5064 - acc: 0.8037 - val_loss: 0.4744 - val_acc: 0.7850\n",
      "Epoch 238/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.8045\n",
      "Epoch 238: val_acc did not improve from 0.81445\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5116 - acc: 0.8046 - val_loss: 0.4689 - val_acc: 0.7991\n",
      "Epoch 239/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5093 - acc: 0.8029\n",
      "Epoch 239: val_acc improved from 0.81445 to 0.81659, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5093 - acc: 0.8029 - val_loss: 0.4626 - val_acc: 0.8166\n",
      "Epoch 240/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8037\n",
      "Epoch 240: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5276 - acc: 0.8037 - val_loss: 0.4820 - val_acc: 0.7909\n",
      "Epoch 241/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.7994\n",
      "Epoch 241: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5141 - acc: 0.7982 - val_loss: 0.4712 - val_acc: 0.7717\n",
      "Epoch 242/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4887 - acc: 0.8012\n",
      "Epoch 242: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4884 - acc: 0.8013 - val_loss: 0.4623 - val_acc: 0.8132\n",
      "Epoch 243/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5095 - acc: 0.7996\n",
      "Epoch 243: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5095 - acc: 0.7996 - val_loss: 0.4665 - val_acc: 0.7785\n",
      "Epoch 244/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8060\n",
      "Epoch 244: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5131 - acc: 0.8064 - val_loss: 0.4649 - val_acc: 0.8021\n",
      "Epoch 245/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.8047\n",
      "Epoch 245: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4772 - acc: 0.8046 - val_loss: 0.4800 - val_acc: 0.7926\n",
      "Epoch 246/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.7978\n",
      "Epoch 246: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4918 - acc: 0.7970 - val_loss: 0.4721 - val_acc: 0.7649\n",
      "Epoch 247/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8027\n",
      "Epoch 247: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5198 - acc: 0.8028 - val_loss: 0.4848 - val_acc: 0.7939\n",
      "Epoch 248/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.8098\n",
      "Epoch 248: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5133 - acc: 0.8098 - val_loss: 0.4688 - val_acc: 0.7935\n",
      "Epoch 249/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.8003\n",
      "Epoch 249: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4980 - acc: 0.8004 - val_loss: 0.4828 - val_acc: 0.8016\n",
      "Epoch 250/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.8034\n",
      "Epoch 250: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5692 - acc: 0.8035 - val_loss: 0.4755 - val_acc: 0.7798\n",
      "Epoch 251/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5434 - acc: 0.8021\n",
      "Epoch 251: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5434 - acc: 0.8021 - val_loss: 0.4793 - val_acc: 0.7713\n",
      "Epoch 252/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.8045\n",
      "Epoch 252: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5433 - acc: 0.8049 - val_loss: 0.4834 - val_acc: 0.7871\n",
      "Epoch 253/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8001\n",
      "Epoch 253: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5100 - acc: 0.8003 - val_loss: 0.4715 - val_acc: 0.7926\n",
      "Epoch 254/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.8008\n",
      "Epoch 254: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4795 - acc: 0.8003 - val_loss: 0.4679 - val_acc: 0.7969\n",
      "Epoch 255/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8064\n",
      "Epoch 255: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4944 - acc: 0.8063 - val_loss: 0.4696 - val_acc: 0.7845\n",
      "Epoch 256/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.8028\n",
      "Epoch 256: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5157 - acc: 0.8028 - val_loss: 0.4741 - val_acc: 0.7798\n",
      "Epoch 257/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4903 - acc: 0.8038\n",
      "Epoch 257: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4911 - acc: 0.8041 - val_loss: 0.4646 - val_acc: 0.7978\n",
      "Epoch 258/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8032\n",
      "Epoch 258: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5000 - acc: 0.8031 - val_loss: 0.4736 - val_acc: 0.7978\n",
      "Epoch 259/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5228 - acc: 0.8106\n",
      "Epoch 259: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5228 - acc: 0.8106 - val_loss: 0.4817 - val_acc: 0.8016\n",
      "Epoch 260/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.8023\n",
      "Epoch 260: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5235 - acc: 0.8028 - val_loss: 0.4688 - val_acc: 0.7969\n",
      "Epoch 261/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8040\n",
      "Epoch 261: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5060 - acc: 0.8041 - val_loss: 0.4706 - val_acc: 0.7700\n",
      "Epoch 262/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.8021\n",
      "Epoch 262: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5136 - acc: 0.8020 - val_loss: 0.4847 - val_acc: 0.7854\n",
      "Epoch 263/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4895 - acc: 0.8024\n",
      "Epoch 263: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4897 - acc: 0.8021 - val_loss: 0.4749 - val_acc: 0.7944\n",
      "Epoch 264/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8028\n",
      "Epoch 264: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5109 - acc: 0.8028 - val_loss: 0.4617 - val_acc: 0.7965\n",
      "Epoch 265/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.8070\n",
      "Epoch 265: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4841 - acc: 0.8071 - val_loss: 0.4757 - val_acc: 0.7828\n",
      "Epoch 266/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.8030\n",
      "Epoch 266: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4969 - acc: 0.8027 - val_loss: 0.4625 - val_acc: 0.7850\n",
      "Epoch 267/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5016 - acc: 0.8030\n",
      "Epoch 267: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5016 - acc: 0.8030 - val_loss: 0.4682 - val_acc: 0.7922\n",
      "Epoch 268/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8067\n",
      "Epoch 268: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5061 - acc: 0.8068 - val_loss: 0.4683 - val_acc: 0.7935\n",
      "Epoch 269/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4819 - acc: 0.7999\n",
      "Epoch 269: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4810 - acc: 0.8001 - val_loss: 0.4597 - val_acc: 0.7952\n",
      "Epoch 270/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.8050\n",
      "Epoch 270: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4969 - acc: 0.8047 - val_loss: 0.4718 - val_acc: 0.7914\n",
      "Epoch 271/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.8047\n",
      "Epoch 271: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5101 - acc: 0.8044 - val_loss: 0.4716 - val_acc: 0.7926\n",
      "Epoch 272/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8063\n",
      "Epoch 272: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4874 - acc: 0.8064 - val_loss: 0.4798 - val_acc: 0.7901\n",
      "Epoch 273/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4854 - acc: 0.8101\n",
      "Epoch 273: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4844 - acc: 0.8097 - val_loss: 0.4652 - val_acc: 0.7995\n",
      "Epoch 274/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8061\n",
      "Epoch 274: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5070 - acc: 0.8061 - val_loss: 0.4734 - val_acc: 0.7918\n",
      "Epoch 275/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.8028\n",
      "Epoch 275: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5074 - acc: 0.8026 - val_loss: 0.4825 - val_acc: 0.7811\n",
      "Epoch 276/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8137\n",
      "Epoch 276: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4999 - acc: 0.8131 - val_loss: 0.4794 - val_acc: 0.7884\n",
      "Epoch 277/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.8057\n",
      "Epoch 277: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4894 - acc: 0.8059 - val_loss: 0.4948 - val_acc: 0.7931\n",
      "Epoch 278/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.8058\n",
      "Epoch 278: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5026 - acc: 0.8057 - val_loss: 0.4748 - val_acc: 0.7901\n",
      "Epoch 279/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.8033\n",
      "Epoch 279: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5069 - acc: 0.8035 - val_loss: 0.4785 - val_acc: 0.7892\n",
      "Epoch 280/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.8033\n",
      "Epoch 280: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5184 - acc: 0.8028 - val_loss: 0.4706 - val_acc: 0.7768\n",
      "Epoch 281/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8009\n",
      "Epoch 281: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5219 - acc: 0.8009 - val_loss: 0.4842 - val_acc: 0.7820\n",
      "Epoch 282/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.8050\n",
      "Epoch 282: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5487 - acc: 0.8050 - val_loss: 0.4828 - val_acc: 0.7956\n",
      "Epoch 283/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8059\n",
      "Epoch 283: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5192 - acc: 0.8060 - val_loss: 0.4598 - val_acc: 0.7965\n",
      "Epoch 284/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8064\n",
      "Epoch 284: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4988 - acc: 0.8065 - val_loss: 0.4707 - val_acc: 0.7811\n",
      "Epoch 285/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8014\n",
      "Epoch 285: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5060 - acc: 0.8016 - val_loss: 0.4709 - val_acc: 0.7931\n",
      "Epoch 286/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.8044\n",
      "Epoch 286: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5164 - acc: 0.8044 - val_loss: 0.4779 - val_acc: 0.7982\n",
      "Epoch 287/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4903 - acc: 0.7977\n",
      "Epoch 287: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4881 - acc: 0.7985 - val_loss: 0.4673 - val_acc: 0.7965\n",
      "Epoch 288/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8056\n",
      "Epoch 288: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5034 - acc: 0.8059 - val_loss: 0.4638 - val_acc: 0.7973\n",
      "Epoch 289/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8100\n",
      "Epoch 289: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4874 - acc: 0.8099 - val_loss: 0.4796 - val_acc: 0.7837\n",
      "Epoch 290/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8036\n",
      "Epoch 290: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.8040 - val_loss: 0.4689 - val_acc: 0.8068\n",
      "Epoch 291/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.8087\n",
      "Epoch 291: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5323 - acc: 0.8091 - val_loss: 0.4617 - val_acc: 0.8025\n",
      "Epoch 292/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.8110\n",
      "Epoch 292: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4676 - acc: 0.8115 - val_loss: 0.4627 - val_acc: 0.8029\n",
      "Epoch 293/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8094\n",
      "Epoch 293: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5260 - acc: 0.8091 - val_loss: 0.4779 - val_acc: 0.7897\n",
      "Epoch 294/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4780 - acc: 0.8055\n",
      "Epoch 294: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4770 - acc: 0.8056 - val_loss: 0.4575 - val_acc: 0.8132\n",
      "Epoch 295/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8125\n",
      "Epoch 295: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5051 - acc: 0.8110 - val_loss: 0.4768 - val_acc: 0.7768\n",
      "Epoch 296/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8127\n",
      "Epoch 296: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4740 - acc: 0.8126 - val_loss: 0.4661 - val_acc: 0.7931\n",
      "Epoch 297/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8121\n",
      "Epoch 297: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5012 - acc: 0.8120 - val_loss: 0.4620 - val_acc: 0.7841\n",
      "Epoch 298/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8014\n",
      "Epoch 298: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4930 - acc: 0.8015 - val_loss: 0.4766 - val_acc: 0.7785\n",
      "Epoch 299/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.8039\n",
      "Epoch 299: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5171 - acc: 0.8041 - val_loss: 0.4818 - val_acc: 0.7700\n",
      "Epoch 300/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.8054\n",
      "Epoch 300: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5919 - acc: 0.8046 - val_loss: 0.4662 - val_acc: 0.7888\n",
      "Epoch 301/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8082\n",
      "Epoch 301: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4740 - acc: 0.8080 - val_loss: 0.4758 - val_acc: 0.7811\n",
      "Epoch 302/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8055\n",
      "Epoch 302: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4821 - acc: 0.8058 - val_loss: 0.4730 - val_acc: 0.7768\n",
      "Epoch 303/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8079\n",
      "Epoch 303: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5461 - acc: 0.8078 - val_loss: 0.4715 - val_acc: 0.7875\n",
      "Epoch 304/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8071\n",
      "Epoch 304: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4986 - acc: 0.8072 - val_loss: 0.4663 - val_acc: 0.7704\n",
      "Epoch 305/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.8050\n",
      "Epoch 305: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5136 - acc: 0.8049 - val_loss: 0.4722 - val_acc: 0.7820\n",
      "Epoch 306/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4978 - acc: 0.8069\n",
      "Epoch 306: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4966 - acc: 0.8068 - val_loss: 0.4723 - val_acc: 0.7905\n",
      "Epoch 307/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8057\n",
      "Epoch 307: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5172 - acc: 0.8055 - val_loss: 0.4867 - val_acc: 0.7871\n",
      "Epoch 308/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5004 - acc: 0.8048\n",
      "Epoch 308: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4982 - acc: 0.8045 - val_loss: 0.4662 - val_acc: 0.7871\n",
      "Epoch 309/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.8092\n",
      "Epoch 309: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5237 - acc: 0.8087 - val_loss: 0.4589 - val_acc: 0.7832\n",
      "Epoch 310/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8078\n",
      "Epoch 310: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4839 - acc: 0.8063 - val_loss: 0.4569 - val_acc: 0.7768\n",
      "Epoch 311/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.8070\n",
      "Epoch 311: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5492 - acc: 0.8066 - val_loss: 0.4592 - val_acc: 0.7867\n",
      "Epoch 312/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8104\n",
      "Epoch 312: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5073 - acc: 0.8107 - val_loss: 0.4644 - val_acc: 0.7973\n",
      "Epoch 313/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.8091\n",
      "Epoch 313: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4865 - acc: 0.8088 - val_loss: 0.4634 - val_acc: 0.7965\n",
      "Epoch 314/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8053\n",
      "Epoch 314: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4929 - acc: 0.8048 - val_loss: 0.4677 - val_acc: 0.7854\n",
      "Epoch 315/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8139\n",
      "Epoch 315: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4677 - acc: 0.8138 - val_loss: 0.4925 - val_acc: 0.7653\n",
      "Epoch 316/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.8125\n",
      "Epoch 316: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4962 - acc: 0.8127 - val_loss: 0.4724 - val_acc: 0.8093\n",
      "Epoch 317/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.8058\n",
      "Epoch 317: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5222 - acc: 0.8058 - val_loss: 0.4620 - val_acc: 0.7961\n",
      "Epoch 318/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.8080\n",
      "Epoch 318: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5075 - acc: 0.8081 - val_loss: 0.4684 - val_acc: 0.7850\n",
      "Epoch 319/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5092 - acc: 0.8081\n",
      "Epoch 319: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5092 - acc: 0.8081 - val_loss: 0.4706 - val_acc: 0.8157\n",
      "Epoch 320/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4913 - acc: 0.8070\n",
      "Epoch 320: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4888 - acc: 0.8062 - val_loss: 0.4624 - val_acc: 0.7871\n",
      "Epoch 321/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8085\n",
      "Epoch 321: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5018 - acc: 0.8078 - val_loss: 0.4605 - val_acc: 0.7837\n",
      "Epoch 322/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4802 - acc: 0.8095\n",
      "Epoch 322: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4802 - acc: 0.8095 - val_loss: 0.4687 - val_acc: 0.7721\n",
      "Epoch 323/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8102\n",
      "Epoch 323: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4805 - acc: 0.8091 - val_loss: 0.4806 - val_acc: 0.7952\n",
      "Epoch 324/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8068\n",
      "Epoch 324: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5230 - acc: 0.8066 - val_loss: 0.4548 - val_acc: 0.8110\n",
      "Epoch 325/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8035\n",
      "Epoch 325: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4867 - acc: 0.8037 - val_loss: 0.4872 - val_acc: 0.7978\n",
      "Epoch 326/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.8095\n",
      "Epoch 326: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5239 - acc: 0.8096 - val_loss: 0.4656 - val_acc: 0.7939\n",
      "Epoch 327/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.8086\n",
      "Epoch 327: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5108 - acc: 0.8080 - val_loss: 0.4732 - val_acc: 0.7850\n",
      "Epoch 328/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4973 - acc: 0.8090\n",
      "Epoch 328: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4999 - acc: 0.8092 - val_loss: 0.4672 - val_acc: 0.7978\n",
      "Epoch 329/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8075\n",
      "Epoch 329: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4868 - acc: 0.8075 - val_loss: 0.4641 - val_acc: 0.8033\n",
      "Epoch 330/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.8040\n",
      "Epoch 330: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5407 - acc: 0.8045 - val_loss: 0.4770 - val_acc: 0.7991\n",
      "Epoch 331/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8092\n",
      "Epoch 331: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5278 - acc: 0.8076 - val_loss: 0.4789 - val_acc: 0.7897\n",
      "Epoch 332/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.8075\n",
      "Epoch 332: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5298 - acc: 0.8079 - val_loss: 0.4732 - val_acc: 0.7897\n",
      "Epoch 333/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.8100\n",
      "Epoch 333: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5223 - acc: 0.8096 - val_loss: 0.4672 - val_acc: 0.7956\n",
      "Epoch 334/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.8064\n",
      "Epoch 334: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4924 - acc: 0.8065 - val_loss: 0.4545 - val_acc: 0.7884\n",
      "Epoch 335/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8122\n",
      "Epoch 335: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4800 - acc: 0.8118 - val_loss: 0.4703 - val_acc: 0.8021\n",
      "Epoch 336/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.8110\n",
      "Epoch 336: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5391 - acc: 0.8110 - val_loss: 0.5150 - val_acc: 0.7691\n",
      "Epoch 337/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4806 - acc: 0.8113\n",
      "Epoch 337: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4823 - acc: 0.8102 - val_loss: 0.4806 - val_acc: 0.8021\n",
      "Epoch 338/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8090\n",
      "Epoch 338: val_acc did not improve from 0.81659\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4989 - acc: 0.8086 - val_loss: 0.4844 - val_acc: 0.7777\n",
      "Epoch 339/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4971 - acc: 0.8167\n",
      "Epoch 339: val_acc improved from 0.81659 to 0.81958, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/1/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4956 - acc: 0.8171 - val_loss: 0.4586 - val_acc: 0.8196\n",
      "Epoch 340/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.8065\n",
      "Epoch 340: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4841 - acc: 0.8051 - val_loss: 0.4697 - val_acc: 0.7850\n",
      "Epoch 341/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.8123\n",
      "Epoch 341: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5064 - acc: 0.8128 - val_loss: 0.4789 - val_acc: 0.8063\n",
      "Epoch 342/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8057\n",
      "Epoch 342: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5046 - acc: 0.8058 - val_loss: 0.4734 - val_acc: 0.8012\n",
      "Epoch 343/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5591 - acc: 0.8073\n",
      "Epoch 343: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5591 - acc: 0.8073 - val_loss: 0.4697 - val_acc: 0.7982\n",
      "Epoch 344/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4990 - acc: 0.8060\n",
      "Epoch 344: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4987 - acc: 0.8060 - val_loss: 0.4748 - val_acc: 0.7815\n",
      "Epoch 345/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4880 - acc: 0.8068\n",
      "Epoch 345: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4880 - acc: 0.8068 - val_loss: 0.4661 - val_acc: 0.7935\n",
      "Epoch 346/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8058\n",
      "Epoch 346: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5254 - acc: 0.8060 - val_loss: 0.4714 - val_acc: 0.7811\n",
      "Epoch 347/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.8088\n",
      "Epoch 347: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5079 - acc: 0.8084 - val_loss: 0.4654 - val_acc: 0.7751\n",
      "Epoch 348/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.8107\n",
      "Epoch 348: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5056 - acc: 0.8111 - val_loss: 0.4536 - val_acc: 0.7965\n",
      "Epoch 349/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8097\n",
      "Epoch 349: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4935 - acc: 0.8095 - val_loss: 0.4646 - val_acc: 0.8063\n",
      "Epoch 350/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.8053\n",
      "Epoch 350: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5104 - acc: 0.8056 - val_loss: 0.4614 - val_acc: 0.7884\n",
      "Epoch 351/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.8060\n",
      "Epoch 351: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5078 - acc: 0.8053 - val_loss: 0.4626 - val_acc: 0.7892\n",
      "Epoch 352/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8091\n",
      "Epoch 352: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5008 - acc: 0.8080 - val_loss: 0.4672 - val_acc: 0.8003\n",
      "Epoch 353/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8082\n",
      "Epoch 353: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5166 - acc: 0.8079 - val_loss: 0.4601 - val_acc: 0.7837\n",
      "Epoch 354/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.8064\n",
      "Epoch 354: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5575 - acc: 0.8060 - val_loss: 0.4643 - val_acc: 0.7867\n",
      "Epoch 355/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.8109\n",
      "Epoch 355: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4847 - acc: 0.8108 - val_loss: 0.4687 - val_acc: 0.7743\n",
      "Epoch 356/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5045 - acc: 0.8084\n",
      "Epoch 356: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5045 - acc: 0.8084 - val_loss: 0.4775 - val_acc: 0.7785\n",
      "Epoch 357/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8093\n",
      "Epoch 357: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4978 - acc: 0.8095 - val_loss: 0.4750 - val_acc: 0.8003\n",
      "Epoch 358/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8025\n",
      "Epoch 358: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5264 - acc: 0.8026 - val_loss: 0.4674 - val_acc: 0.7926\n",
      "Epoch 359/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4704 - acc: 0.8098\n",
      "Epoch 359: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4704 - acc: 0.8098 - val_loss: 0.4638 - val_acc: 0.7777\n",
      "Epoch 360/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.8083\n",
      "Epoch 360: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5154 - acc: 0.8084 - val_loss: 0.4716 - val_acc: 0.7837\n",
      "Epoch 361/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.8045\n",
      "Epoch 361: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5718 - acc: 0.8042 - val_loss: 0.4685 - val_acc: 0.7820\n",
      "Epoch 362/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.8062\n",
      "Epoch 362: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4882 - acc: 0.8062 - val_loss: 0.4712 - val_acc: 0.7837\n",
      "Epoch 363/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8067\n",
      "Epoch 363: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5251 - acc: 0.8071 - val_loss: 0.4719 - val_acc: 0.7918\n",
      "Epoch 364/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8093\n",
      "Epoch 364: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5147 - acc: 0.8093 - val_loss: 0.4756 - val_acc: 0.7939\n",
      "Epoch 365/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4730 - acc: 0.8029\n",
      "Epoch 365: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4727 - acc: 0.8029 - val_loss: 0.4809 - val_acc: 0.7815\n",
      "Epoch 366/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.8081\n",
      "Epoch 366: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5137 - acc: 0.8083 - val_loss: 0.4615 - val_acc: 0.7956\n",
      "Epoch 367/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8105\n",
      "Epoch 367: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5428 - acc: 0.8107 - val_loss: 0.4574 - val_acc: 0.7939\n",
      "Epoch 368/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8128\n",
      "Epoch 368: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4907 - acc: 0.8131 - val_loss: 0.4575 - val_acc: 0.7926\n",
      "Epoch 369/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8063\n",
      "Epoch 369: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5177 - acc: 0.8062 - val_loss: 0.4678 - val_acc: 0.7901\n",
      "Epoch 370/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8071\n",
      "Epoch 370: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4925 - acc: 0.8071 - val_loss: 0.4485 - val_acc: 0.8068\n",
      "Epoch 371/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4918 - acc: 0.8100\n",
      "Epoch 371: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4914 - acc: 0.8100 - val_loss: 0.4516 - val_acc: 0.7939\n",
      "Epoch 372/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8053\n",
      "Epoch 372: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.8053 - val_loss: 0.4718 - val_acc: 0.7897\n",
      "Epoch 373/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.8079\n",
      "Epoch 373: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5160 - acc: 0.8074 - val_loss: 0.4723 - val_acc: 0.7961\n",
      "Epoch 374/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4665 - acc: 0.8089\n",
      "Epoch 374: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4652 - acc: 0.8086 - val_loss: 0.4702 - val_acc: 0.7738\n",
      "Epoch 375/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8056\n",
      "Epoch 375: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4773 - acc: 0.8058 - val_loss: 0.4632 - val_acc: 0.7982\n",
      "Epoch 376/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.8046\n",
      "Epoch 376: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5167 - acc: 0.8043 - val_loss: 0.4760 - val_acc: 0.7721\n",
      "Epoch 377/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5322 - acc: 0.8071\n",
      "Epoch 377: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5322 - acc: 0.8071 - val_loss: 0.4652 - val_acc: 0.7884\n",
      "Epoch 378/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.8088\n",
      "Epoch 378: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5267 - acc: 0.8079 - val_loss: 0.4673 - val_acc: 0.7832\n",
      "Epoch 379/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.8057\n",
      "Epoch 379: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5112 - acc: 0.8058 - val_loss: 0.4533 - val_acc: 0.7995\n",
      "Epoch 380/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.8067\n",
      "Epoch 380: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5303 - acc: 0.8067 - val_loss: 0.4553 - val_acc: 0.7892\n",
      "Epoch 381/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8088\n",
      "Epoch 381: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4895 - acc: 0.8089 - val_loss: 0.4669 - val_acc: 0.7871\n",
      "Epoch 382/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.8128\n",
      "Epoch 382: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4938 - acc: 0.8123 - val_loss: 0.4618 - val_acc: 0.8050\n",
      "Epoch 383/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.8074\n",
      "Epoch 383: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4741 - acc: 0.8076 - val_loss: 0.4542 - val_acc: 0.7901\n",
      "Epoch 384/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8084\n",
      "Epoch 384: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5064 - acc: 0.8087 - val_loss: 0.4564 - val_acc: 0.7956\n",
      "Epoch 385/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8137\n",
      "Epoch 385: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5074 - acc: 0.8143 - val_loss: 0.4490 - val_acc: 0.8021\n",
      "Epoch 386/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.8099\n",
      "Epoch 386: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5041 - acc: 0.8099 - val_loss: 0.4519 - val_acc: 0.7935\n",
      "Epoch 387/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8114\n",
      "Epoch 387: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5220 - acc: 0.8114 - val_loss: 0.4848 - val_acc: 0.7751\n",
      "Epoch 388/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.8083\n",
      "Epoch 388: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5153 - acc: 0.8089 - val_loss: 0.4496 - val_acc: 0.8055\n",
      "Epoch 389/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8135\n",
      "Epoch 389: val_acc did not improve from 0.81958\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4592 - acc: 0.8133 - val_loss: 0.4686 - val_acc: 0.7721\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               24832     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,521\n",
      "Trainable params: 107,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 1.7318 - acc: 0.5915\n",
      "Epoch 1: val_acc improved from -inf to 0.67208, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 3s 8ms/step - loss: 1.7262 - acc: 0.5909 - val_loss: 0.6719 - val_acc: 0.6721\n",
      "Epoch 2/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.6557\n",
      "Epoch 2: val_acc did not improve from 0.67208\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6693 - acc: 0.6556 - val_loss: 0.6737 - val_acc: 0.6674\n",
      "Epoch 3/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6404 - acc: 0.6756\n",
      "Epoch 3: val_acc improved from 0.67208 to 0.68448, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6408 - acc: 0.6745 - val_loss: 0.6493 - val_acc: 0.6845\n",
      "Epoch 4/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6188 - acc: 0.6838\n",
      "Epoch 4: val_acc improved from 0.68448 to 0.70286, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6190 - acc: 0.6830 - val_loss: 0.6335 - val_acc: 0.7029\n",
      "Epoch 5/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.6041 - acc: 0.6958\n",
      "Epoch 5: val_acc improved from 0.70286 to 0.70842, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6040 - acc: 0.6958 - val_loss: 0.6254 - val_acc: 0.7084\n",
      "Epoch 6/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5910 - acc: 0.7065\n",
      "Epoch 6: val_acc improved from 0.70842 to 0.71954, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5917 - acc: 0.7057 - val_loss: 0.6017 - val_acc: 0.7195\n",
      "Epoch 7/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5821 - acc: 0.7147\n",
      "Epoch 7: val_acc improved from 0.71954 to 0.72552, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5827 - acc: 0.7137 - val_loss: 0.6070 - val_acc: 0.7255\n",
      "Epoch 8/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5753 - acc: 0.7150\n",
      "Epoch 8: val_acc did not improve from 0.72552\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5753 - acc: 0.7150 - val_loss: 0.5996 - val_acc: 0.7238\n",
      "Epoch 9/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.7184\n",
      "Epoch 9: val_acc improved from 0.72552 to 0.73194, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5739 - acc: 0.7177 - val_loss: 0.5740 - val_acc: 0.7319\n",
      "Epoch 10/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.7292\n",
      "Epoch 10: val_acc did not improve from 0.73194\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5642 - acc: 0.7283 - val_loss: 0.5892 - val_acc: 0.7247\n",
      "Epoch 11/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5613 - acc: 0.7298\n",
      "Epoch 11: val_acc improved from 0.73194 to 0.73536, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5618 - acc: 0.7289 - val_loss: 0.5781 - val_acc: 0.7354\n",
      "Epoch 12/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5657 - acc: 0.7270\n",
      "Epoch 12: val_acc did not improve from 0.73536\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5657 - acc: 0.7270 - val_loss: 0.5814 - val_acc: 0.7354\n",
      "Epoch 13/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.7316\n",
      "Epoch 13: val_acc improved from 0.73536 to 0.73749, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5578 - acc: 0.7306 - val_loss: 0.5670 - val_acc: 0.7375\n",
      "Epoch 14/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.7378\n",
      "Epoch 14: val_acc did not improve from 0.73749\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5521 - acc: 0.7362 - val_loss: 0.5549 - val_acc: 0.7371\n",
      "Epoch 15/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5574 - acc: 0.7366\n",
      "Epoch 15: val_acc improved from 0.73749 to 0.74476, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5588 - acc: 0.7359 - val_loss: 0.5729 - val_acc: 0.7448\n",
      "Epoch 16/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7388\n",
      "Epoch 16: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5526 - acc: 0.7386 - val_loss: 0.5488 - val_acc: 0.7418\n",
      "Epoch 17/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.7443\n",
      "Epoch 17: val_acc did not improve from 0.74476\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5402 - acc: 0.7432 - val_loss: 0.5494 - val_acc: 0.7379\n",
      "Epoch 18/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7433\n",
      "Epoch 18: val_acc improved from 0.74476 to 0.74519, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5454 - acc: 0.7428 - val_loss: 0.5420 - val_acc: 0.7452\n",
      "Epoch 19/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.7470\n",
      "Epoch 19: val_acc did not improve from 0.74519\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5340 - acc: 0.7471 - val_loss: 0.5288 - val_acc: 0.7418\n",
      "Epoch 20/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7461\n",
      "Epoch 20: val_acc improved from 0.74519 to 0.74647, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5385 - acc: 0.7446 - val_loss: 0.5584 - val_acc: 0.7465\n",
      "Epoch 21/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7499\n",
      "Epoch 21: val_acc improved from 0.74647 to 0.75075, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5366 - acc: 0.7488 - val_loss: 0.5364 - val_acc: 0.7507\n",
      "Epoch 22/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7515\n",
      "Epoch 22: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5311 - acc: 0.7508 - val_loss: 0.5389 - val_acc: 0.7431\n",
      "Epoch 23/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7495\n",
      "Epoch 23: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5358 - acc: 0.7483 - val_loss: 0.5378 - val_acc: 0.7409\n",
      "Epoch 24/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7477\n",
      "Epoch 24: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5393 - acc: 0.7475 - val_loss: 0.5390 - val_acc: 0.7448\n",
      "Epoch 25/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7534\n",
      "Epoch 25: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5434 - acc: 0.7518 - val_loss: 0.5467 - val_acc: 0.7473\n",
      "Epoch 26/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7516\n",
      "Epoch 26: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5360 - acc: 0.7514 - val_loss: 0.5467 - val_acc: 0.7495\n",
      "Epoch 27/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5401 - acc: 0.7492\n",
      "Epoch 27: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5401 - acc: 0.7492 - val_loss: 0.5309 - val_acc: 0.7456\n",
      "Epoch 28/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.7504\n",
      "Epoch 28: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5307 - acc: 0.7500 - val_loss: 0.5703 - val_acc: 0.7499\n",
      "Epoch 29/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.7561\n",
      "Epoch 29: val_acc improved from 0.75075 to 0.75930, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5354 - acc: 0.7554 - val_loss: 0.5378 - val_acc: 0.7593\n",
      "Epoch 30/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.7519\n",
      "Epoch 30: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5285 - acc: 0.7519 - val_loss: 0.5370 - val_acc: 0.7533\n",
      "Epoch 31/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.7551\n",
      "Epoch 31: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5287 - acc: 0.7545 - val_loss: 0.5440 - val_acc: 0.7512\n",
      "Epoch 32/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7539\n",
      "Epoch 32: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5339 - acc: 0.7539 - val_loss: 0.5264 - val_acc: 0.7456\n",
      "Epoch 33/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.7584\n",
      "Epoch 33: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5407 - acc: 0.7576 - val_loss: 0.5205 - val_acc: 0.7490\n",
      "Epoch 34/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7564\n",
      "Epoch 34: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5369 - acc: 0.7560 - val_loss: 0.5340 - val_acc: 0.7567\n",
      "Epoch 35/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7585\n",
      "Epoch 35: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5286 - acc: 0.7580 - val_loss: 0.5292 - val_acc: 0.7542\n",
      "Epoch 36/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.7611\n",
      "Epoch 36: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5257 - acc: 0.7596 - val_loss: 0.5206 - val_acc: 0.7537\n",
      "Epoch 37/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7572\n",
      "Epoch 37: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5201 - acc: 0.7572 - val_loss: 0.5311 - val_acc: 0.7486\n",
      "Epoch 38/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7605\n",
      "Epoch 38: val_acc did not improve from 0.75930\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5277 - acc: 0.7604 - val_loss: 0.5475 - val_acc: 0.7525\n",
      "Epoch 39/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7629\n",
      "Epoch 39: val_acc improved from 0.75930 to 0.76571, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5334 - acc: 0.7615 - val_loss: 0.5220 - val_acc: 0.7657\n",
      "Epoch 40/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7628\n",
      "Epoch 40: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5173 - acc: 0.7619 - val_loss: 0.5270 - val_acc: 0.7525\n",
      "Epoch 41/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.7638\n",
      "Epoch 41: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5224 - acc: 0.7624 - val_loss: 0.5201 - val_acc: 0.7542\n",
      "Epoch 42/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.7619\n",
      "Epoch 42: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5258 - acc: 0.7619 - val_loss: 0.5112 - val_acc: 0.7593\n",
      "Epoch 43/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.7667\n",
      "Epoch 43: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5158 - acc: 0.7660 - val_loss: 0.5324 - val_acc: 0.7589\n",
      "Epoch 44/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5227 - acc: 0.7622\n",
      "Epoch 44: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5227 - acc: 0.7622 - val_loss: 0.5353 - val_acc: 0.7533\n",
      "Epoch 45/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.7689\n",
      "Epoch 45: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5147 - acc: 0.7676 - val_loss: 0.5185 - val_acc: 0.7584\n",
      "Epoch 46/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5173 - acc: 0.7662\n",
      "Epoch 46: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5178 - acc: 0.7654 - val_loss: 0.5436 - val_acc: 0.7644\n",
      "Epoch 47/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7607\n",
      "Epoch 47: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5296 - acc: 0.7587 - val_loss: 0.5261 - val_acc: 0.7401\n",
      "Epoch 48/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.7630\n",
      "Epoch 48: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5256 - acc: 0.7618 - val_loss: 0.5359 - val_acc: 0.7572\n",
      "Epoch 49/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7687\n",
      "Epoch 49: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5153 - acc: 0.7676 - val_loss: 0.5320 - val_acc: 0.7619\n",
      "Epoch 50/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7693\n",
      "Epoch 50: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5206 - acc: 0.7693 - val_loss: 0.5220 - val_acc: 0.7499\n",
      "Epoch 51/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.7686\n",
      "Epoch 51: val_acc improved from 0.76571 to 0.77512, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5293 - acc: 0.7688 - val_loss: 0.5310 - val_acc: 0.7751\n",
      "Epoch 52/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7706\n",
      "Epoch 52: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5249 - acc: 0.7691 - val_loss: 0.5545 - val_acc: 0.7499\n",
      "Epoch 53/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.7647\n",
      "Epoch 53: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5235 - acc: 0.7645 - val_loss: 0.5175 - val_acc: 0.7627\n",
      "Epoch 54/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.7676\n",
      "Epoch 54: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5151 - acc: 0.7670 - val_loss: 0.5415 - val_acc: 0.7623\n",
      "Epoch 55/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7646\n",
      "Epoch 55: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5341 - acc: 0.7637 - val_loss: 0.5484 - val_acc: 0.7619\n",
      "Epoch 56/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7699\n",
      "Epoch 56: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5373 - acc: 0.7681 - val_loss: 0.5207 - val_acc: 0.7576\n",
      "Epoch 57/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.7713\n",
      "Epoch 57: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5095 - acc: 0.7710 - val_loss: 0.5254 - val_acc: 0.7413\n",
      "Epoch 58/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.7696\n",
      "Epoch 58: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5247 - acc: 0.7691 - val_loss: 0.5230 - val_acc: 0.7627\n",
      "Epoch 59/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5157 - acc: 0.7691\n",
      "Epoch 59: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5157 - acc: 0.7691 - val_loss: 0.5246 - val_acc: 0.7559\n",
      "Epoch 60/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.7739\n",
      "Epoch 60: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5291 - acc: 0.7728 - val_loss: 0.5196 - val_acc: 0.7674\n",
      "Epoch 61/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5125 - acc: 0.7684\n",
      "Epoch 61: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5125 - acc: 0.7684 - val_loss: 0.5293 - val_acc: 0.7636\n",
      "Epoch 62/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.7742\n",
      "Epoch 62: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5048 - acc: 0.7737 - val_loss: 0.5106 - val_acc: 0.7610\n",
      "Epoch 63/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7676\n",
      "Epoch 63: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5173 - acc: 0.7678 - val_loss: 0.5313 - val_acc: 0.7631\n",
      "Epoch 64/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.7679\n",
      "Epoch 64: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5107 - acc: 0.7674 - val_loss: 0.5307 - val_acc: 0.7580\n",
      "Epoch 65/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.7680\n",
      "Epoch 65: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5376 - acc: 0.7679 - val_loss: 0.5299 - val_acc: 0.7576\n",
      "Epoch 66/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.7696\n",
      "Epoch 66: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5254 - acc: 0.7690 - val_loss: 0.5405 - val_acc: 0.7657\n",
      "Epoch 67/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.7722\n",
      "Epoch 67: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5207 - acc: 0.7712 - val_loss: 0.5017 - val_acc: 0.7661\n",
      "Epoch 68/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.7727\n",
      "Epoch 68: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5189 - acc: 0.7707 - val_loss: 0.5276 - val_acc: 0.7666\n",
      "Epoch 69/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.7715\n",
      "Epoch 69: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5190 - acc: 0.7707 - val_loss: 0.5219 - val_acc: 0.7486\n",
      "Epoch 70/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.7767\n",
      "Epoch 70: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5295 - acc: 0.7758 - val_loss: 0.5333 - val_acc: 0.7602\n",
      "Epoch 71/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.7750\n",
      "Epoch 71: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5200 - acc: 0.7747 - val_loss: 0.5200 - val_acc: 0.7627\n",
      "Epoch 72/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.7680\n",
      "Epoch 72: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5268 - acc: 0.7665 - val_loss: 0.5374 - val_acc: 0.7507\n",
      "Epoch 73/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7732\n",
      "Epoch 73: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5199 - acc: 0.7733 - val_loss: 0.5243 - val_acc: 0.7661\n",
      "Epoch 74/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7769\n",
      "Epoch 74: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5164 - acc: 0.7764 - val_loss: 0.5284 - val_acc: 0.7550\n",
      "Epoch 75/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7738\n",
      "Epoch 75: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5185 - acc: 0.7732 - val_loss: 0.5322 - val_acc: 0.7738\n",
      "Epoch 76/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7730\n",
      "Epoch 76: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5234 - acc: 0.7712 - val_loss: 0.5594 - val_acc: 0.7563\n",
      "Epoch 77/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.7798\n",
      "Epoch 77: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5044 - acc: 0.7786 - val_loss: 0.5409 - val_acc: 0.7666\n",
      "Epoch 78/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.7691\n",
      "Epoch 78: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5208 - acc: 0.7685 - val_loss: 0.5189 - val_acc: 0.7550\n",
      "Epoch 79/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.7758\n",
      "Epoch 79: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5117 - acc: 0.7758 - val_loss: 0.5191 - val_acc: 0.7525\n",
      "Epoch 80/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7787\n",
      "Epoch 80: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5148 - acc: 0.7774 - val_loss: 0.5164 - val_acc: 0.7738\n",
      "Epoch 81/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7718\n",
      "Epoch 81: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5292 - acc: 0.7719 - val_loss: 0.5054 - val_acc: 0.7555\n",
      "Epoch 82/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5374 - acc: 0.7756\n",
      "Epoch 82: val_acc improved from 0.77512 to 0.78709, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5374 - acc: 0.7756 - val_loss: 0.5135 - val_acc: 0.7871\n",
      "Epoch 83/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.7762\n",
      "Epoch 83: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5188 - acc: 0.7749 - val_loss: 0.5353 - val_acc: 0.7610\n",
      "Epoch 84/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.7810\n",
      "Epoch 84: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5139 - acc: 0.7809 - val_loss: 0.5198 - val_acc: 0.7738\n",
      "Epoch 85/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7831\n",
      "Epoch 85: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5159 - acc: 0.7827 - val_loss: 0.5210 - val_acc: 0.7730\n",
      "Epoch 86/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5201 - acc: 0.7804\n",
      "Epoch 86: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5201 - acc: 0.7804 - val_loss: 0.5191 - val_acc: 0.7798\n",
      "Epoch 87/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.7765\n",
      "Epoch 87: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5227 - acc: 0.7761 - val_loss: 0.5256 - val_acc: 0.7751\n",
      "Epoch 88/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5144 - acc: 0.7741\n",
      "Epoch 88: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5144 - acc: 0.7741 - val_loss: 0.5224 - val_acc: 0.7730\n",
      "Epoch 89/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.7755\n",
      "Epoch 89: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5084 - acc: 0.7755 - val_loss: 0.5238 - val_acc: 0.7614\n",
      "Epoch 90/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7806\n",
      "Epoch 90: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5076 - acc: 0.7808 - val_loss: 0.5248 - val_acc: 0.7670\n",
      "Epoch 91/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7826\n",
      "Epoch 91: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5261 - acc: 0.7811 - val_loss: 0.5208 - val_acc: 0.7649\n",
      "Epoch 92/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5007 - acc: 0.7818\n",
      "Epoch 92: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5003 - acc: 0.7810 - val_loss: 0.5313 - val_acc: 0.7516\n",
      "Epoch 93/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.7817\n",
      "Epoch 93: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4996 - acc: 0.7797 - val_loss: 0.5225 - val_acc: 0.7533\n",
      "Epoch 94/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5142 - acc: 0.7771\n",
      "Epoch 94: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5159 - acc: 0.7754 - val_loss: 0.5338 - val_acc: 0.7516\n",
      "Epoch 95/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.7824\n",
      "Epoch 95: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5255 - acc: 0.7815 - val_loss: 0.5282 - val_acc: 0.7704\n",
      "Epoch 96/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.7800\n",
      "Epoch 96: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4872 - acc: 0.7801 - val_loss: 0.5535 - val_acc: 0.7589\n",
      "Epoch 97/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.7814\n",
      "Epoch 97: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5076 - acc: 0.7809 - val_loss: 0.5249 - val_acc: 0.7726\n",
      "Epoch 98/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7803\n",
      "Epoch 98: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5153 - acc: 0.7804 - val_loss: 0.5374 - val_acc: 0.7777\n",
      "Epoch 99/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.7808\n",
      "Epoch 99: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5521 - acc: 0.7800 - val_loss: 0.5296 - val_acc: 0.7606\n",
      "Epoch 100/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.7789\n",
      "Epoch 100: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5058 - acc: 0.7785 - val_loss: 0.5336 - val_acc: 0.7623\n",
      "Epoch 101/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.7821\n",
      "Epoch 101: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5117 - acc: 0.7808 - val_loss: 0.5197 - val_acc: 0.7580\n",
      "Epoch 102/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5060 - acc: 0.7840\n",
      "Epoch 102: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5060 - acc: 0.7840 - val_loss: 0.5214 - val_acc: 0.7614\n",
      "Epoch 103/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5147 - acc: 0.7796\n",
      "Epoch 103: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5149 - acc: 0.7786 - val_loss: 0.5133 - val_acc: 0.7431\n",
      "Epoch 104/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5234 - acc: 0.7829\n",
      "Epoch 104: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5234 - acc: 0.7829 - val_loss: 0.5462 - val_acc: 0.7687\n",
      "Epoch 105/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7758\n",
      "Epoch 105: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5234 - acc: 0.7758 - val_loss: 0.5346 - val_acc: 0.7589\n",
      "Epoch 106/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.7828\n",
      "Epoch 106: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5086 - acc: 0.7821 - val_loss: 0.5259 - val_acc: 0.7602\n",
      "Epoch 107/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7776\n",
      "Epoch 107: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5317 - acc: 0.7778 - val_loss: 0.5399 - val_acc: 0.7691\n",
      "Epoch 108/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.7827\n",
      "Epoch 108: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5300 - acc: 0.7821 - val_loss: 0.5280 - val_acc: 0.7606\n",
      "Epoch 109/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.7871\n",
      "Epoch 109: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5046 - acc: 0.7867 - val_loss: 0.5313 - val_acc: 0.7871\n",
      "Epoch 110/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7823\n",
      "Epoch 110: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5143 - acc: 0.7818 - val_loss: 0.5179 - val_acc: 0.7721\n",
      "Epoch 111/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.7884\n",
      "Epoch 111: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4991 - acc: 0.7874 - val_loss: 0.5156 - val_acc: 0.7828\n",
      "Epoch 112/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.7805\n",
      "Epoch 112: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5125 - acc: 0.7805 - val_loss: 0.5281 - val_acc: 0.7691\n",
      "Epoch 113/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.7855\n",
      "Epoch 113: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5048 - acc: 0.7852 - val_loss: 0.5394 - val_acc: 0.7747\n",
      "Epoch 114/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.7795\n",
      "Epoch 114: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5499 - acc: 0.7783 - val_loss: 0.5266 - val_acc: 0.7678\n",
      "Epoch 115/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.7841\n",
      "Epoch 115: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5091 - acc: 0.7833 - val_loss: 0.5220 - val_acc: 0.7811\n",
      "Epoch 116/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5108 - acc: 0.7908\n",
      "Epoch 116: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5106 - acc: 0.7902 - val_loss: 0.5207 - val_acc: 0.7687\n",
      "Epoch 117/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7881\n",
      "Epoch 117: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5119 - acc: 0.7872 - val_loss: 0.5244 - val_acc: 0.7636\n",
      "Epoch 118/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.7909\n",
      "Epoch 118: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5020 - acc: 0.7904 - val_loss: 0.5215 - val_acc: 0.7674\n",
      "Epoch 119/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.7883\n",
      "Epoch 119: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5087 - acc: 0.7862 - val_loss: 0.5113 - val_acc: 0.7781\n",
      "Epoch 120/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5166 - acc: 0.7860\n",
      "Epoch 120: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5163 - acc: 0.7849 - val_loss: 0.5229 - val_acc: 0.7606\n",
      "Epoch 121/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7782\n",
      "Epoch 121: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5351 - acc: 0.7776 - val_loss: 0.5367 - val_acc: 0.7845\n",
      "Epoch 122/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7877\n",
      "Epoch 122: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5194 - acc: 0.7869 - val_loss: 0.5060 - val_acc: 0.7850\n",
      "Epoch 123/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.7844\n",
      "Epoch 123: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5096 - acc: 0.7836 - val_loss: 0.5254 - val_acc: 0.7755\n",
      "Epoch 124/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.7835\n",
      "Epoch 124: val_acc improved from 0.78709 to 0.78923, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5217 - acc: 0.7828 - val_loss: 0.5357 - val_acc: 0.7892\n",
      "Epoch 125/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.7821\n",
      "Epoch 125: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5131 - acc: 0.7818 - val_loss: 0.5443 - val_acc: 0.7572\n",
      "Epoch 126/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7838\n",
      "Epoch 126: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5186 - acc: 0.7834 - val_loss: 0.5001 - val_acc: 0.7777\n",
      "Epoch 127/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7888\n",
      "Epoch 127: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5340 - acc: 0.7879 - val_loss: 0.5241 - val_acc: 0.7730\n",
      "Epoch 128/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.7880\n",
      "Epoch 128: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4913 - acc: 0.7877 - val_loss: 0.5203 - val_acc: 0.7627\n",
      "Epoch 129/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7865\n",
      "Epoch 129: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5266 - acc: 0.7863 - val_loss: 0.5278 - val_acc: 0.7631\n",
      "Epoch 130/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5016 - acc: 0.7897\n",
      "Epoch 130: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5151 - acc: 0.7889 - val_loss: 0.5373 - val_acc: 0.7542\n",
      "Epoch 131/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.7876\n",
      "Epoch 131: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5032 - acc: 0.7876 - val_loss: 0.5395 - val_acc: 0.7580\n",
      "Epoch 132/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7880\n",
      "Epoch 132: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5084 - acc: 0.7870 - val_loss: 0.5244 - val_acc: 0.7841\n",
      "Epoch 133/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7880\n",
      "Epoch 133: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5217 - acc: 0.7877 - val_loss: 0.5087 - val_acc: 0.7661\n",
      "Epoch 134/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7866\n",
      "Epoch 134: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5354 - acc: 0.7858 - val_loss: 0.5300 - val_acc: 0.7832\n",
      "Epoch 135/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.7830\n",
      "Epoch 135: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5201 - acc: 0.7830 - val_loss: 0.5406 - val_acc: 0.7674\n",
      "Epoch 136/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.7937\n",
      "Epoch 136: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4934 - acc: 0.7927 - val_loss: 0.5331 - val_acc: 0.7653\n",
      "Epoch 137/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.7848\n",
      "Epoch 137: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5219 - acc: 0.7848 - val_loss: 0.5426 - val_acc: 0.7696\n",
      "Epoch 138/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7895\n",
      "Epoch 138: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5167 - acc: 0.7889 - val_loss: 0.5427 - val_acc: 0.7828\n",
      "Epoch 139/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.7922\n",
      "Epoch 139: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4998 - acc: 0.7903 - val_loss: 0.5289 - val_acc: 0.7657\n",
      "Epoch 140/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7878\n",
      "Epoch 140: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5428 - acc: 0.7867 - val_loss: 0.5322 - val_acc: 0.7666\n",
      "Epoch 141/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.7931\n",
      "Epoch 141: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5195 - acc: 0.7926 - val_loss: 0.5247 - val_acc: 0.7781\n",
      "Epoch 142/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5130 - acc: 0.7894\n",
      "Epoch 142: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5130 - acc: 0.7894 - val_loss: 0.5363 - val_acc: 0.7696\n",
      "Epoch 143/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7876\n",
      "Epoch 143: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5270 - acc: 0.7865 - val_loss: 0.5406 - val_acc: 0.7691\n",
      "Epoch 144/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7904\n",
      "Epoch 144: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5212 - acc: 0.7906 - val_loss: 0.5241 - val_acc: 0.7734\n",
      "Epoch 145/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.7901\n",
      "Epoch 145: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5002 - acc: 0.7894 - val_loss: 0.5287 - val_acc: 0.7584\n",
      "Epoch 146/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.7877\n",
      "Epoch 146: val_acc improved from 0.78923 to 0.79222, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4944 - acc: 0.7879 - val_loss: 0.5537 - val_acc: 0.7922\n",
      "Epoch 147/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.7939\n",
      "Epoch 147: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4935 - acc: 0.7931 - val_loss: 0.5470 - val_acc: 0.7713\n",
      "Epoch 148/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7894\n",
      "Epoch 148: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5186 - acc: 0.7882 - val_loss: 0.5467 - val_acc: 0.7683\n",
      "Epoch 149/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.7945\n",
      "Epoch 149: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5042 - acc: 0.7934 - val_loss: 0.5253 - val_acc: 0.7691\n",
      "Epoch 150/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.7919\n",
      "Epoch 150: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4928 - acc: 0.7918 - val_loss: 0.5313 - val_acc: 0.7584\n",
      "Epoch 151/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.7922\n",
      "Epoch 151: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4937 - acc: 0.7922 - val_loss: 0.5393 - val_acc: 0.7713\n",
      "Epoch 152/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.7886\n",
      "Epoch 152: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4993 - acc: 0.7878 - val_loss: 0.5462 - val_acc: 0.7589\n",
      "Epoch 153/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.7881\n",
      "Epoch 153: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5214 - acc: 0.7880 - val_loss: 0.5478 - val_acc: 0.7691\n",
      "Epoch 154/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5197 - acc: 0.7891\n",
      "Epoch 154: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5197 - acc: 0.7891 - val_loss: 0.5240 - val_acc: 0.7674\n",
      "Epoch 155/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7916\n",
      "Epoch 155: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5148 - acc: 0.7911 - val_loss: 0.5311 - val_acc: 0.7802\n",
      "Epoch 156/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.7927\n",
      "Epoch 156: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5109 - acc: 0.7927 - val_loss: 0.5194 - val_acc: 0.7773\n",
      "Epoch 157/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.7918\n",
      "Epoch 157: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5319 - acc: 0.7919 - val_loss: 0.5492 - val_acc: 0.7760\n",
      "Epoch 158/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7909\n",
      "Epoch 158: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5443 - acc: 0.7897 - val_loss: 0.5281 - val_acc: 0.7815\n",
      "Epoch 159/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5220 - acc: 0.7875\n",
      "Epoch 159: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5220 - acc: 0.7875 - val_loss: 0.5264 - val_acc: 0.7640\n",
      "Epoch 160/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7950\n",
      "Epoch 160: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5291 - acc: 0.7948 - val_loss: 0.5158 - val_acc: 0.7717\n",
      "Epoch 161/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.7908\n",
      "Epoch 161: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5059 - acc: 0.7883 - val_loss: 0.5141 - val_acc: 0.7619\n",
      "Epoch 162/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.7926\n",
      "Epoch 162: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5089 - acc: 0.7929 - val_loss: 0.5531 - val_acc: 0.7905\n",
      "Epoch 163/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.7918\n",
      "Epoch 163: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4937 - acc: 0.7912 - val_loss: 0.5626 - val_acc: 0.7721\n",
      "Epoch 164/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.7942\n",
      "Epoch 164: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5089 - acc: 0.7942 - val_loss: 0.5234 - val_acc: 0.7670\n",
      "Epoch 165/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.7919\n",
      "Epoch 165: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5212 - acc: 0.7912 - val_loss: 0.5267 - val_acc: 0.7674\n",
      "Epoch 166/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7863\n",
      "Epoch 166: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5351 - acc: 0.7856 - val_loss: 0.5272 - val_acc: 0.7807\n",
      "Epoch 167/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.7864\n",
      "Epoch 167: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5129 - acc: 0.7861 - val_loss: 0.5324 - val_acc: 0.7696\n",
      "Epoch 168/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7908\n",
      "Epoch 168: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5271 - acc: 0.7904 - val_loss: 0.5636 - val_acc: 0.7828\n",
      "Epoch 169/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.7960\n",
      "Epoch 169: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4926 - acc: 0.7955 - val_loss: 0.5521 - val_acc: 0.7828\n",
      "Epoch 170/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7946\n",
      "Epoch 170: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5383 - acc: 0.7939 - val_loss: 0.5454 - val_acc: 0.7738\n",
      "Epoch 171/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.7921\n",
      "Epoch 171: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4991 - acc: 0.7916 - val_loss: 0.5468 - val_acc: 0.7815\n",
      "Epoch 172/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.7932\n",
      "Epoch 172: val_acc improved from 0.79222 to 0.79649, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4865 - acc: 0.7931 - val_loss: 0.5576 - val_acc: 0.7965\n",
      "Epoch 173/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.7927\n",
      "Epoch 173: val_acc did not improve from 0.79649\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5207 - acc: 0.7923 - val_loss: 0.5251 - val_acc: 0.7845\n",
      "Epoch 174/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.7991\n",
      "Epoch 174: val_acc did not improve from 0.79649\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5184 - acc: 0.7983 - val_loss: 0.5537 - val_acc: 0.7948\n",
      "Epoch 175/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.7961\n",
      "Epoch 175: val_acc did not improve from 0.79649\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4873 - acc: 0.7945 - val_loss: 0.5277 - val_acc: 0.7815\n",
      "Epoch 176/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7944\n",
      "Epoch 176: val_acc did not improve from 0.79649\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5204 - acc: 0.7942 - val_loss: 0.5232 - val_acc: 0.7867\n",
      "Epoch 177/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7946\n",
      "Epoch 177: val_acc did not improve from 0.79649\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5356 - acc: 0.7947 - val_loss: 0.5357 - val_acc: 0.7781\n",
      "Epoch 178/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.7962\n",
      "Epoch 178: val_acc did not improve from 0.79649\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5089 - acc: 0.7966 - val_loss: 0.5166 - val_acc: 0.7785\n",
      "Epoch 179/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.7976\n",
      "Epoch 179: val_acc improved from 0.79649 to 0.80547, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5048 - acc: 0.7972 - val_loss: 0.5436 - val_acc: 0.8055\n",
      "Epoch 180/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.7925\n",
      "Epoch 180: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4836 - acc: 0.7924 - val_loss: 0.5266 - val_acc: 0.7961\n",
      "Epoch 181/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.7905\n",
      "Epoch 181: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5023 - acc: 0.7905 - val_loss: 0.5456 - val_acc: 0.7995\n",
      "Epoch 182/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5125 - acc: 0.7938\n",
      "Epoch 182: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5125 - acc: 0.7938 - val_loss: 0.5392 - val_acc: 0.7738\n",
      "Epoch 183/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5730 - acc: 0.8003\n",
      "Epoch 183: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5720 - acc: 0.7997 - val_loss: 0.5342 - val_acc: 0.7999\n",
      "Epoch 184/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.7914\n",
      "Epoch 184: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5085 - acc: 0.7910 - val_loss: 0.5118 - val_acc: 0.7717\n",
      "Epoch 185/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.7978\n",
      "Epoch 185: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5221 - acc: 0.7968 - val_loss: 0.5100 - val_acc: 0.7824\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7961\n",
      "Epoch 186: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5415 - acc: 0.7960 - val_loss: 0.5102 - val_acc: 0.7713\n",
      "Epoch 187/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.7950\n",
      "Epoch 187: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4670 - acc: 0.7943 - val_loss: 0.5515 - val_acc: 0.7785\n",
      "Epoch 188/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.7960\n",
      "Epoch 188: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5526 - acc: 0.7957 - val_loss: 0.5575 - val_acc: 0.7918\n",
      "Epoch 189/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.7953\n",
      "Epoch 189: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5120 - acc: 0.7953 - val_loss: 0.5294 - val_acc: 0.7879\n",
      "Epoch 190/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.7921\n",
      "Epoch 190: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5015 - acc: 0.7921 - val_loss: 0.5081 - val_acc: 0.7879\n",
      "Epoch 191/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7948\n",
      "Epoch 191: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5185 - acc: 0.7938 - val_loss: 0.5217 - val_acc: 0.7807\n",
      "Epoch 192/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4963 - acc: 0.7981\n",
      "Epoch 192: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4947 - acc: 0.7979 - val_loss: 0.5219 - val_acc: 0.8038\n",
      "Epoch 193/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.7924\n",
      "Epoch 193: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5072 - acc: 0.7922 - val_loss: 0.5261 - val_acc: 0.7661\n",
      "Epoch 194/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.7937\n",
      "Epoch 194: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5153 - acc: 0.7936 - val_loss: 0.5288 - val_acc: 0.8012\n",
      "Epoch 195/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8035\n",
      "Epoch 195: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5021 - acc: 0.8027 - val_loss: 0.5245 - val_acc: 0.7879\n",
      "Epoch 196/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.7991\n",
      "Epoch 196: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5337 - acc: 0.7990 - val_loss: 0.5189 - val_acc: 0.8021\n",
      "Epoch 197/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.7962\n",
      "Epoch 197: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5223 - acc: 0.7957 - val_loss: 0.5435 - val_acc: 0.7888\n",
      "Epoch 198/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4960 - acc: 0.8041\n",
      "Epoch 198: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4950 - acc: 0.8042 - val_loss: 0.5252 - val_acc: 0.7798\n",
      "Epoch 199/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.7990\n",
      "Epoch 199: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4967 - acc: 0.7983 - val_loss: 0.5295 - val_acc: 0.7674\n",
      "Epoch 200/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4900 - acc: 0.7982\n",
      "Epoch 200: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4898 - acc: 0.7982 - val_loss: 0.5512 - val_acc: 0.7704\n",
      "Epoch 201/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.7989\n",
      "Epoch 201: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5542 - acc: 0.7987 - val_loss: 0.5273 - val_acc: 0.7747\n",
      "Epoch 202/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.8019\n",
      "Epoch 202: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5168 - acc: 0.8005 - val_loss: 0.5174 - val_acc: 0.7850\n",
      "Epoch 203/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4971 - acc: 0.7999\n",
      "Epoch 203: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4971 - acc: 0.7999 - val_loss: 0.5138 - val_acc: 0.7837\n",
      "Epoch 204/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.8016\n",
      "Epoch 204: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4954 - acc: 0.8014 - val_loss: 0.5288 - val_acc: 0.7892\n",
      "Epoch 205/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7989\n",
      "Epoch 205: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5312 - acc: 0.7979 - val_loss: 0.5355 - val_acc: 0.7901\n",
      "Epoch 206/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.7955\n",
      "Epoch 206: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4975 - acc: 0.7957 - val_loss: 0.5315 - val_acc: 0.7841\n",
      "Epoch 207/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7997\n",
      "Epoch 207: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5382 - acc: 0.7980 - val_loss: 0.5150 - val_acc: 0.7879\n",
      "Epoch 208/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8002\n",
      "Epoch 208: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5125 - acc: 0.7998 - val_loss: 0.5246 - val_acc: 0.7922\n",
      "Epoch 209/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4879 - acc: 0.8045\n",
      "Epoch 209: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4872 - acc: 0.8041 - val_loss: 0.5210 - val_acc: 0.7743\n",
      "Epoch 210/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4913 - acc: 0.8002\n",
      "Epoch 210: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4905 - acc: 0.7996 - val_loss: 0.5571 - val_acc: 0.7726\n",
      "Epoch 211/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.7969\n",
      "Epoch 211: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5258 - acc: 0.7969 - val_loss: 0.5438 - val_acc: 0.7914\n",
      "Epoch 212/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7997\n",
      "Epoch 212: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5377 - acc: 0.7988 - val_loss: 0.5200 - val_acc: 0.7760\n",
      "Epoch 213/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8020\n",
      "Epoch 213: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5004 - acc: 0.8017 - val_loss: 0.5344 - val_acc: 0.7794\n",
      "Epoch 214/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.8016\n",
      "Epoch 214: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5105 - acc: 0.8013 - val_loss: 0.5438 - val_acc: 0.7704\n",
      "Epoch 215/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8049\n",
      "Epoch 215: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5013 - acc: 0.8044 - val_loss: 0.5468 - val_acc: 0.7678\n",
      "Epoch 216/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8030\n",
      "Epoch 216: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4949 - acc: 0.8020 - val_loss: 0.5475 - val_acc: 0.7854\n",
      "Epoch 217/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.7987\n",
      "Epoch 217: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4971 - acc: 0.7983 - val_loss: 0.5554 - val_acc: 0.7764\n",
      "Epoch 218/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7960\n",
      "Epoch 218: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5089 - acc: 0.7959 - val_loss: 0.5291 - val_acc: 0.7768\n",
      "Epoch 219/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.7986\n",
      "Epoch 219: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5008 - acc: 0.7973 - val_loss: 0.5061 - val_acc: 0.7734\n",
      "Epoch 220/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4903 - acc: 0.7977\n",
      "Epoch 220: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4890 - acc: 0.7970 - val_loss: 0.5267 - val_acc: 0.7811\n",
      "Epoch 221/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.7988\n",
      "Epoch 221: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4922 - acc: 0.7986 - val_loss: 0.5135 - val_acc: 0.7790\n",
      "Epoch 222/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8019\n",
      "Epoch 222: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4804 - acc: 0.8004 - val_loss: 0.5372 - val_acc: 0.7862\n",
      "Epoch 223/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4922 - acc: 0.7990\n",
      "Epoch 223: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4915 - acc: 0.7980 - val_loss: 0.5571 - val_acc: 0.7764\n",
      "Epoch 224/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.7981\n",
      "Epoch 224: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5465 - acc: 0.7981 - val_loss: 0.5349 - val_acc: 0.7773\n",
      "Epoch 225/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.8002\n",
      "Epoch 225: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5322 - acc: 0.7997 - val_loss: 0.5221 - val_acc: 0.7738\n",
      "Epoch 226/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.8000\n",
      "Epoch 226: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5641 - acc: 0.7994 - val_loss: 0.5058 - val_acc: 0.7743\n",
      "Epoch 227/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5004 - acc: 0.7980\n",
      "Epoch 227: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5004 - acc: 0.7980 - val_loss: 0.5220 - val_acc: 0.7773\n",
      "Epoch 228/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.7994\n",
      "Epoch 228: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4797 - acc: 0.7995 - val_loss: 0.5270 - val_acc: 0.7965\n",
      "Epoch 229/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4907 - acc: 0.8055\n",
      "Epoch 229: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4907 - acc: 0.8055 - val_loss: 0.5410 - val_acc: 0.7845\n",
      "Epoch 230/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.8047\n",
      "Epoch 230: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5135 - acc: 0.8043 - val_loss: 0.5294 - val_acc: 0.7905\n",
      "Epoch 231/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5206 - acc: 0.8007\n",
      "Epoch 231: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5206 - acc: 0.8007 - val_loss: 0.5534 - val_acc: 0.7743\n",
      "Epoch 232/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8008\n",
      "Epoch 232: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5062 - acc: 0.8000 - val_loss: 0.5330 - val_acc: 0.7683\n",
      "Epoch 233/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.7981\n",
      "Epoch 233: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4846 - acc: 0.7982 - val_loss: 0.5463 - val_acc: 0.7691\n",
      "Epoch 234/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.8017\n",
      "Epoch 234: val_acc did not improve from 0.80547\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5394 - acc: 0.8016 - val_loss: 0.5260 - val_acc: 0.7777\n",
      "Epoch 235/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8006\n",
      "Epoch 235: val_acc improved from 0.80547 to 0.80633, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4910 - acc: 0.8004 - val_loss: 0.5237 - val_acc: 0.8063\n",
      "Epoch 236/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.8006\n",
      "Epoch 236: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5120 - acc: 0.7998 - val_loss: 0.5281 - val_acc: 0.7730\n",
      "Epoch 237/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7991\n",
      "Epoch 237: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5237 - acc: 0.7983 - val_loss: 0.5234 - val_acc: 0.7764\n",
      "Epoch 238/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8056\n",
      "Epoch 238: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5474 - acc: 0.8048 - val_loss: 0.5427 - val_acc: 0.7901\n",
      "Epoch 239/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.7970\n",
      "Epoch 239: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5390 - acc: 0.7966 - val_loss: 0.5511 - val_acc: 0.7867\n",
      "Epoch 240/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.7977\n",
      "Epoch 240: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5079 - acc: 0.7965 - val_loss: 0.5422 - val_acc: 0.7713\n",
      "Epoch 241/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4715 - acc: 0.8042\n",
      "Epoch 241: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4715 - acc: 0.8042 - val_loss: 0.5308 - val_acc: 0.7845\n",
      "Epoch 242/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8071\n",
      "Epoch 242: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4973 - acc: 0.8053 - val_loss: 0.5647 - val_acc: 0.7850\n",
      "Epoch 243/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4885 - acc: 0.7986\n",
      "Epoch 243: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4881 - acc: 0.7985 - val_loss: 0.5324 - val_acc: 0.7901\n",
      "Epoch 244/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7989\n",
      "Epoch 244: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5125 - acc: 0.7991 - val_loss: 0.5491 - val_acc: 0.8016\n",
      "Epoch 245/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.7966\n",
      "Epoch 245: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5182 - acc: 0.7969 - val_loss: 0.5613 - val_acc: 0.7730\n",
      "Epoch 246/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8015\n",
      "Epoch 246: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5216 - acc: 0.8012 - val_loss: 0.5392 - val_acc: 0.7738\n",
      "Epoch 247/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8054\n",
      "Epoch 247: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5088 - acc: 0.8044 - val_loss: 0.5038 - val_acc: 0.7824\n",
      "Epoch 248/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8031\n",
      "Epoch 248: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5141 - acc: 0.8024 - val_loss: 0.5187 - val_acc: 0.8033\n",
      "Epoch 249/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8058\n",
      "Epoch 249: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4853 - acc: 0.8057 - val_loss: 0.5546 - val_acc: 0.7914\n",
      "Epoch 250/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4903 - acc: 0.8020\n",
      "Epoch 250: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4888 - acc: 0.8019 - val_loss: 0.5416 - val_acc: 0.7901\n",
      "Epoch 251/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.8015\n",
      "Epoch 251: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5305 - acc: 0.8015 - val_loss: 0.5368 - val_acc: 0.7952\n",
      "Epoch 252/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4908 - acc: 0.8084\n",
      "Epoch 252: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4908 - acc: 0.8084 - val_loss: 0.5367 - val_acc: 0.7858\n",
      "Epoch 253/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8081\n",
      "Epoch 253: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4749 - acc: 0.8069 - val_loss: 0.5522 - val_acc: 0.7764\n",
      "Epoch 254/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.8035\n",
      "Epoch 254: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4944 - acc: 0.8035 - val_loss: 0.5708 - val_acc: 0.7939\n",
      "Epoch 255/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.8010\n",
      "Epoch 255: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4917 - acc: 0.8011 - val_loss: 0.5599 - val_acc: 0.7841\n",
      "Epoch 256/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.8014\n",
      "Epoch 256: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5209 - acc: 0.8014 - val_loss: 0.5476 - val_acc: 0.7811\n",
      "Epoch 257/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5525 - acc: 0.8035\n",
      "Epoch 257: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5525 - acc: 0.8035 - val_loss: 0.5588 - val_acc: 0.7815\n",
      "Epoch 258/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8063\n",
      "Epoch 258: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4777 - acc: 0.8058 - val_loss: 0.5885 - val_acc: 0.7862\n",
      "Epoch 259/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8024\n",
      "Epoch 259: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5109 - acc: 0.8018 - val_loss: 0.5665 - val_acc: 0.7918\n",
      "Epoch 260/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8055\n",
      "Epoch 260: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5126 - acc: 0.8045 - val_loss: 0.5310 - val_acc: 0.7862\n",
      "Epoch 261/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.7978\n",
      "Epoch 261: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4903 - acc: 0.7979 - val_loss: 0.5389 - val_acc: 0.7832\n",
      "Epoch 262/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5145 - acc: 0.8020\n",
      "Epoch 262: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5145 - acc: 0.8020 - val_loss: 0.5158 - val_acc: 0.7858\n",
      "Epoch 263/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8028\n",
      "Epoch 263: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4765 - acc: 0.8022 - val_loss: 0.5281 - val_acc: 0.7854\n",
      "Epoch 264/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.8025\n",
      "Epoch 264: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4852 - acc: 0.8026 - val_loss: 0.5310 - val_acc: 0.7760\n",
      "Epoch 265/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4884 - acc: 0.8043\n",
      "Epoch 265: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4889 - acc: 0.8034 - val_loss: 0.5454 - val_acc: 0.7918\n",
      "Epoch 266/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8050\n",
      "Epoch 266: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5138 - acc: 0.8044 - val_loss: 0.5503 - val_acc: 0.8059\n",
      "Epoch 267/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5038 - acc: 0.8038\n",
      "Epoch 267: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5038 - acc: 0.8038 - val_loss: 0.5186 - val_acc: 0.7764\n",
      "Epoch 268/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8054\n",
      "Epoch 268: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5111 - acc: 0.8056 - val_loss: 0.5541 - val_acc: 0.7768\n",
      "Epoch 269/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5244 - acc: 0.8099\n",
      "Epoch 269: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.8099 - val_loss: 0.5474 - val_acc: 0.7858\n",
      "Epoch 270/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5216 - acc: 0.8036\n",
      "Epoch 270: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5216 - acc: 0.8036 - val_loss: 0.5352 - val_acc: 0.7854\n",
      "Epoch 271/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8062\n",
      "Epoch 271: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4732 - acc: 0.8047 - val_loss: 0.5345 - val_acc: 0.7969\n",
      "Epoch 272/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.8062\n",
      "Epoch 272: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4944 - acc: 0.8062 - val_loss: 0.5562 - val_acc: 0.7640\n",
      "Epoch 273/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4897 - acc: 0.8047\n",
      "Epoch 273: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4909 - acc: 0.8038 - val_loss: 0.5255 - val_acc: 0.7627\n",
      "Epoch 274/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.8046\n",
      "Epoch 274: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4693 - acc: 0.8043 - val_loss: 0.5355 - val_acc: 0.7926\n",
      "Epoch 275/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4862 - acc: 0.8114\n",
      "Epoch 275: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4857 - acc: 0.8108 - val_loss: 0.5537 - val_acc: 0.7867\n",
      "Epoch 276/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.8066\n",
      "Epoch 276: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5605 - acc: 0.8060 - val_loss: 0.5091 - val_acc: 0.7841\n",
      "Epoch 277/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4805 - acc: 0.8013\n",
      "Epoch 277: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4805 - acc: 0.8013 - val_loss: 0.5479 - val_acc: 0.7790\n",
      "Epoch 278/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8049\n",
      "Epoch 278: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4957 - acc: 0.8048 - val_loss: 0.5525 - val_acc: 0.7820\n",
      "Epoch 279/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.8032\n",
      "Epoch 279: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5029 - acc: 0.8032 - val_loss: 0.5341 - val_acc: 0.8012\n",
      "Epoch 280/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8019\n",
      "Epoch 280: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5278 - acc: 0.8015 - val_loss: 0.5278 - val_acc: 0.7790\n",
      "Epoch 281/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8058\n",
      "Epoch 281: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5053 - acc: 0.8052 - val_loss: 0.5450 - val_acc: 0.7832\n",
      "Epoch 282/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8068\n",
      "Epoch 282: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4858 - acc: 0.8069 - val_loss: 0.5064 - val_acc: 0.7909\n",
      "Epoch 283/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8061\n",
      "Epoch 283: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4828 - acc: 0.8062 - val_loss: 0.5235 - val_acc: 0.7884\n",
      "Epoch 284/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.8051\n",
      "Epoch 284: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5414 - acc: 0.8041 - val_loss: 0.5088 - val_acc: 0.8016\n",
      "Epoch 285/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8077\n",
      "Epoch 285: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5146 - acc: 0.8080 - val_loss: 0.5389 - val_acc: 0.8042\n",
      "Epoch 286/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8099\n",
      "Epoch 286: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4911 - acc: 0.8092 - val_loss: 0.5148 - val_acc: 0.7897\n",
      "Epoch 287/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8025\n",
      "Epoch 287: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4914 - acc: 0.8024 - val_loss: 0.5345 - val_acc: 0.7790\n",
      "Epoch 288/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7979\n",
      "Epoch 288: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5223 - acc: 0.7968 - val_loss: 0.5231 - val_acc: 0.7760\n",
      "Epoch 289/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.8034\n",
      "Epoch 289: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4863 - acc: 0.8019 - val_loss: 0.5195 - val_acc: 0.7897\n",
      "Epoch 290/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.7976\n",
      "Epoch 290: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5017 - acc: 0.7976 - val_loss: 0.5147 - val_acc: 0.7726\n",
      "Epoch 291/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8049\n",
      "Epoch 291: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4897 - acc: 0.8048 - val_loss: 0.5273 - val_acc: 0.7760\n",
      "Epoch 292/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.8001\n",
      "Epoch 292: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5117 - acc: 0.7995 - val_loss: 0.5189 - val_acc: 0.7811\n",
      "Epoch 293/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.8020\n",
      "Epoch 293: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4887 - acc: 0.8018 - val_loss: 0.4896 - val_acc: 0.7768\n",
      "Epoch 294/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8074\n",
      "Epoch 294: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4839 - acc: 0.8068 - val_loss: 0.5363 - val_acc: 0.7717\n",
      "Epoch 295/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8093\n",
      "Epoch 295: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4875 - acc: 0.8092 - val_loss: 0.5691 - val_acc: 0.7879\n",
      "Epoch 296/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.8013\n",
      "Epoch 296: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5071 - acc: 0.8016 - val_loss: 0.5324 - val_acc: 0.7828\n",
      "Epoch 297/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8096\n",
      "Epoch 297: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4854 - acc: 0.8096 - val_loss: 0.5655 - val_acc: 0.7956\n",
      "Epoch 298/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4778 - acc: 0.8129\n",
      "Epoch 298: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4791 - acc: 0.8123 - val_loss: 0.5325 - val_acc: 0.7978\n",
      "Epoch 299/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.8086\n",
      "Epoch 299: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5036 - acc: 0.8082 - val_loss: 0.5217 - val_acc: 0.8008\n",
      "Epoch 300/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8101\n",
      "Epoch 300: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5211 - acc: 0.8091 - val_loss: 0.5273 - val_acc: 0.7790\n",
      "Epoch 301/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8013\n",
      "Epoch 301: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5038 - acc: 0.8007 - val_loss: 0.5334 - val_acc: 0.7811\n",
      "Epoch 302/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8023\n",
      "Epoch 302: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5113 - acc: 0.8024 - val_loss: 0.5348 - val_acc: 0.7794\n",
      "Epoch 303/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.8101\n",
      "Epoch 303: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5352 - acc: 0.8099 - val_loss: 0.5361 - val_acc: 0.7918\n",
      "Epoch 304/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4639 - acc: 0.8041\n",
      "Epoch 304: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4642 - acc: 0.8034 - val_loss: 0.5423 - val_acc: 0.7768\n",
      "Epoch 305/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.7998\n",
      "Epoch 305: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5032 - acc: 0.7996 - val_loss: 0.5765 - val_acc: 0.7978\n",
      "Epoch 306/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4717 - acc: 0.8111\n",
      "Epoch 306: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4712 - acc: 0.8110 - val_loss: 0.5501 - val_acc: 0.7820\n",
      "Epoch 307/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4836 - acc: 0.8056\n",
      "Epoch 307: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4830 - acc: 0.8057 - val_loss: 0.5134 - val_acc: 0.7901\n",
      "Epoch 308/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4778 - acc: 0.8028\n",
      "Epoch 308: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4776 - acc: 0.8027 - val_loss: 0.5179 - val_acc: 0.7700\n",
      "Epoch 309/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.8097\n",
      "Epoch 309: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4909 - acc: 0.8091 - val_loss: 0.5208 - val_acc: 0.7845\n",
      "Epoch 310/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.8046\n",
      "Epoch 310: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5169 - acc: 0.8042 - val_loss: 0.5466 - val_acc: 0.7944\n",
      "Epoch 311/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8000\n",
      "Epoch 311: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4844 - acc: 0.8000 - val_loss: 0.5405 - val_acc: 0.7773\n",
      "Epoch 312/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.8075\n",
      "Epoch 312: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5303 - acc: 0.8072 - val_loss: 0.5135 - val_acc: 0.7674\n",
      "Epoch 313/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.8002\n",
      "Epoch 313: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5508 - acc: 0.8000 - val_loss: 0.5421 - val_acc: 0.7653\n",
      "Epoch 314/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4872 - acc: 0.8032\n",
      "Epoch 314: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4872 - acc: 0.8032 - val_loss: 0.5450 - val_acc: 0.7696\n",
      "Epoch 315/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.8053\n",
      "Epoch 315: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5297 - acc: 0.8053 - val_loss: 0.5541 - val_acc: 0.7965\n",
      "Epoch 316/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4863 - acc: 0.8102\n",
      "Epoch 316: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4863 - acc: 0.8102 - val_loss: 0.5390 - val_acc: 0.7790\n",
      "Epoch 317/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8078\n",
      "Epoch 317: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4826 - acc: 0.8077 - val_loss: 0.5464 - val_acc: 0.7738\n",
      "Epoch 318/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.8032\n",
      "Epoch 318: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5261 - acc: 0.8030 - val_loss: 0.5378 - val_acc: 0.7670\n",
      "Epoch 319/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8065\n",
      "Epoch 319: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5462 - acc: 0.8062 - val_loss: 0.5491 - val_acc: 0.7884\n",
      "Epoch 320/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8036\n",
      "Epoch 320: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4998 - acc: 0.8032 - val_loss: 0.5239 - val_acc: 0.7871\n",
      "Epoch 321/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.8067\n",
      "Epoch 321: val_acc improved from 0.80633 to 0.80889, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5201 - acc: 0.8066 - val_loss: 0.5342 - val_acc: 0.8089\n",
      "Epoch 322/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5461 - acc: 0.8089\n",
      "Epoch 322: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5461 - acc: 0.8089 - val_loss: 0.5644 - val_acc: 0.7828\n",
      "Epoch 323/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.7993\n",
      "Epoch 323: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4962 - acc: 0.7994 - val_loss: 0.5509 - val_acc: 0.7674\n",
      "Epoch 324/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.8026\n",
      "Epoch 324: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5166 - acc: 0.8026 - val_loss: 0.5420 - val_acc: 0.7858\n",
      "Epoch 325/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.8059\n",
      "Epoch 325: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5709 - acc: 0.8059 - val_loss: 0.5111 - val_acc: 0.7768\n",
      "Epoch 326/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.8068\n",
      "Epoch 326: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5079 - acc: 0.8067 - val_loss: 0.5165 - val_acc: 0.7841\n",
      "Epoch 327/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8053\n",
      "Epoch 327: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4964 - acc: 0.8051 - val_loss: 0.5285 - val_acc: 0.7747\n",
      "Epoch 328/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8062\n",
      "Epoch 328: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4926 - acc: 0.8053 - val_loss: 0.5373 - val_acc: 0.7922\n",
      "Epoch 329/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5244 - acc: 0.8064\n",
      "Epoch 329: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.8064 - val_loss: 0.5462 - val_acc: 0.7824\n",
      "Epoch 330/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8067\n",
      "Epoch 330: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5258 - acc: 0.8061 - val_loss: 0.5244 - val_acc: 0.7901\n",
      "Epoch 331/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8063\n",
      "Epoch 331: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4775 - acc: 0.8058 - val_loss: 0.5296 - val_acc: 0.7841\n",
      "Epoch 332/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.8127\n",
      "Epoch 332: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4937 - acc: 0.8128 - val_loss: 0.5473 - val_acc: 0.7944\n",
      "Epoch 333/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.8021\n",
      "Epoch 333: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5147 - acc: 0.8014 - val_loss: 0.5328 - val_acc: 0.7657\n",
      "Epoch 334/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4844 - acc: 0.8012\n",
      "Epoch 334: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4842 - acc: 0.8011 - val_loss: 0.5390 - val_acc: 0.7700\n",
      "Epoch 335/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.8082\n",
      "Epoch 335: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5177 - acc: 0.8083 - val_loss: 0.5930 - val_acc: 0.7755\n",
      "Epoch 336/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4978 - acc: 0.8018\n",
      "Epoch 336: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4980 - acc: 0.8013 - val_loss: 0.5440 - val_acc: 0.7832\n",
      "Epoch 337/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8054\n",
      "Epoch 337: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4888 - acc: 0.8053 - val_loss: 0.5549 - val_acc: 0.7730\n",
      "Epoch 338/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8120\n",
      "Epoch 338: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5049 - acc: 0.8118 - val_loss: 0.5783 - val_acc: 0.7794\n",
      "Epoch 339/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.8075\n",
      "Epoch 339: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5204 - acc: 0.8069 - val_loss: 0.5723 - val_acc: 0.7965\n",
      "Epoch 340/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8090\n",
      "Epoch 340: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5060 - acc: 0.8074 - val_loss: 0.5545 - val_acc: 0.7738\n",
      "Epoch 341/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.8015\n",
      "Epoch 341: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5139 - acc: 0.8015 - val_loss: 0.5718 - val_acc: 0.7901\n",
      "Epoch 342/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7988\n",
      "Epoch 342: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5005 - acc: 0.7982 - val_loss: 0.5635 - val_acc: 0.7884\n",
      "Epoch 343/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.8054\n",
      "Epoch 343: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4923 - acc: 0.8055 - val_loss: 0.5501 - val_acc: 0.7790\n",
      "Epoch 344/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.8063\n",
      "Epoch 344: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5369 - acc: 0.8063 - val_loss: 0.5064 - val_acc: 0.7747\n",
      "Epoch 345/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8050\n",
      "Epoch 345: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4772 - acc: 0.8046 - val_loss: 0.5404 - val_acc: 0.7828\n",
      "Epoch 346/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8093\n",
      "Epoch 346: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4966 - acc: 0.8094 - val_loss: 0.5487 - val_acc: 0.7815\n",
      "Epoch 347/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5728 - acc: 0.8054\n",
      "Epoch 347: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5723 - acc: 0.8050 - val_loss: 0.5385 - val_acc: 0.7674\n",
      "Epoch 348/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.8080\n",
      "Epoch 348: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5143 - acc: 0.8067 - val_loss: 0.5210 - val_acc: 0.7760\n",
      "Epoch 349/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.8076\n",
      "Epoch 349: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4867 - acc: 0.8074 - val_loss: 0.5476 - val_acc: 0.7956\n",
      "Epoch 350/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4994 - acc: 0.8080\n",
      "Epoch 350: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4993 - acc: 0.8080 - val_loss: 0.5321 - val_acc: 0.7969\n",
      "Epoch 351/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8075\n",
      "Epoch 351: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4924 - acc: 0.8067 - val_loss: 0.5443 - val_acc: 0.7841\n",
      "Epoch 352/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.8069\n",
      "Epoch 352: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.8060 - val_loss: 0.5396 - val_acc: 0.8003\n",
      "Epoch 353/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.8055\n",
      "Epoch 353: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4868 - acc: 0.8057 - val_loss: 0.5318 - val_acc: 0.7918\n",
      "Epoch 354/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8120\n",
      "Epoch 354: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4637 - acc: 0.8110 - val_loss: 0.5436 - val_acc: 0.7726\n",
      "Epoch 355/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.8084\n",
      "Epoch 355: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5076 - acc: 0.8084 - val_loss: 0.5405 - val_acc: 0.7837\n",
      "Epoch 356/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8099\n",
      "Epoch 356: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4959 - acc: 0.8097 - val_loss: 0.5499 - val_acc: 0.7948\n",
      "Epoch 357/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.8081\n",
      "Epoch 357: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5167 - acc: 0.8074 - val_loss: 0.5453 - val_acc: 0.7738\n",
      "Epoch 358/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8048\n",
      "Epoch 358: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5013 - acc: 0.8048 - val_loss: 0.5522 - val_acc: 0.7879\n",
      "Epoch 359/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.8092\n",
      "Epoch 359: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5329 - acc: 0.8089 - val_loss: 0.5338 - val_acc: 0.7918\n",
      "Epoch 360/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.8065\n",
      "Epoch 360: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4835 - acc: 0.8064 - val_loss: 0.5413 - val_acc: 0.7755\n",
      "Epoch 361/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8094\n",
      "Epoch 361: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4773 - acc: 0.8088 - val_loss: 0.5499 - val_acc: 0.7931\n",
      "Epoch 362/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.8070\n",
      "Epoch 362: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5246 - acc: 0.8069 - val_loss: 0.5366 - val_acc: 0.7935\n",
      "Epoch 363/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8103\n",
      "Epoch 363: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4772 - acc: 0.8103 - val_loss: 0.5713 - val_acc: 0.7854\n",
      "Epoch 364/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.8057\n",
      "Epoch 364: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5261 - acc: 0.8057 - val_loss: 0.5539 - val_acc: 0.7726\n",
      "Epoch 365/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.8094\n",
      "Epoch 365: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5788 - acc: 0.8094 - val_loss: 0.5363 - val_acc: 0.7862\n",
      "Epoch 366/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.8063\n",
      "Epoch 366: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5333 - acc: 0.8062 - val_loss: 0.5465 - val_acc: 0.7832\n",
      "Epoch 367/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8086\n",
      "Epoch 367: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5047 - acc: 0.8083 - val_loss: 0.5324 - val_acc: 0.7850\n",
      "Epoch 368/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8086\n",
      "Epoch 368: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5042 - acc: 0.8072 - val_loss: 0.5254 - val_acc: 0.7687\n",
      "Epoch 369/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.8144\n",
      "Epoch 369: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5518 - acc: 0.8141 - val_loss: 0.5394 - val_acc: 0.8042\n",
      "Epoch 370/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8020\n",
      "Epoch 370: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4838 - acc: 0.8018 - val_loss: 0.5586 - val_acc: 0.7986\n",
      "Epoch 371/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8057\n",
      "Epoch 371: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5114 - acc: 0.8057 - val_loss: 0.5582 - val_acc: 0.7999\n",
      "Epoch 372/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8088\n",
      "Epoch 372: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4966 - acc: 0.8088 - val_loss: 0.5268 - val_acc: 0.7785\n",
      "Epoch 373/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.8130\n",
      "Epoch 373: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5018 - acc: 0.8127 - val_loss: 0.5767 - val_acc: 0.7901\n",
      "Epoch 374/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.8106\n",
      "Epoch 374: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4758 - acc: 0.8102 - val_loss: 0.5702 - val_acc: 0.7982\n",
      "Epoch 375/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8101\n",
      "Epoch 375: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4861 - acc: 0.8094 - val_loss: 0.5702 - val_acc: 0.7726\n",
      "Epoch 376/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.8088\n",
      "Epoch 376: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4896 - acc: 0.8088 - val_loss: 0.5354 - val_acc: 0.8025\n",
      "Epoch 377/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8041\n",
      "Epoch 377: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5501 - acc: 0.8038 - val_loss: 0.5233 - val_acc: 0.7922\n",
      "Epoch 378/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8091\n",
      "Epoch 378: val_acc did not improve from 0.80889\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5017 - acc: 0.8086 - val_loss: 0.5011 - val_acc: 0.7713\n",
      "Epoch 379/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.8045\n",
      "Epoch 379: val_acc improved from 0.80889 to 0.80932, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5755 - acc: 0.8052 - val_loss: 0.5818 - val_acc: 0.8093\n",
      "Epoch 380/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8049\n",
      "Epoch 380: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4852 - acc: 0.8045 - val_loss: 0.5245 - val_acc: 0.7802\n",
      "Epoch 381/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8065\n",
      "Epoch 381: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5020 - acc: 0.8062 - val_loss: 0.5105 - val_acc: 0.7935\n",
      "Epoch 382/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.8116\n",
      "Epoch 382: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4978 - acc: 0.8103 - val_loss: 0.5074 - val_acc: 0.7743\n",
      "Epoch 383/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8080\n",
      "Epoch 383: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4837 - acc: 0.8079 - val_loss: 0.5315 - val_acc: 0.7760\n",
      "Epoch 384/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4785 - acc: 0.8047\n",
      "Epoch 384: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4785 - acc: 0.8047 - val_loss: 0.5204 - val_acc: 0.7777\n",
      "Epoch 385/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8115\n",
      "Epoch 385: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5058 - acc: 0.8109 - val_loss: 0.5332 - val_acc: 0.8059\n",
      "Epoch 386/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.8000\n",
      "Epoch 386: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5162 - acc: 0.7994 - val_loss: 0.5191 - val_acc: 0.7807\n",
      "Epoch 387/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8081\n",
      "Epoch 387: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4834 - acc: 0.8073 - val_loss: 0.5390 - val_acc: 0.7785\n",
      "Epoch 388/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5016 - acc: 0.8063\n",
      "Epoch 388: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5006 - acc: 0.8064 - val_loss: 0.5479 - val_acc: 0.7918\n",
      "Epoch 389/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.8044\n",
      "Epoch 389: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4944 - acc: 0.8042 - val_loss: 0.5211 - val_acc: 0.7828\n",
      "Epoch 390/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8094\n",
      "Epoch 390: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5097 - acc: 0.8086 - val_loss: 0.5191 - val_acc: 0.7747\n",
      "Epoch 391/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4907 - acc: 0.8099\n",
      "Epoch 391: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4903 - acc: 0.8098 - val_loss: 0.5264 - val_acc: 0.7631\n",
      "Epoch 392/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8093\n",
      "Epoch 392: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5013 - acc: 0.8092 - val_loss: 0.5261 - val_acc: 0.7717\n",
      "Epoch 393/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8066\n",
      "Epoch 393: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4906 - acc: 0.8066 - val_loss: 0.5757 - val_acc: 0.7939\n",
      "Epoch 394/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.8075\n",
      "Epoch 394: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4768 - acc: 0.8076 - val_loss: 0.5204 - val_acc: 0.7884\n",
      "Epoch 395/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4794 - acc: 0.8062\n",
      "Epoch 395: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4791 - acc: 0.8063 - val_loss: 0.5690 - val_acc: 0.7871\n",
      "Epoch 396/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4933 - acc: 0.8100\n",
      "Epoch 396: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4930 - acc: 0.8102 - val_loss: 0.5307 - val_acc: 0.8072\n",
      "Epoch 397/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.8047\n",
      "Epoch 397: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5340 - acc: 0.8047 - val_loss: 0.6069 - val_acc: 0.7884\n",
      "Epoch 398/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5641 - acc: 0.8064\n",
      "Epoch 398: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5601 - acc: 0.8067 - val_loss: 0.5362 - val_acc: 0.7952\n",
      "Epoch 399/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8105\n",
      "Epoch 399: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4899 - acc: 0.8097 - val_loss: 0.5345 - val_acc: 0.7909\n",
      "Epoch 400/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8108\n",
      "Epoch 400: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5014 - acc: 0.8104 - val_loss: 0.5286 - val_acc: 0.7858\n",
      "Epoch 401/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8091\n",
      "Epoch 401: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4798 - acc: 0.8091 - val_loss: 0.5316 - val_acc: 0.7884\n",
      "Epoch 402/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8096\n",
      "Epoch 402: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5064 - acc: 0.8091 - val_loss: 0.5942 - val_acc: 0.7978\n",
      "Epoch 403/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8064\n",
      "Epoch 403: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5411 - acc: 0.8050 - val_loss: 0.5018 - val_acc: 0.7807\n",
      "Epoch 404/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8100\n",
      "Epoch 404: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4919 - acc: 0.8098 - val_loss: 0.5566 - val_acc: 0.7867\n",
      "Epoch 405/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8077\n",
      "Epoch 405: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4970 - acc: 0.8077 - val_loss: 0.5428 - val_acc: 0.7696\n",
      "Epoch 406/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.8056\n",
      "Epoch 406: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5320 - acc: 0.8055 - val_loss: 0.5492 - val_acc: 0.7845\n",
      "Epoch 407/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.8100\n",
      "Epoch 407: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5162 - acc: 0.8092 - val_loss: 0.5606 - val_acc: 0.7854\n",
      "Epoch 408/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4730 - acc: 0.8093\n",
      "Epoch 408: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4736 - acc: 0.8086 - val_loss: 0.5311 - val_acc: 0.7991\n",
      "Epoch 409/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8062\n",
      "Epoch 409: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4825 - acc: 0.8049 - val_loss: 0.5499 - val_acc: 0.7905\n",
      "Epoch 410/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.8078\n",
      "Epoch 410: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4808 - acc: 0.8068 - val_loss: 0.5278 - val_acc: 0.7867\n",
      "Epoch 411/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8177\n",
      "Epoch 411: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4844 - acc: 0.8168 - val_loss: 0.5598 - val_acc: 0.7807\n",
      "Epoch 412/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4808 - acc: 0.8110\n",
      "Epoch 412: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4808 - acc: 0.8110 - val_loss: 0.5753 - val_acc: 0.7901\n",
      "Epoch 413/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.8109\n",
      "Epoch 413: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4988 - acc: 0.8097 - val_loss: 0.5606 - val_acc: 0.7773\n",
      "Epoch 414/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8050\n",
      "Epoch 414: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5051 - acc: 0.8048 - val_loss: 0.5329 - val_acc: 0.7640\n",
      "Epoch 415/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8122\n",
      "Epoch 415: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5317 - acc: 0.8114 - val_loss: 0.5340 - val_acc: 0.7905\n",
      "Epoch 416/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8087\n",
      "Epoch 416: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5074 - acc: 0.8082 - val_loss: 0.5351 - val_acc: 0.7956\n",
      "Epoch 417/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.8125\n",
      "Epoch 417: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4783 - acc: 0.8115 - val_loss: 0.5384 - val_acc: 0.7892\n",
      "Epoch 418/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5251 - acc: 0.8072\n",
      "Epoch 418: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5251 - acc: 0.8072 - val_loss: 0.5258 - val_acc: 0.7704\n",
      "Epoch 419/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5256 - acc: 0.8056\n",
      "Epoch 419: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5256 - acc: 0.8056 - val_loss: 0.5387 - val_acc: 0.7850\n",
      "Epoch 420/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8112\n",
      "Epoch 420: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4872 - acc: 0.8112 - val_loss: 0.5673 - val_acc: 0.7738\n",
      "Epoch 421/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8028\n",
      "Epoch 421: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4782 - acc: 0.8020 - val_loss: 0.5414 - val_acc: 0.7948\n",
      "Epoch 422/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5073 - acc: 0.8060\n",
      "Epoch 422: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5055 - acc: 0.8061 - val_loss: 0.5703 - val_acc: 0.7961\n",
      "Epoch 423/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4925 - acc: 0.8078\n",
      "Epoch 423: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4922 - acc: 0.8075 - val_loss: 0.5362 - val_acc: 0.7867\n",
      "Epoch 424/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.8081\n",
      "Epoch 424: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5414 - acc: 0.8073 - val_loss: 0.5300 - val_acc: 0.7858\n",
      "Epoch 425/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8051\n",
      "Epoch 425: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5157 - acc: 0.8047 - val_loss: 0.5444 - val_acc: 0.8003\n",
      "Epoch 426/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8156\n",
      "Epoch 426: val_acc did not improve from 0.80932\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4999 - acc: 0.8148 - val_loss: 0.5391 - val_acc: 0.7909\n",
      "Epoch 427/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.8121\n",
      "Epoch 427: val_acc improved from 0.80932 to 0.81018, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5139 - acc: 0.8118 - val_loss: 0.5568 - val_acc: 0.8102\n",
      "Epoch 428/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.8120\n",
      "Epoch 428: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5092 - acc: 0.8117 - val_loss: 0.5364 - val_acc: 0.7952\n",
      "Epoch 429/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8114\n",
      "Epoch 429: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4823 - acc: 0.8114 - val_loss: 0.5291 - val_acc: 0.7666\n",
      "Epoch 430/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.8067\n",
      "Epoch 430: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5098 - acc: 0.8061 - val_loss: 0.5409 - val_acc: 0.7687\n",
      "Epoch 431/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6397 - acc: 0.8049\n",
      "Epoch 431: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6397 - acc: 0.8049 - val_loss: 0.5349 - val_acc: 0.7807\n",
      "Epoch 432/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8138\n",
      "Epoch 432: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4915 - acc: 0.8138 - val_loss: 0.5377 - val_acc: 0.7897\n",
      "Epoch 433/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8118\n",
      "Epoch 433: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4997 - acc: 0.8113 - val_loss: 0.5489 - val_acc: 0.7841\n",
      "Epoch 434/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5058 - acc: 0.8111\n",
      "Epoch 434: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5058 - acc: 0.8111 - val_loss: 0.5319 - val_acc: 0.7854\n",
      "Epoch 435/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8088\n",
      "Epoch 435: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5101 - acc: 0.8090 - val_loss: 0.5573 - val_acc: 0.8050\n",
      "Epoch 436/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8129\n",
      "Epoch 436: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4853 - acc: 0.8121 - val_loss: 0.5602 - val_acc: 0.7708\n",
      "Epoch 437/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.8048\n",
      "Epoch 437: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5138 - acc: 0.8047 - val_loss: 0.5626 - val_acc: 0.7781\n",
      "Epoch 438/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8114\n",
      "Epoch 438: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5294 - acc: 0.8110 - val_loss: 0.5629 - val_acc: 0.7815\n",
      "Epoch 439/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4789 - acc: 0.8093\n",
      "Epoch 439: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4879 - acc: 0.8081 - val_loss: 0.5498 - val_acc: 0.7862\n",
      "Epoch 440/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8081\n",
      "Epoch 440: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5108 - acc: 0.8068 - val_loss: 0.5819 - val_acc: 0.7837\n",
      "Epoch 441/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5947 - acc: 0.8081\n",
      "Epoch 441: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5947 - acc: 0.8081 - val_loss: 0.5692 - val_acc: 0.7760\n",
      "Epoch 442/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8094\n",
      "Epoch 442: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4946 - acc: 0.8091 - val_loss: 0.5301 - val_acc: 0.7837\n",
      "Epoch 443/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8059\n",
      "Epoch 443: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4860 - acc: 0.8058 - val_loss: 0.5462 - val_acc: 0.7811\n",
      "Epoch 444/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.8155\n",
      "Epoch 444: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5143 - acc: 0.8148 - val_loss: 0.5712 - val_acc: 0.7999\n",
      "Epoch 445/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.8039\n",
      "Epoch 445: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5106 - acc: 0.8033 - val_loss: 0.5397 - val_acc: 0.7845\n",
      "Epoch 446/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5050 - acc: 0.8135\n",
      "Epoch 446: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5032 - acc: 0.8129 - val_loss: 0.5396 - val_acc: 0.7815\n",
      "Epoch 447/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8094\n",
      "Epoch 447: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5017 - acc: 0.8094 - val_loss: 0.5793 - val_acc: 0.7850\n",
      "Epoch 448/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.8166\n",
      "Epoch 448: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5826 - acc: 0.8164 - val_loss: 0.5612 - val_acc: 0.7875\n",
      "Epoch 449/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8149\n",
      "Epoch 449: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5142 - acc: 0.8143 - val_loss: 0.5505 - val_acc: 0.7999\n",
      "Epoch 450/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8063\n",
      "Epoch 450: val_acc did not improve from 0.81018\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5010 - acc: 0.8061 - val_loss: 0.5603 - val_acc: 0.7952\n",
      "Epoch 451/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.8129\n",
      "Epoch 451: val_acc improved from 0.81018 to 0.81231, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/2/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5040 - acc: 0.8128 - val_loss: 0.5635 - val_acc: 0.8123\n",
      "Epoch 452/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.8147\n",
      "Epoch 452: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5501 - acc: 0.8130 - val_loss: 0.5627 - val_acc: 0.7747\n",
      "Epoch 453/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5476 - acc: 0.8063\n",
      "Epoch 453: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5476 - acc: 0.8063 - val_loss: 0.5393 - val_acc: 0.7837\n",
      "Epoch 454/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4741 - acc: 0.8065\n",
      "Epoch 454: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4733 - acc: 0.8063 - val_loss: 0.5408 - val_acc: 0.7781\n",
      "Epoch 455/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8104\n",
      "Epoch 455: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4935 - acc: 0.8100 - val_loss: 0.5779 - val_acc: 0.8089\n",
      "Epoch 456/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8130\n",
      "Epoch 456: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4949 - acc: 0.8128 - val_loss: 0.5585 - val_acc: 0.7897\n",
      "Epoch 457/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.8130\n",
      "Epoch 457: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5208 - acc: 0.8122 - val_loss: 0.5563 - val_acc: 0.7802\n",
      "Epoch 458/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8138\n",
      "Epoch 458: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4918 - acc: 0.8130 - val_loss: 0.5627 - val_acc: 0.7956\n",
      "Epoch 459/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4864 - acc: 0.8129\n",
      "Epoch 459: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4864 - acc: 0.8129 - val_loss: 0.5301 - val_acc: 0.7845\n",
      "Epoch 460/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.8111\n",
      "Epoch 460: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5355 - acc: 0.8103 - val_loss: 0.5418 - val_acc: 0.7956\n",
      "Epoch 461/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5134 - acc: 0.8121\n",
      "Epoch 461: val_acc did not improve from 0.81231\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5134 - acc: 0.8121 - val_loss: 0.5467 - val_acc: 0.7790\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               24832     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,521\n",
      "Trainable params: 107,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 1.7083 - acc: 0.5913\n",
      "Epoch 1: val_acc improved from -inf to 0.68448, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 3s 8ms/step - loss: 1.6932 - acc: 0.5916 - val_loss: 0.6562 - val_acc: 0.6845\n",
      "Epoch 2/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6768 - acc: 0.6451\n",
      "Epoch 2: val_acc improved from 0.68448 to 0.69004, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6776 - acc: 0.6446 - val_loss: 0.6525 - val_acc: 0.6900\n",
      "Epoch 3/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6421 - acc: 0.6633\n",
      "Epoch 3: val_acc did not improve from 0.69004\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6431 - acc: 0.6630 - val_loss: 0.6348 - val_acc: 0.6845\n",
      "Epoch 4/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6202 - acc: 0.6786\n",
      "Epoch 4: val_acc did not improve from 0.69004\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6200 - acc: 0.6789 - val_loss: 0.6178 - val_acc: 0.6845\n",
      "Epoch 5/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6096 - acc: 0.6815\n",
      "Epoch 5: val_acc improved from 0.69004 to 0.71612, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6096 - acc: 0.6815 - val_loss: 0.6248 - val_acc: 0.7161\n",
      "Epoch 6/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.6833\n",
      "Epoch 6: val_acc improved from 0.71612 to 0.73365, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6012 - acc: 0.6833 - val_loss: 0.6020 - val_acc: 0.7336\n",
      "Epoch 7/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5917 - acc: 0.6931\n",
      "Epoch 7: val_acc improved from 0.73365 to 0.74006, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5921 - acc: 0.6924 - val_loss: 0.5827 - val_acc: 0.7401\n",
      "Epoch 8/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5800 - acc: 0.7112\n",
      "Epoch 8: val_acc improved from 0.74006 to 0.74391, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5796 - acc: 0.7108 - val_loss: 0.5509 - val_acc: 0.7439\n",
      "Epoch 9/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.7219\n",
      "Epoch 9: val_acc improved from 0.74391 to 0.74647, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5710 - acc: 0.7212 - val_loss: 0.5502 - val_acc: 0.7465\n",
      "Epoch 10/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.7267\n",
      "Epoch 10: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5654 - acc: 0.7265 - val_loss: 0.5379 - val_acc: 0.7375\n",
      "Epoch 11/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.7332\n",
      "Epoch 11: val_acc improved from 0.74647 to 0.75075, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5556 - acc: 0.7321 - val_loss: 0.5692 - val_acc: 0.7507\n",
      "Epoch 12/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.7327\n",
      "Epoch 12: val_acc did not improve from 0.75075\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5487 - acc: 0.7329 - val_loss: 0.5132 - val_acc: 0.7465\n",
      "Epoch 13/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.7359\n",
      "Epoch 13: val_acc improved from 0.75075 to 0.75973, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5468 - acc: 0.7360 - val_loss: 0.5224 - val_acc: 0.7597\n",
      "Epoch 14/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5480 - acc: 0.7369\n",
      "Epoch 14: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5480 - acc: 0.7369 - val_loss: 0.5270 - val_acc: 0.7499\n",
      "Epoch 15/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.7322\n",
      "Epoch 15: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5468 - acc: 0.7322 - val_loss: 0.5269 - val_acc: 0.7478\n",
      "Epoch 16/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7453\n",
      "Epoch 16: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5366 - acc: 0.7452 - val_loss: 0.5365 - val_acc: 0.7546\n",
      "Epoch 17/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7436\n",
      "Epoch 17: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5372 - acc: 0.7430 - val_loss: 0.5080 - val_acc: 0.7529\n",
      "Epoch 18/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7443\n",
      "Epoch 18: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7438 - val_loss: 0.5069 - val_acc: 0.7542\n",
      "Epoch 19/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.7402\n",
      "Epoch 19: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5361 - acc: 0.7399 - val_loss: 0.5021 - val_acc: 0.7567\n",
      "Epoch 20/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.7462\n",
      "Epoch 20: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5347 - acc: 0.7455 - val_loss: 0.5032 - val_acc: 0.7550\n",
      "Epoch 21/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.7473\n",
      "Epoch 21: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5300 - acc: 0.7472 - val_loss: 0.5013 - val_acc: 0.7525\n",
      "Epoch 22/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.7513\n",
      "Epoch 22: val_acc improved from 0.75973 to 0.76015, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5286 - acc: 0.7505 - val_loss: 0.5275 - val_acc: 0.7602\n",
      "Epoch 23/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7496\n",
      "Epoch 23: val_acc did not improve from 0.76015\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5314 - acc: 0.7493 - val_loss: 0.5200 - val_acc: 0.7576\n",
      "Epoch 24/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5343 - acc: 0.7497\n",
      "Epoch 24: val_acc did not improve from 0.76015\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5335 - acc: 0.7493 - val_loss: 0.5141 - val_acc: 0.7584\n",
      "Epoch 25/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7472\n",
      "Epoch 25: val_acc did not improve from 0.76015\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5292 - acc: 0.7473 - val_loss: 0.5056 - val_acc: 0.7516\n",
      "Epoch 26/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.7508\n",
      "Epoch 26: val_acc improved from 0.76015 to 0.76229, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5281 - acc: 0.7502 - val_loss: 0.4994 - val_acc: 0.7623\n",
      "Epoch 27/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7564\n",
      "Epoch 27: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5300 - acc: 0.7562 - val_loss: 0.5106 - val_acc: 0.7559\n",
      "Epoch 28/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5380 - acc: 0.7565\n",
      "Epoch 28: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5380 - acc: 0.7565 - val_loss: 0.5179 - val_acc: 0.7567\n",
      "Epoch 29/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.7513\n",
      "Epoch 29: val_acc did not improve from 0.76229\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5237 - acc: 0.7509 - val_loss: 0.5080 - val_acc: 0.7503\n",
      "Epoch 30/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.7519\n",
      "Epoch 30: val_acc improved from 0.76229 to 0.76956, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5258 - acc: 0.7525 - val_loss: 0.5054 - val_acc: 0.7696\n",
      "Epoch 31/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.7575\n",
      "Epoch 31: val_acc improved from 0.76956 to 0.77255, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5235 - acc: 0.7575 - val_loss: 0.5145 - val_acc: 0.7726\n",
      "Epoch 32/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7550\n",
      "Epoch 32: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5148 - acc: 0.7551 - val_loss: 0.4953 - val_acc: 0.7576\n",
      "Epoch 33/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.7572\n",
      "Epoch 33: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5120 - acc: 0.7570 - val_loss: 0.5225 - val_acc: 0.7644\n",
      "Epoch 34/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7571\n",
      "Epoch 34: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5242 - acc: 0.7566 - val_loss: 0.5162 - val_acc: 0.7678\n",
      "Epoch 35/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7616\n",
      "Epoch 35: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5249 - acc: 0.7615 - val_loss: 0.5129 - val_acc: 0.7700\n",
      "Epoch 36/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7563\n",
      "Epoch 36: val_acc did not improve from 0.77255\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5173 - acc: 0.7563 - val_loss: 0.5045 - val_acc: 0.7610\n",
      "Epoch 37/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5236 - acc: 0.7598\n",
      "Epoch 37: val_acc improved from 0.77255 to 0.77341, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5236 - acc: 0.7598 - val_loss: 0.5296 - val_acc: 0.7734\n",
      "Epoch 38/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.7610\n",
      "Epoch 38: val_acc improved from 0.77341 to 0.77426, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5221 - acc: 0.7611 - val_loss: 0.5142 - val_acc: 0.7743\n",
      "Epoch 39/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.7574\n",
      "Epoch 39: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5222 - acc: 0.7572 - val_loss: 0.5135 - val_acc: 0.7614\n",
      "Epoch 40/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7595\n",
      "Epoch 40: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5216 - acc: 0.7595 - val_loss: 0.5086 - val_acc: 0.7580\n",
      "Epoch 41/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5228 - acc: 0.7579\n",
      "Epoch 41: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5228 - acc: 0.7579 - val_loss: 0.5155 - val_acc: 0.7563\n",
      "Epoch 42/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.7623\n",
      "Epoch 42: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5261 - acc: 0.7622 - val_loss: 0.5059 - val_acc: 0.7649\n",
      "Epoch 43/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7639\n",
      "Epoch 43: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5326 - acc: 0.7632 - val_loss: 0.4967 - val_acc: 0.7584\n",
      "Epoch 44/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.7646\n",
      "Epoch 44: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5202 - acc: 0.7643 - val_loss: 0.5011 - val_acc: 0.7704\n",
      "Epoch 45/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7591\n",
      "Epoch 45: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5297 - acc: 0.7591 - val_loss: 0.5072 - val_acc: 0.7593\n",
      "Epoch 46/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.7637\n",
      "Epoch 46: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5182 - acc: 0.7630 - val_loss: 0.5133 - val_acc: 0.7738\n",
      "Epoch 47/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7632\n",
      "Epoch 47: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5225 - acc: 0.7630 - val_loss: 0.5111 - val_acc: 0.7610\n",
      "Epoch 48/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.7625\n",
      "Epoch 48: val_acc improved from 0.77426 to 0.77597, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5276 - acc: 0.7625 - val_loss: 0.5202 - val_acc: 0.7760\n",
      "Epoch 49/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.7567\n",
      "Epoch 49: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5442 - acc: 0.7566 - val_loss: 0.5111 - val_acc: 0.7559\n",
      "Epoch 50/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.7597\n",
      "Epoch 50: val_acc improved from 0.77597 to 0.77897, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5257 - acc: 0.7599 - val_loss: 0.5132 - val_acc: 0.7790\n",
      "Epoch 51/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.7616\n",
      "Epoch 51: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5374 - acc: 0.7614 - val_loss: 0.5047 - val_acc: 0.7606\n",
      "Epoch 52/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.7569\n",
      "Epoch 52: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5193 - acc: 0.7566 - val_loss: 0.5181 - val_acc: 0.7726\n",
      "Epoch 53/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7670\n",
      "Epoch 53: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5259 - acc: 0.7669 - val_loss: 0.5146 - val_acc: 0.7670\n",
      "Epoch 54/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5166 - acc: 0.7616\n",
      "Epoch 54: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5147 - acc: 0.7624 - val_loss: 0.5306 - val_acc: 0.7696\n",
      "Epoch 55/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7659\n",
      "Epoch 55: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5146 - acc: 0.7662 - val_loss: 0.5232 - val_acc: 0.7670\n",
      "Epoch 56/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7653\n",
      "Epoch 56: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5210 - acc: 0.7656 - val_loss: 0.5023 - val_acc: 0.7751\n",
      "Epoch 57/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7651\n",
      "Epoch 57: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.7654 - val_loss: 0.5139 - val_acc: 0.7773\n",
      "Epoch 58/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.7662\n",
      "Epoch 58: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5457 - acc: 0.7668 - val_loss: 0.5233 - val_acc: 0.7713\n",
      "Epoch 59/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7661\n",
      "Epoch 59: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5220 - acc: 0.7662 - val_loss: 0.5204 - val_acc: 0.7606\n",
      "Epoch 60/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.7660\n",
      "Epoch 60: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5196 - acc: 0.7654 - val_loss: 0.5158 - val_acc: 0.7572\n",
      "Epoch 61/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.7651\n",
      "Epoch 61: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5453 - acc: 0.7652 - val_loss: 0.5099 - val_acc: 0.7619\n",
      "Epoch 62/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7690\n",
      "Epoch 62: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5288 - acc: 0.7691 - val_loss: 0.5133 - val_acc: 0.7704\n",
      "Epoch 63/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.7676\n",
      "Epoch 63: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5103 - acc: 0.7675 - val_loss: 0.5229 - val_acc: 0.7602\n",
      "Epoch 64/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5548 - acc: 0.7688\n",
      "Epoch 64: val_acc did not improve from 0.77897\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5548 - acc: 0.7688 - val_loss: 0.5088 - val_acc: 0.7683\n",
      "Epoch 65/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.7659\n",
      "Epoch 65: val_acc improved from 0.77897 to 0.78709, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5310 - acc: 0.7659 - val_loss: 0.4936 - val_acc: 0.7871\n",
      "Epoch 66/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5152 - acc: 0.7726\n",
      "Epoch 66: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5152 - acc: 0.7726 - val_loss: 0.5022 - val_acc: 0.7717\n",
      "Epoch 67/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7718\n",
      "Epoch 67: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5408 - acc: 0.7721 - val_loss: 0.4968 - val_acc: 0.7751\n",
      "Epoch 68/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7670\n",
      "Epoch 68: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5371 - acc: 0.7666 - val_loss: 0.5185 - val_acc: 0.7674\n",
      "Epoch 69/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.7727\n",
      "Epoch 69: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5186 - acc: 0.7727 - val_loss: 0.5274 - val_acc: 0.7674\n",
      "Epoch 70/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.7704\n",
      "Epoch 70: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5210 - acc: 0.7708 - val_loss: 0.5242 - val_acc: 0.7730\n",
      "Epoch 71/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7727\n",
      "Epoch 71: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5273 - acc: 0.7726 - val_loss: 0.5184 - val_acc: 0.7555\n",
      "Epoch 72/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5367 - acc: 0.7712\n",
      "Epoch 72: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5359 - acc: 0.7709 - val_loss: 0.5226 - val_acc: 0.7773\n",
      "Epoch 73/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7717\n",
      "Epoch 73: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5232 - acc: 0.7714 - val_loss: 0.5079 - val_acc: 0.7738\n",
      "Epoch 74/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7731\n",
      "Epoch 74: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5289 - acc: 0.7724 - val_loss: 0.5299 - val_acc: 0.7520\n",
      "Epoch 75/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.7684\n",
      "Epoch 75: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.7681 - val_loss: 0.5317 - val_acc: 0.7858\n",
      "Epoch 76/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.7711\n",
      "Epoch 76: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5440 - acc: 0.7711 - val_loss: 0.5359 - val_acc: 0.7811\n",
      "Epoch 77/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7644\n",
      "Epoch 77: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5336 - acc: 0.7640 - val_loss: 0.5209 - val_acc: 0.7747\n",
      "Epoch 78/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.7648\n",
      "Epoch 78: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5540 - acc: 0.7649 - val_loss: 0.5270 - val_acc: 0.7743\n",
      "Epoch 79/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7642\n",
      "Epoch 79: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5366 - acc: 0.7642 - val_loss: 0.5278 - val_acc: 0.7542\n",
      "Epoch 80/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.7719\n",
      "Epoch 80: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5200 - acc: 0.7721 - val_loss: 0.5355 - val_acc: 0.7726\n",
      "Epoch 81/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7687\n",
      "Epoch 81: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5465 - acc: 0.7689 - val_loss: 0.5547 - val_acc: 0.7841\n",
      "Epoch 82/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5402 - acc: 0.7703\n",
      "Epoch 82: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5392 - acc: 0.7697 - val_loss: 0.5268 - val_acc: 0.7670\n",
      "Epoch 83/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7640\n",
      "Epoch 83: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5346 - acc: 0.7641 - val_loss: 0.5262 - val_acc: 0.7589\n",
      "Epoch 84/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.7729\n",
      "Epoch 84: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5263 - acc: 0.7727 - val_loss: 0.5262 - val_acc: 0.7854\n",
      "Epoch 85/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.7705\n",
      "Epoch 85: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5444 - acc: 0.7708 - val_loss: 0.5356 - val_acc: 0.7730\n",
      "Epoch 86/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7700\n",
      "Epoch 86: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5278 - acc: 0.7699 - val_loss: 0.5298 - val_acc: 0.7777\n",
      "Epoch 87/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7698\n",
      "Epoch 87: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5387 - acc: 0.7696 - val_loss: 0.5404 - val_acc: 0.7640\n",
      "Epoch 88/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7704\n",
      "Epoch 88: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5477 - acc: 0.7708 - val_loss: 0.5281 - val_acc: 0.7738\n",
      "Epoch 89/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7704\n",
      "Epoch 89: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5337 - acc: 0.7703 - val_loss: 0.5316 - val_acc: 0.7734\n",
      "Epoch 90/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.7708\n",
      "Epoch 90: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5606 - acc: 0.7709 - val_loss: 0.5319 - val_acc: 0.7666\n",
      "Epoch 91/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.7758\n",
      "Epoch 91: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5582 - acc: 0.7758 - val_loss: 0.5368 - val_acc: 0.7764\n",
      "Epoch 92/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5525 - acc: 0.7694\n",
      "Epoch 92: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5511 - acc: 0.7685 - val_loss: 0.5639 - val_acc: 0.7661\n",
      "Epoch 93/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.7761\n",
      "Epoch 93: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5580 - acc: 0.7761 - val_loss: 0.5373 - val_acc: 0.7764\n",
      "Epoch 94/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7726\n",
      "Epoch 94: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5280 - acc: 0.7730 - val_loss: 0.5582 - val_acc: 0.7811\n",
      "Epoch 95/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.7779\n",
      "Epoch 95: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5544 - acc: 0.7777 - val_loss: 0.5612 - val_acc: 0.7824\n",
      "Epoch 96/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5226 - acc: 0.7787\n",
      "Epoch 96: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5226 - acc: 0.7787 - val_loss: 0.5406 - val_acc: 0.7760\n",
      "Epoch 97/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.7732\n",
      "Epoch 97: val_acc improved from 0.78709 to 0.79778, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5591 - acc: 0.7737 - val_loss: 0.5283 - val_acc: 0.7978\n",
      "Epoch 98/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7742\n",
      "Epoch 98: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5188 - acc: 0.7737 - val_loss: 0.5482 - val_acc: 0.7606\n",
      "Epoch 99/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5484 - acc: 0.7681\n",
      "Epoch 99: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5463 - acc: 0.7679 - val_loss: 0.5340 - val_acc: 0.7867\n",
      "Epoch 100/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5712 - acc: 0.7714\n",
      "Epoch 100: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5712 - acc: 0.7714 - val_loss: 0.5160 - val_acc: 0.7802\n",
      "Epoch 101/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.7680\n",
      "Epoch 101: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5648 - acc: 0.7680 - val_loss: 0.5387 - val_acc: 0.7760\n",
      "Epoch 102/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7725\n",
      "Epoch 102: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5329 - acc: 0.7725 - val_loss: 0.5495 - val_acc: 0.7867\n",
      "Epoch 103/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.7786\n",
      "Epoch 103: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5697 - acc: 0.7785 - val_loss: 0.5440 - val_acc: 0.7785\n",
      "Epoch 104/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7773\n",
      "Epoch 104: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5270 - acc: 0.7769 - val_loss: 0.5929 - val_acc: 0.7738\n",
      "Epoch 105/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.7758\n",
      "Epoch 105: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5300 - acc: 0.7763 - val_loss: 0.5895 - val_acc: 0.7837\n",
      "Epoch 106/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.7725\n",
      "Epoch 106: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5559 - acc: 0.7722 - val_loss: 0.5526 - val_acc: 0.7760\n",
      "Epoch 107/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.7742\n",
      "Epoch 107: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5175 - acc: 0.7736 - val_loss: 0.5787 - val_acc: 0.7580\n",
      "Epoch 108/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.7697\n",
      "Epoch 108: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5389 - acc: 0.7697 - val_loss: 0.5724 - val_acc: 0.7888\n",
      "Epoch 109/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7692\n",
      "Epoch 109: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5421 - acc: 0.7689 - val_loss: 0.5834 - val_acc: 0.7909\n",
      "Epoch 110/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5641 - acc: 0.7728\n",
      "Epoch 110: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5640 - acc: 0.7727 - val_loss: 0.5343 - val_acc: 0.7948\n",
      "Epoch 111/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.7722\n",
      "Epoch 111: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5123 - acc: 0.7719 - val_loss: 0.5577 - val_acc: 0.7807\n",
      "Epoch 112/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.7792\n",
      "Epoch 112: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5330 - acc: 0.7794 - val_loss: 0.5521 - val_acc: 0.7884\n",
      "Epoch 113/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7738\n",
      "Epoch 113: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5362 - acc: 0.7739 - val_loss: 0.5530 - val_acc: 0.7820\n",
      "Epoch 114/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.7796\n",
      "Epoch 114: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5867 - acc: 0.7799 - val_loss: 0.5236 - val_acc: 0.7828\n",
      "Epoch 115/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7760\n",
      "Epoch 115: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5259 - acc: 0.7755 - val_loss: 0.5455 - val_acc: 0.7914\n",
      "Epoch 116/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.7735\n",
      "Epoch 116: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5682 - acc: 0.7731 - val_loss: 0.5302 - val_acc: 0.7597\n",
      "Epoch 117/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5794 - acc: 0.7717\n",
      "Epoch 117: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5808 - acc: 0.7718 - val_loss: 0.5563 - val_acc: 0.7897\n",
      "Epoch 118/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5216 - acc: 0.7741\n",
      "Epoch 118: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5216 - acc: 0.7741 - val_loss: 0.5736 - val_acc: 0.7743\n",
      "Epoch 119/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.7750\n",
      "Epoch 119: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5542 - acc: 0.7750 - val_loss: 0.5547 - val_acc: 0.7956\n",
      "Epoch 120/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7812\n",
      "Epoch 120: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5254 - acc: 0.7810 - val_loss: 0.5499 - val_acc: 0.7743\n",
      "Epoch 121/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5495 - acc: 0.7777\n",
      "Epoch 121: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5495 - acc: 0.7777 - val_loss: 0.5515 - val_acc: 0.7850\n",
      "Epoch 122/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7799\n",
      "Epoch 122: val_acc did not improve from 0.79778\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5523 - acc: 0.7799 - val_loss: 0.5479 - val_acc: 0.7820\n",
      "Epoch 123/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.7799\n",
      "Epoch 123: val_acc improved from 0.79778 to 0.79906, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5473 - acc: 0.7800 - val_loss: 0.5443 - val_acc: 0.7991\n",
      "Epoch 124/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7805\n",
      "Epoch 124: val_acc did not improve from 0.79906\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5254 - acc: 0.7809 - val_loss: 0.5363 - val_acc: 0.7961\n",
      "Epoch 125/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.7735\n",
      "Epoch 125: val_acc did not improve from 0.79906\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5623 - acc: 0.7731 - val_loss: 0.5471 - val_acc: 0.7820\n",
      "Epoch 126/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.7805\n",
      "Epoch 126: val_acc did not improve from 0.79906\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5519 - acc: 0.7807 - val_loss: 0.5291 - val_acc: 0.7888\n",
      "Epoch 127/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5600 - acc: 0.7765\n",
      "Epoch 127: val_acc did not improve from 0.79906\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5592 - acc: 0.7765 - val_loss: 0.5294 - val_acc: 0.7802\n",
      "Epoch 128/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5298 - acc: 0.7793\n",
      "Epoch 128: val_acc did not improve from 0.79906\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5298 - acc: 0.7793 - val_loss: 0.5331 - val_acc: 0.7935\n",
      "Epoch 129/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7810\n",
      "Epoch 129: val_acc improved from 0.79906 to 0.80633, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5254 - acc: 0.7809 - val_loss: 0.5427 - val_acc: 0.8063\n",
      "Epoch 130/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.7763\n",
      "Epoch 130: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5423 - acc: 0.7757 - val_loss: 0.5550 - val_acc: 0.7858\n",
      "Epoch 131/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.7734\n",
      "Epoch 131: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5479 - acc: 0.7732 - val_loss: 0.5606 - val_acc: 0.7824\n",
      "Epoch 132/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5854 - acc: 0.7737\n",
      "Epoch 132: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5837 - acc: 0.7738 - val_loss: 0.5513 - val_acc: 0.7704\n",
      "Epoch 133/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7771\n",
      "Epoch 133: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5336 - acc: 0.7757 - val_loss: 0.5653 - val_acc: 0.7747\n",
      "Epoch 134/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5834 - acc: 0.7765\n",
      "Epoch 134: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5829 - acc: 0.7766 - val_loss: 0.5295 - val_acc: 0.7755\n",
      "Epoch 135/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5849 - acc: 0.7778\n",
      "Epoch 135: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5828 - acc: 0.7772 - val_loss: 0.5596 - val_acc: 0.7777\n",
      "Epoch 136/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.7727\n",
      "Epoch 136: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5338 - acc: 0.7728 - val_loss: 0.5637 - val_acc: 0.7862\n",
      "Epoch 137/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7830\n",
      "Epoch 137: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5288 - acc: 0.7830 - val_loss: 0.5563 - val_acc: 0.7909\n",
      "Epoch 138/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7785\n",
      "Epoch 138: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5484 - acc: 0.7790 - val_loss: 0.5567 - val_acc: 0.8038\n",
      "Epoch 139/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7809\n",
      "Epoch 139: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5226 - acc: 0.7808 - val_loss: 0.5367 - val_acc: 0.7828\n",
      "Epoch 140/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.7784\n",
      "Epoch 140: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5420 - acc: 0.7781 - val_loss: 0.5147 - val_acc: 0.7777\n",
      "Epoch 141/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.7776\n",
      "Epoch 141: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5199 - acc: 0.7778 - val_loss: 0.5452 - val_acc: 0.7815\n",
      "Epoch 142/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5474 - acc: 0.7757\n",
      "Epoch 142: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5474 - acc: 0.7757 - val_loss: 0.5370 - val_acc: 0.7871\n",
      "Epoch 143/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.7772\n",
      "Epoch 143: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5243 - acc: 0.7774 - val_loss: 0.5335 - val_acc: 0.7875\n",
      "Epoch 144/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.7802\n",
      "Epoch 144: val_acc did not improve from 0.80633\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5946 - acc: 0.7802 - val_loss: 0.5698 - val_acc: 0.7854\n",
      "Epoch 145/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7855\n",
      "Epoch 145: val_acc improved from 0.80633 to 0.80761, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5596 - acc: 0.7858 - val_loss: 0.5876 - val_acc: 0.8076\n",
      "Epoch 146/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.7794\n",
      "Epoch 146: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5279 - acc: 0.7794 - val_loss: 0.5484 - val_acc: 0.8016\n",
      "Epoch 147/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5653 - acc: 0.7751\n",
      "Epoch 147: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5653 - acc: 0.7751 - val_loss: 0.5320 - val_acc: 0.7973\n",
      "Epoch 148/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7784\n",
      "Epoch 148: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5343 - acc: 0.7784 - val_loss: 0.5292 - val_acc: 0.7978\n",
      "Epoch 149/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7797\n",
      "Epoch 149: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5409 - acc: 0.7794 - val_loss: 0.5252 - val_acc: 0.7824\n",
      "Epoch 150/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.7744\n",
      "Epoch 150: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5628 - acc: 0.7743 - val_loss: 0.5152 - val_acc: 0.7905\n",
      "Epoch 151/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7876\n",
      "Epoch 151: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5305 - acc: 0.7867 - val_loss: 0.5253 - val_acc: 0.7944\n",
      "Epoch 152/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7772\n",
      "Epoch 152: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5163 - acc: 0.7771 - val_loss: 0.5362 - val_acc: 0.8003\n",
      "Epoch 153/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7787\n",
      "Epoch 153: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5241 - acc: 0.7792 - val_loss: 0.5528 - val_acc: 0.7892\n",
      "Epoch 154/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7767\n",
      "Epoch 154: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5346 - acc: 0.7772 - val_loss: 0.5317 - val_acc: 0.8003\n",
      "Epoch 155/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6172 - acc: 0.7846\n",
      "Epoch 155: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6142 - acc: 0.7839 - val_loss: 0.5325 - val_acc: 0.7832\n",
      "Epoch 156/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6050 - acc: 0.7772\n",
      "Epoch 156: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.6021 - acc: 0.7770 - val_loss: 0.5604 - val_acc: 0.7926\n",
      "Epoch 157/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5719 - acc: 0.7853\n",
      "Epoch 157: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5699 - acc: 0.7852 - val_loss: 0.5245 - val_acc: 0.7794\n",
      "Epoch 158/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.7812\n",
      "Epoch 158: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5511 - acc: 0.7812 - val_loss: 0.5055 - val_acc: 0.8038\n",
      "Epoch 159/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7809\n",
      "Epoch 159: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5394 - acc: 0.7805 - val_loss: 0.5370 - val_acc: 0.7978\n",
      "Epoch 160/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.7830\n",
      "Epoch 160: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5345 - acc: 0.7831 - val_loss: 0.5372 - val_acc: 0.7743\n",
      "Epoch 161/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5233 - acc: 0.7813\n",
      "Epoch 161: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5233 - acc: 0.7813 - val_loss: 0.5539 - val_acc: 0.7965\n",
      "Epoch 162/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.7809\n",
      "Epoch 162: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5431 - acc: 0.7810 - val_loss: 0.5608 - val_acc: 0.7790\n",
      "Epoch 163/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.7790\n",
      "Epoch 163: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5627 - acc: 0.7786 - val_loss: 0.5391 - val_acc: 0.7875\n",
      "Epoch 164/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7782\n",
      "Epoch 164: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5182 - acc: 0.7786 - val_loss: 0.5462 - val_acc: 0.7905\n",
      "Epoch 165/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.7847\n",
      "Epoch 165: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5572 - acc: 0.7846 - val_loss: 0.5604 - val_acc: 0.7802\n",
      "Epoch 166/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7779\n",
      "Epoch 166: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5495 - acc: 0.7780 - val_loss: 0.5459 - val_acc: 0.7986\n",
      "Epoch 167/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.7816\n",
      "Epoch 167: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5585 - acc: 0.7818 - val_loss: 0.5747 - val_acc: 0.7991\n",
      "Epoch 168/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7829\n",
      "Epoch 168: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5319 - acc: 0.7836 - val_loss: 0.6121 - val_acc: 0.7973\n",
      "Epoch 169/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.7846\n",
      "Epoch 169: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5770 - acc: 0.7846 - val_loss: 0.5404 - val_acc: 0.7973\n",
      "Epoch 170/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.7861\n",
      "Epoch 170: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5146 - acc: 0.7864 - val_loss: 0.5500 - val_acc: 0.7999\n",
      "Epoch 171/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7838\n",
      "Epoch 171: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5327 - acc: 0.7834 - val_loss: 0.5600 - val_acc: 0.7879\n",
      "Epoch 172/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.7877\n",
      "Epoch 172: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5543 - acc: 0.7879 - val_loss: 0.5445 - val_acc: 0.7820\n",
      "Epoch 173/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5304 - acc: 0.7840\n",
      "Epoch 173: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5304 - acc: 0.7840 - val_loss: 0.5428 - val_acc: 0.7926\n",
      "Epoch 174/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5803 - acc: 0.7875\n",
      "Epoch 174: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5781 - acc: 0.7874 - val_loss: 0.5791 - val_acc: 0.7734\n",
      "Epoch 175/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.7948\n",
      "Epoch 175: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5230 - acc: 0.7948 - val_loss: 0.5778 - val_acc: 0.8012\n",
      "Epoch 176/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.7841\n",
      "Epoch 176: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5692 - acc: 0.7840 - val_loss: 0.5395 - val_acc: 0.8059\n",
      "Epoch 177/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5695 - acc: 0.7898\n",
      "Epoch 177: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5691 - acc: 0.7900 - val_loss: 0.5424 - val_acc: 0.8029\n",
      "Epoch 178/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5470 - acc: 0.7877\n",
      "Epoch 178: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5470 - acc: 0.7877 - val_loss: 0.5473 - val_acc: 0.7858\n",
      "Epoch 179/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5774 - acc: 0.7864\n",
      "Epoch 179: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5757 - acc: 0.7862 - val_loss: 0.5343 - val_acc: 0.7837\n",
      "Epoch 180/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7811\n",
      "Epoch 180: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5699 - acc: 0.7812 - val_loss: 0.5592 - val_acc: 0.7944\n",
      "Epoch 181/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7882\n",
      "Epoch 181: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5329 - acc: 0.7885 - val_loss: 0.5590 - val_acc: 0.7867\n",
      "Epoch 182/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7888\n",
      "Epoch 182: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5223 - acc: 0.7882 - val_loss: 0.5546 - val_acc: 0.7867\n",
      "Epoch 183/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7860\n",
      "Epoch 183: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5217 - acc: 0.7865 - val_loss: 0.5678 - val_acc: 0.8021\n",
      "Epoch 184/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5613 - acc: 0.7847\n",
      "Epoch 184: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5610 - acc: 0.7848 - val_loss: 0.5550 - val_acc: 0.7978\n",
      "Epoch 185/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7888\n",
      "Epoch 185: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5549 - acc: 0.7880 - val_loss: 0.5570 - val_acc: 0.7944\n",
      "Epoch 186/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.7869\n",
      "Epoch 186: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5560 - acc: 0.7865 - val_loss: 0.5983 - val_acc: 0.7922\n",
      "Epoch 187/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5640 - acc: 0.7817\n",
      "Epoch 187: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5640 - acc: 0.7817 - val_loss: 0.5885 - val_acc: 0.7875\n",
      "Epoch 188/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5536 - acc: 0.7934\n",
      "Epoch 188: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5526 - acc: 0.7935 - val_loss: 0.5466 - val_acc: 0.7738\n",
      "Epoch 189/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5600 - acc: 0.7851\n",
      "Epoch 189: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5573 - acc: 0.7852 - val_loss: 0.5608 - val_acc: 0.7871\n",
      "Epoch 190/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7864\n",
      "Epoch 190: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5361 - acc: 0.7864 - val_loss: 0.5599 - val_acc: 0.7961\n",
      "Epoch 191/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.7896\n",
      "Epoch 191: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5460 - acc: 0.7895 - val_loss: 0.5554 - val_acc: 0.7832\n",
      "Epoch 192/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5144 - acc: 0.7862\n",
      "Epoch 192: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5144 - acc: 0.7862 - val_loss: 0.5557 - val_acc: 0.7935\n",
      "Epoch 193/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.7877\n",
      "Epoch 193: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5850 - acc: 0.7882 - val_loss: 0.5650 - val_acc: 0.7828\n",
      "Epoch 194/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.7894\n",
      "Epoch 194: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5561 - acc: 0.7886 - val_loss: 0.5411 - val_acc: 0.7721\n",
      "Epoch 195/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7878\n",
      "Epoch 195: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5273 - acc: 0.7881 - val_loss: 0.5552 - val_acc: 0.7956\n",
      "Epoch 196/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5977 - acc: 0.7912\n",
      "Epoch 196: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5954 - acc: 0.7914 - val_loss: 0.5411 - val_acc: 0.7785\n",
      "Epoch 197/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7842\n",
      "Epoch 197: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5271 - acc: 0.7839 - val_loss: 0.5525 - val_acc: 0.7871\n",
      "Epoch 198/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.7878\n",
      "Epoch 198: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5666 - acc: 0.7867 - val_loss: 0.5621 - val_acc: 0.7918\n",
      "Epoch 199/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.7886\n",
      "Epoch 199: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5179 - acc: 0.7885 - val_loss: 0.5454 - val_acc: 0.7875\n",
      "Epoch 200/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.7909\n",
      "Epoch 200: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5072 - acc: 0.7908 - val_loss: 0.5404 - val_acc: 0.8003\n",
      "Epoch 201/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.7923\n",
      "Epoch 201: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5696 - acc: 0.7924 - val_loss: 0.5257 - val_acc: 0.7892\n",
      "Epoch 202/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.7877\n",
      "Epoch 202: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5514 - acc: 0.7877 - val_loss: 0.5194 - val_acc: 0.7901\n",
      "Epoch 203/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.7852\n",
      "Epoch 203: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5265 - acc: 0.7850 - val_loss: 0.5255 - val_acc: 0.7935\n",
      "Epoch 204/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5025 - acc: 0.7901\n",
      "Epoch 204: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5009 - acc: 0.7904 - val_loss: 0.5305 - val_acc: 0.7922\n",
      "Epoch 205/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.7911\n",
      "Epoch 205: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5648 - acc: 0.7913 - val_loss: 0.5225 - val_acc: 0.7875\n",
      "Epoch 206/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.7937\n",
      "Epoch 206: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5542 - acc: 0.7936 - val_loss: 0.5127 - val_acc: 0.7781\n",
      "Epoch 207/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.7949\n",
      "Epoch 207: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5136 - acc: 0.7952 - val_loss: 0.5144 - val_acc: 0.7982\n",
      "Epoch 208/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5647 - acc: 0.7913\n",
      "Epoch 208: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5607 - acc: 0.7914 - val_loss: 0.5291 - val_acc: 0.7948\n",
      "Epoch 209/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7944\n",
      "Epoch 209: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5299 - acc: 0.7945 - val_loss: 0.5314 - val_acc: 0.7999\n",
      "Epoch 210/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.7886\n",
      "Epoch 210: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5451 - acc: 0.7886 - val_loss: 0.5308 - val_acc: 0.7871\n",
      "Epoch 211/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.7906\n",
      "Epoch 211: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5138 - acc: 0.7900 - val_loss: 0.5282 - val_acc: 0.7837\n",
      "Epoch 212/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.7917\n",
      "Epoch 212: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5571 - acc: 0.7920 - val_loss: 0.5006 - val_acc: 0.8042\n",
      "Epoch 213/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.7945\n",
      "Epoch 213: val_acc did not improve from 0.80761\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5636 - acc: 0.7944 - val_loss: 0.5530 - val_acc: 0.8063\n",
      "Epoch 214/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.7989\n",
      "Epoch 214: val_acc improved from 0.80761 to 0.81573, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5216 - acc: 0.7991 - val_loss: 0.5423 - val_acc: 0.8157\n",
      "Epoch 215/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5209 - acc: 0.7911\n",
      "Epoch 215: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5209 - acc: 0.7911 - val_loss: 0.5651 - val_acc: 0.7820\n",
      "Epoch 216/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.7961\n",
      "Epoch 216: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5367 - acc: 0.7958 - val_loss: 0.5450 - val_acc: 0.8033\n",
      "Epoch 217/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7956\n",
      "Epoch 217: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5378 - acc: 0.7958 - val_loss: 0.5534 - val_acc: 0.8050\n",
      "Epoch 218/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7974\n",
      "Epoch 218: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5247 - acc: 0.7974 - val_loss: 0.5232 - val_acc: 0.7935\n",
      "Epoch 219/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.7888\n",
      "Epoch 219: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5270 - acc: 0.7890 - val_loss: 0.5230 - val_acc: 0.7969\n",
      "Epoch 220/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5243 - acc: 0.7956\n",
      "Epoch 220: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5243 - acc: 0.7956 - val_loss: 0.5316 - val_acc: 0.7995\n",
      "Epoch 221/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4958 - acc: 0.7933\n",
      "Epoch 221: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4958 - acc: 0.7933 - val_loss: 0.5066 - val_acc: 0.7888\n",
      "Epoch 222/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7936\n",
      "Epoch 222: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5252 - acc: 0.7936 - val_loss: 0.5473 - val_acc: 0.8085\n",
      "Epoch 223/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7939\n",
      "Epoch 223: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5403 - acc: 0.7931 - val_loss: 0.5042 - val_acc: 0.7824\n",
      "Epoch 224/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7930\n",
      "Epoch 224: val_acc did not improve from 0.81573\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5166 - acc: 0.7936 - val_loss: 0.4967 - val_acc: 0.7854\n",
      "Epoch 225/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.7971\n",
      "Epoch 225: val_acc improved from 0.81573 to 0.81873, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/3/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5460 - acc: 0.7963 - val_loss: 0.5219 - val_acc: 0.8187\n",
      "Epoch 226/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.7977\n",
      "Epoch 226: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5056 - acc: 0.7967 - val_loss: 0.5336 - val_acc: 0.7965\n",
      "Epoch 227/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.7949\n",
      "Epoch 227: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5120 - acc: 0.7951 - val_loss: 0.5519 - val_acc: 0.8127\n",
      "Epoch 228/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.7937\n",
      "Epoch 228: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5554 - acc: 0.7937 - val_loss: 0.5373 - val_acc: 0.8076\n",
      "Epoch 229/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5073 - acc: 0.8002\n",
      "Epoch 229: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5069 - acc: 0.8003 - val_loss: 0.5360 - val_acc: 0.7939\n",
      "Epoch 230/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7973\n",
      "Epoch 230: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5510 - acc: 0.7975 - val_loss: 0.5650 - val_acc: 0.7944\n",
      "Epoch 231/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8057\n",
      "Epoch 231: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4927 - acc: 0.8060 - val_loss: 0.5355 - val_acc: 0.7973\n",
      "Epoch 232/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.7958\n",
      "Epoch 232: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5113 - acc: 0.7959 - val_loss: 0.5283 - val_acc: 0.7986\n",
      "Epoch 233/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7986\n",
      "Epoch 233: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5319 - acc: 0.7985 - val_loss: 0.5157 - val_acc: 0.7961\n",
      "Epoch 234/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8004\n",
      "Epoch 234: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5061 - acc: 0.8006 - val_loss: 0.5309 - val_acc: 0.8097\n",
      "Epoch 235/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4980 - acc: 0.7996\n",
      "Epoch 235: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4980 - acc: 0.7996 - val_loss: 0.5155 - val_acc: 0.7965\n",
      "Epoch 236/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.7984\n",
      "Epoch 236: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5053 - acc: 0.7986 - val_loss: 0.5172 - val_acc: 0.7879\n",
      "Epoch 237/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.7921\n",
      "Epoch 237: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5013 - acc: 0.7924 - val_loss: 0.5264 - val_acc: 0.8029\n",
      "Epoch 238/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5813 - acc: 0.7892\n",
      "Epoch 238: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5810 - acc: 0.7892 - val_loss: 0.5118 - val_acc: 0.7854\n",
      "Epoch 239/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.7965\n",
      "Epoch 239: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5171 - acc: 0.7960 - val_loss: 0.5173 - val_acc: 0.7781\n",
      "Epoch 240/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7952\n",
      "Epoch 240: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5129 - acc: 0.7954 - val_loss: 0.5517 - val_acc: 0.7931\n",
      "Epoch 241/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7998\n",
      "Epoch 241: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5185 - acc: 0.7999 - val_loss: 0.5347 - val_acc: 0.7858\n",
      "Epoch 242/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7925\n",
      "Epoch 242: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5264 - acc: 0.7922 - val_loss: 0.5356 - val_acc: 0.7982\n",
      "Epoch 243/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.7947\n",
      "Epoch 243: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5203 - acc: 0.7939 - val_loss: 0.5428 - val_acc: 0.7905\n",
      "Epoch 244/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.7935\n",
      "Epoch 244: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5553 - acc: 0.7941 - val_loss: 0.5379 - val_acc: 0.8033\n",
      "Epoch 245/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.7944\n",
      "Epoch 245: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5790 - acc: 0.7942 - val_loss: 0.5277 - val_acc: 0.7965\n",
      "Epoch 246/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7998\n",
      "Epoch 246: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5096 - acc: 0.7988 - val_loss: 0.5460 - val_acc: 0.7828\n",
      "Epoch 247/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.7906\n",
      "Epoch 247: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5638 - acc: 0.7905 - val_loss: 0.5217 - val_acc: 0.8063\n",
      "Epoch 248/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.7931\n",
      "Epoch 248: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5472 - acc: 0.7935 - val_loss: 0.5494 - val_acc: 0.8063\n",
      "Epoch 249/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5147 - acc: 0.7943\n",
      "Epoch 249: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5146 - acc: 0.7943 - val_loss: 0.5528 - val_acc: 0.8059\n",
      "Epoch 250/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.7997\n",
      "Epoch 250: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4964 - acc: 0.7999 - val_loss: 0.5672 - val_acc: 0.7995\n",
      "Epoch 251/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.8008\n",
      "Epoch 251: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5325 - acc: 0.8007 - val_loss: 0.5198 - val_acc: 0.7995\n",
      "Epoch 252/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.7939\n",
      "Epoch 252: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5737 - acc: 0.7940 - val_loss: 0.5406 - val_acc: 0.8008\n",
      "Epoch 253/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.7924\n",
      "Epoch 253: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5890 - acc: 0.7926 - val_loss: 0.5093 - val_acc: 0.7931\n",
      "Epoch 254/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7939\n",
      "Epoch 254: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5349 - acc: 0.7938 - val_loss: 0.5104 - val_acc: 0.7815\n",
      "Epoch 255/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7974\n",
      "Epoch 255: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5376 - acc: 0.7973 - val_loss: 0.5612 - val_acc: 0.7892\n",
      "Epoch 256/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.7951\n",
      "Epoch 256: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5519 - acc: 0.7952 - val_loss: 0.5350 - val_acc: 0.7982\n",
      "Epoch 257/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.7983\n",
      "Epoch 257: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6031 - acc: 0.7985 - val_loss: 0.5124 - val_acc: 0.7871\n",
      "Epoch 258/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.7989\n",
      "Epoch 258: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5146 - acc: 0.7986 - val_loss: 0.4971 - val_acc: 0.7879\n",
      "Epoch 259/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.7990\n",
      "Epoch 259: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5543 - acc: 0.7993 - val_loss: 0.5250 - val_acc: 0.7867\n",
      "Epoch 260/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7926\n",
      "Epoch 260: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5111 - acc: 0.7928 - val_loss: 0.5476 - val_acc: 0.7858\n",
      "Epoch 261/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.7925\n",
      "Epoch 261: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5427 - acc: 0.7928 - val_loss: 0.5242 - val_acc: 0.7807\n",
      "Epoch 262/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.7949\n",
      "Epoch 262: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5148 - acc: 0.7944 - val_loss: 0.5429 - val_acc: 0.7867\n",
      "Epoch 263/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8012\n",
      "Epoch 263: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5040 - acc: 0.8017 - val_loss: 0.5433 - val_acc: 0.8029\n",
      "Epoch 264/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8030\n",
      "Epoch 264: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5065 - acc: 0.8032 - val_loss: 0.5326 - val_acc: 0.7986\n",
      "Epoch 265/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5282 - acc: 0.7970\n",
      "Epoch 265: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5282 - acc: 0.7970 - val_loss: 0.5565 - val_acc: 0.7939\n",
      "Epoch 266/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8029\n",
      "Epoch 266: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5132 - acc: 0.8024 - val_loss: 0.5274 - val_acc: 0.7871\n",
      "Epoch 267/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.8004\n",
      "Epoch 267: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5179 - acc: 0.8006 - val_loss: 0.5540 - val_acc: 0.7969\n",
      "Epoch 268/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.7997\n",
      "Epoch 268: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5040 - acc: 0.7999 - val_loss: 0.5526 - val_acc: 0.7944\n",
      "Epoch 269/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7982\n",
      "Epoch 269: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5182 - acc: 0.7979 - val_loss: 0.5419 - val_acc: 0.7888\n",
      "Epoch 270/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.7978\n",
      "Epoch 270: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5265 - acc: 0.7980 - val_loss: 0.5182 - val_acc: 0.8046\n",
      "Epoch 271/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.7969\n",
      "Epoch 271: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5565 - acc: 0.7971 - val_loss: 0.5598 - val_acc: 0.8089\n",
      "Epoch 272/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4931 - acc: 0.7976\n",
      "Epoch 272: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4923 - acc: 0.7978 - val_loss: 0.5263 - val_acc: 0.7969\n",
      "Epoch 273/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.8010\n",
      "Epoch 273: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5655 - acc: 0.8007 - val_loss: 0.5330 - val_acc: 0.7867\n",
      "Epoch 274/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.7987\n",
      "Epoch 274: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5391 - acc: 0.7988 - val_loss: 0.5534 - val_acc: 0.8050\n",
      "Epoch 275/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.8017\n",
      "Epoch 275: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5567 - acc: 0.8012 - val_loss: 0.5661 - val_acc: 0.8042\n",
      "Epoch 276/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4952 - acc: 0.8038\n",
      "Epoch 276: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4936 - acc: 0.8035 - val_loss: 0.5377 - val_acc: 0.7871\n",
      "Epoch 277/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7964\n",
      "Epoch 277: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5351 - acc: 0.7959 - val_loss: 0.5553 - val_acc: 0.7879\n",
      "Epoch 278/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7965\n",
      "Epoch 278: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5323 - acc: 0.7969 - val_loss: 0.5495 - val_acc: 0.8063\n",
      "Epoch 279/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7965\n",
      "Epoch 279: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5400 - acc: 0.7960 - val_loss: 0.5189 - val_acc: 0.8050\n",
      "Epoch 280/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.7970\n",
      "Epoch 280: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5229 - acc: 0.7971 - val_loss: 0.5095 - val_acc: 0.7991\n",
      "Epoch 281/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7909\n",
      "Epoch 281: val_acc did not improve from 0.81873\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5534 - acc: 0.7903 - val_loss: 0.5178 - val_acc: 0.7944\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               24832     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,521\n",
      "Trainable params: 107,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 1.5631 - acc: 0.5904\n",
      "Epoch 1: val_acc improved from -inf to 0.69175, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 3s 8ms/step - loss: 1.5588 - acc: 0.5906 - val_loss: 0.6711 - val_acc: 0.6917\n",
      "Epoch 2/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6776 - acc: 0.6417\n",
      "Epoch 2: val_acc improved from 0.69175 to 0.71313, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6773 - acc: 0.6415 - val_loss: 0.6537 - val_acc: 0.7131\n",
      "Epoch 3/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6383 - acc: 0.6745\n",
      "Epoch 3: val_acc improved from 0.71313 to 0.73108, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6384 - acc: 0.6739 - val_loss: 0.6222 - val_acc: 0.7311\n",
      "Epoch 4/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6108 - acc: 0.6925\n",
      "Epoch 4: val_acc did not improve from 0.73108\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6109 - acc: 0.6917 - val_loss: 0.6094 - val_acc: 0.7281\n",
      "Epoch 5/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5926 - acc: 0.7031\n",
      "Epoch 5: val_acc improved from 0.73108 to 0.73279, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5934 - acc: 0.7022 - val_loss: 0.6096 - val_acc: 0.7328\n",
      "Epoch 6/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.7079\n",
      "Epoch 6: val_acc improved from 0.73279 to 0.73835, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5940 - acc: 0.7080 - val_loss: 0.5811 - val_acc: 0.7383\n",
      "Epoch 7/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5811 - acc: 0.7120\n",
      "Epoch 7: val_acc improved from 0.73835 to 0.74006, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5811 - acc: 0.7120 - val_loss: 0.5986 - val_acc: 0.7401\n",
      "Epoch 8/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5777 - acc: 0.7170\n",
      "Epoch 8: val_acc improved from 0.74006 to 0.74177, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5778 - acc: 0.7165 - val_loss: 0.5917 - val_acc: 0.7418\n",
      "Epoch 9/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.7173\n",
      "Epoch 9: val_acc improved from 0.74177 to 0.75631, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5650 - acc: 0.7172 - val_loss: 0.5738 - val_acc: 0.7563\n",
      "Epoch 10/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.7242\n",
      "Epoch 10: val_acc did not improve from 0.75631\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5713 - acc: 0.7236 - val_loss: 0.5566 - val_acc: 0.7546\n",
      "Epoch 11/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.7269\n",
      "Epoch 11: val_acc did not improve from 0.75631\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5652 - acc: 0.7261 - val_loss: 0.5666 - val_acc: 0.7533\n",
      "Epoch 12/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.7276\n",
      "Epoch 12: val_acc improved from 0.75631 to 0.75802, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5599 - acc: 0.7275 - val_loss: 0.5363 - val_acc: 0.7580\n",
      "Epoch 13/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.7284\n",
      "Epoch 13: val_acc improved from 0.75802 to 0.76186, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5604 - acc: 0.7277 - val_loss: 0.5615 - val_acc: 0.7619\n",
      "Epoch 14/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.7302\n",
      "Epoch 14: val_acc did not improve from 0.76186\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5582 - acc: 0.7293 - val_loss: 0.5626 - val_acc: 0.7580\n",
      "Epoch 15/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7323\n",
      "Epoch 15: val_acc improved from 0.76186 to 0.77170, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5501 - acc: 0.7318 - val_loss: 0.5639 - val_acc: 0.7717\n",
      "Epoch 16/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7301\n",
      "Epoch 16: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5566 - acc: 0.7299 - val_loss: 0.5557 - val_acc: 0.7567\n",
      "Epoch 17/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.7336\n",
      "Epoch 17: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5499 - acc: 0.7336 - val_loss: 0.5390 - val_acc: 0.7572\n",
      "Epoch 18/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.7293\n",
      "Epoch 18: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5579 - acc: 0.7288 - val_loss: 0.5507 - val_acc: 0.7670\n",
      "Epoch 19/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7372\n",
      "Epoch 19: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5524 - acc: 0.7365 - val_loss: 0.5468 - val_acc: 0.7678\n",
      "Epoch 20/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.7329\n",
      "Epoch 20: val_acc did not improve from 0.77170\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5488 - acc: 0.7330 - val_loss: 0.5392 - val_acc: 0.7644\n",
      "Epoch 21/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7416\n",
      "Epoch 21: val_acc improved from 0.77170 to 0.77512, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5452 - acc: 0.7414 - val_loss: 0.5365 - val_acc: 0.7751\n",
      "Epoch 22/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5447 - acc: 0.7402\n",
      "Epoch 22: val_acc did not improve from 0.77512\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5447 - acc: 0.7402 - val_loss: 0.5245 - val_acc: 0.7678\n",
      "Epoch 23/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7472\n",
      "Epoch 23: val_acc improved from 0.77512 to 0.77597, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5343 - acc: 0.7469 - val_loss: 0.5310 - val_acc: 0.7760\n",
      "Epoch 24/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5364 - acc: 0.7417\n",
      "Epoch 24: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5364 - acc: 0.7417 - val_loss: 0.5405 - val_acc: 0.7670\n",
      "Epoch 25/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7445\n",
      "Epoch 25: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5404 - acc: 0.7441 - val_loss: 0.5161 - val_acc: 0.7734\n",
      "Epoch 26/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7425\n",
      "Epoch 26: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5452 - acc: 0.7423 - val_loss: 0.5281 - val_acc: 0.7717\n",
      "Epoch 27/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7473\n",
      "Epoch 27: val_acc improved from 0.77597 to 0.78196, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5374 - acc: 0.7471 - val_loss: 0.5473 - val_acc: 0.7820\n",
      "Epoch 28/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7418\n",
      "Epoch 28: val_acc improved from 0.78196 to 0.78794, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5507 - acc: 0.7416 - val_loss: 0.5493 - val_acc: 0.7879\n",
      "Epoch 29/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7446\n",
      "Epoch 29: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5387 - acc: 0.7445 - val_loss: 0.5390 - val_acc: 0.7610\n",
      "Epoch 30/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7463\n",
      "Epoch 30: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5375 - acc: 0.7462 - val_loss: 0.5281 - val_acc: 0.7798\n",
      "Epoch 31/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7474\n",
      "Epoch 31: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5379 - acc: 0.7473 - val_loss: 0.5271 - val_acc: 0.7807\n",
      "Epoch 32/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7475\n",
      "Epoch 32: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5383 - acc: 0.7473 - val_loss: 0.5368 - val_acc: 0.7773\n",
      "Epoch 33/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.7510\n",
      "Epoch 33: val_acc improved from 0.78794 to 0.78965, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5273 - acc: 0.7508 - val_loss: 0.5331 - val_acc: 0.7897\n",
      "Epoch 34/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7497\n",
      "Epoch 34: val_acc did not improve from 0.78965\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5413 - acc: 0.7495 - val_loss: 0.5229 - val_acc: 0.7841\n",
      "Epoch 35/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.7506\n",
      "Epoch 35: val_acc did not improve from 0.78965\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5296 - acc: 0.7506 - val_loss: 0.5442 - val_acc: 0.7854\n",
      "Epoch 36/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7492\n",
      "Epoch 36: val_acc improved from 0.78965 to 0.79521, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5492 - acc: 0.7495 - val_loss: 0.5331 - val_acc: 0.7952\n",
      "Epoch 37/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7549\n",
      "Epoch 37: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5206 - acc: 0.7550 - val_loss: 0.5286 - val_acc: 0.7875\n",
      "Epoch 38/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5326 - acc: 0.7521\n",
      "Epoch 38: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5326 - acc: 0.7521 - val_loss: 0.5265 - val_acc: 0.7824\n",
      "Epoch 39/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7531\n",
      "Epoch 39: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5350 - acc: 0.7529 - val_loss: 0.5320 - val_acc: 0.7845\n",
      "Epoch 40/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.7509\n",
      "Epoch 40: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5434 - acc: 0.7505 - val_loss: 0.5299 - val_acc: 0.7837\n",
      "Epoch 41/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5314 - acc: 0.7519\n",
      "Epoch 41: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5314 - acc: 0.7519 - val_loss: 0.5370 - val_acc: 0.7922\n",
      "Epoch 42/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.7568\n",
      "Epoch 42: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5353 - acc: 0.7566 - val_loss: 0.5293 - val_acc: 0.7854\n",
      "Epoch 43/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5358 - acc: 0.7554\n",
      "Epoch 43: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5358 - acc: 0.7554 - val_loss: 0.5132 - val_acc: 0.7832\n",
      "Epoch 44/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7597\n",
      "Epoch 44: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5246 - acc: 0.7594 - val_loss: 0.5442 - val_acc: 0.7888\n",
      "Epoch 45/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.7564\n",
      "Epoch 45: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5231 - acc: 0.7562 - val_loss: 0.5491 - val_acc: 0.7901\n",
      "Epoch 46/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.7609\n",
      "Epoch 46: val_acc did not improve from 0.79521\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5292 - acc: 0.7599 - val_loss: 0.5187 - val_acc: 0.7858\n",
      "Epoch 47/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.7630\n",
      "Epoch 47: val_acc improved from 0.79521 to 0.79820, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5287 - acc: 0.7629 - val_loss: 0.5473 - val_acc: 0.7982\n",
      "Epoch 48/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.7559\n",
      "Epoch 48: val_acc improved from 0.79820 to 0.79863, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5348 - acc: 0.7560 - val_loss: 0.5193 - val_acc: 0.7986\n",
      "Epoch 49/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7648\n",
      "Epoch 49: val_acc did not improve from 0.79863\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5195 - acc: 0.7648 - val_loss: 0.5330 - val_acc: 0.7982\n",
      "Epoch 50/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7579\n",
      "Epoch 50: val_acc improved from 0.79863 to 0.79949, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5357 - acc: 0.7579 - val_loss: 0.5350 - val_acc: 0.7995\n",
      "Epoch 51/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.7583\n",
      "Epoch 51: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5255 - acc: 0.7586 - val_loss: 0.5367 - val_acc: 0.7935\n",
      "Epoch 52/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7616\n",
      "Epoch 52: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5459 - acc: 0.7609 - val_loss: 0.5343 - val_acc: 0.7828\n",
      "Epoch 53/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.7579\n",
      "Epoch 53: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5241 - acc: 0.7580 - val_loss: 0.5382 - val_acc: 0.7879\n",
      "Epoch 54/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7650\n",
      "Epoch 54: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5278 - acc: 0.7650 - val_loss: 0.5399 - val_acc: 0.7931\n",
      "Epoch 55/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.7575\n",
      "Epoch 55: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5128 - acc: 0.7575 - val_loss: 0.5399 - val_acc: 0.7956\n",
      "Epoch 56/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7620\n",
      "Epoch 56: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5262 - acc: 0.7616 - val_loss: 0.5154 - val_acc: 0.7952\n",
      "Epoch 57/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7622\n",
      "Epoch 57: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5226 - acc: 0.7610 - val_loss: 0.5418 - val_acc: 0.7939\n",
      "Epoch 58/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7637\n",
      "Epoch 58: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5241 - acc: 0.7633 - val_loss: 0.5197 - val_acc: 0.7931\n",
      "Epoch 59/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7625\n",
      "Epoch 59: val_acc improved from 0.79949 to 0.80504, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5264 - acc: 0.7628 - val_loss: 0.5339 - val_acc: 0.8050\n",
      "Epoch 60/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5187 - acc: 0.7599\n",
      "Epoch 60: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5187 - acc: 0.7599 - val_loss: 0.5479 - val_acc: 0.7837\n",
      "Epoch 61/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5299 - acc: 0.7632\n",
      "Epoch 61: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5299 - acc: 0.7632 - val_loss: 0.5222 - val_acc: 0.7918\n",
      "Epoch 62/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.7654\n",
      "Epoch 62: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5200 - acc: 0.7656 - val_loss: 0.5401 - val_acc: 0.8033\n",
      "Epoch 63/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7690\n",
      "Epoch 63: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5193 - acc: 0.7674 - val_loss: 0.5346 - val_acc: 0.7815\n",
      "Epoch 64/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7640\n",
      "Epoch 64: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5443 - acc: 0.7640 - val_loss: 0.5403 - val_acc: 0.7944\n",
      "Epoch 65/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7680\n",
      "Epoch 65: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5298 - acc: 0.7679 - val_loss: 0.5184 - val_acc: 0.8029\n",
      "Epoch 66/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7598\n",
      "Epoch 66: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5329 - acc: 0.7588 - val_loss: 0.5180 - val_acc: 0.7867\n",
      "Epoch 67/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.7662\n",
      "Epoch 67: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5100 - acc: 0.7662 - val_loss: 0.5272 - val_acc: 0.7867\n",
      "Epoch 68/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5328 - acc: 0.7614\n",
      "Epoch 68: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5328 - acc: 0.7614 - val_loss: 0.5469 - val_acc: 0.7973\n",
      "Epoch 69/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.7629\n",
      "Epoch 69: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5265 - acc: 0.7616 - val_loss: 0.5279 - val_acc: 0.8003\n",
      "Epoch 70/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7649\n",
      "Epoch 70: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5320 - acc: 0.7650 - val_loss: 0.5573 - val_acc: 0.7875\n",
      "Epoch 71/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7636\n",
      "Epoch 71: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5342 - acc: 0.7637 - val_loss: 0.5435 - val_acc: 0.7867\n",
      "Epoch 72/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.7609\n",
      "Epoch 72: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5346 - acc: 0.7609 - val_loss: 0.5447 - val_acc: 0.8012\n",
      "Epoch 73/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7664\n",
      "Epoch 73: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5335 - acc: 0.7665 - val_loss: 0.5326 - val_acc: 0.8003\n",
      "Epoch 74/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7643\n",
      "Epoch 74: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5524 - acc: 0.7628 - val_loss: 0.5448 - val_acc: 0.7918\n",
      "Epoch 75/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5402 - acc: 0.7672\n",
      "Epoch 75: val_acc did not improve from 0.80504\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5402 - acc: 0.7672 - val_loss: 0.5493 - val_acc: 0.7824\n",
      "Epoch 76/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.7685\n",
      "Epoch 76: val_acc improved from 0.80504 to 0.81274, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5451 - acc: 0.7683 - val_loss: 0.5285 - val_acc: 0.8127\n",
      "Epoch 77/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7666\n",
      "Epoch 77: val_acc did not improve from 0.81274\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5142 - acc: 0.7657 - val_loss: 0.5543 - val_acc: 0.7986\n",
      "Epoch 78/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7652\n",
      "Epoch 78: val_acc did not improve from 0.81274\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5327 - acc: 0.7649 - val_loss: 0.5350 - val_acc: 0.7794\n",
      "Epoch 79/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.7674\n",
      "Epoch 79: val_acc did not improve from 0.81274\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5313 - acc: 0.7674 - val_loss: 0.5173 - val_acc: 0.8021\n",
      "Epoch 80/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7716\n",
      "Epoch 80: val_acc improved from 0.81274 to 0.81488, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5176 - acc: 0.7717 - val_loss: 0.5194 - val_acc: 0.8149\n",
      "Epoch 81/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.7703\n",
      "Epoch 81: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5113 - acc: 0.7703 - val_loss: 0.5412 - val_acc: 0.8029\n",
      "Epoch 82/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.7707\n",
      "Epoch 82: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5095 - acc: 0.7705 - val_loss: 0.5485 - val_acc: 0.7931\n",
      "Epoch 83/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.7724\n",
      "Epoch 83: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4932 - acc: 0.7725 - val_loss: 0.5656 - val_acc: 0.8050\n",
      "Epoch 84/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.7673\n",
      "Epoch 84: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5124 - acc: 0.7676 - val_loss: 0.5435 - val_acc: 0.7978\n",
      "Epoch 85/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.7746\n",
      "Epoch 85: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5092 - acc: 0.7743 - val_loss: 0.5417 - val_acc: 0.8072\n",
      "Epoch 86/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.7713\n",
      "Epoch 86: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5318 - acc: 0.7712 - val_loss: 0.5251 - val_acc: 0.8089\n",
      "Epoch 87/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7673\n",
      "Epoch 87: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5152 - acc: 0.7678 - val_loss: 0.5424 - val_acc: 0.8059\n",
      "Epoch 88/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.7720\n",
      "Epoch 88: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5248 - acc: 0.7724 - val_loss: 0.5482 - val_acc: 0.7991\n",
      "Epoch 89/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.7757\n",
      "Epoch 89: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5485 - acc: 0.7751 - val_loss: 0.5454 - val_acc: 0.7738\n",
      "Epoch 90/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.7728\n",
      "Epoch 90: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5394 - acc: 0.7727 - val_loss: 0.5372 - val_acc: 0.8068\n",
      "Epoch 91/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7724\n",
      "Epoch 91: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5406 - acc: 0.7719 - val_loss: 0.5431 - val_acc: 0.8016\n",
      "Epoch 92/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7758\n",
      "Epoch 92: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5349 - acc: 0.7753 - val_loss: 0.5287 - val_acc: 0.7764\n",
      "Epoch 93/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.7788\n",
      "Epoch 93: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5254 - acc: 0.7782 - val_loss: 0.5364 - val_acc: 0.8029\n",
      "Epoch 94/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.7712\n",
      "Epoch 94: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5458 - acc: 0.7711 - val_loss: 0.5334 - val_acc: 0.8029\n",
      "Epoch 95/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5137 - acc: 0.7732\n",
      "Epoch 95: val_acc did not improve from 0.81488\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5137 - acc: 0.7732 - val_loss: 0.5367 - val_acc: 0.7888\n",
      "Epoch 96/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.7776\n",
      "Epoch 96: val_acc improved from 0.81488 to 0.81616, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5215 - acc: 0.7771 - val_loss: 0.5493 - val_acc: 0.8162\n",
      "Epoch 97/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.7749\n",
      "Epoch 97: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5263 - acc: 0.7749 - val_loss: 0.5567 - val_acc: 0.8085\n",
      "Epoch 98/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.7765\n",
      "Epoch 98: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5059 - acc: 0.7766 - val_loss: 0.5628 - val_acc: 0.8003\n",
      "Epoch 99/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.7749\n",
      "Epoch 99: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5167 - acc: 0.7750 - val_loss: 0.5539 - val_acc: 0.8003\n",
      "Epoch 100/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.7779\n",
      "Epoch 100: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5122 - acc: 0.7771 - val_loss: 0.5423 - val_acc: 0.8008\n",
      "Epoch 101/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7744\n",
      "Epoch 101: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5219 - acc: 0.7738 - val_loss: 0.5573 - val_acc: 0.7969\n",
      "Epoch 102/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.7808\n",
      "Epoch 102: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5208 - acc: 0.7807 - val_loss: 0.5797 - val_acc: 0.7888\n",
      "Epoch 103/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.7729\n",
      "Epoch 103: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5308 - acc: 0.7720 - val_loss: 0.5686 - val_acc: 0.7717\n",
      "Epoch 104/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.7729\n",
      "Epoch 104: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5150 - acc: 0.7730 - val_loss: 0.5286 - val_acc: 0.8102\n",
      "Epoch 105/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.7753\n",
      "Epoch 105: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5242 - acc: 0.7753 - val_loss: 0.5579 - val_acc: 0.8072\n",
      "Epoch 106/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.7796\n",
      "Epoch 106: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5023 - acc: 0.7796 - val_loss: 0.5829 - val_acc: 0.8123\n",
      "Epoch 107/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.7802\n",
      "Epoch 107: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5004 - acc: 0.7799 - val_loss: 0.5634 - val_acc: 0.8127\n",
      "Epoch 108/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.7824\n",
      "Epoch 108: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5001 - acc: 0.7823 - val_loss: 0.5496 - val_acc: 0.8110\n",
      "Epoch 109/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.7839\n",
      "Epoch 109: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5138 - acc: 0.7838 - val_loss: 0.5782 - val_acc: 0.7982\n",
      "Epoch 110/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7697\n",
      "Epoch 110: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5304 - acc: 0.7700 - val_loss: 0.5333 - val_acc: 0.8110\n",
      "Epoch 111/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.7709\n",
      "Epoch 111: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5318 - acc: 0.7703 - val_loss: 0.5334 - val_acc: 0.8093\n",
      "Epoch 112/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7842\n",
      "Epoch 112: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5161 - acc: 0.7838 - val_loss: 0.5455 - val_acc: 0.7931\n",
      "Epoch 113/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7817\n",
      "Epoch 113: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5363 - acc: 0.7818 - val_loss: 0.5494 - val_acc: 0.8063\n",
      "Epoch 114/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.7799\n",
      "Epoch 114: val_acc did not improve from 0.81616\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5141 - acc: 0.7794 - val_loss: 0.5744 - val_acc: 0.7914\n",
      "Epoch 115/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7823\n",
      "Epoch 115: val_acc improved from 0.81616 to 0.82257, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5145 - acc: 0.7821 - val_loss: 0.5695 - val_acc: 0.8226\n",
      "Epoch 116/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7728\n",
      "Epoch 116: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5227 - acc: 0.7731 - val_loss: 0.5768 - val_acc: 0.8110\n",
      "Epoch 117/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7699\n",
      "Epoch 117: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5316 - acc: 0.7695 - val_loss: 0.5672 - val_acc: 0.8110\n",
      "Epoch 118/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7753\n",
      "Epoch 118: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5186 - acc: 0.7749 - val_loss: 0.5681 - val_acc: 0.8174\n",
      "Epoch 119/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7746\n",
      "Epoch 119: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5150 - acc: 0.7747 - val_loss: 0.5850 - val_acc: 0.8102\n",
      "Epoch 120/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7746\n",
      "Epoch 120: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5326 - acc: 0.7750 - val_loss: 0.5708 - val_acc: 0.7807\n",
      "Epoch 121/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5590 - acc: 0.7730\n",
      "Epoch 121: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5589 - acc: 0.7731 - val_loss: 0.5801 - val_acc: 0.8003\n",
      "Epoch 122/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7788\n",
      "Epoch 122: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5200 - acc: 0.7784 - val_loss: 0.5620 - val_acc: 0.8157\n",
      "Epoch 123/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7721\n",
      "Epoch 123: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5144 - acc: 0.7722 - val_loss: 0.5629 - val_acc: 0.8093\n",
      "Epoch 124/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.7759\n",
      "Epoch 124: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5141 - acc: 0.7756 - val_loss: 0.6009 - val_acc: 0.8089\n",
      "Epoch 125/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.7765\n",
      "Epoch 125: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5271 - acc: 0.7768 - val_loss: 0.5832 - val_acc: 0.8132\n",
      "Epoch 126/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.7726\n",
      "Epoch 126: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5308 - acc: 0.7716 - val_loss: 0.5813 - val_acc: 0.8110\n",
      "Epoch 127/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.7776\n",
      "Epoch 127: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5164 - acc: 0.7771 - val_loss: 0.5495 - val_acc: 0.8059\n",
      "Epoch 128/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.7707\n",
      "Epoch 128: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5121 - acc: 0.7696 - val_loss: 0.6275 - val_acc: 0.8089\n",
      "Epoch 129/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.7767\n",
      "Epoch 129: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5312 - acc: 0.7764 - val_loss: 0.5783 - val_acc: 0.7914\n",
      "Epoch 130/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.7733\n",
      "Epoch 130: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5270 - acc: 0.7730 - val_loss: 0.5296 - val_acc: 0.7832\n",
      "Epoch 131/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.7829\n",
      "Epoch 131: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5066 - acc: 0.7827 - val_loss: 0.5475 - val_acc: 0.8093\n",
      "Epoch 132/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.7796\n",
      "Epoch 132: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5552 - acc: 0.7798 - val_loss: 0.5868 - val_acc: 0.8170\n",
      "Epoch 133/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.7786\n",
      "Epoch 133: val_acc did not improve from 0.82257\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5436 - acc: 0.7786 - val_loss: 0.5512 - val_acc: 0.8068\n",
      "Epoch 134/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.7817\n",
      "Epoch 134: val_acc improved from 0.82257 to 0.82343, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5199 - acc: 0.7818 - val_loss: 0.5447 - val_acc: 0.8234\n",
      "Epoch 135/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7801\n",
      "Epoch 135: val_acc did not improve from 0.82343\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5414 - acc: 0.7800 - val_loss: 0.6004 - val_acc: 0.8217\n",
      "Epoch 136/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.7824\n",
      "Epoch 136: val_acc did not improve from 0.82343\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5192 - acc: 0.7826 - val_loss: 0.5738 - val_acc: 0.8097\n",
      "Epoch 137/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.7822\n",
      "Epoch 137: val_acc improved from 0.82343 to 0.82386, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5117 - acc: 0.7825 - val_loss: 0.5490 - val_acc: 0.8239\n",
      "Epoch 138/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.7832\n",
      "Epoch 138: val_acc did not improve from 0.82386\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5214 - acc: 0.7821 - val_loss: 0.5818 - val_acc: 0.8003\n",
      "Epoch 139/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.7781\n",
      "Epoch 139: val_acc did not improve from 0.82386\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5377 - acc: 0.7783 - val_loss: 0.5903 - val_acc: 0.8162\n",
      "Epoch 140/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7827\n",
      "Epoch 140: val_acc improved from 0.82386 to 0.82728, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5226 - acc: 0.7828 - val_loss: 0.5700 - val_acc: 0.8273\n",
      "Epoch 141/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.7841\n",
      "Epoch 141: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5570 - acc: 0.7838 - val_loss: 0.5443 - val_acc: 0.8140\n",
      "Epoch 142/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7800\n",
      "Epoch 142: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5310 - acc: 0.7798 - val_loss: 0.5603 - val_acc: 0.8174\n",
      "Epoch 143/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.7808\n",
      "Epoch 143: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5122 - acc: 0.7807 - val_loss: 0.5479 - val_acc: 0.8166\n",
      "Epoch 144/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.7785\n",
      "Epoch 144: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5374 - acc: 0.7786 - val_loss: 0.6128 - val_acc: 0.8038\n",
      "Epoch 145/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.7807\n",
      "Epoch 145: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5785 - acc: 0.7805 - val_loss: 0.5713 - val_acc: 0.8153\n",
      "Epoch 146/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.7819\n",
      "Epoch 146: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5196 - acc: 0.7820 - val_loss: 0.5915 - val_acc: 0.8230\n",
      "Epoch 147/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7825\n",
      "Epoch 147: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5340 - acc: 0.7825 - val_loss: 0.5957 - val_acc: 0.8153\n",
      "Epoch 148/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5199 - acc: 0.7809\n",
      "Epoch 148: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5199 - acc: 0.7809 - val_loss: 0.6002 - val_acc: 0.8132\n",
      "Epoch 149/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.7856\n",
      "Epoch 149: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5218 - acc: 0.7848 - val_loss: 0.6458 - val_acc: 0.8115\n",
      "Epoch 150/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7861\n",
      "Epoch 150: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5181 - acc: 0.7863 - val_loss: 0.6223 - val_acc: 0.8170\n",
      "Epoch 151/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.7849\n",
      "Epoch 151: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5082 - acc: 0.7852 - val_loss: 0.6061 - val_acc: 0.8200\n",
      "Epoch 152/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.7805\n",
      "Epoch 152: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5350 - acc: 0.7804 - val_loss: 0.6002 - val_acc: 0.8068\n",
      "Epoch 153/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5532 - acc: 0.7829\n",
      "Epoch 153: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5532 - acc: 0.7829 - val_loss: 0.6419 - val_acc: 0.8239\n",
      "Epoch 154/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7818\n",
      "Epoch 154: val_acc did not improve from 0.82728\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5378 - acc: 0.7812 - val_loss: 0.5899 - val_acc: 0.8162\n",
      "Epoch 155/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7827\n",
      "Epoch 155: val_acc improved from 0.82728 to 0.82856, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5072 - acc: 0.7832 - val_loss: 0.5612 - val_acc: 0.8286\n",
      "Epoch 156/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5135 - acc: 0.7835\n",
      "Epoch 156: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5129 - acc: 0.7830 - val_loss: 0.5818 - val_acc: 0.8106\n",
      "Epoch 157/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.7853\n",
      "Epoch 157: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5018 - acc: 0.7849 - val_loss: 0.5706 - val_acc: 0.7850\n",
      "Epoch 158/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5148 - acc: 0.7789\n",
      "Epoch 158: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5148 - acc: 0.7789 - val_loss: 0.6196 - val_acc: 0.8063\n",
      "Epoch 159/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.7853\n",
      "Epoch 159: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5066 - acc: 0.7849 - val_loss: 0.6045 - val_acc: 0.8123\n",
      "Epoch 160/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7827\n",
      "Epoch 160: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5188 - acc: 0.7816 - val_loss: 0.6191 - val_acc: 0.8012\n",
      "Epoch 161/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.7791\n",
      "Epoch 161: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5199 - acc: 0.7792 - val_loss: 0.5979 - val_acc: 0.8136\n",
      "Epoch 162/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5266 - acc: 0.7838\n",
      "Epoch 162: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5266 - acc: 0.7838 - val_loss: 0.5692 - val_acc: 0.8093\n",
      "Epoch 163/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.7778\n",
      "Epoch 163: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5137 - acc: 0.7781 - val_loss: 0.6230 - val_acc: 0.8200\n",
      "Epoch 164/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5061 - acc: 0.7858\n",
      "Epoch 164: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5061 - acc: 0.7858 - val_loss: 0.5526 - val_acc: 0.8080\n",
      "Epoch 165/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7795\n",
      "Epoch 165: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5094 - acc: 0.7789 - val_loss: 0.5827 - val_acc: 0.7935\n",
      "Epoch 166/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.7840\n",
      "Epoch 166: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5152 - acc: 0.7841 - val_loss: 0.5526 - val_acc: 0.8170\n",
      "Epoch 167/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.7791\n",
      "Epoch 167: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5001 - acc: 0.7790 - val_loss: 0.6124 - val_acc: 0.8106\n",
      "Epoch 168/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5096 - acc: 0.7817\n",
      "Epoch 168: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5096 - acc: 0.7817 - val_loss: 0.5546 - val_acc: 0.8166\n",
      "Epoch 169/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7831\n",
      "Epoch 169: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5237 - acc: 0.7831 - val_loss: 0.5513 - val_acc: 0.8243\n",
      "Epoch 170/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.7827\n",
      "Epoch 170: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5539 - acc: 0.7825 - val_loss: 0.6179 - val_acc: 0.8115\n",
      "Epoch 171/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.7916\n",
      "Epoch 171: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5147 - acc: 0.7917 - val_loss: 0.5952 - val_acc: 0.7926\n",
      "Epoch 172/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7887\n",
      "Epoch 172: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5311 - acc: 0.7889 - val_loss: 0.5319 - val_acc: 0.8260\n",
      "Epoch 173/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7821\n",
      "Epoch 173: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5117 - acc: 0.7818 - val_loss: 0.5991 - val_acc: 0.8183\n",
      "Epoch 174/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.7795\n",
      "Epoch 174: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5125 - acc: 0.7792 - val_loss: 0.5842 - val_acc: 0.8140\n",
      "Epoch 175/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.7819\n",
      "Epoch 175: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5163 - acc: 0.7819 - val_loss: 0.5545 - val_acc: 0.8046\n",
      "Epoch 176/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.7818\n",
      "Epoch 176: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5345 - acc: 0.7818 - val_loss: 0.6075 - val_acc: 0.8221\n",
      "Epoch 177/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.7890\n",
      "Epoch 177: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5092 - acc: 0.7892 - val_loss: 0.5572 - val_acc: 0.8196\n",
      "Epoch 178/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.7767\n",
      "Epoch 178: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5044 - acc: 0.7769 - val_loss: 0.6192 - val_acc: 0.8106\n",
      "Epoch 179/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.7915\n",
      "Epoch 179: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5161 - acc: 0.7917 - val_loss: 0.5430 - val_acc: 0.7926\n",
      "Epoch 180/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.7846\n",
      "Epoch 180: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5109 - acc: 0.7832 - val_loss: 0.5458 - val_acc: 0.8145\n",
      "Epoch 181/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7883\n",
      "Epoch 181: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5170 - acc: 0.7873 - val_loss: 0.5905 - val_acc: 0.8166\n",
      "Epoch 182/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.7900\n",
      "Epoch 182: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5230 - acc: 0.7901 - val_loss: 0.5644 - val_acc: 0.8247\n",
      "Epoch 183/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.7845\n",
      "Epoch 183: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4982 - acc: 0.7846 - val_loss: 0.6368 - val_acc: 0.8230\n",
      "Epoch 184/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.7873\n",
      "Epoch 184: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4981 - acc: 0.7867 - val_loss: 0.5705 - val_acc: 0.8106\n",
      "Epoch 185/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7858\n",
      "Epoch 185: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5261 - acc: 0.7861 - val_loss: 0.5336 - val_acc: 0.8183\n",
      "Epoch 186/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.7857\n",
      "Epoch 186: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5430 - acc: 0.7862 - val_loss: 0.6607 - val_acc: 0.8286\n",
      "Epoch 187/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4900 - acc: 0.7826\n",
      "Epoch 187: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4892 - acc: 0.7829 - val_loss: 0.5948 - val_acc: 0.8192\n",
      "Epoch 188/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.7820\n",
      "Epoch 188: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5246 - acc: 0.7821 - val_loss: 0.5952 - val_acc: 0.8226\n",
      "Epoch 189/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.7868\n",
      "Epoch 189: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5231 - acc: 0.7870 - val_loss: 0.5494 - val_acc: 0.8200\n",
      "Epoch 190/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.7870\n",
      "Epoch 190: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5045 - acc: 0.7871 - val_loss: 0.6386 - val_acc: 0.8166\n",
      "Epoch 191/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.7842\n",
      "Epoch 191: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5273 - acc: 0.7838 - val_loss: 0.7069 - val_acc: 0.8076\n",
      "Epoch 192/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.7858\n",
      "Epoch 192: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5493 - acc: 0.7861 - val_loss: 0.6062 - val_acc: 0.8247\n",
      "Epoch 193/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.7927\n",
      "Epoch 193: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5216 - acc: 0.7917 - val_loss: 0.6014 - val_acc: 0.8029\n",
      "Epoch 194/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7858\n",
      "Epoch 194: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5171 - acc: 0.7855 - val_loss: 0.6294 - val_acc: 0.8119\n",
      "Epoch 195/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.7832\n",
      "Epoch 195: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5410 - acc: 0.7827 - val_loss: 0.6449 - val_acc: 0.8162\n",
      "Epoch 196/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.7832\n",
      "Epoch 196: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5279 - acc: 0.7831 - val_loss: 0.5683 - val_acc: 0.8170\n",
      "Epoch 197/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7830\n",
      "Epoch 197: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5216 - acc: 0.7828 - val_loss: 0.6202 - val_acc: 0.8187\n",
      "Epoch 198/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.7879\n",
      "Epoch 198: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5461 - acc: 0.7885 - val_loss: 0.5448 - val_acc: 0.8187\n",
      "Epoch 199/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7823\n",
      "Epoch 199: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5323 - acc: 0.7826 - val_loss: 0.5775 - val_acc: 0.8281\n",
      "Epoch 200/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.7874\n",
      "Epoch 200: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5003 - acc: 0.7867 - val_loss: 0.5907 - val_acc: 0.8183\n",
      "Epoch 201/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.7815\n",
      "Epoch 201: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5321 - acc: 0.7816 - val_loss: 0.6182 - val_acc: 0.8213\n",
      "Epoch 202/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.7847\n",
      "Epoch 202: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5079 - acc: 0.7845 - val_loss: 0.5822 - val_acc: 0.8226\n",
      "Epoch 203/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.7892\n",
      "Epoch 203: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4891 - acc: 0.7893 - val_loss: 0.5683 - val_acc: 0.8226\n",
      "Epoch 204/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.7817\n",
      "Epoch 204: val_acc did not improve from 0.82856\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5582 - acc: 0.7827 - val_loss: 0.5855 - val_acc: 0.8234\n",
      "Epoch 205/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7912\n",
      "Epoch 205: val_acc improved from 0.82856 to 0.83070, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5184 - acc: 0.7903 - val_loss: 0.6153 - val_acc: 0.8307\n",
      "Epoch 206/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7912\n",
      "Epoch 206: val_acc did not improve from 0.83070\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5190 - acc: 0.7896 - val_loss: 0.5737 - val_acc: 0.8123\n",
      "Epoch 207/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5367 - acc: 0.7807\n",
      "Epoch 207: val_acc improved from 0.83070 to 0.83540, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5355 - acc: 0.7803 - val_loss: 0.6195 - val_acc: 0.8354\n",
      "Epoch 208/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.7836\n",
      "Epoch 208: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5099 - acc: 0.7829 - val_loss: 0.5761 - val_acc: 0.8063\n",
      "Epoch 209/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7897\n",
      "Epoch 209: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5249 - acc: 0.7895 - val_loss: 0.5759 - val_acc: 0.8136\n",
      "Epoch 210/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7849\n",
      "Epoch 210: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5175 - acc: 0.7850 - val_loss: 0.5616 - val_acc: 0.8157\n",
      "Epoch 211/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5123 - acc: 0.7890\n",
      "Epoch 211: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5123 - acc: 0.7890 - val_loss: 0.5720 - val_acc: 0.8200\n",
      "Epoch 212/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.7852\n",
      "Epoch 212: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5209 - acc: 0.7852 - val_loss: 0.5451 - val_acc: 0.8170\n",
      "Epoch 213/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.7855\n",
      "Epoch 213: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5420 - acc: 0.7851 - val_loss: 0.5921 - val_acc: 0.8243\n",
      "Epoch 214/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.7826\n",
      "Epoch 214: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5081 - acc: 0.7826 - val_loss: 0.5797 - val_acc: 0.7909\n",
      "Epoch 215/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7850\n",
      "Epoch 215: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5236 - acc: 0.7850 - val_loss: 0.5421 - val_acc: 0.8127\n",
      "Epoch 216/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7843\n",
      "Epoch 216: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5145 - acc: 0.7846 - val_loss: 0.5912 - val_acc: 0.8119\n",
      "Epoch 217/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.7889\n",
      "Epoch 217: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5128 - acc: 0.7888 - val_loss: 0.5523 - val_acc: 0.8192\n",
      "Epoch 218/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.7806\n",
      "Epoch 218: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5709 - acc: 0.7797 - val_loss: 0.6702 - val_acc: 0.7807\n",
      "Epoch 219/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.7893\n",
      "Epoch 219: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4970 - acc: 0.7889 - val_loss: 0.6464 - val_acc: 0.8136\n",
      "Epoch 220/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.7814\n",
      "Epoch 220: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5145 - acc: 0.7815 - val_loss: 0.5993 - val_acc: 0.8093\n",
      "Epoch 221/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7927\n",
      "Epoch 221: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5362 - acc: 0.7921 - val_loss: 0.5858 - val_acc: 0.8068\n",
      "Epoch 222/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7826\n",
      "Epoch 222: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5163 - acc: 0.7826 - val_loss: 0.5317 - val_acc: 0.8038\n",
      "Epoch 223/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7835\n",
      "Epoch 223: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5378 - acc: 0.7836 - val_loss: 0.6878 - val_acc: 0.7909\n",
      "Epoch 224/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7877\n",
      "Epoch 224: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5099 - acc: 0.7881 - val_loss: 0.6007 - val_acc: 0.8234\n",
      "Epoch 225/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.7902\n",
      "Epoch 225: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5725 - acc: 0.7892 - val_loss: 0.6908 - val_acc: 0.8260\n",
      "Epoch 226/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.7871\n",
      "Epoch 226: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4867 - acc: 0.7879 - val_loss: 0.6608 - val_acc: 0.8273\n",
      "Epoch 227/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5038 - acc: 0.7924\n",
      "Epoch 227: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5029 - acc: 0.7921 - val_loss: 0.7012 - val_acc: 0.8153\n",
      "Epoch 228/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.7885\n",
      "Epoch 228: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5149 - acc: 0.7886 - val_loss: 0.6782 - val_acc: 0.8119\n",
      "Epoch 229/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.7907\n",
      "Epoch 229: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4927 - acc: 0.7905 - val_loss: 0.6341 - val_acc: 0.8260\n",
      "Epoch 230/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.7877\n",
      "Epoch 230: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5352 - acc: 0.7879 - val_loss: 0.5807 - val_acc: 0.8196\n",
      "Epoch 231/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.7912\n",
      "Epoch 231: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5034 - acc: 0.7904 - val_loss: 0.5526 - val_acc: 0.8038\n",
      "Epoch 232/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7834\n",
      "Epoch 232: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5071 - acc: 0.7830 - val_loss: 0.6252 - val_acc: 0.8102\n",
      "Epoch 233/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7899\n",
      "Epoch 233: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5372 - acc: 0.7891 - val_loss: 0.5785 - val_acc: 0.8221\n",
      "Epoch 234/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.7902\n",
      "Epoch 234: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5146 - acc: 0.7897 - val_loss: 0.6973 - val_acc: 0.8281\n",
      "Epoch 235/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.7935\n",
      "Epoch 235: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5135 - acc: 0.7931 - val_loss: 0.6211 - val_acc: 0.8294\n",
      "Epoch 236/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.7931\n",
      "Epoch 236: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4827 - acc: 0.7932 - val_loss: 0.6001 - val_acc: 0.8123\n",
      "Epoch 237/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.7840\n",
      "Epoch 237: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5007 - acc: 0.7831 - val_loss: 0.5930 - val_acc: 0.8132\n",
      "Epoch 238/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.7930\n",
      "Epoch 238: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5168 - acc: 0.7917 - val_loss: 0.5864 - val_acc: 0.8162\n",
      "Epoch 239/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7929\n",
      "Epoch 239: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5086 - acc: 0.7920 - val_loss: 0.6064 - val_acc: 0.8256\n",
      "Epoch 240/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.7914\n",
      "Epoch 240: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4898 - acc: 0.7911 - val_loss: 0.6733 - val_acc: 0.8132\n",
      "Epoch 241/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7861\n",
      "Epoch 241: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5162 - acc: 0.7865 - val_loss: 0.6326 - val_acc: 0.8256\n",
      "Epoch 242/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.7874\n",
      "Epoch 242: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5377 - acc: 0.7873 - val_loss: 0.6527 - val_acc: 0.8136\n",
      "Epoch 243/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7866\n",
      "Epoch 243: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5128 - acc: 0.7863 - val_loss: 0.6041 - val_acc: 0.8166\n",
      "Epoch 244/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7901\n",
      "Epoch 244: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5501 - acc: 0.7902 - val_loss: 0.5836 - val_acc: 0.8209\n",
      "Epoch 245/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7873\n",
      "Epoch 245: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5434 - acc: 0.7873 - val_loss: 0.5693 - val_acc: 0.8230\n",
      "Epoch 246/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5673 - acc: 0.7944\n",
      "Epoch 246: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5646 - acc: 0.7940 - val_loss: 0.5063 - val_acc: 0.8247\n",
      "Epoch 247/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5067 - acc: 0.7924\n",
      "Epoch 247: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5067 - acc: 0.7924 - val_loss: 0.6020 - val_acc: 0.8234\n",
      "Epoch 248/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.7857\n",
      "Epoch 248: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5096 - acc: 0.7860 - val_loss: 0.6257 - val_acc: 0.8264\n",
      "Epoch 249/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5254 - acc: 0.7864\n",
      "Epoch 249: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5254 - acc: 0.7864 - val_loss: 0.6648 - val_acc: 0.8089\n",
      "Epoch 250/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.7809\n",
      "Epoch 250: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5109 - acc: 0.7810 - val_loss: 0.6150 - val_acc: 0.8145\n",
      "Epoch 251/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.7828\n",
      "Epoch 251: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5052 - acc: 0.7826 - val_loss: 0.5517 - val_acc: 0.8221\n",
      "Epoch 252/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.7882\n",
      "Epoch 252: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5025 - acc: 0.7878 - val_loss: 0.5954 - val_acc: 0.8247\n",
      "Epoch 253/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5014 - acc: 0.7955\n",
      "Epoch 253: val_acc did not improve from 0.83540\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5014 - acc: 0.7955 - val_loss: 0.6381 - val_acc: 0.8166\n",
      "Epoch 254/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.7944\n",
      "Epoch 254: val_acc improved from 0.83540 to 0.83754, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5097 - acc: 0.7943 - val_loss: 0.6182 - val_acc: 0.8375\n",
      "Epoch 255/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4976 - acc: 0.7918\n",
      "Epoch 255: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4976 - acc: 0.7918 - val_loss: 0.5712 - val_acc: 0.8286\n",
      "Epoch 256/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4906 - acc: 0.7920\n",
      "Epoch 256: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4924 - acc: 0.7918 - val_loss: 0.6946 - val_acc: 0.8055\n",
      "Epoch 257/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5016 - acc: 0.7909\n",
      "Epoch 257: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5013 - acc: 0.7909 - val_loss: 0.6482 - val_acc: 0.8328\n",
      "Epoch 258/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.7858\n",
      "Epoch 258: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5308 - acc: 0.7862 - val_loss: 0.5812 - val_acc: 0.8230\n",
      "Epoch 259/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.7868\n",
      "Epoch 259: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5127 - acc: 0.7870 - val_loss: 0.6059 - val_acc: 0.8230\n",
      "Epoch 260/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7894\n",
      "Epoch 260: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5341 - acc: 0.7894 - val_loss: 0.5889 - val_acc: 0.8247\n",
      "Epoch 261/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.7924\n",
      "Epoch 261: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5034 - acc: 0.7924 - val_loss: 0.6215 - val_acc: 0.8260\n",
      "Epoch 262/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.7873\n",
      "Epoch 262: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5223 - acc: 0.7867 - val_loss: 0.6794 - val_acc: 0.8136\n",
      "Epoch 263/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5178 - acc: 0.7928\n",
      "Epoch 263: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5178 - acc: 0.7928 - val_loss: 0.6045 - val_acc: 0.8264\n",
      "Epoch 264/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.7934\n",
      "Epoch 264: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4864 - acc: 0.7936 - val_loss: 0.6518 - val_acc: 0.8200\n",
      "Epoch 265/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.7917\n",
      "Epoch 265: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4912 - acc: 0.7918 - val_loss: 0.6885 - val_acc: 0.8290\n",
      "Epoch 266/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.7900\n",
      "Epoch 266: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5102 - acc: 0.7895 - val_loss: 0.5917 - val_acc: 0.8132\n",
      "Epoch 267/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.7928\n",
      "Epoch 267: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5083 - acc: 0.7928 - val_loss: 0.6362 - val_acc: 0.8256\n",
      "Epoch 268/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.7888\n",
      "Epoch 268: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5166 - acc: 0.7878 - val_loss: 0.6471 - val_acc: 0.8217\n",
      "Epoch 269/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.7909\n",
      "Epoch 269: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4999 - acc: 0.7910 - val_loss: 0.5782 - val_acc: 0.8162\n",
      "Epoch 270/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7969\n",
      "Epoch 270: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5170 - acc: 0.7969 - val_loss: 0.6421 - val_acc: 0.8089\n",
      "Epoch 271/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.7966\n",
      "Epoch 271: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5051 - acc: 0.7966 - val_loss: 0.6152 - val_acc: 0.8200\n",
      "Epoch 272/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.7875\n",
      "Epoch 272: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5320 - acc: 0.7866 - val_loss: 0.5690 - val_acc: 0.8204\n",
      "Epoch 273/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.7885\n",
      "Epoch 273: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5016 - acc: 0.7878 - val_loss: 0.7173 - val_acc: 0.8174\n",
      "Epoch 274/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.7937\n",
      "Epoch 274: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5268 - acc: 0.7940 - val_loss: 0.6823 - val_acc: 0.8234\n",
      "Epoch 275/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7849\n",
      "Epoch 275: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5370 - acc: 0.7848 - val_loss: 0.6087 - val_acc: 0.8012\n",
      "Epoch 276/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.7897\n",
      "Epoch 276: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5378 - acc: 0.7896 - val_loss: 0.5633 - val_acc: 0.8204\n",
      "Epoch 277/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.7895\n",
      "Epoch 277: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5189 - acc: 0.7893 - val_loss: 0.5130 - val_acc: 0.8183\n",
      "Epoch 278/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7981\n",
      "Epoch 278: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5407 - acc: 0.7981 - val_loss: 0.5425 - val_acc: 0.8209\n",
      "Epoch 279/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7875\n",
      "Epoch 279: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5435 - acc: 0.7874 - val_loss: 0.6452 - val_acc: 0.8213\n",
      "Epoch 280/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7955\n",
      "Epoch 280: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5205 - acc: 0.7945 - val_loss: 0.5967 - val_acc: 0.8179\n",
      "Epoch 281/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.7864\n",
      "Epoch 281: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5106 - acc: 0.7863 - val_loss: 0.6632 - val_acc: 0.7978\n",
      "Epoch 282/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.7873\n",
      "Epoch 282: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5219 - acc: 0.7873 - val_loss: 0.6190 - val_acc: 0.8251\n",
      "Epoch 283/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.7903\n",
      "Epoch 283: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5077 - acc: 0.7904 - val_loss: 0.5221 - val_acc: 0.8371\n",
      "Epoch 284/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7878\n",
      "Epoch 284: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5353 - acc: 0.7877 - val_loss: 0.5509 - val_acc: 0.8234\n",
      "Epoch 285/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.7890\n",
      "Epoch 285: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5076 - acc: 0.7886 - val_loss: 0.5769 - val_acc: 0.8145\n",
      "Epoch 286/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.7843\n",
      "Epoch 286: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5377 - acc: 0.7834 - val_loss: 0.5504 - val_acc: 0.8003\n",
      "Epoch 287/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.7898\n",
      "Epoch 287: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5497 - acc: 0.7886 - val_loss: 0.5973 - val_acc: 0.8166\n",
      "Epoch 288/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4921 - acc: 0.7931\n",
      "Epoch 288: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4921 - acc: 0.7931 - val_loss: 0.5756 - val_acc: 0.8153\n",
      "Epoch 289/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.7895\n",
      "Epoch 289: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5209 - acc: 0.7895 - val_loss: 0.5639 - val_acc: 0.8247\n",
      "Epoch 290/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.7941\n",
      "Epoch 290: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4973 - acc: 0.7939 - val_loss: 0.5547 - val_acc: 0.8337\n",
      "Epoch 291/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.7944\n",
      "Epoch 291: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5161 - acc: 0.7942 - val_loss: 0.5662 - val_acc: 0.8038\n",
      "Epoch 292/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.7965\n",
      "Epoch 292: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5185 - acc: 0.7965 - val_loss: 0.5816 - val_acc: 0.8217\n",
      "Epoch 293/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7949\n",
      "Epoch 293: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5223 - acc: 0.7947 - val_loss: 0.6029 - val_acc: 0.8354\n",
      "Epoch 294/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.7907\n",
      "Epoch 294: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4977 - acc: 0.7904 - val_loss: 0.5655 - val_acc: 0.8204\n",
      "Epoch 295/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.7892\n",
      "Epoch 295: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5278 - acc: 0.7886 - val_loss: 0.5366 - val_acc: 0.8123\n",
      "Epoch 296/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.7948\n",
      "Epoch 296: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5302 - acc: 0.7952 - val_loss: 0.5791 - val_acc: 0.8298\n",
      "Epoch 297/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8002\n",
      "Epoch 297: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4926 - acc: 0.8004 - val_loss: 0.5850 - val_acc: 0.8217\n",
      "Epoch 298/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.7898\n",
      "Epoch 298: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4928 - acc: 0.7895 - val_loss: 0.6087 - val_acc: 0.8204\n",
      "Epoch 299/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.7919\n",
      "Epoch 299: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4953 - acc: 0.7917 - val_loss: 0.5983 - val_acc: 0.8149\n",
      "Epoch 300/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.7922\n",
      "Epoch 300: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5135 - acc: 0.7922 - val_loss: 0.5440 - val_acc: 0.8226\n",
      "Epoch 301/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.7924\n",
      "Epoch 301: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5133 - acc: 0.7919 - val_loss: 0.5516 - val_acc: 0.8234\n",
      "Epoch 302/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5298 - acc: 0.7940\n",
      "Epoch 302: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5298 - acc: 0.7940 - val_loss: 0.5324 - val_acc: 0.8213\n",
      "Epoch 303/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.7903\n",
      "Epoch 303: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5157 - acc: 0.7904 - val_loss: 0.5608 - val_acc: 0.8213\n",
      "Epoch 304/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.7885\n",
      "Epoch 304: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5063 - acc: 0.7888 - val_loss: 0.5563 - val_acc: 0.8247\n",
      "Epoch 305/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.7912\n",
      "Epoch 305: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5685 - acc: 0.7898 - val_loss: 0.5312 - val_acc: 0.8021\n",
      "Epoch 306/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.7909\n",
      "Epoch 306: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4822 - acc: 0.7910 - val_loss: 0.5702 - val_acc: 0.8119\n",
      "Epoch 307/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.7893\n",
      "Epoch 307: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5185 - acc: 0.7888 - val_loss: 0.5988 - val_acc: 0.8153\n",
      "Epoch 308/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5066 - acc: 0.7942\n",
      "Epoch 308: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5066 - acc: 0.7942 - val_loss: 0.5791 - val_acc: 0.8281\n",
      "Epoch 309/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.7954\n",
      "Epoch 309: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5214 - acc: 0.7951 - val_loss: 0.5320 - val_acc: 0.8042\n",
      "Epoch 310/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.7965\n",
      "Epoch 310: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5004 - acc: 0.7967 - val_loss: 0.5633 - val_acc: 0.8264\n",
      "Epoch 311/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.7952\n",
      "Epoch 311: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5341 - acc: 0.7957 - val_loss: 0.5541 - val_acc: 0.8320\n",
      "Epoch 312/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.7948\n",
      "Epoch 312: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5173 - acc: 0.7948 - val_loss: 0.5421 - val_acc: 0.8209\n",
      "Epoch 313/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.7940\n",
      "Epoch 313: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5194 - acc: 0.7934 - val_loss: 0.5480 - val_acc: 0.8162\n",
      "Epoch 314/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.7945\n",
      "Epoch 314: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4970 - acc: 0.7947 - val_loss: 0.5504 - val_acc: 0.8226\n",
      "Epoch 315/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.7956\n",
      "Epoch 315: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5065 - acc: 0.7943 - val_loss: 0.5236 - val_acc: 0.7978\n",
      "Epoch 316/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.7953\n",
      "Epoch 316: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4820 - acc: 0.7940 - val_loss: 0.5438 - val_acc: 0.8204\n",
      "Epoch 317/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7893\n",
      "Epoch 317: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5536 - acc: 0.7893 - val_loss: 0.5178 - val_acc: 0.8226\n",
      "Epoch 318/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.7911\n",
      "Epoch 318: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4995 - acc: 0.7910 - val_loss: 0.5245 - val_acc: 0.8192\n",
      "Epoch 319/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7959\n",
      "Epoch 319: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5284 - acc: 0.7958 - val_loss: 0.5963 - val_acc: 0.8239\n",
      "Epoch 320/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.7917\n",
      "Epoch 320: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5081 - acc: 0.7917 - val_loss: 0.5800 - val_acc: 0.8286\n",
      "Epoch 321/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.7893\n",
      "Epoch 321: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5024 - acc: 0.7886 - val_loss: 0.6849 - val_acc: 0.8204\n",
      "Epoch 322/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.7993\n",
      "Epoch 322: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4917 - acc: 0.7995 - val_loss: 0.5829 - val_acc: 0.8183\n",
      "Epoch 323/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.7948\n",
      "Epoch 323: val_acc did not improve from 0.83754\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4983 - acc: 0.7927 - val_loss: 0.5948 - val_acc: 0.8063\n",
      "Epoch 324/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5115 - acc: 0.8019\n",
      "Epoch 324: val_acc improved from 0.83754 to 0.83796, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/4/256/best_model.h5\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5115 - acc: 0.8019 - val_loss: 0.6415 - val_acc: 0.8380\n",
      "Epoch 325/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.8006\n",
      "Epoch 325: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5201 - acc: 0.8005 - val_loss: 0.6036 - val_acc: 0.8149\n",
      "Epoch 326/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.7941\n",
      "Epoch 326: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5079 - acc: 0.7936 - val_loss: 0.5953 - val_acc: 0.8226\n",
      "Epoch 327/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5227 - acc: 0.7902\n",
      "Epoch 327: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5225 - acc: 0.7903 - val_loss: 0.5452 - val_acc: 0.8174\n",
      "Epoch 328/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5034 - acc: 0.7916\n",
      "Epoch 328: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5034 - acc: 0.7916 - val_loss: 0.5805 - val_acc: 0.8239\n",
      "Epoch 329/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.7943\n",
      "Epoch 329: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5044 - acc: 0.7936 - val_loss: 0.5839 - val_acc: 0.8174\n",
      "Epoch 330/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7974\n",
      "Epoch 330: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5337 - acc: 0.7975 - val_loss: 0.5158 - val_acc: 0.8200\n",
      "Epoch 331/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.8025\n",
      "Epoch 331: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5043 - acc: 0.8025 - val_loss: 0.5386 - val_acc: 0.8192\n",
      "Epoch 332/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4954 - acc: 0.7939\n",
      "Epoch 332: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4954 - acc: 0.7939 - val_loss: 0.6204 - val_acc: 0.8209\n",
      "Epoch 333/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7984\n",
      "Epoch 333: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5346 - acc: 0.7981 - val_loss: 0.6637 - val_acc: 0.8221\n",
      "Epoch 334/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.7950\n",
      "Epoch 334: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5167 - acc: 0.7945 - val_loss: 0.6221 - val_acc: 0.8033\n",
      "Epoch 335/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.7963\n",
      "Epoch 335: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.4935 - acc: 0.7962 - val_loss: 0.6109 - val_acc: 0.8192\n",
      "Epoch 336/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.7955\n",
      "Epoch 336: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5113 - acc: 0.7951 - val_loss: 0.5930 - val_acc: 0.8234\n",
      "Epoch 337/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7913\n",
      "Epoch 337: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5191 - acc: 0.7910 - val_loss: 0.5815 - val_acc: 0.8239\n",
      "Epoch 338/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.7879\n",
      "Epoch 338: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5006 - acc: 0.7881 - val_loss: 0.5860 - val_acc: 0.8196\n",
      "Epoch 339/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.7949\n",
      "Epoch 339: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5013 - acc: 0.7942 - val_loss: 0.5689 - val_acc: 0.8132\n",
      "Epoch 340/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4979 - acc: 0.8009\n",
      "Epoch 340: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4979 - acc: 0.8009 - val_loss: 0.6111 - val_acc: 0.8324\n",
      "Epoch 341/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4966 - acc: 0.7985\n",
      "Epoch 341: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4965 - acc: 0.7986 - val_loss: 0.6128 - val_acc: 0.8371\n",
      "Epoch 342/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4831 - acc: 0.7974\n",
      "Epoch 342: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4831 - acc: 0.7974 - val_loss: 0.5978 - val_acc: 0.8166\n",
      "Epoch 343/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.7942\n",
      "Epoch 343: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5409 - acc: 0.7941 - val_loss: 0.5961 - val_acc: 0.8102\n",
      "Epoch 344/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7969\n",
      "Epoch 344: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5269 - acc: 0.7973 - val_loss: 0.6244 - val_acc: 0.8303\n",
      "Epoch 345/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.7961\n",
      "Epoch 345: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5166 - acc: 0.7952 - val_loss: 0.6615 - val_acc: 0.8371\n",
      "Epoch 346/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5254 - acc: 0.7936\n",
      "Epoch 346: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5254 - acc: 0.7936 - val_loss: 0.5769 - val_acc: 0.8213\n",
      "Epoch 347/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.8014\n",
      "Epoch 347: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5120 - acc: 0.8009 - val_loss: 0.5611 - val_acc: 0.8196\n",
      "Epoch 348/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.7959\n",
      "Epoch 348: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.4819 - acc: 0.7960 - val_loss: 0.6162 - val_acc: 0.8350\n",
      "Epoch 349/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.7964\n",
      "Epoch 349: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5082 - acc: 0.7964 - val_loss: 0.5914 - val_acc: 0.8209\n",
      "Epoch 350/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.7918\n",
      "Epoch 350: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5363 - acc: 0.7917 - val_loss: 0.7444 - val_acc: 0.8226\n",
      "Epoch 351/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.7942\n",
      "Epoch 351: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5088 - acc: 0.7943 - val_loss: 0.6748 - val_acc: 0.8273\n",
      "Epoch 352/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.7959\n",
      "Epoch 352: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5060 - acc: 0.7960 - val_loss: 0.6675 - val_acc: 0.8303\n",
      "Epoch 353/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.8003\n",
      "Epoch 353: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5469 - acc: 0.7998 - val_loss: 0.6747 - val_acc: 0.8149\n",
      "Epoch 354/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7971\n",
      "Epoch 354: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5059 - acc: 0.7973 - val_loss: 0.5969 - val_acc: 0.8264\n",
      "Epoch 355/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.7996\n",
      "Epoch 355: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5516 - acc: 0.7996 - val_loss: 0.5730 - val_acc: 0.8068\n",
      "Epoch 356/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7923\n",
      "Epoch 356: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5373 - acc: 0.7923 - val_loss: 0.6402 - val_acc: 0.8029\n",
      "Epoch 357/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7998\n",
      "Epoch 357: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5312 - acc: 0.7999 - val_loss: 0.5884 - val_acc: 0.8187\n",
      "Epoch 358/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.7959\n",
      "Epoch 358: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5337 - acc: 0.7959 - val_loss: 0.5718 - val_acc: 0.8230\n",
      "Epoch 359/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.7962\n",
      "Epoch 359: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4786 - acc: 0.7957 - val_loss: 0.5301 - val_acc: 0.8076\n",
      "Epoch 360/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.8005\n",
      "Epoch 360: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5462 - acc: 0.8002 - val_loss: 0.5748 - val_acc: 0.8145\n",
      "Epoch 361/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.7883\n",
      "Epoch 361: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 9ms/step - loss: 0.5444 - acc: 0.7885 - val_loss: 0.5801 - val_acc: 0.8226\n",
      "Epoch 362/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8000\n",
      "Epoch 362: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4995 - acc: 0.8003 - val_loss: 0.5749 - val_acc: 0.8187\n",
      "Epoch 363/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8009\n",
      "Epoch 363: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4974 - acc: 0.8010 - val_loss: 0.5891 - val_acc: 0.8294\n",
      "Epoch 364/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.7886\n",
      "Epoch 364: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4953 - acc: 0.7893 - val_loss: 0.6203 - val_acc: 0.8281\n",
      "Epoch 365/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.7953\n",
      "Epoch 365: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5073 - acc: 0.7952 - val_loss: 0.5701 - val_acc: 0.8230\n",
      "Epoch 366/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5600 - acc: 0.7988\n",
      "Epoch 366: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5600 - acc: 0.7988 - val_loss: 0.7256 - val_acc: 0.8234\n",
      "Epoch 367/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.7912\n",
      "Epoch 367: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5137 - acc: 0.7901 - val_loss: 0.6697 - val_acc: 0.8123\n",
      "Epoch 368/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7951\n",
      "Epoch 368: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5294 - acc: 0.7953 - val_loss: 0.6322 - val_acc: 0.8380\n",
      "Epoch 369/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4965 - acc: 0.8004\n",
      "Epoch 369: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4965 - acc: 0.8004 - val_loss: 0.5606 - val_acc: 0.8153\n",
      "Epoch 370/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8019\n",
      "Epoch 370: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5000 - acc: 0.8010 - val_loss: 0.6292 - val_acc: 0.8311\n",
      "Epoch 371/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.7962\n",
      "Epoch 371: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4928 - acc: 0.7959 - val_loss: 0.5244 - val_acc: 0.8149\n",
      "Epoch 372/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.7961\n",
      "Epoch 372: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4977 - acc: 0.7962 - val_loss: 0.5890 - val_acc: 0.8273\n",
      "Epoch 373/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.7925\n",
      "Epoch 373: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5332 - acc: 0.7922 - val_loss: 0.6245 - val_acc: 0.8157\n",
      "Epoch 374/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4820 - acc: 0.7932\n",
      "Epoch 374: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4822 - acc: 0.7925 - val_loss: 0.5915 - val_acc: 0.8179\n",
      "Epoch 375/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7910\n",
      "Epoch 375: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5131 - acc: 0.7898 - val_loss: 0.5730 - val_acc: 0.8204\n",
      "Epoch 376/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.7960\n",
      "Epoch 376: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5244 - acc: 0.7957 - val_loss: 0.6167 - val_acc: 0.7986\n",
      "Epoch 377/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.7952\n",
      "Epoch 377: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6090 - acc: 0.7943 - val_loss: 0.6045 - val_acc: 0.7982\n",
      "Epoch 378/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.7917\n",
      "Epoch 378: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5127 - acc: 0.7918 - val_loss: 0.5717 - val_acc: 0.8281\n",
      "Epoch 379/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.7960\n",
      "Epoch 379: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5362 - acc: 0.7962 - val_loss: 0.5828 - val_acc: 0.7956\n",
      "Epoch 380/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7959\n",
      "Epoch 380: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5175 - acc: 0.7957 - val_loss: 0.5473 - val_acc: 0.8213\n",
      "Epoch 381/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.7941\n",
      "Epoch 381: val_acc did not improve from 0.83796\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5289 - acc: 0.7941 - val_loss: 0.5783 - val_acc: 0.8157\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 256)               24832     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,521\n",
      "Trainable params: 107,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 1.5817 - acc: 0.5930\n",
      "Epoch 1: val_acc improved from -inf to 0.66510, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 5s 10ms/step - loss: 1.5749 - acc: 0.5926 - val_loss: 0.6800 - val_acc: 0.6651\n",
      "Epoch 2/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6725 - acc: 0.6492\n",
      "Epoch 2: val_acc improved from 0.66510 to 0.67237, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6731 - acc: 0.6489 - val_loss: 0.6661 - val_acc: 0.6724\n",
      "Epoch 3/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6401 - acc: 0.6681\n",
      "Epoch 3: val_acc improved from 0.67237 to 0.67280, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6397 - acc: 0.6679 - val_loss: 0.6494 - val_acc: 0.6728\n",
      "Epoch 4/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6172 - acc: 0.6810\n",
      "Epoch 4: val_acc improved from 0.67280 to 0.67836, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6173 - acc: 0.6805 - val_loss: 0.6386 - val_acc: 0.6784\n",
      "Epoch 5/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6059 - acc: 0.6817\n",
      "Epoch 5: val_acc improved from 0.67836 to 0.72070, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6056 - acc: 0.6813 - val_loss: 0.6243 - val_acc: 0.7207\n",
      "Epoch 6/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.6898\n",
      "Epoch 6: val_acc improved from 0.72070 to 0.72883, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.6003 - acc: 0.6892 - val_loss: 0.5949 - val_acc: 0.7288\n",
      "Epoch 7/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.7081\n",
      "Epoch 7: val_acc did not improve from 0.72883\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5802 - acc: 0.7077 - val_loss: 0.5856 - val_acc: 0.7237\n",
      "Epoch 8/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.7189\n",
      "Epoch 8: val_acc did not improve from 0.72883\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5702 - acc: 0.7183 - val_loss: 0.5850 - val_acc: 0.7275\n",
      "Epoch 9/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5645 - acc: 0.7283\n",
      "Epoch 9: val_acc improved from 0.72883 to 0.74423, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5645 - acc: 0.7283 - val_loss: 0.6002 - val_acc: 0.7442\n",
      "Epoch 10/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5564 - acc: 0.7280\n",
      "Epoch 10: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5567 - acc: 0.7278 - val_loss: 0.5876 - val_acc: 0.7087\n",
      "Epoch 11/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.7345\n",
      "Epoch 11: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5522 - acc: 0.7331 - val_loss: 0.5666 - val_acc: 0.7374\n",
      "Epoch 12/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7381\n",
      "Epoch 12: val_acc did not improve from 0.74423\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5507 - acc: 0.7375 - val_loss: 0.5778 - val_acc: 0.7365\n",
      "Epoch 13/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7432\n",
      "Epoch 13: val_acc improved from 0.74423 to 0.74594, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5416 - acc: 0.7425 - val_loss: 0.5773 - val_acc: 0.7459\n",
      "Epoch 14/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7407\n",
      "Epoch 14: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5424 - acc: 0.7408 - val_loss: 0.5599 - val_acc: 0.7425\n",
      "Epoch 15/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7454\n",
      "Epoch 15: val_acc improved from 0.74594 to 0.74893, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5447 - acc: 0.7450 - val_loss: 0.5388 - val_acc: 0.7489\n",
      "Epoch 16/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.7443\n",
      "Epoch 16: val_acc did not improve from 0.74893\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5349 - acc: 0.7452 - val_loss: 0.5675 - val_acc: 0.7489\n",
      "Epoch 17/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.7458\n",
      "Epoch 17: val_acc improved from 0.74893 to 0.75706, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5428 - acc: 0.7451 - val_loss: 0.5569 - val_acc: 0.7571\n",
      "Epoch 18/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7473\n",
      "Epoch 18: val_acc did not improve from 0.75706\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5292 - acc: 0.7467 - val_loss: 0.5197 - val_acc: 0.7464\n",
      "Epoch 19/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.7463\n",
      "Epoch 19: val_acc did not improve from 0.75706\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5420 - acc: 0.7464 - val_loss: 0.5567 - val_acc: 0.7438\n",
      "Epoch 20/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.7494\n",
      "Epoch 20: val_acc did not improve from 0.75706\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5363 - acc: 0.7494 - val_loss: 0.5293 - val_acc: 0.7447\n",
      "Epoch 21/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7550\n",
      "Epoch 21: val_acc improved from 0.75706 to 0.75920, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5267 - acc: 0.7551 - val_loss: 0.5499 - val_acc: 0.7592\n",
      "Epoch 22/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.7632\n",
      "Epoch 22: val_acc did not improve from 0.75920\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5309 - acc: 0.7622 - val_loss: 0.5402 - val_acc: 0.7528\n",
      "Epoch 23/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.7538\n",
      "Epoch 23: val_acc did not improve from 0.75920\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5241 - acc: 0.7535 - val_loss: 0.5406 - val_acc: 0.7562\n",
      "Epoch 24/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.7549\n",
      "Epoch 24: val_acc did not improve from 0.75920\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5401 - acc: 0.7546 - val_loss: 0.5377 - val_acc: 0.7588\n",
      "Epoch 25/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.7575\n",
      "Epoch 25: val_acc did not improve from 0.75920\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5256 - acc: 0.7569 - val_loss: 0.5121 - val_acc: 0.7549\n",
      "Epoch 26/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.7610\n",
      "Epoch 26: val_acc did not improve from 0.75920\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5217 - acc: 0.7603 - val_loss: 0.5663 - val_acc: 0.7528\n",
      "Epoch 27/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.7593\n",
      "Epoch 27: val_acc did not improve from 0.75920\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5470 - acc: 0.7589 - val_loss: 0.5367 - val_acc: 0.7519\n",
      "Epoch 28/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.7666\n",
      "Epoch 28: val_acc improved from 0.75920 to 0.76262, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5170 - acc: 0.7667 - val_loss: 0.5327 - val_acc: 0.7626\n",
      "Epoch 29/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.7603\n",
      "Epoch 29: val_acc improved from 0.76262 to 0.77032, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5266 - acc: 0.7610 - val_loss: 0.5119 - val_acc: 0.7703\n",
      "Epoch 30/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7673\n",
      "Epoch 30: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5145 - acc: 0.7676 - val_loss: 0.5380 - val_acc: 0.7686\n",
      "Epoch 31/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7640\n",
      "Epoch 31: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5284 - acc: 0.7638 - val_loss: 0.5346 - val_acc: 0.7665\n",
      "Epoch 32/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.7633\n",
      "Epoch 32: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5174 - acc: 0.7630 - val_loss: 0.5270 - val_acc: 0.7472\n",
      "Epoch 33/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.7698\n",
      "Epoch 33: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5291 - acc: 0.7695 - val_loss: 0.5329 - val_acc: 0.7665\n",
      "Epoch 34/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.7669\n",
      "Epoch 34: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5088 - acc: 0.7668 - val_loss: 0.5138 - val_acc: 0.7481\n",
      "Epoch 35/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5129 - acc: 0.7701\n",
      "Epoch 35: val_acc did not improve from 0.77032\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5129 - acc: 0.7701 - val_loss: 0.5481 - val_acc: 0.7524\n",
      "Epoch 36/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7662\n",
      "Epoch 36: val_acc improved from 0.77032 to 0.77203, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5226 - acc: 0.7662 - val_loss: 0.5225 - val_acc: 0.7720\n",
      "Epoch 37/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5167 - acc: 0.7610\n",
      "Epoch 37: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5167 - acc: 0.7610 - val_loss: 0.5268 - val_acc: 0.7605\n",
      "Epoch 38/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5380 - acc: 0.7641\n",
      "Epoch 38: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5380 - acc: 0.7641 - val_loss: 0.5090 - val_acc: 0.7601\n",
      "Epoch 39/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.7676\n",
      "Epoch 39: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5031 - acc: 0.7678 - val_loss: 0.5235 - val_acc: 0.7575\n",
      "Epoch 40/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.7705\n",
      "Epoch 40: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5102 - acc: 0.7691 - val_loss: 0.5362 - val_acc: 0.7601\n",
      "Epoch 41/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.7667\n",
      "Epoch 41: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5356 - acc: 0.7666 - val_loss: 0.5322 - val_acc: 0.7596\n",
      "Epoch 42/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.7667\n",
      "Epoch 42: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5209 - acc: 0.7667 - val_loss: 0.5137 - val_acc: 0.7686\n",
      "Epoch 43/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.7736\n",
      "Epoch 43: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5118 - acc: 0.7726 - val_loss: 0.5307 - val_acc: 0.7451\n",
      "Epoch 44/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.7691\n",
      "Epoch 44: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5345 - acc: 0.7687 - val_loss: 0.5322 - val_acc: 0.7601\n",
      "Epoch 45/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.7713\n",
      "Epoch 45: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5116 - acc: 0.7717 - val_loss: 0.5295 - val_acc: 0.7502\n",
      "Epoch 46/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.7697\n",
      "Epoch 46: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5255 - acc: 0.7687 - val_loss: 0.5269 - val_acc: 0.7481\n",
      "Epoch 47/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.7761\n",
      "Epoch 47: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5208 - acc: 0.7760 - val_loss: 0.5199 - val_acc: 0.7575\n",
      "Epoch 48/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.7700\n",
      "Epoch 48: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5068 - acc: 0.7696 - val_loss: 0.5141 - val_acc: 0.7553\n",
      "Epoch 49/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.7728\n",
      "Epoch 49: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5114 - acc: 0.7731 - val_loss: 0.5227 - val_acc: 0.7690\n",
      "Epoch 50/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7712\n",
      "Epoch 50: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5274 - acc: 0.7714 - val_loss: 0.5156 - val_acc: 0.7686\n",
      "Epoch 51/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7721\n",
      "Epoch 51: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5250 - acc: 0.7718 - val_loss: 0.5440 - val_acc: 0.7601\n",
      "Epoch 52/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.7755\n",
      "Epoch 52: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5069 - acc: 0.7757 - val_loss: 0.5185 - val_acc: 0.7630\n",
      "Epoch 53/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.7743\n",
      "Epoch 53: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5262 - acc: 0.7743 - val_loss: 0.5266 - val_acc: 0.7485\n",
      "Epoch 54/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.7750\n",
      "Epoch 54: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5040 - acc: 0.7755 - val_loss: 0.5115 - val_acc: 0.7558\n",
      "Epoch 55/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.7777\n",
      "Epoch 55: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5109 - acc: 0.7773 - val_loss: 0.5091 - val_acc: 0.7643\n",
      "Epoch 56/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.7701\n",
      "Epoch 56: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5202 - acc: 0.7698 - val_loss: 0.5064 - val_acc: 0.7592\n",
      "Epoch 57/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7714\n",
      "Epoch 57: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5216 - acc: 0.7706 - val_loss: 0.5098 - val_acc: 0.7468\n",
      "Epoch 58/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.7729\n",
      "Epoch 58: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5097 - acc: 0.7728 - val_loss: 0.5301 - val_acc: 0.7660\n",
      "Epoch 59/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7758\n",
      "Epoch 59: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5233 - acc: 0.7757 - val_loss: 0.5188 - val_acc: 0.7579\n",
      "Epoch 60/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.7783\n",
      "Epoch 60: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5287 - acc: 0.7779 - val_loss: 0.5135 - val_acc: 0.7485\n",
      "Epoch 61/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.7745\n",
      "Epoch 61: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5097 - acc: 0.7747 - val_loss: 0.5025 - val_acc: 0.7575\n",
      "Epoch 62/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7784\n",
      "Epoch 62: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5242 - acc: 0.7784 - val_loss: 0.5333 - val_acc: 0.7549\n",
      "Epoch 63/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.7823\n",
      "Epoch 63: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5138 - acc: 0.7826 - val_loss: 0.5116 - val_acc: 0.7699\n",
      "Epoch 64/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.7756\n",
      "Epoch 64: val_acc did not improve from 0.77203\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5172 - acc: 0.7751 - val_loss: 0.5236 - val_acc: 0.7588\n",
      "Epoch 65/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7715\n",
      "Epoch 65: val_acc improved from 0.77203 to 0.77545, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5249 - acc: 0.7718 - val_loss: 0.5040 - val_acc: 0.7754\n",
      "Epoch 66/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.7764\n",
      "Epoch 66: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5285 - acc: 0.7765 - val_loss: 0.5032 - val_acc: 0.7558\n",
      "Epoch 67/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.7733\n",
      "Epoch 67: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5314 - acc: 0.7734 - val_loss: 0.5178 - val_acc: 0.7609\n",
      "Epoch 68/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.7720\n",
      "Epoch 68: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5046 - acc: 0.7723 - val_loss: 0.5155 - val_acc: 0.7613\n",
      "Epoch 69/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7814\n",
      "Epoch 69: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5076 - acc: 0.7814 - val_loss: 0.5334 - val_acc: 0.7541\n",
      "Epoch 70/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.7761\n",
      "Epoch 70: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5402 - acc: 0.7764 - val_loss: 0.5119 - val_acc: 0.7609\n",
      "Epoch 71/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7810\n",
      "Epoch 71: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5322 - acc: 0.7809 - val_loss: 0.5132 - val_acc: 0.7528\n",
      "Epoch 72/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.7760\n",
      "Epoch 72: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5139 - acc: 0.7757 - val_loss: 0.5122 - val_acc: 0.7519\n",
      "Epoch 73/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.7787\n",
      "Epoch 73: val_acc did not improve from 0.77545\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5267 - acc: 0.7791 - val_loss: 0.5278 - val_acc: 0.7571\n",
      "Epoch 74/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.7771\n",
      "Epoch 74: val_acc improved from 0.77545 to 0.77887, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5163 - acc: 0.7762 - val_loss: 0.5285 - val_acc: 0.7789\n",
      "Epoch 75/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5296 - acc: 0.7797\n",
      "Epoch 75: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5276 - acc: 0.7800 - val_loss: 0.5252 - val_acc: 0.7583\n",
      "Epoch 76/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5312 - acc: 0.7767\n",
      "Epoch 76: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5312 - acc: 0.7767 - val_loss: 0.5086 - val_acc: 0.7579\n",
      "Epoch 77/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7829\n",
      "Epoch 77: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5365 - acc: 0.7829 - val_loss: 0.5167 - val_acc: 0.7601\n",
      "Epoch 78/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.7856\n",
      "Epoch 78: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5175 - acc: 0.7852 - val_loss: 0.5270 - val_acc: 0.7635\n",
      "Epoch 79/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5001 - acc: 0.7831\n",
      "Epoch 79: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4993 - acc: 0.7828 - val_loss: 0.5196 - val_acc: 0.7485\n",
      "Epoch 80/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.7869\n",
      "Epoch 80: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5168 - acc: 0.7870 - val_loss: 0.5249 - val_acc: 0.7652\n",
      "Epoch 81/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.7758\n",
      "Epoch 81: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5677 - acc: 0.7755 - val_loss: 0.5034 - val_acc: 0.7695\n",
      "Epoch 82/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.7789\n",
      "Epoch 82: val_acc did not improve from 0.77887\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5306 - acc: 0.7789 - val_loss: 0.5049 - val_acc: 0.7494\n",
      "Epoch 83/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.7835\n",
      "Epoch 83: val_acc improved from 0.77887 to 0.78229, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5037 - acc: 0.7836 - val_loss: 0.5359 - val_acc: 0.7823\n",
      "Epoch 84/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7824\n",
      "Epoch 84: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5469 - acc: 0.7825 - val_loss: 0.4986 - val_acc: 0.7823\n",
      "Epoch 85/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.7818\n",
      "Epoch 85: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5351 - acc: 0.7817 - val_loss: 0.4980 - val_acc: 0.7682\n",
      "Epoch 86/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5231 - acc: 0.7901\n",
      "Epoch 86: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5231 - acc: 0.7901 - val_loss: 0.5165 - val_acc: 0.7669\n",
      "Epoch 87/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.7803\n",
      "Epoch 87: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5412 - acc: 0.7795 - val_loss: 0.5221 - val_acc: 0.7609\n",
      "Epoch 88/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.7812\n",
      "Epoch 88: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5220 - acc: 0.7805 - val_loss: 0.5187 - val_acc: 0.7643\n",
      "Epoch 89/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7836\n",
      "Epoch 89: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5119 - acc: 0.7826 - val_loss: 0.5268 - val_acc: 0.7669\n",
      "Epoch 90/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.7830\n",
      "Epoch 90: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5228 - acc: 0.7827 - val_loss: 0.5198 - val_acc: 0.7464\n",
      "Epoch 91/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7803\n",
      "Epoch 91: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5293 - acc: 0.7798 - val_loss: 0.5118 - val_acc: 0.7729\n",
      "Epoch 92/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5108 - acc: 0.7838\n",
      "Epoch 92: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5098 - acc: 0.7835 - val_loss: 0.5163 - val_acc: 0.7609\n",
      "Epoch 93/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7802\n",
      "Epoch 93: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5241 - acc: 0.7796 - val_loss: 0.5169 - val_acc: 0.7618\n",
      "Epoch 94/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5169 - acc: 0.7848\n",
      "Epoch 94: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5169 - acc: 0.7848 - val_loss: 0.5324 - val_acc: 0.7536\n",
      "Epoch 95/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.7913\n",
      "Epoch 95: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5045 - acc: 0.7910 - val_loss: 0.5119 - val_acc: 0.7643\n",
      "Epoch 96/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7900\n",
      "Epoch 96: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5249 - acc: 0.7897 - val_loss: 0.5058 - val_acc: 0.7759\n",
      "Epoch 97/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.7849\n",
      "Epoch 97: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5368 - acc: 0.7848 - val_loss: 0.5085 - val_acc: 0.7742\n",
      "Epoch 98/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.7856\n",
      "Epoch 98: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5401 - acc: 0.7855 - val_loss: 0.4935 - val_acc: 0.7742\n",
      "Epoch 99/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.7864\n",
      "Epoch 99: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5285 - acc: 0.7863 - val_loss: 0.5006 - val_acc: 0.7635\n",
      "Epoch 100/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.7883\n",
      "Epoch 100: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5259 - acc: 0.7881 - val_loss: 0.5092 - val_acc: 0.7699\n",
      "Epoch 101/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7864\n",
      "Epoch 101: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5325 - acc: 0.7857 - val_loss: 0.5115 - val_acc: 0.7772\n",
      "Epoch 102/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.7873\n",
      "Epoch 102: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5210 - acc: 0.7872 - val_loss: 0.5222 - val_acc: 0.7707\n",
      "Epoch 103/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7892\n",
      "Epoch 103: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5355 - acc: 0.7893 - val_loss: 0.5302 - val_acc: 0.7699\n",
      "Epoch 104/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7815\n",
      "Epoch 104: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5169 - acc: 0.7822 - val_loss: 0.5459 - val_acc: 0.7737\n",
      "Epoch 105/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7855\n",
      "Epoch 105: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5159 - acc: 0.7850 - val_loss: 0.5223 - val_acc: 0.7665\n",
      "Epoch 106/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.7922\n",
      "Epoch 106: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5169 - acc: 0.7920 - val_loss: 0.5029 - val_acc: 0.7669\n",
      "Epoch 107/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7819\n",
      "Epoch 107: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5369 - acc: 0.7819 - val_loss: 0.5052 - val_acc: 0.7639\n",
      "Epoch 108/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5436 - acc: 0.7888\n",
      "Epoch 108: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5436 - acc: 0.7888 - val_loss: 0.5044 - val_acc: 0.7626\n",
      "Epoch 109/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5729 - acc: 0.7814\n",
      "Epoch 109: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5717 - acc: 0.7815 - val_loss: 0.5212 - val_acc: 0.7660\n",
      "Epoch 110/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7855\n",
      "Epoch 110: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5114 - acc: 0.7854 - val_loss: 0.5189 - val_acc: 0.7669\n",
      "Epoch 111/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.7795\n",
      "Epoch 111: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5690 - acc: 0.7795 - val_loss: 0.5019 - val_acc: 0.7746\n",
      "Epoch 112/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7883\n",
      "Epoch 112: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5204 - acc: 0.7881 - val_loss: 0.5174 - val_acc: 0.7673\n",
      "Epoch 113/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5659 - acc: 0.7817\n",
      "Epoch 113: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5659 - acc: 0.7817 - val_loss: 0.4818 - val_acc: 0.7695\n",
      "Epoch 114/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.7932\n",
      "Epoch 114: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5059 - acc: 0.7925 - val_loss: 0.5125 - val_acc: 0.7613\n",
      "Epoch 115/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5064 - acc: 0.7885\n",
      "Epoch 115: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5054 - acc: 0.7886 - val_loss: 0.5129 - val_acc: 0.7737\n",
      "Epoch 116/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.7918\n",
      "Epoch 116: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5159 - acc: 0.7926 - val_loss: 0.5098 - val_acc: 0.7682\n",
      "Epoch 117/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5156 - acc: 0.7935\n",
      "Epoch 117: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5156 - acc: 0.7935 - val_loss: 0.5083 - val_acc: 0.7678\n",
      "Epoch 118/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5692 - acc: 0.7872\n",
      "Epoch 118: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5689 - acc: 0.7873 - val_loss: 0.5060 - val_acc: 0.7648\n",
      "Epoch 119/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.7905\n",
      "Epoch 119: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5263 - acc: 0.7905 - val_loss: 0.4938 - val_acc: 0.7763\n",
      "Epoch 120/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.7877\n",
      "Epoch 120: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5139 - acc: 0.7883 - val_loss: 0.5112 - val_acc: 0.7618\n",
      "Epoch 121/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7893\n",
      "Epoch 121: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5173 - acc: 0.7893 - val_loss: 0.5226 - val_acc: 0.7733\n",
      "Epoch 122/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.7870\n",
      "Epoch 122: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5487 - acc: 0.7876 - val_loss: 0.5090 - val_acc: 0.7814\n",
      "Epoch 123/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7911\n",
      "Epoch 123: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5230 - acc: 0.7908 - val_loss: 0.5141 - val_acc: 0.7682\n",
      "Epoch 124/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.7839\n",
      "Epoch 124: val_acc did not improve from 0.78229\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5141 - acc: 0.7832 - val_loss: 0.5161 - val_acc: 0.7784\n",
      "Epoch 125/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.7907\n",
      "Epoch 125: val_acc improved from 0.78229 to 0.79299, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5180 - acc: 0.7910 - val_loss: 0.5002 - val_acc: 0.7930\n",
      "Epoch 126/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.7958\n",
      "Epoch 126: val_acc improved from 0.79299 to 0.79427, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5302 - acc: 0.7964 - val_loss: 0.5014 - val_acc: 0.7943\n",
      "Epoch 127/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7876\n",
      "Epoch 127: val_acc did not improve from 0.79427\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5397 - acc: 0.7875 - val_loss: 0.4992 - val_acc: 0.7669\n",
      "Epoch 128/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.7856\n",
      "Epoch 128: val_acc did not improve from 0.79427\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5119 - acc: 0.7863 - val_loss: 0.5020 - val_acc: 0.7823\n",
      "Epoch 129/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.7918\n",
      "Epoch 129: val_acc did not improve from 0.79427\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5087 - acc: 0.7916 - val_loss: 0.5058 - val_acc: 0.7682\n",
      "Epoch 130/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.7883\n",
      "Epoch 130: val_acc did not improve from 0.79427\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5846 - acc: 0.7885 - val_loss: 0.4862 - val_acc: 0.7849\n",
      "Epoch 131/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5210 - acc: 0.7948\n",
      "Epoch 131: val_acc improved from 0.79427 to 0.80539, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5210 - acc: 0.7948 - val_loss: 0.5088 - val_acc: 0.8054\n",
      "Epoch 132/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.7891\n",
      "Epoch 132: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5036 - acc: 0.7895 - val_loss: 0.4839 - val_acc: 0.7669\n",
      "Epoch 133/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.7908\n",
      "Epoch 133: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5067 - acc: 0.7904 - val_loss: 0.5213 - val_acc: 0.7806\n",
      "Epoch 134/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.7925\n",
      "Epoch 134: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5181 - acc: 0.7918 - val_loss: 0.5014 - val_acc: 0.7678\n",
      "Epoch 135/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5188 - acc: 0.7921\n",
      "Epoch 135: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5188 - acc: 0.7921 - val_loss: 0.4998 - val_acc: 0.7733\n",
      "Epoch 136/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.7891\n",
      "Epoch 136: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4801 - acc: 0.7884 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 137/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.7892\n",
      "Epoch 137: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5331 - acc: 0.7887 - val_loss: 0.5041 - val_acc: 0.7648\n",
      "Epoch 138/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.7957\n",
      "Epoch 138: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5704 - acc: 0.7955 - val_loss: 0.4984 - val_acc: 0.7571\n",
      "Epoch 139/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.7891\n",
      "Epoch 139: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5306 - acc: 0.7897 - val_loss: 0.5148 - val_acc: 0.7814\n",
      "Epoch 140/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.7944\n",
      "Epoch 140: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5087 - acc: 0.7940 - val_loss: 0.4943 - val_acc: 0.7806\n",
      "Epoch 141/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7890\n",
      "Epoch 141: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5254 - acc: 0.7889 - val_loss: 0.5063 - val_acc: 0.7699\n",
      "Epoch 142/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.7911\n",
      "Epoch 142: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5142 - acc: 0.7912 - val_loss: 0.5006 - val_acc: 0.7652\n",
      "Epoch 143/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5716 - acc: 0.7901\n",
      "Epoch 143: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5691 - acc: 0.7904 - val_loss: 0.4898 - val_acc: 0.7729\n",
      "Epoch 144/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7907\n",
      "Epoch 144: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5347 - acc: 0.7905 - val_loss: 0.5000 - val_acc: 0.7648\n",
      "Epoch 145/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.7928\n",
      "Epoch 145: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5137 - acc: 0.7921 - val_loss: 0.4979 - val_acc: 0.7772\n",
      "Epoch 146/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.7894\n",
      "Epoch 146: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5358 - acc: 0.7885 - val_loss: 0.4981 - val_acc: 0.7707\n",
      "Epoch 147/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.7976\n",
      "Epoch 147: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5266 - acc: 0.7975 - val_loss: 0.4963 - val_acc: 0.7793\n",
      "Epoch 148/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7957\n",
      "Epoch 148: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5479 - acc: 0.7954 - val_loss: 0.5123 - val_acc: 0.7648\n",
      "Epoch 149/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7979\n",
      "Epoch 149: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5356 - acc: 0.7980 - val_loss: 0.4886 - val_acc: 0.7703\n",
      "Epoch 150/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4902 - acc: 0.7990\n",
      "Epoch 150: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4902 - acc: 0.7990 - val_loss: 0.4921 - val_acc: 0.7767\n",
      "Epoch 151/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.7973\n",
      "Epoch 151: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5209 - acc: 0.7971 - val_loss: 0.4963 - val_acc: 0.7767\n",
      "Epoch 152/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7966\n",
      "Epoch 152: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5138 - acc: 0.7963 - val_loss: 0.4941 - val_acc: 0.7793\n",
      "Epoch 153/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7933\n",
      "Epoch 153: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5375 - acc: 0.7935 - val_loss: 0.4840 - val_acc: 0.7921\n",
      "Epoch 154/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.7939\n",
      "Epoch 154: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5958 - acc: 0.7936 - val_loss: 0.4943 - val_acc: 0.7716\n",
      "Epoch 155/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.7906\n",
      "Epoch 155: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5190 - acc: 0.7906 - val_loss: 0.5033 - val_acc: 0.7754\n",
      "Epoch 156/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5663 - acc: 0.7939\n",
      "Epoch 156: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5663 - acc: 0.7939 - val_loss: 0.5003 - val_acc: 0.7960\n",
      "Epoch 157/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7921\n",
      "Epoch 157: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5448 - acc: 0.7925 - val_loss: 0.4873 - val_acc: 0.7951\n",
      "Epoch 158/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5196 - acc: 0.7926\n",
      "Epoch 158: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5196 - acc: 0.7926 - val_loss: 0.5245 - val_acc: 0.7789\n",
      "Epoch 159/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.7953\n",
      "Epoch 159: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5180 - acc: 0.7947 - val_loss: 0.4983 - val_acc: 0.7831\n",
      "Epoch 160/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.7932\n",
      "Epoch 160: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5134 - acc: 0.7931 - val_loss: 0.4915 - val_acc: 0.7806\n",
      "Epoch 161/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7918\n",
      "Epoch 161: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5336 - acc: 0.7913 - val_loss: 0.5035 - val_acc: 0.7660\n",
      "Epoch 162/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7968\n",
      "Epoch 162: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5472 - acc: 0.7963 - val_loss: 0.4980 - val_acc: 0.7669\n",
      "Epoch 163/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7912\n",
      "Epoch 163: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5066 - acc: 0.7915 - val_loss: 0.5058 - val_acc: 0.7930\n",
      "Epoch 164/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.8013\n",
      "Epoch 164: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5540 - acc: 0.8014 - val_loss: 0.4879 - val_acc: 0.7904\n",
      "Epoch 165/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.7947\n",
      "Epoch 165: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5592 - acc: 0.7947 - val_loss: 0.4842 - val_acc: 0.7776\n",
      "Epoch 166/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7946\n",
      "Epoch 166: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5109 - acc: 0.7944 - val_loss: 0.4985 - val_acc: 0.7857\n",
      "Epoch 167/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.7954\n",
      "Epoch 167: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5043 - acc: 0.7954 - val_loss: 0.5085 - val_acc: 0.7814\n",
      "Epoch 168/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7967\n",
      "Epoch 168: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5268 - acc: 0.7967 - val_loss: 0.5009 - val_acc: 0.7720\n",
      "Epoch 169/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.7937\n",
      "Epoch 169: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5213 - acc: 0.7933 - val_loss: 0.4981 - val_acc: 0.7746\n",
      "Epoch 170/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.7980\n",
      "Epoch 170: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5138 - acc: 0.7982 - val_loss: 0.4882 - val_acc: 0.7639\n",
      "Epoch 171/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.8023\n",
      "Epoch 171: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5188 - acc: 0.8022 - val_loss: 0.5033 - val_acc: 0.7806\n",
      "Epoch 172/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8002\n",
      "Epoch 172: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5255 - acc: 0.8003 - val_loss: 0.5040 - val_acc: 0.7810\n",
      "Epoch 173/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.8023\n",
      "Epoch 173: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5050 - acc: 0.8028 - val_loss: 0.4949 - val_acc: 0.7819\n",
      "Epoch 174/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8013\n",
      "Epoch 174: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4991 - acc: 0.8008 - val_loss: 0.5096 - val_acc: 0.7831\n",
      "Epoch 175/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.7986\n",
      "Epoch 175: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5056 - acc: 0.7976 - val_loss: 0.4990 - val_acc: 0.7673\n",
      "Epoch 176/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7938\n",
      "Epoch 176: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5228 - acc: 0.7938 - val_loss: 0.5007 - val_acc: 0.7733\n",
      "Epoch 177/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.7923\n",
      "Epoch 177: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5178 - acc: 0.7921 - val_loss: 0.4932 - val_acc: 0.7861\n",
      "Epoch 178/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.7957\n",
      "Epoch 178: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5536 - acc: 0.7953 - val_loss: 0.4940 - val_acc: 0.7836\n",
      "Epoch 179/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5673 - acc: 0.8009\n",
      "Epoch 179: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5676 - acc: 0.8000 - val_loss: 0.5025 - val_acc: 0.7733\n",
      "Epoch 180/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7971\n",
      "Epoch 180: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5400 - acc: 0.7978 - val_loss: 0.5009 - val_acc: 0.7793\n",
      "Epoch 181/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.7972\n",
      "Epoch 181: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5148 - acc: 0.7972 - val_loss: 0.4944 - val_acc: 0.7891\n",
      "Epoch 182/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.8066\n",
      "Epoch 182: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4934 - acc: 0.8068 - val_loss: 0.4960 - val_acc: 0.7968\n",
      "Epoch 183/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.8032\n",
      "Epoch 183: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5114 - acc: 0.8031 - val_loss: 0.5020 - val_acc: 0.7819\n",
      "Epoch 184/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.7983\n",
      "Epoch 184: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5658 - acc: 0.7984 - val_loss: 0.5134 - val_acc: 0.7879\n",
      "Epoch 185/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.7958\n",
      "Epoch 185: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5270 - acc: 0.7961 - val_loss: 0.5063 - val_acc: 0.7772\n",
      "Epoch 186/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.8016\n",
      "Epoch 186: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5240 - acc: 0.8017 - val_loss: 0.5012 - val_acc: 0.7806\n",
      "Epoch 187/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.8021\n",
      "Epoch 187: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5300 - acc: 0.8013 - val_loss: 0.4971 - val_acc: 0.7686\n",
      "Epoch 188/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8027\n",
      "Epoch 188: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5539 - acc: 0.8029 - val_loss: 0.5062 - val_acc: 0.7998\n",
      "Epoch 189/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8022\n",
      "Epoch 189: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4826 - acc: 0.8023 - val_loss: 0.4907 - val_acc: 0.7754\n",
      "Epoch 190/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8015\n",
      "Epoch 190: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5301 - acc: 0.8001 - val_loss: 0.5085 - val_acc: 0.7695\n",
      "Epoch 191/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5161 - acc: 0.8043\n",
      "Epoch 191: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5161 - acc: 0.8043 - val_loss: 0.4966 - val_acc: 0.7887\n",
      "Epoch 192/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.8021\n",
      "Epoch 192: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5461 - acc: 0.8008 - val_loss: 0.4978 - val_acc: 0.7913\n",
      "Epoch 193/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.8033\n",
      "Epoch 193: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5336 - acc: 0.8031 - val_loss: 0.4948 - val_acc: 0.7908\n",
      "Epoch 194/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8002\n",
      "Epoch 194: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5075 - acc: 0.8009 - val_loss: 0.5126 - val_acc: 0.7836\n",
      "Epoch 195/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.8001\n",
      "Epoch 195: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5182 - acc: 0.8002 - val_loss: 0.4925 - val_acc: 0.7891\n",
      "Epoch 196/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.7963\n",
      "Epoch 196: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4984 - acc: 0.7961 - val_loss: 0.5046 - val_acc: 0.7827\n",
      "Epoch 197/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.8005\n",
      "Epoch 197: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5324 - acc: 0.8007 - val_loss: 0.4908 - val_acc: 0.7981\n",
      "Epoch 198/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5049 - acc: 0.7978\n",
      "Epoch 198: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5049 - acc: 0.7978 - val_loss: 0.4962 - val_acc: 0.7703\n",
      "Epoch 199/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8023\n",
      "Epoch 199: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5276 - acc: 0.8021 - val_loss: 0.4961 - val_acc: 0.7772\n",
      "Epoch 200/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.8072\n",
      "Epoch 200: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.5313 - acc: 0.8069 - val_loss: 0.4980 - val_acc: 0.7840\n",
      "Epoch 201/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5604 - acc: 0.7995\n",
      "Epoch 201: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5575 - acc: 0.7994 - val_loss: 0.4895 - val_acc: 0.7861\n",
      "Epoch 202/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8025\n",
      "Epoch 202: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5230 - acc: 0.8028 - val_loss: 0.5024 - val_acc: 0.8015\n",
      "Epoch 203/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8032\n",
      "Epoch 203: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5002 - acc: 0.8033 - val_loss: 0.4931 - val_acc: 0.7763\n",
      "Epoch 204/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.7997\n",
      "Epoch 204: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5317 - acc: 0.7998 - val_loss: 0.4967 - val_acc: 0.8011\n",
      "Epoch 205/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.8049\n",
      "Epoch 205: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5252 - acc: 0.8047 - val_loss: 0.4961 - val_acc: 0.7836\n",
      "Epoch 206/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.8018\n",
      "Epoch 206: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5120 - acc: 0.8012 - val_loss: 0.4962 - val_acc: 0.7806\n",
      "Epoch 207/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.7975\n",
      "Epoch 207: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5208 - acc: 0.7977 - val_loss: 0.4978 - val_acc: 0.7742\n",
      "Epoch 208/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8051\n",
      "Epoch 208: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4814 - acc: 0.8058 - val_loss: 0.5057 - val_acc: 0.7964\n",
      "Epoch 209/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.8015\n",
      "Epoch 209: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4982 - acc: 0.8013 - val_loss: 0.5154 - val_acc: 0.7669\n",
      "Epoch 210/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.8060\n",
      "Epoch 210: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5502 - acc: 0.8064 - val_loss: 0.5138 - val_acc: 0.7789\n",
      "Epoch 211/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.8023\n",
      "Epoch 211: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5132 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7879\n",
      "Epoch 212/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8011\n",
      "Epoch 212: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4946 - acc: 0.8009 - val_loss: 0.5006 - val_acc: 0.7836\n",
      "Epoch 213/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8012\n",
      "Epoch 213: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4989 - acc: 0.8013 - val_loss: 0.5023 - val_acc: 0.7879\n",
      "Epoch 214/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7979\n",
      "Epoch 214: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5375 - acc: 0.7985 - val_loss: 0.5029 - val_acc: 0.7746\n",
      "Epoch 215/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.7974\n",
      "Epoch 215: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5381 - acc: 0.7973 - val_loss: 0.4975 - val_acc: 0.7754\n",
      "Epoch 216/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.8043\n",
      "Epoch 216: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5619 - acc: 0.8038 - val_loss: 0.5048 - val_acc: 0.7772\n",
      "Epoch 217/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8008\n",
      "Epoch 217: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5742 - acc: 0.8002 - val_loss: 0.5031 - val_acc: 0.7849\n",
      "Epoch 218/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.8031\n",
      "Epoch 218: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5450 - acc: 0.8033 - val_loss: 0.5141 - val_acc: 0.7926\n",
      "Epoch 219/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.8038\n",
      "Epoch 219: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5478 - acc: 0.8037 - val_loss: 0.5140 - val_acc: 0.7720\n",
      "Epoch 220/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.8050\n",
      "Epoch 220: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5256 - acc: 0.8046 - val_loss: 0.4863 - val_acc: 0.7853\n",
      "Epoch 221/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.8026\n",
      "Epoch 221: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5419 - acc: 0.8017 - val_loss: 0.4943 - val_acc: 0.7686\n",
      "Epoch 222/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.8026\n",
      "Epoch 222: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5428 - acc: 0.8028 - val_loss: 0.4993 - val_acc: 0.7913\n",
      "Epoch 223/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7983\n",
      "Epoch 223: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5338 - acc: 0.7986 - val_loss: 0.5114 - val_acc: 0.7844\n",
      "Epoch 224/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.8077\n",
      "Epoch 224: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5778 - acc: 0.8073 - val_loss: 0.4930 - val_acc: 0.8024\n",
      "Epoch 225/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8106\n",
      "Epoch 225: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5223 - acc: 0.8101 - val_loss: 0.4983 - val_acc: 0.7806\n",
      "Epoch 226/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8054\n",
      "Epoch 226: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4915 - acc: 0.8053 - val_loss: 0.5201 - val_acc: 0.7956\n",
      "Epoch 227/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7991\n",
      "Epoch 227: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5245 - acc: 0.7992 - val_loss: 0.4905 - val_acc: 0.7763\n",
      "Epoch 228/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.8038\n",
      "Epoch 228: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4839 - acc: 0.8041 - val_loss: 0.5033 - val_acc: 0.7985\n",
      "Epoch 229/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.8027\n",
      "Epoch 229: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5359 - acc: 0.8030 - val_loss: 0.4942 - val_acc: 0.7814\n",
      "Epoch 230/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8038\n",
      "Epoch 230: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5024 - acc: 0.8040 - val_loss: 0.5156 - val_acc: 0.8024\n",
      "Epoch 231/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5153 - acc: 0.8017\n",
      "Epoch 231: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5153 - acc: 0.8017 - val_loss: 0.5191 - val_acc: 0.7797\n",
      "Epoch 232/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8036\n",
      "Epoch 232: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5344 - acc: 0.8037 - val_loss: 0.5075 - val_acc: 0.7707\n",
      "Epoch 233/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.8040\n",
      "Epoch 233: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5268 - acc: 0.8039 - val_loss: 0.5033 - val_acc: 0.7780\n",
      "Epoch 234/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.7989\n",
      "Epoch 234: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5247 - acc: 0.7985 - val_loss: 0.5159 - val_acc: 0.7780\n",
      "Epoch 235/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8002\n",
      "Epoch 235: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5100 - acc: 0.7994 - val_loss: 0.5046 - val_acc: 0.7857\n",
      "Epoch 236/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.8044\n",
      "Epoch 236: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4997 - acc: 0.8046 - val_loss: 0.5052 - val_acc: 0.7725\n",
      "Epoch 237/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5178 - acc: 0.8031\n",
      "Epoch 237: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5178 - acc: 0.8031 - val_loss: 0.5094 - val_acc: 0.7861\n",
      "Epoch 238/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.8007\n",
      "Epoch 238: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5340 - acc: 0.8009 - val_loss: 0.5046 - val_acc: 0.7737\n",
      "Epoch 239/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5412 - acc: 0.8046\n",
      "Epoch 239: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.8046 - val_loss: 0.5092 - val_acc: 0.7836\n",
      "Epoch 240/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8049\n",
      "Epoch 240: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5095 - acc: 0.8050 - val_loss: 0.5099 - val_acc: 0.7750\n",
      "Epoch 241/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8061\n",
      "Epoch 241: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5107 - acc: 0.8056 - val_loss: 0.5086 - val_acc: 0.7733\n",
      "Epoch 242/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8032\n",
      "Epoch 242: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5225 - acc: 0.8034 - val_loss: 0.5110 - val_acc: 0.8054\n",
      "Epoch 243/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4922 - acc: 0.8053\n",
      "Epoch 243: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4907 - acc: 0.8052 - val_loss: 0.5049 - val_acc: 0.7776\n",
      "Epoch 244/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8033\n",
      "Epoch 244: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4941 - acc: 0.8033 - val_loss: 0.5000 - val_acc: 0.7840\n",
      "Epoch 245/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8083\n",
      "Epoch 245: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4901 - acc: 0.8074 - val_loss: 0.5088 - val_acc: 0.7913\n",
      "Epoch 246/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5004 - acc: 0.8035\n",
      "Epoch 246: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4980 - acc: 0.8043 - val_loss: 0.4895 - val_acc: 0.7938\n",
      "Epoch 247/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8087\n",
      "Epoch 247: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5203 - acc: 0.8092 - val_loss: 0.5032 - val_acc: 0.7926\n",
      "Epoch 248/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8032\n",
      "Epoch 248: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4784 - acc: 0.8038 - val_loss: 0.4874 - val_acc: 0.7780\n",
      "Epoch 249/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8037\n",
      "Epoch 249: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5146 - acc: 0.8032 - val_loss: 0.5022 - val_acc: 0.7866\n",
      "Epoch 250/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.8007\n",
      "Epoch 250: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5172 - acc: 0.8000 - val_loss: 0.5175 - val_acc: 0.7750\n",
      "Epoch 251/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.8054\n",
      "Epoch 251: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5121 - acc: 0.8050 - val_loss: 0.5123 - val_acc: 0.7737\n",
      "Epoch 252/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8059\n",
      "Epoch 252: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5048 - acc: 0.8060 - val_loss: 0.5009 - val_acc: 0.7802\n",
      "Epoch 253/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.8067\n",
      "Epoch 253: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5072 - acc: 0.8064 - val_loss: 0.4977 - val_acc: 0.7840\n",
      "Epoch 254/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8100\n",
      "Epoch 254: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4988 - acc: 0.8100 - val_loss: 0.5080 - val_acc: 0.8037\n",
      "Epoch 255/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.8088\n",
      "Epoch 255: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5448 - acc: 0.8088 - val_loss: 0.5021 - val_acc: 0.7823\n",
      "Epoch 256/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.8041\n",
      "Epoch 256: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4910 - acc: 0.8040 - val_loss: 0.5015 - val_acc: 0.7780\n",
      "Epoch 257/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.8053\n",
      "Epoch 257: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5170 - acc: 0.8055 - val_loss: 0.4862 - val_acc: 0.7759\n",
      "Epoch 258/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5068 - acc: 0.8041\n",
      "Epoch 258: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5068 - acc: 0.8041 - val_loss: 0.4877 - val_acc: 0.7926\n",
      "Epoch 259/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8054\n",
      "Epoch 259: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4935 - acc: 0.8062 - val_loss: 0.4806 - val_acc: 0.7789\n",
      "Epoch 260/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.8053\n",
      "Epoch 260: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4901 - acc: 0.8048 - val_loss: 0.4944 - val_acc: 0.7643\n",
      "Epoch 261/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.8037\n",
      "Epoch 261: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5229 - acc: 0.8037 - val_loss: 0.4912 - val_acc: 0.7725\n",
      "Epoch 262/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.8073\n",
      "Epoch 262: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5088 - acc: 0.8074 - val_loss: 0.4991 - val_acc: 0.8003\n",
      "Epoch 263/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.8029\n",
      "Epoch 263: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5417 - acc: 0.8033 - val_loss: 0.5105 - val_acc: 0.7823\n",
      "Epoch 264/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.8139\n",
      "Epoch 264: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4928 - acc: 0.8140 - val_loss: 0.5103 - val_acc: 0.7806\n",
      "Epoch 265/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5273 - acc: 0.8038\n",
      "Epoch 265: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5273 - acc: 0.8038 - val_loss: 0.4991 - val_acc: 0.7789\n",
      "Epoch 266/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.8095\n",
      "Epoch 266: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5467 - acc: 0.8096 - val_loss: 0.5012 - val_acc: 0.7844\n",
      "Epoch 267/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8051\n",
      "Epoch 267: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4961 - acc: 0.8047 - val_loss: 0.4942 - val_acc: 0.7840\n",
      "Epoch 268/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8069\n",
      "Epoch 268: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5116 - acc: 0.8057 - val_loss: 0.4881 - val_acc: 0.7934\n",
      "Epoch 269/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.8097\n",
      "Epoch 269: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5003 - acc: 0.8103 - val_loss: 0.4911 - val_acc: 0.8015\n",
      "Epoch 270/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8100\n",
      "Epoch 270: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4787 - acc: 0.8105 - val_loss: 0.4964 - val_acc: 0.7806\n",
      "Epoch 271/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.8073\n",
      "Epoch 271: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5089 - acc: 0.8071 - val_loss: 0.4964 - val_acc: 0.7780\n",
      "Epoch 272/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8081\n",
      "Epoch 272: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5145 - acc: 0.8084 - val_loss: 0.4848 - val_acc: 0.8037\n",
      "Epoch 273/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.8094\n",
      "Epoch 273: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4924 - acc: 0.8093 - val_loss: 0.5001 - val_acc: 0.7866\n",
      "Epoch 274/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.8099\n",
      "Epoch 274: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4933 - acc: 0.8097 - val_loss: 0.4935 - val_acc: 0.8028\n",
      "Epoch 275/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4966 - acc: 0.8112\n",
      "Epoch 275: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4952 - acc: 0.8111 - val_loss: 0.4937 - val_acc: 0.7861\n",
      "Epoch 276/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.8127\n",
      "Epoch 276: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5042 - acc: 0.8125 - val_loss: 0.4975 - val_acc: 0.7763\n",
      "Epoch 277/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.8109\n",
      "Epoch 277: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5331 - acc: 0.8107 - val_loss: 0.5011 - val_acc: 0.7810\n",
      "Epoch 278/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8086\n",
      "Epoch 278: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4814 - acc: 0.8093 - val_loss: 0.4829 - val_acc: 0.7742\n",
      "Epoch 279/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4814 - acc: 0.8136\n",
      "Epoch 279: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4810 - acc: 0.8137 - val_loss: 0.5086 - val_acc: 0.7904\n",
      "Epoch 280/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.8080\n",
      "Epoch 280: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4639 - acc: 0.8077 - val_loss: 0.5040 - val_acc: 0.7921\n",
      "Epoch 281/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8060\n",
      "Epoch 281: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4917 - acc: 0.8058 - val_loss: 0.5084 - val_acc: 0.7725\n",
      "Epoch 282/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.8078\n",
      "Epoch 282: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4963 - acc: 0.8069 - val_loss: 0.5115 - val_acc: 0.7797\n",
      "Epoch 283/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5125 - acc: 0.8132\n",
      "Epoch 283: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5120 - acc: 0.8134 - val_loss: 0.5222 - val_acc: 0.7874\n",
      "Epoch 284/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.8131\n",
      "Epoch 284: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5083 - acc: 0.8132 - val_loss: 0.4942 - val_acc: 0.7943\n",
      "Epoch 285/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4889 - acc: 0.8081\n",
      "Epoch 285: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4872 - acc: 0.8086 - val_loss: 0.4897 - val_acc: 0.8037\n",
      "Epoch 286/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.8112\n",
      "Epoch 286: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5244 - acc: 0.8115 - val_loss: 0.4966 - val_acc: 0.7908\n",
      "Epoch 287/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8091\n",
      "Epoch 287: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5025 - acc: 0.8092 - val_loss: 0.5024 - val_acc: 0.8050\n",
      "Epoch 288/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.8102\n",
      "Epoch 288: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5054 - acc: 0.8100 - val_loss: 0.5044 - val_acc: 0.7861\n",
      "Epoch 289/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.8156\n",
      "Epoch 289: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4816 - acc: 0.8150 - val_loss: 0.5105 - val_acc: 0.7827\n",
      "Epoch 290/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8077\n",
      "Epoch 290: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5222 - acc: 0.8081 - val_loss: 0.4995 - val_acc: 0.7904\n",
      "Epoch 291/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8113\n",
      "Epoch 291: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5156 - acc: 0.8103 - val_loss: 0.4971 - val_acc: 0.7797\n",
      "Epoch 292/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4897 - acc: 0.8106\n",
      "Epoch 292: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4868 - acc: 0.8106 - val_loss: 0.5198 - val_acc: 0.7879\n",
      "Epoch 293/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4981 - acc: 0.8123\n",
      "Epoch 293: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4981 - acc: 0.8123 - val_loss: 0.5279 - val_acc: 0.7977\n",
      "Epoch 294/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4991 - acc: 0.8087\n",
      "Epoch 294: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4991 - acc: 0.8087 - val_loss: 0.5028 - val_acc: 0.7870\n",
      "Epoch 295/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.8068\n",
      "Epoch 295: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5091 - acc: 0.8065 - val_loss: 0.4887 - val_acc: 0.7737\n",
      "Epoch 296/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5121 - acc: 0.8042\n",
      "Epoch 296: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5121 - acc: 0.8042 - val_loss: 0.4859 - val_acc: 0.7891\n",
      "Epoch 297/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4821 - acc: 0.8081\n",
      "Epoch 297: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4817 - acc: 0.8083 - val_loss: 0.5080 - val_acc: 0.7814\n",
      "Epoch 298/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8105\n",
      "Epoch 298: val_acc did not improve from 0.80539\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4913 - acc: 0.8105 - val_loss: 0.5035 - val_acc: 0.7759\n",
      "Epoch 299/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5036 - acc: 0.8106\n",
      "Epoch 299: val_acc improved from 0.80539 to 0.80710, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5036 - acc: 0.8106 - val_loss: 0.5033 - val_acc: 0.8071\n",
      "Epoch 300/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8093\n",
      "Epoch 300: val_acc did not improve from 0.80710\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5225 - acc: 0.8088 - val_loss: 0.5017 - val_acc: 0.7733\n",
      "Epoch 301/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8127\n",
      "Epoch 301: val_acc did not improve from 0.80710\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4867 - acc: 0.8131 - val_loss: 0.4891 - val_acc: 0.7994\n",
      "Epoch 302/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5108 - acc: 0.8089\n",
      "Epoch 302: val_acc did not improve from 0.80710\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5103 - acc: 0.8090 - val_loss: 0.4892 - val_acc: 0.7853\n",
      "Epoch 303/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8164\n",
      "Epoch 303: val_acc improved from 0.80710 to 0.81437, saving model to train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/5/256/best_model.h5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4810 - acc: 0.8167 - val_loss: 0.5141 - val_acc: 0.8144\n",
      "Epoch 304/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.8108\n",
      "Epoch 304: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4968 - acc: 0.8108 - val_loss: 0.5005 - val_acc: 0.7891\n",
      "Epoch 305/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.8115\n",
      "Epoch 305: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5179 - acc: 0.8117 - val_loss: 0.4965 - val_acc: 0.7810\n",
      "Epoch 306/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.8148\n",
      "Epoch 306: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5169 - acc: 0.8145 - val_loss: 0.5106 - val_acc: 0.7797\n",
      "Epoch 307/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.8100\n",
      "Epoch 307: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5316 - acc: 0.8095 - val_loss: 0.4842 - val_acc: 0.7814\n",
      "Epoch 308/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4781 - acc: 0.8148\n",
      "Epoch 308: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4773 - acc: 0.8139 - val_loss: 0.5150 - val_acc: 0.7896\n",
      "Epoch 309/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8111\n",
      "Epoch 309: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5119 - acc: 0.8100 - val_loss: 0.4948 - val_acc: 0.7784\n",
      "Epoch 310/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8169\n",
      "Epoch 310: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5016 - acc: 0.8165 - val_loss: 0.4808 - val_acc: 0.7750\n",
      "Epoch 311/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8114\n",
      "Epoch 311: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4848 - acc: 0.8116 - val_loss: 0.4860 - val_acc: 0.7857\n",
      "Epoch 312/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.8085\n",
      "Epoch 312: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5178 - acc: 0.8092 - val_loss: 0.4823 - val_acc: 0.7917\n",
      "Epoch 313/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5029 - acc: 0.8120\n",
      "Epoch 313: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5029 - acc: 0.8120 - val_loss: 0.5033 - val_acc: 0.7763\n",
      "Epoch 314/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8156\n",
      "Epoch 314: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5280 - acc: 0.8153 - val_loss: 0.5083 - val_acc: 0.7921\n",
      "Epoch 315/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.8060\n",
      "Epoch 315: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5131 - acc: 0.8062 - val_loss: 0.5080 - val_acc: 0.7814\n",
      "Epoch 316/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8043\n",
      "Epoch 316: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5066 - acc: 0.8052 - val_loss: 0.4904 - val_acc: 0.7660\n",
      "Epoch 317/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.8056\n",
      "Epoch 317: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5128 - acc: 0.8057 - val_loss: 0.5123 - val_acc: 0.7870\n",
      "Epoch 318/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.8139\n",
      "Epoch 318: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4971 - acc: 0.8141 - val_loss: 0.4849 - val_acc: 0.7819\n",
      "Epoch 319/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.8125\n",
      "Epoch 319: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5543 - acc: 0.8125 - val_loss: 0.4921 - val_acc: 0.7810\n",
      "Epoch 320/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.8034\n",
      "Epoch 320: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5347 - acc: 0.8033 - val_loss: 0.4910 - val_acc: 0.7720\n",
      "Epoch 321/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8111\n",
      "Epoch 321: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4931 - acc: 0.8111 - val_loss: 0.4979 - val_acc: 0.7742\n",
      "Epoch 322/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5147 - acc: 0.8051\n",
      "Epoch 322: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5139 - acc: 0.8049 - val_loss: 0.4916 - val_acc: 0.7870\n",
      "Epoch 323/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4678 - acc: 0.8144\n",
      "Epoch 323: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4666 - acc: 0.8137 - val_loss: 0.5059 - val_acc: 0.7699\n",
      "Epoch 324/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.8115\n",
      "Epoch 324: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5219 - acc: 0.8116 - val_loss: 0.5063 - val_acc: 0.7810\n",
      "Epoch 325/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.8140\n",
      "Epoch 325: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4899 - acc: 0.8142 - val_loss: 0.4888 - val_acc: 0.7938\n",
      "Epoch 326/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8116\n",
      "Epoch 326: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4844 - acc: 0.8115 - val_loss: 0.5011 - val_acc: 0.7754\n",
      "Epoch 327/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8171\n",
      "Epoch 327: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4956 - acc: 0.8167 - val_loss: 0.5206 - val_acc: 0.7849\n",
      "Epoch 328/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4832 - acc: 0.8150\n",
      "Epoch 328: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4832 - acc: 0.8150 - val_loss: 0.5565 - val_acc: 0.7831\n",
      "Epoch 329/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8074\n",
      "Epoch 329: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4868 - acc: 0.8072 - val_loss: 0.4949 - val_acc: 0.7759\n",
      "Epoch 330/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4937 - acc: 0.8183\n",
      "Epoch 330: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4937 - acc: 0.8183 - val_loss: 0.4998 - val_acc: 0.7981\n",
      "Epoch 331/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.8114\n",
      "Epoch 331: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5236 - acc: 0.8116 - val_loss: 0.4932 - val_acc: 0.7857\n",
      "Epoch 332/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8128\n",
      "Epoch 332: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4983 - acc: 0.8132 - val_loss: 0.4996 - val_acc: 0.7849\n",
      "Epoch 333/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5051 - acc: 0.8181\n",
      "Epoch 333: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5051 - acc: 0.8181 - val_loss: 0.5108 - val_acc: 0.7737\n",
      "Epoch 334/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8086\n",
      "Epoch 334: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4854 - acc: 0.8084 - val_loss: 0.5136 - val_acc: 0.7678\n",
      "Epoch 335/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.8054\n",
      "Epoch 335: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5530 - acc: 0.8056 - val_loss: 0.4904 - val_acc: 0.7759\n",
      "Epoch 336/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.8132\n",
      "Epoch 336: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5600 - acc: 0.8130 - val_loss: 0.5235 - val_acc: 0.7874\n",
      "Epoch 337/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8115\n",
      "Epoch 337: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5293 - acc: 0.8102 - val_loss: 0.4982 - val_acc: 0.7682\n",
      "Epoch 338/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8173\n",
      "Epoch 338: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4866 - acc: 0.8174 - val_loss: 0.5041 - val_acc: 0.7981\n",
      "Epoch 339/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5125 - acc: 0.8148\n",
      "Epoch 339: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5117 - acc: 0.8145 - val_loss: 0.4752 - val_acc: 0.7985\n",
      "Epoch 340/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5235 - acc: 0.8115\n",
      "Epoch 340: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5235 - acc: 0.8115 - val_loss: 0.4815 - val_acc: 0.7990\n",
      "Epoch 341/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8129\n",
      "Epoch 341: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4945 - acc: 0.8115 - val_loss: 0.4949 - val_acc: 0.7784\n",
      "Epoch 342/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4967 - acc: 0.8123\n",
      "Epoch 342: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4967 - acc: 0.8123 - val_loss: 0.4999 - val_acc: 0.7742\n",
      "Epoch 343/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4789 - acc: 0.8128\n",
      "Epoch 343: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4764 - acc: 0.8122 - val_loss: 0.4797 - val_acc: 0.8033\n",
      "Epoch 344/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4680 - acc: 0.8157\n",
      "Epoch 344: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4676 - acc: 0.8157 - val_loss: 0.4756 - val_acc: 0.7810\n",
      "Epoch 345/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.8116\n",
      "Epoch 345: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5283 - acc: 0.8114 - val_loss: 0.5031 - val_acc: 0.7763\n",
      "Epoch 346/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8174\n",
      "Epoch 346: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4903 - acc: 0.8170 - val_loss: 0.5173 - val_acc: 0.7887\n",
      "Epoch 347/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8071\n",
      "Epoch 347: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4609 - acc: 0.8073 - val_loss: 0.5280 - val_acc: 0.7836\n",
      "Epoch 348/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5048 - acc: 0.8146\n",
      "Epoch 348: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5048 - acc: 0.8146 - val_loss: 0.4921 - val_acc: 0.7956\n",
      "Epoch 349/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.8079\n",
      "Epoch 349: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5161 - acc: 0.8080 - val_loss: 0.5061 - val_acc: 0.7917\n",
      "Epoch 350/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8140\n",
      "Epoch 350: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4692 - acc: 0.8139 - val_loss: 0.4934 - val_acc: 0.7806\n",
      "Epoch 351/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8170\n",
      "Epoch 351: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4967 - acc: 0.8163 - val_loss: 0.5052 - val_acc: 0.7908\n",
      "Epoch 352/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.8142\n",
      "Epoch 352: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5234 - acc: 0.8150 - val_loss: 0.5022 - val_acc: 0.7985\n",
      "Epoch 353/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8142\n",
      "Epoch 353: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4632 - acc: 0.8142 - val_loss: 0.5121 - val_acc: 0.7754\n",
      "Epoch 354/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.8146\n",
      "Epoch 354: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5181 - acc: 0.8148 - val_loss: 0.5125 - val_acc: 0.7844\n",
      "Epoch 355/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8105\n",
      "Epoch 355: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5370 - acc: 0.8105 - val_loss: 0.5100 - val_acc: 0.7797\n",
      "Epoch 356/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8176\n",
      "Epoch 356: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4960 - acc: 0.8181 - val_loss: 0.4884 - val_acc: 0.7840\n",
      "Epoch 357/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4865 - acc: 0.8120\n",
      "Epoch 357: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4865 - acc: 0.8120 - val_loss: 0.5026 - val_acc: 0.7750\n",
      "Epoch 358/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8143\n",
      "Epoch 358: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4909 - acc: 0.8143 - val_loss: 0.5064 - val_acc: 0.7784\n",
      "Epoch 359/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.8127\n",
      "Epoch 359: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5341 - acc: 0.8125 - val_loss: 0.4846 - val_acc: 0.7806\n",
      "Epoch 360/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.8177\n",
      "Epoch 360: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5152 - acc: 0.8177 - val_loss: 0.4853 - val_acc: 0.7802\n",
      "Epoch 361/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.8105\n",
      "Epoch 361: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5250 - acc: 0.8106 - val_loss: 0.5179 - val_acc: 0.7802\n",
      "Epoch 362/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4915 - acc: 0.8073\n",
      "Epoch 362: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4915 - acc: 0.8073 - val_loss: 0.5077 - val_acc: 0.7784\n",
      "Epoch 363/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8096\n",
      "Epoch 363: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5151 - acc: 0.8104 - val_loss: 0.5080 - val_acc: 0.7981\n",
      "Epoch 364/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8147\n",
      "Epoch 364: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5412 - acc: 0.8147 - val_loss: 0.5044 - val_acc: 0.7849\n",
      "Epoch 365/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.8140\n",
      "Epoch 365: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4927 - acc: 0.8141 - val_loss: 0.5123 - val_acc: 0.7840\n",
      "Epoch 366/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8111\n",
      "Epoch 366: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5310 - acc: 0.8110 - val_loss: 0.5024 - val_acc: 0.7742\n",
      "Epoch 367/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.8110\n",
      "Epoch 367: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4887 - acc: 0.8110 - val_loss: 0.5092 - val_acc: 0.8007\n",
      "Epoch 368/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4804 - acc: 0.8228\n",
      "Epoch 368: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4800 - acc: 0.8229 - val_loss: 0.5025 - val_acc: 0.7750\n",
      "Epoch 369/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.8162\n",
      "Epoch 369: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5037 - acc: 0.8162 - val_loss: 0.5037 - val_acc: 0.7866\n",
      "Epoch 370/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.8151\n",
      "Epoch 370: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5235 - acc: 0.8150 - val_loss: 0.4990 - val_acc: 0.7754\n",
      "Epoch 371/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.8162\n",
      "Epoch 371: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5143 - acc: 0.8154 - val_loss: 0.5124 - val_acc: 0.7951\n",
      "Epoch 372/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.8162\n",
      "Epoch 372: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4934 - acc: 0.8158 - val_loss: 0.5065 - val_acc: 0.7819\n",
      "Epoch 373/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8141\n",
      "Epoch 373: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4979 - acc: 0.8138 - val_loss: 0.4968 - val_acc: 0.7776\n",
      "Epoch 374/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8132\n",
      "Epoch 374: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5017 - acc: 0.8137 - val_loss: 0.5168 - val_acc: 0.7951\n",
      "Epoch 375/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.8156\n",
      "Epoch 375: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5320 - acc: 0.8157 - val_loss: 0.5036 - val_acc: 0.8109\n",
      "Epoch 376/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5291 - acc: 0.8137\n",
      "Epoch 376: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5291 - acc: 0.8137 - val_loss: 0.4939 - val_acc: 0.7879\n",
      "Epoch 377/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8131\n",
      "Epoch 377: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4832 - acc: 0.8128 - val_loss: 0.4882 - val_acc: 0.7866\n",
      "Epoch 378/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8137\n",
      "Epoch 378: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4916 - acc: 0.8137 - val_loss: 0.4852 - val_acc: 0.7900\n",
      "Epoch 379/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8143\n",
      "Epoch 379: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5146 - acc: 0.8145 - val_loss: 0.5097 - val_acc: 0.8050\n",
      "Epoch 380/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8143\n",
      "Epoch 380: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4898 - acc: 0.8140 - val_loss: 0.4841 - val_acc: 0.8028\n",
      "Epoch 381/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8179\n",
      "Epoch 381: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5285 - acc: 0.8174 - val_loss: 0.4848 - val_acc: 0.7844\n",
      "Epoch 382/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8164\n",
      "Epoch 382: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4559 - acc: 0.8165 - val_loss: 0.4826 - val_acc: 0.7985\n",
      "Epoch 383/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.8162\n",
      "Epoch 383: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5113 - acc: 0.8166 - val_loss: 0.5000 - val_acc: 0.7853\n",
      "Epoch 384/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.8177\n",
      "Epoch 384: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5002 - acc: 0.8178 - val_loss: 0.4932 - val_acc: 0.7840\n",
      "Epoch 385/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5052 - acc: 0.8132\n",
      "Epoch 385: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5052 - acc: 0.8132 - val_loss: 0.4999 - val_acc: 0.8015\n",
      "Epoch 386/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.8174\n",
      "Epoch 386: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5091 - acc: 0.8174 - val_loss: 0.4845 - val_acc: 0.7678\n",
      "Epoch 387/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8125\n",
      "Epoch 387: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4874 - acc: 0.8134 - val_loss: 0.4811 - val_acc: 0.7896\n",
      "Epoch 388/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4903 - acc: 0.8183\n",
      "Epoch 388: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4899 - acc: 0.8183 - val_loss: 0.4920 - val_acc: 0.7780\n",
      "Epoch 389/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.8153\n",
      "Epoch 389: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5410 - acc: 0.8154 - val_loss: 0.4840 - val_acc: 0.7960\n",
      "Epoch 390/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.8194\n",
      "Epoch 390: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5262 - acc: 0.8192 - val_loss: 0.4907 - val_acc: 0.7802\n",
      "Epoch 391/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.8128\n",
      "Epoch 391: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5368 - acc: 0.8125 - val_loss: 0.4868 - val_acc: 0.7879\n",
      "Epoch 392/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8127\n",
      "Epoch 392: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4867 - acc: 0.8128 - val_loss: 0.4932 - val_acc: 0.7973\n",
      "Epoch 393/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.8115\n",
      "Epoch 393: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5444 - acc: 0.8114 - val_loss: 0.4958 - val_acc: 0.7840\n",
      "Epoch 394/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8173\n",
      "Epoch 394: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5065 - acc: 0.8174 - val_loss: 0.4983 - val_acc: 0.7990\n",
      "Epoch 395/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8198\n",
      "Epoch 395: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5086 - acc: 0.8196 - val_loss: 0.5068 - val_acc: 0.7904\n",
      "Epoch 396/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8173\n",
      "Epoch 396: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5018 - acc: 0.8167 - val_loss: 0.5084 - val_acc: 0.7733\n",
      "Epoch 397/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5367 - acc: 0.8111\n",
      "Epoch 397: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5367 - acc: 0.8111 - val_loss: 0.4741 - val_acc: 0.7853\n",
      "Epoch 398/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.8169\n",
      "Epoch 398: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5253 - acc: 0.8171 - val_loss: 0.4882 - val_acc: 0.8015\n",
      "Epoch 399/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5015 - acc: 0.8186\n",
      "Epoch 399: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5015 - acc: 0.8186 - val_loss: 0.4891 - val_acc: 0.8092\n",
      "Epoch 400/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8178\n",
      "Epoch 400: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4775 - acc: 0.8170 - val_loss: 0.4900 - val_acc: 0.7814\n",
      "Epoch 401/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.8150\n",
      "Epoch 401: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5049 - acc: 0.8154 - val_loss: 0.4999 - val_acc: 0.8144\n",
      "Epoch 402/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.8171\n",
      "Epoch 402: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5228 - acc: 0.8171 - val_loss: 0.4867 - val_acc: 0.7960\n",
      "Epoch 403/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8140\n",
      "Epoch 403: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4961 - acc: 0.8137 - val_loss: 0.5123 - val_acc: 0.7797\n",
      "Epoch 404/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4532 - acc: 0.8154\n",
      "Epoch 404: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4532 - acc: 0.8149 - val_loss: 0.5097 - val_acc: 0.7729\n",
      "Epoch 405/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.8089\n",
      "Epoch 405: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5122 - acc: 0.8089 - val_loss: 0.4907 - val_acc: 0.7823\n",
      "Epoch 406/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4892 - acc: 0.8132\n",
      "Epoch 406: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4887 - acc: 0.8135 - val_loss: 0.4971 - val_acc: 0.7840\n",
      "Epoch 407/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8204\n",
      "Epoch 407: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4864 - acc: 0.8204 - val_loss: 0.4889 - val_acc: 0.7793\n",
      "Epoch 408/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8167\n",
      "Epoch 408: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4862 - acc: 0.8172 - val_loss: 0.5072 - val_acc: 0.7956\n",
      "Epoch 409/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5050 - acc: 0.8173\n",
      "Epoch 409: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5050 - acc: 0.8173 - val_loss: 0.4808 - val_acc: 0.7853\n",
      "Epoch 410/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.8169\n",
      "Epoch 410: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4971 - acc: 0.8174 - val_loss: 0.4819 - val_acc: 0.7823\n",
      "Epoch 411/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.8158\n",
      "Epoch 411: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4961 - acc: 0.8158 - val_loss: 0.4908 - val_acc: 0.8011\n",
      "Epoch 412/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.8118\n",
      "Epoch 412: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5398 - acc: 0.8112 - val_loss: 0.5000 - val_acc: 0.7802\n",
      "Epoch 413/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8169\n",
      "Epoch 413: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4866 - acc: 0.8172 - val_loss: 0.4874 - val_acc: 0.7938\n",
      "Epoch 414/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.8167\n",
      "Epoch 414: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5202 - acc: 0.8167 - val_loss: 0.4830 - val_acc: 0.8024\n",
      "Epoch 415/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.8097\n",
      "Epoch 415: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5064 - acc: 0.8096 - val_loss: 0.5221 - val_acc: 0.7836\n",
      "Epoch 416/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8135\n",
      "Epoch 416: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5022 - acc: 0.8137 - val_loss: 0.4937 - val_acc: 0.8020\n",
      "Epoch 417/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8138\n",
      "Epoch 417: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.5155 - acc: 0.8131 - val_loss: 0.5045 - val_acc: 0.7861\n",
      "Epoch 418/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5004 - acc: 0.8148\n",
      "Epoch 418: val_acc did not improve from 0.81437\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.4987 - acc: 0.8149 - val_loss: 0.4867 - val_acc: 0.7921\n",
      "92/92 [==============================] - 1s 4ms/step - loss: 0.4546 - acc: 0.8003\n"
     ]
    }
   ],
   "source": [
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=range(1, 6))\n",
    "    training_time = pd.DataFrame(index=lags, columns=[f'CPU_Time_{i}' for i in range(1, 6)] + [f'Wall_Time_{i}' for i in range(1, 6)])\n",
    "\n",
    "    for lag in lags:\n",
    "        train_temp_dir = train_dir + '_' + str(lag)\n",
    "        train = tf.data.Dataset.load(train_temp_dir)\n",
    "        flattened_train = train.unbatch()\n",
    "\n",
    "        train_data = list(flattened_train.as_numpy_iterator())\n",
    "        train_size = len(list(train_data))\n",
    "\n",
    "        kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_data), 1):\n",
    "            train_fold_data = ([train_data[i][0] for i in train_index], [train_data[i][1] for i in train_index])\n",
    "            val_fold_data = ([train_data[i][0] for i in val_index], [train_data[i][1] for i in val_index])\n",
    "\n",
    "            train_fold = tf.data.Dataset.from_tensor_slices(train_fold_data).batch(32)\n",
    "            val_fold = tf.data.Dataset.from_tensor_slices(val_fold_data).batch(32)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(momentum=0.01), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_fold, epochs=epochs, validation_data=val_fold, callbacks=myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "\n",
    "            training_time.loc[lag, f'CPU_Time_{fold}'] = cpu_time\n",
    "            training_time.loc[lag, f'Wall_Time_{fold}'] = wall_time\n",
    "\n",
    "            recap.loc[lag, fold] = history.history['acc'][-1]\n",
    "\n",
    "\n",
    "    # Evaluate on the test dataset after cross-validation\n",
    "    test_temp_dir = test_dir + '_' + str(lag)\n",
    "    test_ds = tf.data.Dataset.load(test_temp_dir)\n",
    "    results = model.evaluate(test_ds, callbacks=myCallbacks(log_path))\n",
    "\n",
    "    recap[f'test_{lag}'] = results[1]\n",
    "\n",
    "    log_recap_dir = os.path.join(log_dir, 'Recap')\n",
    "    if not os.path.exists(log_recap_dir):\n",
    "        os.makedirs(log_recap_dir)\n",
    "\n",
    "    recap.to_csv(os.path.join(log_recap_dir, 'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_recap_dir, 'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "LAG = 256\n",
    "\n",
    "test_dir = f\"datasets/tf_batch/rwb/segment_1 seconds/test_{LAG}\"\n",
    "test_ds = tf.data.Dataset.load(test_dir)\n",
    "model_dir = [f\"train_logs/logs7/RWB_ANN_256_128_128_256_RMSprop/{i}/{LAG}/best_model.h5\" for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_test = test_ds.unbatch()\n",
    "test_data = list(flattened_test.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_value = np.array([test_data[i][0] for i in range(len(test_data))])\n",
    "test_data_label = np.array([test_data[i][1] for i in range(len(test_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 96)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_value.reshape(test_data_value.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 1s 4ms/step - loss: 0.5171 - acc: 0.8208\n",
      "0.5170807242393494 0.8207934498786926\n",
      "92/92 [==============================] - 1s 3ms/step\n",
      "92/92 [==============================] - 1s 4ms/step - loss: 0.4822 - acc: 0.8122\n",
      "0.48223522305488586 0.8122435212135315\n",
      "92/92 [==============================] - 0s 3ms/step\n",
      "92/92 [==============================] - 1s 3ms/step - loss: 0.5471 - acc: 0.8187\n",
      "0.5471169948577881 0.8187414407730103\n",
      "92/92 [==============================] - 0s 3ms/step\n",
      "92/92 [==============================] - 1s 3ms/step - loss: 0.5160 - acc: 0.8129\n",
      "0.515957772731781 0.8129274845123291\n",
      "92/92 [==============================] - 0s 3ms/step\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 0.4971 - acc: 0.8211\n",
      "0.49707362055778503 0.821135401725769\n",
      "92/92 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, model_path in enumerate(model_dir):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(loss, acc)\n",
    "    pred = model.predict(test_data_value.reshape(test_data_value.shape[0], -1))\n",
    "    results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAHHCAYAAAAlCIV9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABznElEQVR4nO3dd1QU198G8GcpSy+CVEWKvYAaY8GKFRu2GGPHbiyxxfqLBbvRRI3R2Es0tsSaGDWCiR2NDbtEsYAKaiwgKnXv+wfvThwXBHYp7vp8zpmjc+fOzN3Z2d0vt41CCCFARERERAbLqLALQERERET5iwEfERERkYFjwEdERERk4BjwERERERk4BnxEREREBo4BHxEREZGBY8BHREREZOAY8BEREREZOAZ8RERERAbO4AI+hUKBkJCQwi4G5cLff/8NpVKJu3fvFnZRSM+lpqbCw8MDP/zwQ5Z55s6di3LlykGlUhVgyfRfTq7tm+7cuQOFQoF169blb8H0yLx58+Dj4wNjY2NUqVKlsIuTr9atWweFQoE7d+7ket+QkBAoFIps8/Xq1QteXl65L9w73LhxA82aNYOdnR0UCgV27dqV430PHToEhUKBQ4cOZZs3ICAAAQEBWpdTG7kK+NRvoHoxMTFBsWLF0KtXL9y/fz+/yqiTEydOICQkBM+fP9fpOF5eXrLXbmVlhRo1amD9+vWyfBUqVEDlypU19t+5cycUCgUaNGigsW3NmjVQKBQ4cOAAAM3rrFAo4OzsjIYNG2Lfvn25LnuNGjWgUCiwdOnSTLerz2dubp7p+xgQEIBKlSrJ0tTX44svvtDIr77pt23blqPyffXVV+jSpQs8PT0BACqVCuvWrUObNm3g4eEBKysrVKpUCTNmzEBSUpLG/m9fK/UyZ86cTM+3detW+Pv7w8rKCvb29qhduzb+/PPPbMv5999/Y/DgwahWrRpMTU2z/EKKiYnB1KlTUaNGDRQpUgRFixZFQEAAwsLCMs1/9uxZtG7dGq6urrC2toafnx8WLVqE9PT0bMsUGRmJkSNHonbt2jA3N8/yC/bJkyeYN28e6tevDycnJ9jb26NWrVrYunVrpse9ceMGOnfujOLFi8PS0hLlypXDtGnT8OrVq2zLdPDgQfTp0wdlypSBpaUlfHx80K9fP8TGxmrkDQgIyPS9a968eabHPnfuHNq0aQMHBwdYWlqiUqVKWLRokbTd1NQUo0aNwsyZMzO9VxISEvD1119j3LhxMDL67+vv7fPb2tqiQYMG+P333zWO8ebn89ixYxrbhRDw8PCAQqFA69atZdsSExMxZcoUVKpUCVZWVnB0dESVKlUwfPhwPHjwIOuL+h7I7tq+T3bs2IHPPvsMPj4+sLS0RNmyZfHll19m+jvw9ne7evn8888zPXZYWBgaNWoEOzs72NjYoFq1all+jt504MABjB07FnXq1MHatWsxa9YsXV8m5YPg4GBcunQJM2fOxIYNG/Dxxx8XdpEkz58/h7Ozc65+X99kos1Jp02bBm9vbyQlJeHkyZNYt24djh07hsuXL8Pc3FybQ+abEydOYOrUqejVqxfs7e11OlaVKlXw5ZdfAgBiY2OxatUqBAcHIzk5Gf379wcA1K1bF6tXr0Z8fDzs7OykfY8fPw4TExOcPn0aqampMDU1lW0zNjaGv7+/7Hzq6yyEwMOHD7Fu3Tq0bNkSv/32m8YPSVZu3LiB06dPw8vLCxs3bsSgQYOyzJucnIw5c+bg+++/z/E1WblyJSZMmAB3d/cc7/OmiIgIhIWF4cSJE1Laq1ev0Lt3b9SqVQuff/45nJ2dER4ejilTpuDgwYP4888/NYKtpk2bomfPnrK0qlWrapwvJCQE06ZNQ8eOHdGrVy+kpqbi8uXLOfqDZe/evVi1ahX8/Pzg4+ODf/75J9N8u3fvxtdff4127dohODgYaWlpWL9+PZo2bYo1a9agd+/eUt6zZ8+idu3aKF26NMaNGwdLS0vs27cPw4cPR1RUFL777rt3lik8PByLFi1ChQoVUL58eURERGSZ76uvvkLLli0xceJEmJiYYPv27ejcuTOuXr2KqVOnSnljYmJQo0YN2NnZYejQoXBwcJCu/9mzZ7F79+53lmncuHF4+vQpPv30U5QuXRq3bt3C4sWLsWfPHkRERMDV1VWWv3jx4pg9e7YsLbP76cCBAwgKCkLVqlUxadIkWFtbIyoqCvfu3ZPl6927N8aPH49NmzahT58+sm1r1qxBWloaunTponF89T0khMDdu3exdOlSBAUFYd++fQgMDNTIb25ujk2bNqFu3bqy9MOHD+PevXswMzOTpaempqJ+/fq4fv06goOD8cUXXyAxMRFXrlzBpk2b0L59e60/RwXlXdf2fTJgwAC4u7uje/fuKFGiBC5duoTFixdj7969OHfuHCwsLGT53/xuVytTpozGcdeuXYu+ffuiadOmmDVrFoyNjREZGYmYmJhsy/Tnn3/CyMgIq1evhlKp1O0FEoCM35+8rKl//fq19F05dOjQPDtuXpk8eXKO/ujOksiFtWvXCgDi9OnTsvRx48YJAGLr1q25OVy+ACCmTJkirc+bN08AELdv39bpuJ6enqJVq1aytEePHglra2tRvnx5Ke3HH38UAMTevXtleWvVqiW6du0qAIjw8HDZtjJlyoiqVatK61ld56dPnwpTU1PRtWvXHJd78uTJwtnZWWzfvl0oFIpMr4P6fFWqVBFmZmbi/v37su0NGjQQFStWlKV5enqKihUrChMTE/HFF1/Itv31118CgPjll1+yLd+wYcNEiRIlhEqlktKSk5PF8ePHNfJOnTpVABChoaGydABiyJAh2Z4rPDxcKBQKMX/+/GzzZiYuLk68evVKCCHEkCFDRFYfn8uXL4vHjx/L0pKSkkS5cuVE8eLFZen9+/cXSqVSPHnyRJZev359YWtrm22Znjx5IhISEoQQ777Xb926Je7cuSNLU6lUolGjRsLMzEwkJiZK6TNnzhQAxOXLl2X5e/bsKQCIp0+fvrNMhw8fFunp6RppAMRXX30lS8/s3spMfHy8cHFxEe3bt9c4dmZat24t6tWrp5Hu5+cnunfvrpGe2T109epVAUC0aNFClq7+vHTo0EEULVpUpKamyrb3799fVKtWTeM74+effxYAxMaNGzXO//r1axEfH5/t68qJ1NRUkZycnCfHykxW1/Ztt2/fFgDE2rVr860sWfnrr7800tTfzStXrpSlZ/bdnpnbt28LCwsLMWzYMK3K1Lt3b2FlZaXVvplRqVTS99H7SP050ea3d8qUKVl+v+anu3fvCgBi3rx5Wu2v/u3L7P57W4MGDUSDBg1yfOxLly4JExMTMW3atBz/vr4tT/rw1atXDwAQFRUlS79+/To6duwIBwcHmJub4+OPP8avv/4qy5OamoqpU6eidOnSMDc3h6OjI+rWrYvQ0FApT1Zt3dm134eEhGDMmDEAAG9vb6mqXt3k9e+//+L69etaR8xOTk4oV66c7HWr/9o/fvy4lJaUlIRz586hQ4cO8PHxkW17/Pgx/vnnH41agszY29vDwsICJiY5r5jdtGkTOnbsiNatW8POzg6bNm3KMu///vc/pKenZ9kU+jYvLy/07NkTK1eu1Lo5ateuXWjUqJGsxk6pVKJ27doaedu3bw8AuHbtWqbHev369TubmhYuXAhXV1cMHz4cQggkJibmqqwuLi4aNQOZqVixIooWLSpLMzMzQ8uWLXHv3j28ePFCSk9ISIC5ublG7bObm1uOzuXg4AAbG5ts83l7e0tN5moKhQLt2rVDcnIybt26JSsTkPF63y6TkZFRtrUT9evXlzWXqtMcHByyfO/S0tLe+X5s2rQJDx8+xMyZM2FkZISXL1++8y/7pk2b4tixY3j69KmUdvv2bVy8eBFNmjR5Z/nVypcvj6JFi2p8r6l16dIFT548kX1XpaSkYNu2bejatatGfvVx6tSpo7HN3Nwctra20nqvXr1gbW2NW7duITAwEFZWVnB3d8e0adMghJDyqfvJffPNN1i4cCFKliwJMzMzXL16FUBGrVK9evWk7gtt27bVeA/U/aWuX7+OTp06wdbWFo6Ojhg+fHimn6fMrm1OXbx4Eb169YKPjw/Mzc3h6uqKPn364MmTJxp5Dx06hI8//hjm5uYoWbIkli9fnuO+XZn9XmT3/ZGSkoKXL19mecxly5YhPT0d06ZNA5DRPP/me/EuCoUCa9euxcuXL6XfIXXfxrS0NEyfPl1677y8vPC///0PycnJsmN4eXmhdevW+OOPP/Dxxx/DwsICy5cvz/Kc6q44Fy9eRIMGDWBpaYlSpUpJTYGHDx9GzZo1YWFhgbJly2ba5eT8+fNo0aIFbG1tYW1tjcaNG+PkyZMa+a5cuYJGjRrBwsICxYsXx4wZM7L8fO7bt0+6J21sbNCqVStcuXIlR9fxbW/HAG9+HlasWCFd0+rVq+P06dPvPFZISIj0HTlmzBgoFArZsXN6LTKjLouFhQVq1KiBo0eP5vq1Dh8+HO3bt5fiLW3kScCnDqCKFCkipV25cgW1atXCtWvXMH78eHz77bewsrJCu3btsHPnTilfSEgIpk6dioYNG2Lx4sX46quvUKJECZw7d07ncnXo0EFqulmwYAE2bNiADRs2wMnJCQCwePFilC9fHn///bdWx09LS8O9e/dkr9vHxwfu7u6yvj2nT59GSkoKateujdq1a8sCPnVTZmYBX3x8PP799188fvwYV65cwaBBg5CYmIju3bvnqHynTp3CzZs30aVLFyiVSnTo0AEbN27MMr+3t3euA7ivvvoKaWlpOQ4S33T//n1ER0fjo48+ylH+uLg4ANAIpoCMflVWVlawsLBAhQoVMg1sDx48iOrVq2PRokVwcnKCjY0N3NzcsHjx4lyXXRtxcXGwtLSEpaWllBYQEICEhAQMHDgQ165dw927d7Fs2TLs2LEDEyZMKJAyAfJrqv6x7Nu3LyIiIhATE4OtW7di6dKlGDZsGKysrHJ9nsTERCQmJmb63v3zzz/Sl7+rqysmTZqE1NRUWZ6wsDDY2tri/v37KFu2LKytrWFra4tBgwZlGpRUq1YNQghZVwH1/3N6v8XHx+PZs2eyz/ebvLy84O/vj82bN0tp+/btQ3x8PDp37qyRX/1jsn79+hwFCunp6WjevDlcXFwwd+5cVKtWDVOmTMGUKVM08q5duxbff/89BgwYgG+//RYODg4ICwtDYGAgHj16hJCQEIwaNQonTpxAnTp1Mu3n2alTJyQlJWH27Nlo2bIlFi1ahAEDBmjky+za5lRoaChu3bqF3r174/vvv0fnzp2xZcsWtGzZUnZNzp8/j+bNm+PJkyeYOnUq+vbti2nTpuWqA/3b3vX98eeff8LS0hLW1tbw8vLKtCtFWFgYypUrh71796J48eKwsbGBo6MjJk2alG2z4oYNG1CvXj2YmZlJv0P169cHAPTr1w+TJ0/GRx99hAULFqBBgwaYPXt2pvdQZGQkunTpgqZNm+K7777LduDHs2fP0Lp1a9SsWRNz586FmZkZOnfujK1bt6Jz585o2bIl5syZg5cvX6Jjx46yP0avXLmCevXq4cKFCxg7diwmTZqE27dvIyAgAKdOnZJd14YNGyIiIgLjx4/HiBEjsH79+kyv4YYNG9CqVStYW1vj66+/xqRJk3D16lXUrVtXq8EdWdm0aRPmzZuHgQMHYsaMGbhz5w46dOig8b3ypg4dOmDBggUAMv6Y27BhAxYuXJira5GZ1atXY+DAgXB1dcXcuXNRp04dtGnTJkfdANR++eUXnDhxAnPnzs3xPpnKTXWguoo2LCxMPH78WMTExIht27YJJycnYWZmJmJiYqS8jRs3Fr6+viIpKUlKU6lUonbt2qJ06dJSWuXKlbOtTs+q6jM4OFh4enrK0pCLJl11tXFOql89PT1Fs2bNxOPHj8Xjx4/FpUuXRI8ePTJtCvr000+FhYWFSElJEUIIMXv2bOHt7S2EEOKHH34Qzs7OUt7Ro0cLALJmVPV1fnsxMzMT69aty7asakOHDhUeHh5Sc+mBAwcEAHH+/HlZvjebkKOiooSJiYms2SKrJl31+9a7d29hbm4uHjx4IITIeZNuWFiYACB+++23HL2eJk2aCFtbW/Hs2TNZeu3atcXChQvF7t27xdKlS0WlSpUEAPHDDz9IeZ4+fSoACEdHR2FtbS3mzZsntm7dKpo3by4AiGXLluWoDGrvatLNzI0bN4S5ubno0aOHLD0tLU0MHTpUmJqaSu+zsbGxWLp0aa7KI0Tuuy88efJEODs7Z9o8N336dGFhYSG7/95ujs2N6dOnCwDi4MGDsvQ+ffqIkJAQsX37drF+/XrRpk0bAUB06tRJls/Pz09YWloKS0tL8cUXX4jt27eLL774QgAQnTt31jjfgwcPBADx9ddfS2kTJ04UAMSLFy808gMQffv2FY8fPxaPHj0SZ86cke6Nt5t33vy8LF68WNjY2EhNa59++qlo2LChEEKzqfDVq1eibNmyAoDw9PQUvXr1EqtXrxYPHz7UKE9wcLAAIOsuoVKpRKtWrYRSqZS6DKibTW1tbcWjR49kx6hSpYpwdnaWdRe4cOGCMDIyEj179pTS1N+Dbdq0ke0/ePBgAUBcuHAh22ubmcyadDNrgty8ebMAII4cOSKlBQUFCUtLS9n34o0bN4SJiYnWTX19+/YVxsbG4p9//pGlBwUFia+//lrs2rVLrF69WtSrV08AEGPHjpXls7W1FUWKFBFmZmZi0qRJYtu2bVI3nfHjx2d7/uDgYI0m3YiICAFA9OvXT5au/l34888/pTRPT08BQOzfvz9Hr7dBgwYCgNi0aZOUdv36dQFAGBkZiZMnT0rpf/zxh8Z71a5dO6FUKkVUVJSU9uDBA2FjYyPq168vpY0YMUIAEKdOnZLSHj16JOzs7GTfRy9evBD29vaif//+snLGxcUJOzs7WXpOm3TfjgHU95yjo6Os68nu3btz9Fuj3v/tz3xOr8XbTbopKSnC2dlZVKlSRdbNYsWKFQJAjpp0X716JUqUKCEmTJggO4c2TbpaBXxvL15eXuKPP/6Q8j158kQoFAoxffp0KUBSL+p+WPfu3RNCZNyUXl5eGh/CN+VXwJcb6g/b20vv3r01vsS+++47WV+91q1bi27dugkhMr5wAUiv19/fXwoG1dTXecmSJSI0NFSEhoaKn376STRv3lyYmJiI7du3Z1ve1NRU4eTkJEaPHi2lpaWlCWdnZ1nam+dT9xl8O4DLLuB7O0jM6Q25detWAUAcO3Ys29ej7lf2ZhCXleTkZFGpUiVhb28vvTfR0dHSe7ZlyxYpb3p6uqhQoYJG37rs5Cbge/nypahSpYooUqSIRv9IIYRYsGCBaN26tfjxxx/F1q1bRbt27YSJiYnYuXNnrsqUm3s9PT1dNG/eXCiVShEREaGxfcOGDSIwMFCsWLFCbN++XfTp00coFArx/fff56pMQmT03zMxMdEI4rLSv39/jb6uPj4+AoD4/PPPZXkHDhwo+zypvX79WgAQY8aMkdIGDRokTExMMj1nZp9tU1NTMXbsWI0+g29+Xh49eiRMTEzEzz//LBISEoSFhYXURyyzvmHPnz8XY8aMkX2fGBkZiaFDh8r+OFYHfJGRkbL99+3bJwCIzZs3CyH++4Hq3bu3LJ86KHs7aBFCiMDAQFG0aFFpXf3j+uZ3uBBCXLt2TQAQs2fPzvbaZia7PnyvX78Wjx8/lvItXLhQCJHxPWVhYZFpX+WgoCCtAr6NGzdmeT3eplKpRGBgoDAxMZFVYhgZGQkAYs6cObL8zZs3FxYWFlJf2qxkFvDNmjVLABBXr16VpcfGxgoA4ssvv5TSPD09NX4r3qVBgwbC2tpa1j9aCCHs7e01vs+fP38uAIhJkyYJITLeA0tLy0w/swMHDhRGRkZSn9MyZcqIWrVqaeRT/8Gg/j7asWOHFMS+HRc0a9ZMlCpVStpX14Bv8ODBsnzqP/i/++67dx4vs4AvN9fi7YDvxIkTmVYopKSkCDs7uxwFfJMnTxZubm7SH6q6BHxaNekuWbIEoaGh2LZtG1q2bIl///1XNiLt5s2bEEJg0qRJcHJyki3q5ohHjx4ByBiJ+vz5c5QpUwa+vr4YM2YMLl68qE2x8l3NmjURGhqK/fv345tvvoG9vT2ePXum0afpzX584v+bPtT9dipVqgRbW1scP34cSUlJOHv2bJb992rUqIEmTZqgSZMm6NatG37//XdUqFABQ4cORUpKyjvLeuDAATx+/Bg1atTAzZs3cfPmTdy+fRsNGzbE5s2b39kEMXHixFw10/r4+KBHjx5YsWJFplNvZEdk07y1detWTJw4EX379n3nKGM1pVKJoUOH4vnz5zh79iwASP3hTE1N0bFjRymvkZERPvvsM9y7dw/R0dG5Lnt20tPTpZGw27Zt0xiFOWfOHHz99dfYvHkzevbsiU6dOmHnzp2oW7cuhgwZgrS0tDwvEwB88cUX2L9/P1atWqUxjdCWLVswYMAArFq1Cv3790eHDh2wevVqBAcHY9y4cZn2t8rK9evX0b59e1SqVAmrVq3K0T7q0ZJv9ilSv39vj65V95ULDw+XpavvqZz091Jr27YtQkND8fvvv0t9xV69eqXRH/FNTk5OaNKkCTZt2oQdO3YgPT1ddn+9zc7ODnPnzsWdO3dw584drF69GmXLlsXixYsxffp0WV4jIyP4+PjI0tQjR99u/vL29patq+e0LFu2rEYZypcvj3///Vejv1rp0qVl6yVLloSRkZHGubS5tmpPnz7F8OHDpf6wTk5OUtnj4+MBZPw2vH79GqVKldLYP7O07Bw9ehR9+/ZFYGAgZs6cmW1+hUKBkSNHIi0tTTafWlb3YJcuXfD69WucP38+12W7e/cujIyMNF6Xq6sr7O3tNeYmfft9zk7x4sU13ic7Ozt4eHhopAEZTcBARt/yV69eZXn/qFQqqUny7t27GvcOoHnv3bhxAwDQqFEjjbjgwIEDUkyQF0qUKCFbV3fLUL++3MjNtXib+v17+/qYmppqfLYzc+fOHcybNw8zZ86EtbV1rsv+Nq2mZalRo4Y0N027du1Qt25ddO3aFZGRkbC2tpaCidGjR2c6nQHw3we3fv36iIqKwu7du3HgwAGsWrUKCxYswLJly9CvXz8AGR/AzIKCnMxTlpeKFi0qdfgODAxEuXLl0Lp1a3z33XcYNWqUlK9y5cqwsbHBsWPH0LJlSzx9+lQahGBkZISaNWvi2LFjKFmyJFJSUnI0YEO9b8OGDfHdd9/hxo0bqFixYpZ51X31OnXqlOn2w4cPo2HDhplu8/HxQffu3bFixQqMHz8+R2X76quvsGHDBmk6kpxwdHQE8O4PYWhoKHr27IlWrVph2bJlOTouAOkLTd2xXD1wyN7eHsbGxrK8zs7OUjne/qLQVf/+/bFnzx5s3LgRjRo10tj+ww8/oFGjRhof5jZt2mDUqFG4c+eOVj9y7zJ16lT88MMPmDNnDnr06JFpmapWrYrixYtrlGndunU4f/58jgY+xMTESBOY7t27N0eDSwDN9w7ImKblypUrGgNJ3nzv3qRef7O/lqOjI9LS0vDixYtMy1K8eHHpdbVs2RJFixbF0KFD0bBhQ3To0CHL8nbt2hX9+/dHXFwcWrRokePpnzw9PdGnTx+0b98ePj4+2LhxI2bMmJGjfd+WkwE+uZVVQJfZtc2pTp064cSJExgzZgyqVKki/V40b948XybCvnDhAtq0aYNKlSph27ZtOR7wltU9eOPGjRzfg7mR0+A5t+/z29912aVn98e3LtTv74YNGzSmZgKQq8GI2SmM15cfJk+ejGLFiiEgIED6w0vdF/Xx48e4c+cOSpQo8c4/St+k86ANY2NjzJ49Gw8ePJA6v6sjV1NTU6mG6u3lzS9cBwcH9O7dG5s3b0ZMTAz8/PxkT8soUqRIphNm5uTJDNr8FZpTrVq1QoMGDTBr1izZX8vGxsaoVasWjh8/jmPHjsHW1ha+vr7SdvXADfXgjZwGfACkGp93jWh8+fIldu/ejc8++wy//PKLxuLm5vbOwRvAf7V8X3/9dY7KVbJkSXTv3h3Lly/PcS1fuXLlAGSMnszMqVOn0L59e3z88cf4+eefc/WFoB51qh6gY2RkhCpVquDx48cataPqASrqvHllzJgxWLt2LRYsWJDpvG8A8PDhw0z/cFF3Ls7rGr4lS5YgJCQEI0aMwLhx4/KtTE+ePEGzZs2QnJyMP/74A25ubjku49vvHZAxUACAxnyJWb136nuqfPnyUlp299vbBg4ciJIlS2LixInv/KFo3749jIyMcPLkyUxH52anSJEiKFmypMbnRqVSyUZPA5Dmfszu6QLqASKRkZEa265fv46iRYtqDL5R18Co3bx5EyqVSuNcmV3bnHj27BkOHjyI8ePHY+rUqWjfvj2aNm2qUdPh7OwMc3Nz3Lx5U+MYmaVlJSoqCs2bN4ezszP27t2bqxqSvLgHc8LT0xMqlUrj2j98+BDPnz/XGFlfUJycnGBpaZnl/WNkZCQFxZ6enhrlBzTvvZIlSwLIeH8ziwkK+qkTOZWba/E29fv39vVJTU3N0fdQdHQ0bt68CR8fH3h7e8Pb21v6LRk8eDC8vb2lWRVyIk9G6QYEBKBGjRpYuHAhkpKS4OzsjICAgCx//B8/fiz9/+3mIWtra5QqVUo2JL1kyZK4fv26bL8LFy7IRrtmRf2lllnAqOu0LACkJq6VK1fK0uvWrYvHjx9j7dq1qFmzpiwCr127NiIjI7F79244Ojrm+IszNTUVBw4cgFKpfOc+O3fuxMuXLzFkyBB07NhRY2ndujW2b9+uMez/TW8GcOq/KLIzceJEpKam5ngkUbFixeDh4YEzZ85obLt27RpatWoFLy8v7NmzJ8u/bN+8J9RevHiBhQsXomjRotKXNAB89tlnSE9Px48//iilJSUlYePGjahQoUKeTno7b948fPPNN/jf//6H4cOHZ5mvTJkyCA0NlX0O0tPT8fPPP8PGxkb6kswLW7duxbBhw9CtWzfMnz//nWU6f/68xsTSmzdvhpGREfz8/N55npcvX6Jly5a4f/8+9u7dm2lzD5Ax/cvb96AQQqrlerN1QF1TvXr1aln+VatWwcTEROPH4uzZs1AoFLLJzNX/z+x+y4yJiQm+/PJLXLt27Z2TTVtbW2Pp0qUICQlBUFBQlvkuXLiAf//9VyP97t27uHr1aqZNRm+OIBdCYPHixTA1NUXjxo3fWXY3NzdUqVIFP/74o+y77/Llyzhw4ABatmypsc+SJUtk6+oJ2Fu0aCFLz+za5oS61uXt4Fk9EvLNfE2aNMGuXbtkswXcvHkzx08aiouLQ7NmzWBkZIQ//vgjy2Ds6dOnGn/cpKamYs6cOVAqlbJWkM8++wyA/B5UqVRYu3YtHBwcZN81OaV+H96+BurPZ6tWrXJ9zLxgbGyMZs2aYffu3bIm/YcPH0qTjaunEWrZsiVOnjwpm+3i8ePHGpUKgYGBsLW1xaxZszIdLZvZd/n7IDfX4m0ff/wxnJycsGzZMllFw7p163L09K8ZM2Zg586dskXd9WPs2LHYuXNnrmZNyLM61DFjxuDTTz/FunXr8Pnnn2PJkiWoW7cufH190b9/f/j4+ODhw4cIDw/HvXv3cOHCBQAZjyILCAhAtWrV4ODggDNnzmDbtm2yWa779OmD+fPnIzAwEH379sWjR4+wbNkyVKxYMdvoVv0h/Oqrr9C5c2eYmpoiKCgIVlZWWLx4MaZOnYq//vpL678uWrRogUqVKmH+/PkYMmSI9AQNda1deHi4xrN9a9WqBYVCgZMnTyIoKCjLWsh9+/bh+vXrADL6tWzatAk3btzA+PHjs7zBgIzmXEdHx0znsgMymuZWrlyJ33///Z1NVepm2sjIyHc2H6upg8Q3A6rstG3bFjt37oQQQroOL168QGBgIJ49e4YxY8ZoPN6qZMmS0o/NkiVLsGvXLgQFBaFEiRKIjY3FmjVrEB0djQ0bNsj6Vw4cOBCrVq3CkCFD8M8//6BEiRLYsGED7t69i99++012joCAABw+fFj243T37l1s2LABwH9Bgzo48fT0lJpHd+7cibFjx6J06dIoX748fvrpJ9mxmzZtKjULjR8/Ht27d0fNmjUxYMAAWFhYYPPmzTh79ixmzJgheyJLr1698OOPP+L27dtSrUt8fLz0w6z+A2jx4sWwt7eHvb299Dn6+++/0bNnTzg6OqJx48YaX8a1a9eWalrGjBkjzZU1dOhQODo6Ys+ePdi3bx/69esnC4zV0yq9+Rnq1q0b/v77b/Tp0wfXrl2TzXtmbW0tNfmfO3cOXbp0QZcuXVCqVCm8fv0aO3fuxPHjxzFgwADZ9ClVq1ZFnz59pCdlNGjQAIcOHcIvv/yS6ZNeQkNDUadOHanbAJDR8lCpUiWEhYXl+CkRvXr1wuTJk7PtqhAcHJztsUJDQzFlyhS0adMGtWrVkubZW7NmDZKTkzW+J8zNzbF//34EBwejZs2a2LdvH37//Xf873//y1Ft0rx589CiRQv4+/ujb9++eP36Nb7//nvY2dll+rzx27dvo02bNmjevDnCw8Px008/oWvXrhp9PDO7tjlha2uL+vXrY+7cuUhNTUWxYsVw4MCBTGs6QkJCcODAAdSpUweDBg1Ceno6Fi9ejEqVKmX5NJk3NW/eHLdu3cLYsWNx7Ngx2TRZLi4uaNq0KQDg119/xYwZM9CxY0d4e3vj6dOn2LRpEy5fvoxZs2bJmh7btm2Lxo0bY/bs2fj3339RuXJl7Nq1C8eOHcPy5cs1nqySE5UrV0ZwcDBWrFiB58+fo0GDBvj777/x448/ol27dll2uykIM2bMQGhoKOrWrYvBgwfDxMQEy5cvR3JysuyP+rFjx2LDhg1o3rw5hg8fDisrK6xYsQKenp6y/vi2trZYunQpevTogY8++gidO3eGk5MToqOj8fvvv6NOnToFNkVWbuX0WrzN1NQUM2bMwMCBA9GoUSN89tlnuH37NtauXZujPnyZtf6pu4xUr149x92nJLkZ4ZHVEyCEyBj1V7JkSVGyZEmRlpYmhMgYvdmzZ0/h6uoqTE1NRbFixUTr1q3Ftm3bpP1mzJghatSoIezt7YWFhYUoV66cmDlzpjSlidpPP/0kfHx8hFKpFFWqVBF//PFHjkbpCpExJUSxYsWkUVbqUUO5nZYlq+lj1q1bpzEa7eXLl9IUAgcOHNDYx8/PL8upDTIbDW1ubi6qVKkili5dqjHq6k0PHz4UJiYmGtN/vOnVq1fC0tJStG/fXna+zN5X9WjBd43SfdONGzeEsbFxjkcRnTt3TgAQR48eldLUI6WyWoKDg6W8Bw4cEE2bNpXuMXt7e9GsWTON6T/UHj58KIKDg4WDg4MwMzMTNWvWzHSag2rVqglXV1dZmnp0VGbLm6Ot1PdVVsvb99v+/ftFgwYNRNGiRYVSqRS+vr6ZThPzySefCAsLC9m0NO+6Vm9+NrIaYa9e3h5JeerUKdGiRQvpupYpU0bMnDlT46kSX375pVAoFOLatWtSWlYj2t8u061bt8Snn34qvLy8hLm5ubC0tBTVqlUTy5Yty/QeT0lJESEhIcLT01OYmpqKUqVKiQULFmjke/78uVAqlWLVqlUa2+bPny+sra01RtYDWT+tJSQkRPa+vevz8qa3PyO3bt0SkydPFrVq1RLOzs7CxMREODk5iVatWsmm3xDivxGdUVFRolmzZsLS0lK4uLiIKVOmyEYNZzWNhFpYWJioU6eOsLCwELa2tiIoKEhjRKj6fr169aro2LGjsLGxEUWKFBFDhw4Vr1+/luV917V9W2ajdO/duyfat28v7O3thZ2dnfj000+lEcVvf28fPHhQVK1aVSiVSlGyZEmxatUq8eWXXwpzc/Nsz/2ue/3Nz+qZM2dEUFCQKFasmFAqlcLa2lrUrVtX/Pzzz5ke98WLF2L48OHC1dVV+qz+9NNP2ZZHiMxH6QqRMaPC1KlThbe3tzA1NRUeHh5iwoQJslHbQuT8iSBqWT3FJqvjZPYZOHfunAgMDBTW1tbC0tJSNGzYUJw4cUJj34sXL4oGDRoIc3NzUaxYMTF9+nSxevVq2e+t2l9//SUCAwOFnZ2dMDc3FyVLlhS9evUSZ86ckfLoOko3s89DZvfY2961f06uRVZP2vjhhx+Et7e3MDMzEx9//LE4cuRIrp+08fY58n1aFqL80KhRo0wfd1VYEhIShImJiVi8eHFhF0Umsyl1Clv16tVFx44dC7sYMgsWLBBubm6Zzvn2/Plz4eDgkKOApTBlFRzkB/WP69uPA8zMu65tQWjbtq1s+g4iyrk86cNHpItZs2Zh69atORqEUxCOHDmCYsWKoX///oVdFMmVK1fw+vXrLAdaFIaEhARcuHBBetTU+yA1NRXz58/HxIkTM+33aWdnh7Fjx2LevHn5MirUkGV3bfPa69evZes3btzA3r1739vO/UTvO4UQejZOmYjIwPXq1Qvbtm3L9fOetaHuh/n48WOtplrJL25ubtJzd+/evYulS5ciOTkZ58+fz3IgEBFlLe8mviEiIsojzZs3x+bNmxEXFwczMzP4+/tj1qxZDPaItMQaPiIiIiIDxz58RERERAaOAR8RERGRgWMfPpJRqVR48OABbGxs8vWxdERElD+EEHjx4gXc3d1z/JxVbSQlJWk8qlIbSqUS5ubmeVAiehcGfCTz4MGDLJ8LSERE+iMmJgbFixfPl2MnJSXB29MacY80n7udW66urrh9+zaDvnzGgI9kbGxsAAC16o6HiUnuHxVEpA/SzdibhQxXWloSTh+cLX2f54eUlBTEPUrH3bNesLXR/vOU8EIFz2p3kJKSwoAvnzHgIxl1M66JiRlMTPjhI8OkMGXAR4avILrlWNsoYG2j/XlUYNehgsKAj4iIiLSSLlRI12Fyt3TBJ94UFAZ8REREpBUVBFTQPuLTZV/KHbZrEBERERk41vARERGRVlRQQZdGWd32ptxgwEdERERaSRcC6To8oVWXfSl32KRLREREZOBYw0dERERa4aAN/cGAj4iIiLSigkA6Az69wCZdIiIiIgPHGj4iIiLSCpt09QcDPiIiItIKR+nqDzbpEhERERk41vARERGRVlT/v+iyPxUMBnxERESklXQdR+nqsi/lDgM+IiIi0kq6yFh02Z8KBvvwERERERk41vARERGRVtiHT38w4CMiIiKtqKBAOhQ67U8Fg026RERERAaONXxERESkFZXIWHTZnwoGAz4iIiLSSrqOTbq67Eu5wyZdIiIiIgPHGj4iIiLSCmv49AcDPiIiItKKSiigEjqM0tVhX8odNukSERERGTjW8BEREZFW2KSrPxjwERERkVbSYYR0HRoL0/OwLPRuDPiIiIhIK0LHPnyCffgKDPvwERERERk4BnxERESkFXUfPl2W3Dhy5AiCgoLg7u4OhUKBXbt2ybYrFIpMl3nz5kl5vLy8NLbPmTNHdpyLFy+iXr16MDc3h4eHB+bOnav1NXpfsEmXiIiItJIujJAudOjDl8tHq718+RKVK1dGnz590KFDB43tsbGxsvV9+/ahb9+++OSTT2Tp06ZNQ//+/aV1Gxsb6f8JCQlo1qwZmjRpgmXLluHSpUvo06cP7O3tMWDAgNwV+D3CgI+IiIj0QosWLdCiRYsst7u6usrWd+/ejYYNG8LHx0eWbmNjo5FXbePGjUhJScGaNWugVCpRsWJFREREYP78+Xod8LFJl4iIiLSiggIqGOmw5N+gjYcPH+L3339H3759NbbNmTMHjo6OqFq1KubNm4e0tDRpW3h4OOrXrw+lUimlBQYGIjIyEs+ePcu38uY31vARERGRVvJqHr6EhARZupmZGczMzHQq248//ggbGxuNpt9hw4bho48+goODA06cOIEJEyYgNjYW8+fPBwDExcXB29tbto+Li4u0rUiRIjqVq7Aw4CMiIqJC5eHhIVufMmUKQkJCdDrmmjVr0K1bN5ibm8vSR40aJf3fz88PSqUSAwcOxOzZs3UOMt9nDPiIiIhIK7oP2sgYtRETEwNbW1spXdfA6+jRo4iMjMTWrVuzzVuzZk2kpaXhzp07KFu2LFxdXfHw4UNZHvV6Vv3+9AH78BEREZFWMvrw6bYAgK2trWzRNeBbvXo1qlWrhsqVK2ebNyIiAkZGRnB2dgYA+Pv748iRI0hNTZXyhIaGomzZsnrbnAsw4CMiIiI9kZiYiIiICERERAAAbt++jYiICERHR0t5EhIS8Msvv6Bfv34a+4eHh2PhwoW4cOECbt26hY0bN2LkyJHo3r27FMx17doVSqUSffv2xZUrV7B161Z89913sqZgfcQmXSIiItKKSsdn6aqQu4n4zpw5g4YNG0rr6iAsODgY69atAwBs2bIFQgh06dJFY38zMzNs2bIFISEhSE5Ohre3N0aOHCkL5uzs7HDgwAEMGTIE1apVQ9GiRTF58mS9npIFABRCiFxOe0iGLCEhAXZ2dqgbMAUmJubZ70Ckh9LN2bhBhistNQnhf0xBfHy8rF9cXlL/VmyJqABLG2Otj/PqRTo6V7mar2WlDKzhIyIiIq2o59PTfn/WORUU/plLREREZOBYw0dERERaSRcKpAsdJl7WYV/KHQZ8REREpJV0HQdtpLNJt8CwSZeIiIjIwLGGj4iIiLSiEkZQ6fCkDRUnCikwDPiIiIhIK2zS1R9s0iUiIiIycKzhIyIiIq2ooNtIW1XeFYWywYCPiIiItKL7xMtsaCwovNJEREREBo41fERERKSVdGGEdB1G6eqyL+UOAz4iIiLSigoKqKBLHz4+aaOgMOAjIiIirbCGT3/wShMREREZONbwERERkVZ0n3iZ9U4FhQEfERERaUUlFFDpMg+fDvtS7jC0JiIiIjJwrOEjIiIirah0bNLlxMsFhwEfERERaUUljKDSYaStLvtS7vBKExERERk41vARERGRVtKhQLoOkyfrsi/lDgM+IiIi0gqbdPUHrzQRERGRgWMNHxEREWklHbo1y6bnXVEoGwz4iIiISCts0tUfDPiIiIhIK+nCCOk6BG267Eu5wytNREREZOBYw0dERERaEVBApUMfPsFpWQoMAz4iIiLSCpt09QevNBEREZGBYw0fERERaUUlFFAJ7ZtlddmXcocBHxEREWklHUZI16GxUJd9KXd4pYmIiIgMHGv4iIiISCts0tUfDPiIiIhIKyoYQaVDY6Eu+1Lu8EoTERERGTjW8BEREZFW0oUC6To0y+qyL+UOa/iIiIhIK+o+fLosuXHkyBEEBQXB3d0dCoUCu3btkm3v1asXFAqFbGnevLksz9OnT9GtWzfY2trC3t4effv2RWJioizPxYsXUa9ePZibm8PDwwNz587V6vq8TxjwERERkVaEMIJKh0Xk8kkbL1++ROXKlbFkyZIs8zRv3hyxsbHSsnnzZtn2bt264cqVKwgNDcWePXtw5MgRDBgwQNqekJCAZs2awdPTE2fPnsW8efMQEhKCFStW5O7ivGfYpEtERER6oUWLFmjRosU785iZmcHV1TXTbdeuXcP+/ftx+vRpfPzxxwCA77//Hi1btsQ333wDd3d3bNy4ESkpKVizZg2USiUqVqyIiIgIzJ8/XxYY6hvW8BEREZFW0qHQeclrhw4dgrOzM8qWLYtBgwbhyZMn0rbw8HDY29tLwR4ANGnSBEZGRjh16pSUp379+lAqlVKewMBAREZG4tmzZ3le3oLCGj4iIiLSikroNpeeSmT8m5CQIEs3MzODmZlZro/XvHlzdOjQAd7e3oiKisL//vc/tGjRAuHh4TA2NkZcXBycnZ1l+5iYmMDBwQFxcXEAgLi4OHh7e8vyuLi4SNuKFCmS63K9DxjwERERUaHy8PCQrU+ZMgUhISG5Pk7nzp2l//v6+sLPzw8lS5bEoUOH0LhxY12LqdcY8Bk4Ly8vjBgxAiNGjCjsonywOre+gP6fncX2/RXww8Zab20VmD36AGpUvo/JCxvj+FlPacuQHidRqfRDeBV/hugH9hg4sV2Blpsop7o2v4ABn5zGtrCKWLzVHwDg7pSAQZ+egm+phzA1ScffV4pj0SZ/PHthKe03c8gBlPJ4giK2SXjxUomz14ph+fbqeBJvVVgvhXJJPfhCl/0BICYmBra2tlK6NrV7mfHx8UHRokVx8+ZNNG7cGK6urnj06JEsT1paGp4+fSr1+3N1dcXDhw9ledTrWfUN1Afsw0eUj8p6P0brRpGIis68CeCT5lcg3tGHZf+R0jh0yjvL7USFrazXYwQ1uIabMQ5SmrkyFfNG7IMQwMhvW2Lo10EwNVZh1hehUCiElO98pBumrmiMHhM7YvKyJnB3SsDUzw8WxssgLamg0HkBAFtbW9mSVwHfvXv38OTJE7i5uQEA/P398fz5c5w9e1bK8+eff0KlUqFmzZpSniNHjiA1NVXKExoairJly+ptcy7AgK/QpaSkFHYRKJ+Ym6Xif4MOY/7qOnjxUvPLq2SJJ/i0xWXMW1k30/2XbKiF3WEVEPvIJr+LSqQVC7NUTOz3F75ZXw+Jr/7r4F6p1EO4Fk3EnLUNcPu+A27fd8DstQ1Q1vMxPir3QMq3LcwXV2854+FTG1yJcsGm/ZVRwecRjI1VhfFySA8kJiYiIiICERERAIDbt28jIiIC0dHRSExMxJgxY3Dy5EncuXMHBw8eRNu2bVGqVCkEBgYCAMqXL4/mzZujf//++Pvvv3H8+HEMHToUnTt3hru7OwCga9euUCqV6Nu3L65cuYKtW7fiu+++w6hRowrrZecJBny5FBAQgGHDhmHs2LFwcHCAq6urrJ9BdHQ02rZtC2tra9ja2qJTp06yquGQkBBUqVIFq1atgre3N8zNzQEACoUCy5cvR+vWrWFpaYny5csjPDwcN2/eREBAAKysrFC7dm1ERUVJx4qKikLbtm3h4uICa2trVK9eHWFhYQV2LejdhgeH4+QFD5y7Ukxjm5kyDV8NPoxFP/rjWbxlJnsTvf+Gdz2BkxdL4Ow1+T1uapIOCCA1zVhKS0k1hhAK+JaKy/RYNpZJaFLzJq5EuSA9nT9N+kL9pA1dltw4c+YMqlatiqpVqwIARo0ahapVq2Ly5MkwNjbGxYsX0aZNG5QpUwZ9+/ZFtWrVcPToUVmN4caNG1GuXDk0btwYLVu2RN26dWVz7NnZ2eHAgQO4ffs2qlWrhi+//BKTJ0/W6ylZAPbh08qPP/6IUaNG4dSpUwgPD0evXr1Qp04dNG7cWAr2Dh8+jLS0NAwZMgSfffYZDh06JO1/8+ZNbN++HTt27ICx8X9fiNOnT8f8+fMxf/58jBs3Dl27doWPjw8mTJiAEiVKoE+fPhg6dCj27dsHIOMvnZYtW2LmzJkwMzPD+vXrERQUhMjISJQoUaKgLwu9oWGtWyjl9QSDpwRlun1wt1O4csMZJ855Zrqd6H3XqHoUypT4F5/PbKux7eotZ7xONsHAT/7Gyp3VoYDAgE9Ow9hYwMHutSzvgE/+RvuGV2FhloYrUc6Y8H2zgnoJlAfyqg9fTgUEBEAIkeX2P/74I9tjODg4YNOmTe/M4+fnh6NHj+aqbO87Bnxa8PPzw5QpUwAApUuXxuLFi3HwYEa/k0uXLuH27dvSiKP169ejYsWKOH36NKpXrw4goxl3/fr1cHJykh23d+/e6NSpEwBg3Lhx8Pf3x6RJk6Sq6OHDh6N3795S/sqVK6Ny5crS+vTp07Fz5078+uuvGDp0aI5eS3JyMpKTk6X1t4fGU+45OSRiSPeTGPt1c6Sman7E/KtGo0qFWAycqPlDSaQPnIokYmjncIye3wIpaZr3eHyiBUKWN8bIbsfRodEVCKHAwb9LIvKuI97+rd76hx/2HisLF4cX6BV0HhP6HP7/oI/PWCXKSwz4tODn5ydbd3Nzw6NHj3Dt2jV4eHjIhpdXqFAB9vb2uHbtmhTweXp6agR7bx9XPeePr6+vLC0pKQkJCQmwtbVFYmIiQkJC8PvvvyM2NhZpaWl4/fo1oqOjc/xaZs+ejalTp+Y4P2WvjPcTFLFLwrLpu6U0Y2MBv7JxaNf0Gn49WA7uzgn4dflPsv2mDPsTlyJd8OWslgVdZKJcKev5Lxxsk7By0i4pzdhYwK90HNo3vIqmg3rjzNXi6PbVZ7CzTkJ6ugKJr82w45uN+POxrexY8YnmiE80x72HdoiOK4Jf5m5GBZ9HuHrLpYBfFWlDhdw/D/ft/algMODTgqmpqWxdoVBApcp5J2Mrq8ynHHjzuAqFIss09blGjx6N0NBQfPPNNyhVqhQsLCzQsWPHXA0EmTBhgqwjakJCgsZ8SJQ75664o++E9rK0Mf2PIuaBHbb87of4F2bY81c52fbVs3di6cYaCD/Ppnh6/5295o7eUzrI0sb1PoLoWHts3u8na6aLT8zop1y13APY27zGiQtZ3+PqEbxKk/R8KDXlB/HGSFtt96eCwYAvD5UvXx4xMTGIiYmRgqarV6/i+fPnqFChQp6f7/jx4+jVqxfat88ILhITE3Hnzp1cHUPb2cwpa6+TTHHnnnzoflKyCRISzaT0zAZqPHpijbjH/43IdXdOgIV5KhzsXsNMmYaSJTIeD3T3vj3S0o019icqKK+Tlbj9wEGWlpRsgoSXZlJ689r/IDrOHs9fmKOiz0MM7XwSv4RVQsxDewBAee9HKOf1GJduuuLFSyXcnV+gT9szuP/IFldYu6c3VELHGj4d9qXcYcCXh5o0aQJfX19069YNCxcuRFpaGgYPHowGDRrIntuXV0qXLo0dO3YgKCgICoUCkyZNylVNI73fvux3DFXK/zeiccXMjCbiriM/xcN/OVULvd9KuD7HgA6nYWOVjLgn1vhpbxX8ElpJ2p6UYoJ6H91BrzbnYGGWhifxFvj7cnFM/b2qbHQvEeUNBnx5SKFQYPfu3fjiiy9Qv359GBkZoXnz5vj+++/z5Xzz589Hnz59ULt2bRQtWhTjxo3joIv3VHb98hr36JPrfYjeJyO+aS1bX7GjBlbsqJFl/tv3HTDq21b5XSzKZwU9Spe0pxDvGt9MH5yEhATY2dmhbsAUmJiYF3ZxiPJFujl/ZMhwpaUmIfyPKYiPj5c9riwvqX8r2h7oA1MrZfY7ZCH1ZQp2N1uTr2WlDPzWIyIiIjJwbNIlIiIirah0HKXLaVkKDgM+IiIi0gpH6eoPNukSERERGTjW8BEREZFWWMOnPxjwERERkVYY8OkPNukSERERGTjW8BEREZFWWMOnPxjwERERkVYEdJtahU9+KDgM+IiIiEgrrOHTH+zDR0RERGTgWMNHREREWmENn/5gwEdERERaYcCnP9ikS0RERGTgWMNHREREWmENn/5gwEdERERaEUIBoUPQpsu+lDts0iUiIiIycKzhIyIiIq2ooNBp4mVd9qXcYcBHREREWmEfPv3BJl0iIiIiA8caPiIiItIKB23oDwZ8REREpBU26eoPBnxERESkFdbw6Q/24SMiIiIycKzhIyIiIq0IHZt0WcNXcBjwERERkVYEACF0258KBpt0iYiIiAwca/iIiIhIKyoooOCTNvQCAz4iIiLSCkfp6g826RIREREZONbwERERkVZUQgEFJ17WCwz4iIiISCtC6DhKl8N0CwybdImIiEgvHDlyBEFBQXB3d4dCocCuXbukbampqRg3bhx8fX1hZWUFd3d39OzZEw8ePJAdw8vLCwqFQrbMmTNHlufixYuoV68ezM3N4eHhgblz5xbEy8tXDPiIiIhIK+pBG7osufHy5UtUrlwZS5Ys0dj26tUrnDt3DpMmTcK5c+ewY8cOREZGok2bNhp5p02bhtjYWGn54osvpG0JCQlo1qwZPD09cfbsWcybNw8hISFYsWJF7i/Qe4RNukRERKSVgh6l26JFC7Ro0SLTbXZ2dggNDZWlLV68GDVq1EB0dDRKlCghpdvY2MDV1TXT42zcuBEpKSlYs2YNlEolKlasiIiICMyfPx8DBgzIVXnfJ6zhIyIiIq2o/v/RarosQEat2ptLcnJynpQvPj4eCoUC9vb2svQ5c+bA0dERVatWxbx585CWliZtCw8PR/369aFUKqW0wMBAREZG4tmzZ3lSrsLAgI+IiIgKlYeHB+zs7KRl9uzZOh8zKSkJ48aNQ5cuXWBrayulDxs2DFu2bMFff/2FgQMHYtasWRg7dqy0PS4uDi4uLrJjqdfj4uJ0LldhYZMuERERaSWvRunGxMTIgjIzMzOdypWamopOnTpBCIGlS5fKto0aNUr6v5+fH5RKJQYOHIjZs2frfN73GQM+IiIi0kpGwKdLH76Mf21tbWUBny7Uwd7du3fx559/ZnvcmjVrIi0tDXfu3EHZsmXh6uqKhw8fyvKo17Pq96cP2KRLREREBkEd7N24cQNhYWFwdHTMdp+IiAgYGRnB2dkZAODv748jR44gNTVVyhMaGoqyZcuiSJEi+Vb2/MYaPiIiItJKQY/STUxMxM2bN6X127dvIyIiAg4ODnBzc0PHjh1x7tw57NmzB+np6VKfOwcHByiVSoSHh+PUqVNo2LAhbGxsEB4ejpEjR6J79+5SMNe1a1dMnToVffv2xbhx43D58mV89913WLBggdav833AgI+IiIi0Iv5/0WX/3Dhz5gwaNmworav74wUHByMkJAS//vorAKBKlSqy/f766y8EBATAzMwMW7ZsQUhICJKTk+Ht7Y2RI0fK+vXZ2dnhwIEDGDJkCKpVq4aiRYti8uTJej0lC8CAj4iIiPREQEAAxDtGibxrGwB89NFHOHnyZLbn8fPzw9GjR3NdvvcZAz4iIiLSSkE36ZL2GPARERGRdgq6TZe0xoCPiIiItKNjDR9Yw1dgOC0LERERkYFjDR8RERFpJa+etEH5jwEfERERaYWDNvQHm3SJiIiIDBxr+IiIiEg7QqHbwAvW8BUYBnxERESkFfbh0x9s0iUiIiIycKzhIyIiIu1w4mW9YbABn/oByjnRpk2bfCwJERGRYeIoXf1hsAFfu3btcpRPoVAgPT09fwtDREREVIgMNuBTqVSFXQQiIiLDx2ZZvWCwAV9WkpKSYG5uXtjFICIi0nts0tUfH8Qo3fT0dEyfPh3FihWDtbU1bt26BQCYNGkSVq9eXcilIyIi0lMiDxYqEB9EwDdz5kysW7cOc+fOhVKplNIrVaqEVatWFWLJiIiIiPLfBxHwrV+/HitWrEC3bt1gbGwspVeuXBnXr18vxJIRERHpM0UeLFQQPog+fPfv30epUqU00lUqFVJTUwuhRERERAaA8/DpjQ+ihq9ChQo4evSoRvq2bdtQtWrVQigRERERUcH5IGr4Jk+ejODgYNy/fx8qlQo7duxAZGQk1q9fjz179hR28YiIiPQTa/j0xgdRw9e2bVv89ttvCAsLg5WVFSZPnoxr167ht99+Q9OmTQu7eERERPpJKHRfqEB8EDV8AFCvXj2EhoYWdjGIiIiICtwHE/ABwJkzZ3Dt2jUAGf36qlWrVsglIiIi0l9CZCy67E8F44MI+O7du4cuXbrg+PHjsLe3BwA8f/4ctWvXxpYtW1C8ePHCLSAREZE+Yh8+vfFB9OHr168fUlNTce3aNTx9+hRPnz7FtWvXoFKp0K9fv8IuHhEREVG++iBq+A4fPowTJ06gbNmyUlrZsmXx/fffo169eoVYMiIiIj2m68ALDtooMB9EwOfh4ZHpBMvp6elwd3cvhBIRERHpP4XIWHTZnwrGB9GkO2/ePHzxxRc4c+aMlHbmzBkMHz4c33zzTSGWjIiISI+JPFioQBhsDV+RIkWgUPxXVfzy5UvUrFkTJiYZLzktLQ0mJibo06cP2rVrV0ilJCIiIsp/BhvwLVy4sLCLQEREZNjYh09vGGzAFxwcXNhFICIiMmyclkVvGGzAl5WkpCSkpKTI0mxtbQupNERERET574MYtPHy5UsMHToUzs7OsLKyQpEiRWQLERERaYGDNvTGBxHwjR07Fn/++SeWLl0KMzMzrFq1ClOnToW7uzvWr19f2MUjIiLSTwz49MYH0aT722+/Yf369QgICEDv3r1Rr149lCpVCp6enti4cSO6detW2EUkIiIiyjcfRA3f06dP4ePjAyCjv97Tp08BAHXr1sWRI0cKs2hERET6Sz1KV5eFCsQHEfD5+Pjg9u3bAIBy5crh559/BpBR82dvb1+IJSMiItJf6idt6LJQwfggAr7evXvjwoULAIDx48djyZIlMDc3x8iRIzFmzJhCLh0RERHlxJEjRxAUFAR3d3coFArs2rVLtl0IgcmTJ8PNzQ0WFhZo0qQJbty4Icvz9OlTdOvWDba2trC3t0ffvn2RmJgoy3Px4kXUq1cP5ubm8PDwwNy5c/P7peW7DyLgGzlyJIYNGwYAaNKkCa5fv45Nmzbh/PnzGD58eCGXjoiISE8V8KCNly9fonLlyliyZEmm2+fOnYtFixZh2bJlOHXqFKysrBAYGIikpCQpT7du3XDlyhWEhoZiz549OHLkCAYMGCBtT0hIQLNmzeDp6YmzZ89i3rx5CAkJwYoVK3JX2PfMBzFo422enp7w9PQs7GIQERFRLrRo0QItWrTIdJsQAgsXLsTEiRPRtm1bAMD69evh4uKCXbt2oXPnzrh27Rr279+P06dP4+OPPwYAfP/992jZsiW++eYbuLu7Y+PGjUhJScGaNWugVCpRsWJFREREYP78+bLAUN8YbMC3aNGiHOdV1/4RERFRzimgWz889ZCNhIQEWbqZmRnMzMxydazbt28jLi4OTZo0kdLs7OxQs2ZNhIeHo3PnzggPD4e9vb0U7AEZLX9GRkY4deoU2rdvj/DwcNSvXx9KpVLKExgYiK+//hrPnj3T2/l7DTbgW7BgQY7yKRQKBnxERESFyMPDQ7Y+ZcoUhISE5OoYcXFxAAAXFxdZuouLi7QtLi4Ozs7Osu0mJiZwcHCQ5fH29tY4hnobA773jHpULmnH5FAETBSmhV0Monxx8EFEYReBKN8kvFChSJkCOpmuU6v8/74xMTGyx5zmtnaPsvdBDNogIiKifJBHgzZsbW1lizYBn6urKwDg4cOHsvSHDx9K21xdXfHo0SPZ9rS0NDx9+lSWJ7NjvHkOfcSAj4iIiPSet7c3XF1dcfDgQSktISEBp06dgr+/PwDA398fz58/x9mzZ6U8f/75J1QqFWrWrCnlOXLkCFJTU6U8oaGhKFu2rN425wIM+IiIiEhbBTwtS2JiIiIiIhAREQEgo/tWREQEoqOjoVAoMGLECMyYMQO//vorLl26hJ49e8Ld3R3t2rUDAJQvXx7NmzdH//798ffff+P48eMYOnQoOnfuDHd3dwBA165doVQq0bdvX1y5cgVbt27Fd999h1GjRulwoQqfwfbhIyIiovyl69MycrvvmTNn0LBhQ2ldHYQFBwdj3bp1GDt2LF6+fIkBAwbg+fPnqFu3Lvbv3w9zc3Npn40bN2Lo0KFo3LgxjIyM8Mknn8hm9rCzs8OBAwcwZMgQVKtWDUWLFsXkyZP1ekoWAFAIIfhgE5IkJCTAzs4OAWjLQRtksP7goA0yYBmDNm4hPj5eNhAiT8/x/78VXjNnwuiNYCq3VElJuPPVV/laVsrwwTTpHj16FN27d4e/vz/u378PANiwYQOOHTtWyCUjIiLSUwXcpEva+yACvu3btyMwMBAWFhY4f/48kpOTAQDx8fGYNWtWIZeOiIhITzHg0xsfRMA3Y8YMLFu2DCtXroSp6X/NlHXq1MG5c+cKsWRERERE+e+DGLQRGRmJ+vXra6Tb2dnh+fPnBV8gIiIiA1DQgzZIex9EDZ+rqytu3rypkX7s2DH4+PgUQomIiIgMgPpJG7osVCA+iICvf//+GD58OE6dOgWFQoEHDx5g48aNGD16NAYNGlTYxSMiItJP7MOnNz6IJt3x48dDpVKhcePGePXqFerXrw8zMzOMHj0aX3zxRWEXj4iIiChffRABn0KhwFdffYUxY8bg5s2bSExMRIUKFWBtbV3YRSMiItJb7MOnPz6IgE9NqVSiQoUKhV0MIiIiw6BrsywDvgLzQQR8DRs2hEKRdcfQP//8swBLQ0RERFSwPoiAr0qVKrL11NRURERE4PLlywgODi6cQhEREek7HZt0WcNXcD6IgG/BggWZpoeEhCAxMbGAS0NERGQg2KSrNz6IaVmy0r17d6xZs6awi0FERESUrz6IGr6shIeHw9zcvLCLQUREpJ9Yw6c3PoiAr0OHDrJ1IQRiY2Nx5swZTJo0qZBKRUREpN84LYv++CACPjs7O9m6kZERypYti2nTpqFZs2aFVCoiIiKigmHwAV96ejp69+4NX19fFClSpLCLQ0RERFTgDH7QhrGxMZo1a4bnz58XdlGIiIgMC5+lqzcMPuADgEqVKuHWrVuFXQwiIiKDou7Dp8tCBeODCPhmzJiB0aNHY8+ePYiNjUVCQoJsISIiIjJkBt2Hb9q0afjyyy/RsmVLAECbNm1kj1gTQkChUCA9Pb2wikhERKTfWEunFww64Js6dSo+//xz/PXXX4VdFCIiIsPDefj0hkEHfEJk3EkNGjQo5JIQERERFR6DDvgAyJpwiYiIKO9w4mX9YfABX5kyZbIN+p4+fVpApSEiIjIgbNLVGwYf8E2dOlXjSRtEREREHxKDD/g6d+4MZ2fnwi4GERGRwWGTrv4w6ICP/feIiIjyEZt09YZBT7ysHqVLRERE9CEz6Bo+lUpV2EUgIiIyXKzh0xsGHfARERFR/mEfPv3BgI+IiIi0wxo+vWHQffiIiIiIiDV8REREpC3W8OkNBnxERESkFfbh0x9s0iUiIiIycKzhIyIiIu2wSVdvMOAjIiIirbBJV3+wSZeIiIj0gpeXFxQKhcYyZMgQAEBAQIDGts8//1x2jOjoaLRq1QqWlpZwdnbGmDFjkJaWVhgvp0Cxho+IiIi0U8BNuqdPn0Z6erq0fvnyZTRt2hSffvqplNa/f39MmzZNWre0tJT+n56ejlatWsHV1RUnTpxAbGwsevbsCVNTU8yaNUv716EHGPARERGRdgo44HNycpKtz5kzByVLlkSDBg2kNEtLS7i6uma6/4EDB3D16lWEhYXBxcUFVapUwfTp0zFu3DiEhIRAqVTm+iXoCzbpEhERUaFKSEiQLcnJydnuk5KSgp9++gl9+vSBQqGQ0jdu3IiiRYuiUqVKmDBhAl69eiVtCw8Ph6+vL1xcXKS0wMBAJCQk4MqVK3n7ot4zrOEjIiIirSj+f9FlfwDw8PCQpU+ZMgUhISHv3HfXrl14/vw5evXqJaV17doVnp6ecHd3x8WLFzFu3DhERkZix44dAIC4uDhZsAdAWo+Li9Phlbz/GPARERGRdvKoSTcmJga2trZSspmZWba7rl69Gi1atIC7u7uUNmDAAOn/vr6+cHNzQ+PGjREVFYWSJUvqUFD9xyZdIiIi0op6WhZdFgCwtbWVLdkFfHfv3kVYWBj69ev3znw1a9YEANy8eRMA4OrqiocPH8ryqNez6vdnKBjwERERkV5Zu3YtnJ2d0apVq3fmi4iIAAC4ubkBAPz9/XHp0iU8evRIyhMaGgpbW1tUqFAh38r7PmCTLhEREWmnEJ60oVKpsHbtWgQHB8PE5L8wJioqCps2bULLli3h6OiIixcvYuTIkahfvz78/PwAAM2aNUOFChXQo0cPzJ07F3FxcZg4cSKGDBmSo2ZkfcaAj4iIiLRXwE/LCAsLQ3R0NPr06SNLVyqVCAsLw8KFC/Hy5Ut4eHjgk08+wcSJE6U8xsbG2LNnDwYNGgR/f39YWVkhODhYNm+foWLAR0RERHqjWbNmEEIzyvTw8MDhw4ez3d/T0xN79+7Nj6K91xjwERERkVb4LF39wYCPiIiItFMIffhIOxylS0RERGTgWMNHREREWmGTrv5gwEdERETaYZOu3mCTLhEREZGBYw0fERERaYVNuvqDAR8RERFph026eoMBHxEREWmHAZ/eYB8+IiIiIgPHGj4iIiLSCvvw6Q8GfERERKQdNunqDTbpEhERERk41vARERGRVhRCQCG0r6bTZV/KHQZ8REREpB026eoNNukSERERGTjW8BEREZFWOEpXfzDgIyIiIu2wSVdvsEmXiIiIyMCxho+IiIi0wiZd/cGAj4iIiLTDJl29wYCPiIiItMIaPv3BPnxEREREBo41fERERKQdNunqDQZ8REREpDU2y+oHNukSERERGTjW8BEREZF2hMhYdNmfCgQDPiIiItIKR+nqDzbpEhERERk41vARERGRdjhKV28w4CMiIiKtKFQZiy77U8Fgky4RERGRgWMN33vGy8sLI0aMwIgRIwq7KJSHun8Zhx5fPpSlxdw0Q7/65d7KKTDjp9uo3ugFQvp4IXy/XcEVkigLl05a4ZcfnHHjkiWePjTFlNW3UbtFvLT92WMTrJ7pjrOHbfAy3hiVaiViyIx7KOaTIuX5bmxxnD9qgycPTWFhqUL5j1+i71cPUKJ0suxcB7Y6YMcKJ9y7ZQZL63TUb/0cQ2ffL7DXSrnEJl29wYCvkKxbtw4jRozA8+fPZemnT5+GlZVV4RSK8tWd6+YY/5mPtJ6ertDI077/v5ylgN47Sa+M4FPxNQK7PMW0vt6ybUIAU/t4w9hEIGTtLVhaq7BjhRPGf1YKKw9fh7llRptdab/XaNThGZyKpeLFM2P89K0r/telJH48dRXGxhnH2r7cCduXO6HfxAco99ErJL0ywsMYZUG/XMoFjtLVHwz43jNOTk6FXQTKJ+npwLPHpllu96n4Gp8MfIwvWpTGlgtXC7BkRO9WvdELVG/0ItNt92+Z4dpZKyz/6zq8yiYBAL6Ycw+dK1fEXzvt0aLbUwBAy+5PpH1cPYDgcbEY1KQcHsYo4e6VghfPjfHj126Y+uMtVK2XKOX1qZCUj6+MdMZ5+PQG+/Bpaf/+/ahbty7s7e3h6OiI1q1bIyoqCgBw6NAhKBQKWe1dREQEFAoF7ty5g0OHDqF3796Ij4+HQqGAQqFASEgIgIwm3YULFwIAhBAICQlBiRIlYGZmBnd3dwwbNkw6ppeXF2bMmIGePXvC2toanp6e+PXXX/H48WO0bdsW1tbW8PPzw5kzZwrqstA7FPNOwaZzV7Au/BrGLb4Lp2L/NXeZWagwfsldLPmq2DuDQqL3TWpKRk210uy/3vdGRoCpUuDKaetM90l6ZYQDWx3gWiIZTu6pAIBzR2ygEsC/caboV78culWrgBkDPfHoPj8PRHmBAZ+WXr58iVGjRuHMmTM4ePAgjIyM0L59e6hU2Q85ql27NhYuXAhbW1vExsYiNjYWo0eP1si3fft2LFiwAMuXL8eNGzewa9cu+Pr6yvIsWLAAderUwfnz59GqVSv06NEDPXv2RPfu3XHu3DmULFkSPXv2hMjir6jk5GQkJCTIFsp7189Z4psRHviqmw++H18MriVS8O3Om7CwSgcADAy5j6tnrBD+B/vskX7xKJUE52IpWDPbDS+eGyM1RYGti53xb6wSTx/KG5F+W+eItqV80baUH07/aYvZW6Jgqsz4boq7q4RQAVsWueDzafcxccUdvHhmggmdS0pBJb1/1E26uixUMNikq6VPPvlEtr5mzRo4OTnh6tXsm+KUSiXs7OygUCjg6uqaZb7o6Gi4urqiSZMmMDU1RYkSJVCjRg1ZnpYtW2LgwIEAgMmTJ2Pp0qWoXr06Pv30UwDAuHHj4O/vj4cPH2Z6rtmzZ2Pq1KnZlpl0c+YvW+n/t69Z4Pp5K2z4+yrqt3mO+CcmqFInEYOblSnEEhJpx8QUmLz6NuaPKoGOFXxhZCxQtd4LVG+UoNFa16jDM3xU/wWePjLFtqXOmDnQCwt234DSXEAlgLRUIwyefh/VAjKajycsvYMulSvhwglrfByQeZMyFTIO2tAbrOHT0o0bN9ClSxf4+PjA1tYWXl5eADKCtLzy6aef4vXr1/Dx8UH//v2xc+dOpKWlyfL4+flJ/3dxcQEAWS2gOu3Ro0eZnmPChAmIj4+XlpiYmDwrP2XtZYIx7t0yg7tXCqrUSYSbVwp2XL+MvdEXsDf6AgBg0so7mLvtZiGXlCh7pf1eY2lYJHZcv4jNEZcxa9MtJDwzhlsJ+QhcK1sVivmkwLfWS0xceQcxN81wfF9GrbaDc8Z3W4ky//XZs3dMh61DGpt1SRISEiJ1hVIv5cr9N9tBUlIShgwZAkdHR1hbW+OTTz7Bw4fyGRKio6PRqlUrWFpawtnZGWPGjNH4bTVErOHTUlBQEDw9PbFy5Uq4u7tDpVKhUqVKSElJgbV1Rr+VN5tRU1NTc30ODw8PREZGIiwsDKGhoRg8eDDmzZuHw4cPw9Q04wtQ/S8AKBSKLNOyamo2MzODmZlZrstGujG3TIe7ZwoObjfBkV/tsW+Tg2z7ir/+wfIQd5w8YJvFEYjeP1a2Gd8z928pceOCJYLHxGWZVwgAQoHUlIx6h4rVXwIA7kWZSf36Ep4ZI+GpCVyK5f77kwpGYYzSrVixIsLCwqR1E5P/QpmRI0fi999/xy+//AI7OzsMHToUHTp0wPHjxwEA6enpaNWqFVxdXXHixAnExsaiZ8+eMDU1xaxZs7R/IXqAAZ8Wnjx5gsjISKxcuRL16tUDABw7dkzarh5pGxsbiyJFigDIGLTxJqVSifT09GzPZWFhgaCgIAQFBWHIkCEoV64cLl26hI8++iiPXg0VhP6TH+DkAVs8uqeEo2sqeoyOQ7oKOLSzCOKfmmQ6UOPRfSUexjAYp8L3+qURHtz+716Mi1Ei6rIFbOzT4Fw8FUd+s4OdYzqci6Xg9jVzLJtcHP7N46Wm2di7Shz+1R7VGryAnUMaHsea4ufFLlBaqFCjcUa/4eIlk+EfGI+lk4th+NwYWNmosGaWG4qXSkLlOmzOfW8VwihdExOTTLsoxcfHY/Xq1di0aRMaNWoEAFi7di3Kly+PkydPolatWjhw4ACuXr2KsLAwuLi4oEqVKpg+fTrGjRuHkJAQKJWGOw0QAz4tFClSBI6OjlixYgXc3NwQHR2N8ePHS9tLlSoFDw8PhISEYObMmfjnn3/w7bffyo7h5eWFxMREHDx4EJUrV4alpSUsLS1ledatW4f09HTUrFkTlpaW+Omnn2BhYQFPT88CeZ2Ud4q6pWLCD3dhUyQd8U9McOW0FUa0Lo34p/wI0vvvnwuWGNuxlLS+PKQYAKBpp6cYvTAaTx+aYnlIMTz/1wQOzmlo8ulTdB3xXzOa0kyFy6essXOlExLjjWFfNA2+tRKxYPcN2Bf9ryltzKK7WD6lGCb39IHCCPCrlYiZG2/BhC26Bu/tAYPvan26ceMG3N3dYW5uDn9/f8yePRslSpTA2bNnkZqaiiZNmkh5y5UrhxIlSiA8PBy1atVCeHg4fH19pe5OABAYGIhBgwbhypUrqFq1av68wPcAf220YGRkhC1btmDYsGGoVKkSypYti0WLFiEgIABARpPq5s2bMWjQIPj5+aF69eqYMWOGNJACyBip+/nnn+Ozzz7DkydPMGXKFGlqFjV7e3vMmTMHo0aNQnp6Onx9ffHbb7/B0dGxAF8t5YXZg3IXpAe6V86nkhDlXuXaifjjQUSW29v1+xft+v2b5XZH1zTM+OlWtuexslFh1PwYjJrPvsT6Iq+adD08PGTpmf0mAkDNmjWxbt06lC1bFrGxsZg6dSrq1auHy5cvIy4uDkqlEvb29rJ9XFxcEBeX0b0gLi5OFuypt6u3GTIGfFpq0qSJxojcN/vs1alTBxcvXsxyOwAsXboUS5culaXduXNH+n+7du3Qrl27LMvwZt6szuHl5ZXllCxEREQ6yaNRujExMbC1/a/Pcla1ey1atJD+7+fnh5o1a8LT0xM///wzLCwsdCiI4eMoXSIiIipUtra2siWngwnt7e1RpkwZ3Lx5E66urkhJSdF4ZOmb05K5urpqjNpVr79rmjRDwICPiIiItFLYEy8nJiYiKioKbm5uqFatGkxNTXHw4EFpe2RkJKKjo+Hv7w8A8Pf3x6VLl2RTlYWGhsLW1hYVKlTQrTDvOTbpEhERkXZUImPRZf9cGD16tDQt2oMHDzBlyhQYGxujS5cusLOzQ9++fTFq1Cg4ODjA1tYWX3zxBfz9/VGrVi0AQLNmzVChQgX06NEDc+fORVxcHCZOnIghQ4YY/BRlDPiIiIhIOwX8pI179+6hS5cuePLkCZycnFC3bl2cPHlSmg5twYIFMDIywieffILk5GQEBgbihx9+kPY3NjbGnj17MGjQIPj7+8PKygrBwcGYNm2aDi9CPzDgIyIiIr2wZcuWd243NzfHkiVLsGTJkizzeHp6Yu/evXldtPceAz4iIiLSigI6TsuSZyWh7DDgIyIiIu0UwpM2SDscpUtERERk4FjDR0RERFrJqydtUP5jwEdERETaKeBRuqQ9NukSERERGTjW8BEREZFWFEJAocPAC132pdxhwEdERETaUf3/osv+VCDYpEtERERk4FjDR0RERFphk67+YMBHRERE2uEoXb3BgI+IiIi0wydt6A324SMiIiIycKzhIyIiIq3wSRv6gwEfERERaYdNunqDTbpEREREBo41fERERKQVhSpj0WV/KhgM+IiIiEg7bNLVG2zSJSIiIjJwrOEjIiIi7XDiZb3BgI+IiIi0wker6Q826RIREREZONbwERERkXY4aENvMOAjIiIi7QgAukytwnivwDDgIyIiIq2wD5/+YB8+IiIiIgPHGj4iIiLSjoCOffjyrCSUDQZ8REREpB0O2tAbbNIlIiIiMnCs4SMiIiLtqAAodNyfCgQDPiIiItIKR+nqDzbpEhERERk41vARERGRdjhoQ28w4CMiIiLtMODTG2zSJSIiIjJwrOEjIiIi7bCGT28w4CMiIiLtcFoWvcGAj4iIiLTCaVn0B/vwERERkV6YPXs2qlevDhsbGzg7O6Ndu3aIjIyU5QkICIBCoZAtn3/+uSxPdHQ0WrVqBUtLSzg7O2PMmDFIS0sryJdS4FjDR0RERNop4D58hw8fxpAhQ1C9enWkpaXhf//7H5o1a4arV6/CyspKyte/f39MmzZNWre0tJT+n56ejlatWsHV1RUnTpxAbGwsevbsCVNTU8yaNUv71/KeY8BHRERE2lEJQKFDwKfK3b779++Xra9btw7Ozs44e/Ys6tevL6VbWlrC1dU102McOHAAV69eRVhYGFxcXFClShVMnz4d48aNQ0hICJRKZe5fhx5gky4REREVqoSEBNmSnJyco/3i4+MBAA4ODrL0jRs3omjRoqhUqRImTJiAV69eSdvCw8Ph6+sLFxcXKS0wMBAJCQm4cuVKHrya9xNr+IiIiEg7edSk6+HhIUueMmUKQkJC3rmrSqXCiBEjUKdOHVSqVElK79q1Kzw9PeHu7o6LFy9i3LhxiIyMxI4dOwAAcXFxsmAPgLQeFxen/Wt5zzHgIyIiIi3pGPAhY9+YmBjY2tpKqWZmZtnuOWTIEFy+fBnHjh2TpQ8YMED6v6+vL9zc3NC4cWNERUWhZMmSOpRVv7FJl4iIiAqVra2tbMku4Bs6dCj27NmDv/76C8WLF39n3po1awIAbt68CQBwdXXFw4cPZXnU61n1+zMEDPiIiIhIO+omXV2WXJ1OYOjQodi5cyf+/PNPeHt7Z7tPREQEAMDNzQ0A4O/vj0uXLuHRo0dSntDQUNja2qJChQq5Ko8+YZMuERERaUcloG6W1X7/nBsyZAg2bdqE3bt3w8bGRupzZ2dnBwsLC0RFRWHTpk1o2bIlHB0dcfHiRYwcORL169eHn58fAKBZs2aoUKECevTogblz5yIuLg4TJ07EkCFDctSUrK9Yw0dERER6YenSpYiPj0dAQADc3NykZevWrQAApVKJsLAwNGvWDOXKlcOXX36JTz75BL/99pt0DGNjY+zZswfGxsbw9/dH9+7d0bNnT9m8fYaINXxERESkHaHKWHTZPzfZs2kC9vDwwOHDh7M9jqenJ/bu3Zurc+s7BnxERESknQJ+0gZpjwEfERERaaeA+/CR9tiHj4iIiMjAsYaPiIiItMMmXb3BgI+IiIi0I6BjwJdnJaFssEmXiIiIyMCxho+IiIi0wyZdvcGAj4iIiLSjUgHQYR4+lQ77Uq6wSZeIiIjIwLGGj4iIiLTDJl29wYCPiIiItMOAT2+wSZeIiIjIwLGGj4iIiLTDR6vpDQZ8REREpBUhVBBC+5G2uuxLucOAj4iIiLQjhG61dOzDV2DYh4+IiIjIwLGGj4iIiLQjdOzDxxq+AsOAj4iIiLSjUgEKHfrhsQ9fgWGTLhEREZGBYw0fERERaYdNunqDAR8RERFpRahUEDo06XJaloLDJl0iIiIiA8caPiIiItIOm3T1BgM+IiIi0o5KAAoGfPqATbpEREREBo41fERERKQdIQDoMg8fa/gKCgM+IiIi0opQCQgdmnQFA74Cw4CPiIiItCNU0K2Gj9OyFBT24SMiIiIycKzhIyIiIq2wSVd/MOAjIiIi7bBJV28w4CMZ9V9baUjVaS5NovdZwgv+yJDhSkjMuL8LovZM19+KNKTmXWHonRjwkcyLFy8AAMewt5BLQpR/ipQp7BIQ5b8XL17Azs4uX46tVCrh6uqKY3G6/1a4urpCqVTmQanoXRSCDej0BpVKhQcPHsDGxgYKhaKwi/NBSEhIgIeHB2JiYmBra1vYxSHKU7y/C54QAi9evIC7uzuMjPJvbGZSUhJSUlJ0Po5SqYS5uXkelIjehTV8JGNkZITixYsXdjE+SLa2tvxBJIPF+7tg5VfN3pvMzc0ZqOkRTstCREREZOAY8BEREREZOAZ8RIXMzMwMU6ZMgZmZWWEXhSjP8f4mej9w0AYRERGRgWMNHxEREZGBY8BHREREZOAY8BEREREZOAZ8RAbIy8sLCxcuLOxiEEl4TxIVLgZ8RESUZ9atWwd7e3uN9NOnT2PAgAEFXyAiAsAnbRAVipSUFD47kj4oTk5OhV0Eog8aa/iIciAgIADDhg3D2LFj4eDgAFdXV4SEhEjbo6Oj0bZtW1hbW8PW1hadOnXCw4cPpe0hISGoUqUKVq1aBW9vb+lxRAqFAsuXL0fr1q1haWmJ8uXLIzw8HDdv3kRAQACsrKxQu3ZtREVFSceKiopC27Zt4eLiAmtra1SvXh1hYWEFdi3IsO3fvx9169aFvb09HB0d0bp1a+n+O3ToEBQKBZ4/fy7lj4iIgEKhwJ07d3Do0CH07t0b8fHxUCgUUCgU0ufkzSZdIQRCQkJQokQJmJmZwd3dHcOGDZOO6eXlhRkzZqBnz56wtraGp6cnfv31Vzx+/Fj6nPn5+eHMmTMFdVmI9B4DPqIc+vHHH2FlZYVTp05h7ty5mDZtGkJDQ6FSqdC2bVs8ffoUhw8fRmhoKG7duoXPPvtMtv/Nmzexfft27NixAxEREVL69OnT0bNnT0RERKBcuXLo2rUrBg4ciAkTJuDMmTMQQmDo0KFS/sTERLRs2RIHDx7E+fPn0bx5cwQFBSE6OrqgLgUZsJcvX2LUqFE4c+YMDh48CCMjI7Rv3x4qlSrbfWvXro2FCxfC1tYWsbGxiI2NxejRozXybd++HQsWLMDy5ctx48YN7Nq1C76+vrI8CxYsQJ06dXD+/Hm0atUKPXr0QM+ePdG9e3ecO3cOJUuWRM+ePcGpZIlySBBRtho0aCDq1q0rS6tevboYN26cOHDggDA2NhbR0dHStitXrggA4u+//xZCCDFlyhRhamoqHj16JDsGADFx4kRpPTw8XAAQq1evltI2b94szM3N31m+ihUriu+//15a9/T0FAsWLMj16yR62+PHjwUAcenSJfHXX38JAOLZs2fS9vPnzwsA4vbt20IIIdauXSvs7Ow0jvPmPfntt9+KMmXKiJSUlEzP6enpKbp37y6tx8bGCgBi0qRJUpr6sxIbG6vzayT6ELCGjyiH/Pz8ZOtubm549OgRrl27Bg8PD3h4eEjbKlSoAHt7e1y7dk1K8/T0zLQf05vHdXFxAQBZbYeLiwuSkpKQkJAAIKOGb/To0Shfvjzs7e1hbW2Na9eusYaP8sSNGzfQpUsX+Pj4wNbWFl5eXgCQp/fXp59+itevX8PHxwf9+/fHzp07kZaWJsuTk88FADx69CjPykVkyBjwEeWQqampbF2hUOSomUvNysoq2+MqFIos09TnGj16NHbu3IlZs2bh6NGjiIiIgK+vL1JSUnJcFqKsBAUF4enTp1i5ciVOnTqFU6dOAcgYaGRklPGTId5oRk1NTc31OTw8PBAZGYkffvgBFhYWGDx4MOrXry87Vm4/F0T0bgz4iHRUvnx5xMTEICYmRkq7evUqnj9/jgoVKuT5+Y4fP45evXqhffv28PX1haurK+7cuZPn56EPz5MnTxAZGYmJEyeicePGKF++PJ49eyZtV9dQx8bGSmlv9kcFAKVSifT09GzPZWFhgaCgICxatAiHDh1CeHg4Ll26lDcvhIg0cFoWIh01adIEvr6+6NatGxYuXIi0tDQMHjwYDRo0wMcff5zn5ytdujR27NiBoKAgKBQKTJo0ibUclCeKFCkCR0dHrFixAm5uboiOjsb48eOl7aVKlYKHhwdCQkIwc+ZM/PPPP/j2229lx/Dy8kJiYiIOHjyIypUrw9LSEpaWlrI869atQ3p6OmrWrAlLS0v89NNPsLCwgKenZ4G8TqIPEWv4iHSkUCiwe/duFClSBPXr10eTJk3g4+ODrVu35sv55s+fjyJFiqB27doICgpCYGAgPvroo3w5F31YjIyMsGXLFpw9exaVKlXCyJEjMW/ePGm7qakpNm/ejOvXr8PPzw9ff/01ZsyYITtG7dq18fnnn+Ozzz6Dk5MT5s6dq3Eee3t7rFy5EnXq1IGfnx/CwsLw22+/wdHRMd9fI9GHSiEEx7QTERERGTLW8BEREREZOAZ8RERERAaOAR8RERGRgWPAR0RERGTgGPARERERGTgGfEREREQGjgEfERERkYFjwEdE751evXqhXbt20npAQABGjBhR4OU4dOgQFAoFnj9/nmUehUKBXbt25fiYISEhqFKlik7lunPnDhQKhcZjzYiIssKAj4hypFevXlAoFFAoFFAqlShVqhSmTZuGtLS0fD/3jh07MH369BzlzUmQRkT0oeGzdIkox5o3b461a9ciOTkZe/fuxZAhQ2BqaooJEyZo5E1JSYFSqcyT8zo4OOTJcYiIPlSs4SOiHDMzM4Orqys8PT0xaNAgNGnSBL/++iuA/5phZ86cCXd3d5QtWxYAEBMTg06dOsHe3h4ODg5o27Yt7ty5Ix0zPT0do0aNgr29PRwdHTF27Fi8/cTHt5t0k5OTMW7cOHh4eMDMzAylSpXC6tWrcefOHTRs2BAAUKRIESgUCvTq1QsAoFKpMHv2bHh7e8PCwgKVK1fGtm3bZOfZu3cvypQpAwsLCzRs2FBWzpwaN24cypQpA0tLS/j4+GDSpElITU3VyLd8+XJ4eHjA0tISnTp1Qnx8vGz7qlWrUL58eZibm6NcuXL44Ycfcl0WIiI1BnxEpDULCwukpKRI6wcPHkRkZCRCQ0OxZ88epKamIjAwEDY2Njh69CiOHz8Oa2trNG/eXNrv22+/xbp167BmzRocO3YMT58+xc6dO9953p49e2Lz5s1YtGgRrl27huXLl8Pa2hoeHh7Yvn07ACAyMhKxsbH47rvvAACzZ8/G+vXrsWzZMly5cgUjR45E9+7dcfjwYQAZgWmHDh0QFBSEiIgI9OvXD+PHj8/1NbGxscG6detw9epVfPfdd1i5ciUWLFggy3Pz5k38/PPP+O2337B//36cP38egwcPlrZv3LgRkydPxsyZM3Ht2jXMmjULkyZNwo8//pjr8hARAQAEEVEOBAcHi7Zt2wohhFCpVCI0NFSYmZmJ0aNHS9tdXFxEcnKytM+GDRtE2bJlhUqlktKSk5OFhYWF+OOPP4QQQri5uYm5c+dK21NTU0Xx4sWlcwkhRIMGDcTw4cOFEEJERkYKACI0NDTTcv71118CgHj27JmUlpSUJCwtLcWJEydkefv27Su6dOkihBBiwoQJokKFCrLt48aN0zjW2wCInTt3Zrl93rx5olq1atL6lClThLGxsbh3756Utm/fPmFkZCRiY2OFEEKULFlSbNq0SXac6dOnC39/fyGEELdv3xYAxPnz57M8LxHRm9iHj4hybM+ePbC2tkZqaipUKhW6du2KkJAQabuvr6+s396FCxdw8+ZN2NjYyI6TlJSEqKgoxMfHIzY2FjVr1pS2mZiY4OOPP9Zo1lWLiIiAsbExGjRokONy37x5E69evULTpk1l6SkpKahatSoA4Nq1a7JyAIC/v3+Oz6G2detWLFq0CFFRUUhMTERaWhpsbW1leUqUKIFixYrJzqNSqRAZGQkbGxtERUWhb9++6N+/v5QnLS0NdnZ2uS4PERHAQRtElAsNGzbE0qVLoVQq4e7uDhMT+VeIlZWVbD0xMRHVqlXDxo0bNY7l5OSkVRksLCxyvU9iYiIA4Pfff5cFWkBGv8S8Eh4ejm7dumHq1KkIDAyEnZ0dtmzZgm+//TbXZV25cqVGAGpsbJxnZSWiDwsDPiLKMSsrK5QqVSrH+T/66CNs3boVzs7OGrVcam5ubjh16hTq168PIKMm6+zZs/joo48yze/r6wuVSoXDhw+jSZMmGtvVNYzp6elSWoUKFWBmZobo6OgsawbLly8vDUBRO3nyZPYv8g0nTpyAp6cnvvrqKynt7t27Gvmio6Px4MEDuLu7S+cxMjJC2bJl4eLiAnd3d9y6dQvdunXL1fmJiLLCQRtElG+6deuGokWLom3btjh69Chu376NQ4cOYdiwYbh37x4AYPjw4ZgzZw527dqF69evY/Dgwe+cQ8/LywvBwcHo06cPdu3aJR3z559/BgB4enpCoVBgz549ePz4MRITE2FjY4PRo0dj5MiR+PHHHxEVFYVz587h+++/lwZCfP7557hx4wbGjBmDyMhIbNq0CevWrcvV6y1dujSio6OxZcsWREVFYdGiRZkOQDE3N0dwcDAuXLiAo0ePYtiwYejUqRNcXV0BAFOnTsXs2bOxaNEi/PPPP7h06RLWrl2L+fPn56o8RERqDPiIKN9YWlriyJEjKFGiBDp06IDy5cujb9++SEpKkmr8vvzyS/To0QPBwcHw9/eHjY0N2rdv/87jLl26FB07dsTgwYNRrlw59O/fHy9fvgQAFCtWDFOnTsX48ePh4uKCoUOHAgCmT5+OSZMmYfbs2ShfvjyaN2+O33//Hd7e3gAy+tVt374du3btQuXKlbFs2TLMmjUrV6+3TZs2GDlyJIYOHYoqVargxIkTmDRpkka+UqVKoUOHDmjZsiWaNWsGPz8/2bQr/fr1w6pVq7B27Vr4+vqiQYMGWLdunVRWIqLcUoisekYTERERkUFgDR8RERGRgWPAR0RERGTgGPARERERGTgGfEREREQGjgEfERERkYFjwEdERERk4BjwERERERk4BnxEREREBo4BHxEREZGBY8BHREREZOAY8BEREREZOAZ8RERERAbu/wApCK6jcvHXIgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_data_label.astype(bool), pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'autism'])\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Result: RWB ANN (256,128,128,256)(RMSprop) lag 256 for model in fold {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: RWB ANN (256,128,128,256)(RMSprop) lag 256 for model in fold 1\n",
      "Precision: 0.846081208687441\n",
      "Recall: 0.9005025125628141\n",
      "Accuracy: 0.8207934336525308\n",
      "F1-score: 0.872444011684518\n",
      "\n",
      "\n",
      "Result: RWB ANN (256,128,128,256)(RMSprop) lag 256 for model in fold 2\n",
      "Precision: 0.8028583438419504\n",
      "Recall: 0.9597989949748744\n",
      "Accuracy: 0.8122435020519836\n",
      "F1-score: 0.8743419546807051\n",
      "\n",
      "\n",
      "Result: RWB ANN (256,128,128,256)(RMSprop) lag 256 for model in fold 3\n",
      "Precision: 0.809322033898305\n",
      "Recall: 0.9597989949748744\n",
      "Accuracy: 0.8187414500683995\n",
      "F1-score: 0.8781609195402298\n",
      "\n",
      "\n",
      "Result: RWB ANN (256,128,128,256)(RMSprop) lag 256 for model in fold 4\n",
      "Precision: 0.7970358172087278\n",
      "Recall: 0.9728643216080402\n",
      "Accuracy: 0.8129274965800274\n",
      "F1-score: 0.8762163385381307\n",
      "\n",
      "\n",
      "Result: RWB ANN (256,128,128,256)(RMSprop) lag 256 for model in fold 5\n",
      "Precision: 0.8182212581344902\n",
      "Recall: 0.9477386934673366\n",
      "Accuracy: 0.8211354309165527\n",
      "F1-score: 0.8782305005820722\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    test_data_label = test_data_label.astype(bool)  \n",
    "\n",
    "    print(f\"Result: RWB ANN (256,128,128,256)(RMSprop) lag 256 for model in fold {i}\")\n",
    "\n",
    "    TP = np.sum((test_data_label == True) & (pred == True))\n",
    "    FP = np.sum((test_data_label == False) & (pred == True))\n",
    "    TN = np.sum((test_data_label == False) & (pred == False))\n",
    "    FN = np.sum((test_data_label == True) & (pred == False))  \n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
