{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, lag, excluded_name=[]):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):        \n",
    "        folder = os.path.join(directory, foldername)\n",
    "        # print(folder)\n",
    "        if str(lag) in folder:\n",
    "            # print(os.listdir(folder))\n",
    "            for name in os.listdir(folder):\n",
    "                if name in excluded_name:\n",
    "                    # print(name)\n",
    "                    continue\n",
    "                filename = os.path.join(folder, name)\n",
    "                # print(filename)\n",
    "                for files in os.listdir(filename):\n",
    "                    rel_path = os.path.join(filename, files)\n",
    "                    # print(rel_path)\n",
    "                    temp_label = folder\n",
    "                    if \"autism\" in temp_label:\n",
    "                        label = 'autism'\n",
    "                    else:\n",
    "                        label = 'normal'\n",
    "\n",
    "                    temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                    rwb = np.load(rel_path)\n",
    "                    rwb.astype(np.float64).reshape(-1,1)\n",
    "                                    \n",
    "                    temp_data.loc[0, \"data\"] = rwb\n",
    "                    temp_data['label'] = label\n",
    "                    data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"autism\": 1, \"normal\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_value(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "    missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "    clean_data = data.drop(index=data.index[missing_indices])\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data, des_path):\n",
    "    if not os.path.exists(des_path):\n",
    "        os.makedirs(des_path)\n",
    "    data.save(des_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, train_split: float):\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "        data['data'],\n",
    "        data[['label', 'label_map']],\n",
    "        train_size=train_split,\n",
    "        stratify=data['label_map']\n",
    "    )\n",
    "\n",
    "    train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "    test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "    train_df[\"data\"] = train_x\n",
    "    train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "    test_df[\"data\"] = test_x\n",
    "    test_df[['label', 'label_map']] = test_y\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    # loading extracted feature & label\n",
    "    # x = get_dataset(path, lag, excluded_name)\n",
    "\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = data[\"label_map\"].values\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list))\n",
    "\n",
    "    # train_size = int(train_split * len(labels_list))  \n",
    "    # test_size = len(labels_list) - train_size  \n",
    "\n",
    "    # train_dataset = dataset.take(train_size)\n",
    "    # test_dataset = dataset.skip(train_size)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/features/rwb/segment_1 seconds\"\n",
    "\n",
    "train_dir = \"datasets/tf_batch/rwb/segment_1 seconds/train\"\n",
    "test_dir = \"datasets/tf_batch/rwb/segment_1 seconds/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15434, 3)\n",
      "(14618, 3)\n",
      "train:  (11694, 3)\n",
      "test:  (2924, 3)\n"
     ]
    }
   ],
   "source": [
    "excluded = [\"zyad\"]\n",
    "train_split = 0.8\n",
    "LAG = [256]\n",
    "\n",
    "for lag in LAG:\n",
    "    data = get_dataset(data_dir, lag, excluded)\n",
    "    print(data.shape)\n",
    "    data = remove_missing_value(data)\n",
    "    print(data.shape)\n",
    "    train_data, test_data = get_train_test(data, train_split)\n",
    "    print(\"train: \", train_data.shape)\n",
    "    print(\"test: \", test_data.shape)\n",
    "    train_batch = get_batch(train_data)\n",
    "    test_batch = get_batch(test_data)\n",
    "    tf.data.Dataset.save(train_batch, f\"{train_dir}_{lag}\")\n",
    "    tf.data.Dataset.save(test_batch, f\"{test_dir}_{lag}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load(\"datasets/features/rwb/segment_1 seconds/normal_256/amer/Amer_segment_1.csv_bispectrum.npy\")\n",
    "\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(96,)))\n",
    "    model.add(layers.Reshape((96, 1)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [256]\n",
    "# folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000\n",
    "log_dirs = [f\"train_logs/logs7/RWB_ANN_512_256_256_512_Adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 378,881\n",
      "Trainable params: 378,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 1.5331 - acc: 0.5964\n",
      "Epoch 1: val_acc improved from -inf to 0.69047, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 1.5182 - acc: 0.5973 - val_loss: 0.6643 - val_acc: 0.6905\n",
      "Epoch 2/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.6690 - acc: 0.6415\n",
      "Epoch 2: val_acc did not improve from 0.69047\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6696 - acc: 0.6432 - val_loss: 0.6432 - val_acc: 0.6905\n",
      "Epoch 3/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6415 - acc: 0.6622\n",
      "Epoch 3: val_acc did not improve from 0.69047\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6411 - acc: 0.6633 - val_loss: 0.6280 - val_acc: 0.6896\n",
      "Epoch 4/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6319 - acc: 0.6703\n",
      "Epoch 4: val_acc improved from 0.69047 to 0.70158, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6320 - acc: 0.6719 - val_loss: 0.6073 - val_acc: 0.7016\n",
      "Epoch 5/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.6801\n",
      "Epoch 5: val_acc improved from 0.70158 to 0.71184, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6175 - acc: 0.6811 - val_loss: 0.5913 - val_acc: 0.7118\n",
      "Epoch 6/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6029 - acc: 0.6916\n",
      "Epoch 6: val_acc improved from 0.71184 to 0.72210, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6023 - acc: 0.6927 - val_loss: 0.5874 - val_acc: 0.7221\n",
      "Epoch 7/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5933 - acc: 0.6980\n",
      "Epoch 7: val_acc improved from 0.72210 to 0.72723, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5931 - acc: 0.6982 - val_loss: 0.5750 - val_acc: 0.7272\n",
      "Epoch 8/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5894 - acc: 0.7059\n",
      "Epoch 8: val_acc did not improve from 0.72723\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5894 - acc: 0.7059 - val_loss: 0.5698 - val_acc: 0.7238\n",
      "Epoch 9/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.7068\n",
      "Epoch 9: val_acc improved from 0.72723 to 0.73236, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5860 - acc: 0.7070 - val_loss: 0.5738 - val_acc: 0.7324\n",
      "Epoch 10/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.7109\n",
      "Epoch 10: val_acc did not improve from 0.73236\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5805 - acc: 0.7116 - val_loss: 0.5750 - val_acc: 0.7217\n",
      "Epoch 11/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.7147\n",
      "Epoch 11: val_acc improved from 0.73236 to 0.73749, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5761 - acc: 0.7149 - val_loss: 0.5542 - val_acc: 0.7375\n",
      "Epoch 12/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5699 - acc: 0.7164\n",
      "Epoch 12: val_acc did not improve from 0.73749\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5700 - acc: 0.7164 - val_loss: 0.5611 - val_acc: 0.7375\n",
      "Epoch 13/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5617 - acc: 0.7196\n",
      "Epoch 13: val_acc improved from 0.73749 to 0.74391, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5612 - acc: 0.7199 - val_loss: 0.5626 - val_acc: 0.7439\n",
      "Epoch 14/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.7198\n",
      "Epoch 14: val_acc improved from 0.74391 to 0.74434, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5594 - acc: 0.7201 - val_loss: 0.5468 - val_acc: 0.7443\n",
      "Epoch 15/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5522 - acc: 0.7197\n",
      "Epoch 15: val_acc did not improve from 0.74434\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5524 - acc: 0.7198 - val_loss: 0.5478 - val_acc: 0.7426\n",
      "Epoch 16/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7224\n",
      "Epoch 16: val_acc improved from 0.74434 to 0.74605, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5506 - acc: 0.7228 - val_loss: 0.5372 - val_acc: 0.7460\n",
      "Epoch 17/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.7293\n",
      "Epoch 17: val_acc did not improve from 0.74605\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5423 - acc: 0.7297 - val_loss: 0.5417 - val_acc: 0.7452\n",
      "Epoch 18/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.7270\n",
      "Epoch 18: val_acc did not improve from 0.74605\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5451 - acc: 0.7278 - val_loss: 0.5350 - val_acc: 0.7413\n",
      "Epoch 19/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.7309\n",
      "Epoch 19: val_acc did not improve from 0.74605\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5375 - acc: 0.7311 - val_loss: 0.5443 - val_acc: 0.7383\n",
      "Epoch 20/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.7343\n",
      "Epoch 20: val_acc improved from 0.74605 to 0.75203, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5307 - acc: 0.7349 - val_loss: 0.5289 - val_acc: 0.7520\n",
      "Epoch 21/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7322\n",
      "Epoch 21: val_acc did not improve from 0.75203\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5298 - acc: 0.7324 - val_loss: 0.5230 - val_acc: 0.7443\n",
      "Epoch 22/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.7342\n",
      "Epoch 22: val_acc improved from 0.75203 to 0.75331, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5268 - acc: 0.7343 - val_loss: 0.5249 - val_acc: 0.7533\n",
      "Epoch 23/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5185 - acc: 0.7453\n",
      "Epoch 23: val_acc did not improve from 0.75331\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5192 - acc: 0.7448 - val_loss: 0.5218 - val_acc: 0.7533\n",
      "Epoch 24/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.7366\n",
      "Epoch 24: val_acc did not improve from 0.75331\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5262 - acc: 0.7366 - val_loss: 0.5137 - val_acc: 0.7529\n",
      "Epoch 25/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7421\n",
      "Epoch 25: val_acc improved from 0.75331 to 0.76528, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5141 - acc: 0.7428 - val_loss: 0.5198 - val_acc: 0.7653\n",
      "Epoch 26/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.7395\n",
      "Epoch 26: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5244 - acc: 0.7395 - val_loss: 0.5343 - val_acc: 0.7482\n",
      "Epoch 27/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.7453\n",
      "Epoch 27: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5166 - acc: 0.7454 - val_loss: 0.5118 - val_acc: 0.7473\n",
      "Epoch 28/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.7423\n",
      "Epoch 28: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5118 - acc: 0.7424 - val_loss: 0.5098 - val_acc: 0.7614\n",
      "Epoch 29/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.7442\n",
      "Epoch 29: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5116 - acc: 0.7445 - val_loss: 0.5055 - val_acc: 0.7602\n",
      "Epoch 30/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5017 - acc: 0.7496\n",
      "Epoch 30: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5017 - acc: 0.7489 - val_loss: 0.5103 - val_acc: 0.7482\n",
      "Epoch 31/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5082 - acc: 0.7474\n",
      "Epoch 31: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5082 - acc: 0.7474 - val_loss: 0.4967 - val_acc: 0.7640\n",
      "Epoch 32/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.7492\n",
      "Epoch 32: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4993 - acc: 0.7490 - val_loss: 0.4987 - val_acc: 0.7580\n",
      "Epoch 33/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.7518\n",
      "Epoch 33: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4964 - acc: 0.7519 - val_loss: 0.4981 - val_acc: 0.7636\n",
      "Epoch 34/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.7585\n",
      "Epoch 34: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4893 - acc: 0.7585 - val_loss: 0.4946 - val_acc: 0.7640\n",
      "Epoch 35/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.7520\n",
      "Epoch 35: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4964 - acc: 0.7523 - val_loss: 0.4978 - val_acc: 0.7623\n",
      "Epoch 36/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.7572\n",
      "Epoch 36: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4870 - acc: 0.7573 - val_loss: 0.4873 - val_acc: 0.7653\n",
      "Epoch 37/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4909 - acc: 0.7570\n",
      "Epoch 37: val_acc did not improve from 0.76528\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4919 - acc: 0.7556 - val_loss: 0.4890 - val_acc: 0.7623\n",
      "Epoch 38/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4911 - acc: 0.7539\n",
      "Epoch 38: val_acc improved from 0.76528 to 0.76571, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4908 - acc: 0.7542 - val_loss: 0.4886 - val_acc: 0.7657\n",
      "Epoch 39/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.7581\n",
      "Epoch 39: val_acc did not improve from 0.76571\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4901 - acc: 0.7580 - val_loss: 0.5091 - val_acc: 0.7563\n",
      "Epoch 40/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.7639\n",
      "Epoch 40: val_acc improved from 0.76571 to 0.76828, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4848 - acc: 0.7641 - val_loss: 0.4861 - val_acc: 0.7683\n",
      "Epoch 41/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.7640\n",
      "Epoch 41: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4824 - acc: 0.7633 - val_loss: 0.4992 - val_acc: 0.7580\n",
      "Epoch 42/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.7637\n",
      "Epoch 42: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4845 - acc: 0.7637 - val_loss: 0.4957 - val_acc: 0.7657\n",
      "Epoch 43/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4781 - acc: 0.7662\n",
      "Epoch 43: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4780 - acc: 0.7660 - val_loss: 0.4973 - val_acc: 0.7614\n",
      "Epoch 44/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4730 - acc: 0.7649\n",
      "Epoch 44: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4724 - acc: 0.7654 - val_loss: 0.4980 - val_acc: 0.7670\n",
      "Epoch 45/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.7643\n",
      "Epoch 45: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4797 - acc: 0.7644 - val_loss: 0.4947 - val_acc: 0.7619\n",
      "Epoch 46/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4778 - acc: 0.7636\n",
      "Epoch 46: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4771 - acc: 0.7637 - val_loss: 0.4924 - val_acc: 0.7593\n",
      "Epoch 47/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4782 - acc: 0.7680\n",
      "Epoch 47: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4795 - acc: 0.7679 - val_loss: 0.4984 - val_acc: 0.7627\n",
      "Epoch 48/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.7686\n",
      "Epoch 48: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4798 - acc: 0.7683 - val_loss: 0.4931 - val_acc: 0.7619\n",
      "Epoch 49/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4759 - acc: 0.7659\n",
      "Epoch 49: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4763 - acc: 0.7653 - val_loss: 0.4937 - val_acc: 0.7670\n",
      "Epoch 50/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.7647\n",
      "Epoch 50: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4774 - acc: 0.7645 - val_loss: 0.4858 - val_acc: 0.7619\n",
      "Epoch 51/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.7671\n",
      "Epoch 51: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4691 - acc: 0.7674 - val_loss: 0.4891 - val_acc: 0.7614\n",
      "Epoch 52/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.7673\n",
      "Epoch 52: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4825 - acc: 0.7678 - val_loss: 0.4980 - val_acc: 0.7619\n",
      "Epoch 53/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.7727\n",
      "Epoch 53: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4649 - acc: 0.7726 - val_loss: 0.4809 - val_acc: 0.7657\n",
      "Epoch 54/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.7705\n",
      "Epoch 54: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4661 - acc: 0.7707 - val_loss: 0.4902 - val_acc: 0.7670\n",
      "Epoch 55/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.7717\n",
      "Epoch 55: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4620 - acc: 0.7718 - val_loss: 0.4917 - val_acc: 0.7602\n",
      "Epoch 56/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4675 - acc: 0.7708\n",
      "Epoch 56: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4675 - acc: 0.7707 - val_loss: 0.4890 - val_acc: 0.7610\n",
      "Epoch 57/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4647 - acc: 0.7745\n",
      "Epoch 57: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4649 - acc: 0.7740 - val_loss: 0.4986 - val_acc: 0.7674\n",
      "Epoch 58/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4646 - acc: 0.7736\n",
      "Epoch 58: val_acc did not improve from 0.76828\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4652 - acc: 0.7732 - val_loss: 0.4873 - val_acc: 0.7644\n",
      "Epoch 59/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.7704\n",
      "Epoch 59: val_acc improved from 0.76828 to 0.77469, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4663 - acc: 0.7706 - val_loss: 0.4822 - val_acc: 0.7747\n",
      "Epoch 60/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.7732\n",
      "Epoch 60: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4645 - acc: 0.7728 - val_loss: 0.4793 - val_acc: 0.7636\n",
      "Epoch 61/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.7758\n",
      "Epoch 61: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4666 - acc: 0.7758 - val_loss: 0.4834 - val_acc: 0.7691\n",
      "Epoch 62/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4677 - acc: 0.7682\n",
      "Epoch 62: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4687 - acc: 0.7676 - val_loss: 0.4810 - val_acc: 0.7721\n",
      "Epoch 63/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4669 - acc: 0.7727\n",
      "Epoch 63: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4666 - acc: 0.7728 - val_loss: 0.4712 - val_acc: 0.7717\n",
      "Epoch 64/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4607 - acc: 0.7782\n",
      "Epoch 64: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4612 - acc: 0.7776 - val_loss: 0.4797 - val_acc: 0.7717\n",
      "Epoch 65/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4588 - acc: 0.7794\n",
      "Epoch 65: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4580 - acc: 0.7799 - val_loss: 0.4848 - val_acc: 0.7704\n",
      "Epoch 66/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.7793\n",
      "Epoch 66: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4567 - acc: 0.7792 - val_loss: 0.4878 - val_acc: 0.7614\n",
      "Epoch 67/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4624 - acc: 0.7775\n",
      "Epoch 67: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4620 - acc: 0.7771 - val_loss: 0.4803 - val_acc: 0.7691\n",
      "Epoch 68/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.7786\n",
      "Epoch 68: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4566 - acc: 0.7784 - val_loss: 0.4895 - val_acc: 0.7730\n",
      "Epoch 69/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.4569 - acc: 0.7776\n",
      "Epoch 69: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4586 - acc: 0.7759 - val_loss: 0.4782 - val_acc: 0.7678\n",
      "Epoch 70/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4590 - acc: 0.7780\n",
      "Epoch 70: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4588 - acc: 0.7786 - val_loss: 0.4836 - val_acc: 0.7713\n",
      "Epoch 71/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4521 - acc: 0.7811\n",
      "Epoch 71: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4520 - acc: 0.7811 - val_loss: 0.4884 - val_acc: 0.7708\n",
      "Epoch 72/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4523 - acc: 0.7798\n",
      "Epoch 72: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4541 - acc: 0.7787 - val_loss: 0.4759 - val_acc: 0.7708\n",
      "Epoch 73/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.7818\n",
      "Epoch 73: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4564 - acc: 0.7818 - val_loss: 0.4780 - val_acc: 0.7734\n",
      "Epoch 74/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.7875\n",
      "Epoch 74: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4439 - acc: 0.7874 - val_loss: 0.4947 - val_acc: 0.7704\n",
      "Epoch 75/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4407 - acc: 0.7888\n",
      "Epoch 75: val_acc did not improve from 0.77469\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4423 - acc: 0.7877 - val_loss: 0.4930 - val_acc: 0.7708\n",
      "Epoch 76/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4560 - acc: 0.7814\n",
      "Epoch 76: val_acc improved from 0.77469 to 0.77640, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4568 - acc: 0.7808 - val_loss: 0.4920 - val_acc: 0.7764\n",
      "Epoch 77/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.7797\n",
      "Epoch 77: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4489 - acc: 0.7800 - val_loss: 0.4955 - val_acc: 0.7734\n",
      "Epoch 78/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.7832\n",
      "Epoch 78: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4538 - acc: 0.7833 - val_loss: 0.4861 - val_acc: 0.7700\n",
      "Epoch 79/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.7834\n",
      "Epoch 79: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4442 - acc: 0.7835 - val_loss: 0.4882 - val_acc: 0.7760\n",
      "Epoch 80/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.7798\n",
      "Epoch 80: val_acc improved from 0.77640 to 0.78324, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4526 - acc: 0.7801 - val_loss: 0.4819 - val_acc: 0.7832\n",
      "Epoch 81/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.7834\n",
      "Epoch 81: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4460 - acc: 0.7834 - val_loss: 0.4756 - val_acc: 0.7777\n",
      "Epoch 82/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.7809\n",
      "Epoch 82: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4468 - acc: 0.7808 - val_loss: 0.4935 - val_acc: 0.7687\n",
      "Epoch 83/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4463 - acc: 0.7849\n",
      "Epoch 83: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4463 - acc: 0.7849 - val_loss: 0.4948 - val_acc: 0.7781\n",
      "Epoch 84/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4425 - acc: 0.7828\n",
      "Epoch 84: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4430 - acc: 0.7823 - val_loss: 0.4858 - val_acc: 0.7764\n",
      "Epoch 85/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.7818\n",
      "Epoch 85: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4381 - acc: 0.7814 - val_loss: 0.4943 - val_acc: 0.7738\n",
      "Epoch 86/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.4411 - acc: 0.7898\n",
      "Epoch 86: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4419 - acc: 0.7893 - val_loss: 0.4748 - val_acc: 0.7738\n",
      "Epoch 87/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4526 - acc: 0.7874\n",
      "Epoch 87: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4521 - acc: 0.7871 - val_loss: 0.4788 - val_acc: 0.7696\n",
      "Epoch 88/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.7802\n",
      "Epoch 88: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4465 - acc: 0.7805 - val_loss: 0.4846 - val_acc: 0.7751\n",
      "Epoch 89/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.7855\n",
      "Epoch 89: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4410 - acc: 0.7857 - val_loss: 0.4736 - val_acc: 0.7726\n",
      "Epoch 90/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.7815\n",
      "Epoch 90: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4500 - acc: 0.7816 - val_loss: 0.4792 - val_acc: 0.7785\n",
      "Epoch 91/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.7899\n",
      "Epoch 91: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4382 - acc: 0.7894 - val_loss: 0.4704 - val_acc: 0.7798\n",
      "Epoch 92/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4427 - acc: 0.7852\n",
      "Epoch 92: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4427 - acc: 0.7852 - val_loss: 0.4790 - val_acc: 0.7760\n",
      "Epoch 93/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4368 - acc: 0.7866\n",
      "Epoch 93: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4387 - acc: 0.7854 - val_loss: 0.4806 - val_acc: 0.7743\n",
      "Epoch 94/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.7873\n",
      "Epoch 94: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4394 - acc: 0.7879 - val_loss: 0.4797 - val_acc: 0.7730\n",
      "Epoch 95/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.7896\n",
      "Epoch 95: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4417 - acc: 0.7888 - val_loss: 0.4765 - val_acc: 0.7760\n",
      "Epoch 96/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.7911\n",
      "Epoch 96: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4366 - acc: 0.7910 - val_loss: 0.4826 - val_acc: 0.7623\n",
      "Epoch 97/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4346 - acc: 0.7896\n",
      "Epoch 97: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4352 - acc: 0.7889 - val_loss: 0.4806 - val_acc: 0.7734\n",
      "Epoch 98/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.7868\n",
      "Epoch 98: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4364 - acc: 0.7870 - val_loss: 0.4677 - val_acc: 0.7773\n",
      "Epoch 99/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.7908\n",
      "Epoch 99: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4363 - acc: 0.7907 - val_loss: 0.4847 - val_acc: 0.7627\n",
      "Epoch 100/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.7788\n",
      "Epoch 100: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4417 - acc: 0.7790 - val_loss: 0.4730 - val_acc: 0.7717\n",
      "Epoch 101/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.7897\n",
      "Epoch 101: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4326 - acc: 0.7898 - val_loss: 0.4754 - val_acc: 0.7794\n",
      "Epoch 102/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4426 - acc: 0.7975\n",
      "Epoch 102: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4429 - acc: 0.7966 - val_loss: 0.4888 - val_acc: 0.7606\n",
      "Epoch 103/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.7875\n",
      "Epoch 103: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4362 - acc: 0.7870 - val_loss: 0.4844 - val_acc: 0.7661\n",
      "Epoch 104/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.7853\n",
      "Epoch 104: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4479 - acc: 0.7851 - val_loss: 0.4801 - val_acc: 0.7657\n",
      "Epoch 105/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.7848\n",
      "Epoch 105: val_acc did not improve from 0.78324\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4473 - acc: 0.7849 - val_loss: 0.4853 - val_acc: 0.7734\n",
      "Epoch 106/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4339 - acc: 0.7911\n",
      "Epoch 106: val_acc improved from 0.78324 to 0.78709, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4336 - acc: 0.7906 - val_loss: 0.4674 - val_acc: 0.7871\n",
      "Epoch 107/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.7871\n",
      "Epoch 107: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4359 - acc: 0.7878 - val_loss: 0.4771 - val_acc: 0.7760\n",
      "Epoch 108/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.7914\n",
      "Epoch 108: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4308 - acc: 0.7914 - val_loss: 0.4728 - val_acc: 0.7781\n",
      "Epoch 109/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4411 - acc: 0.7935\n",
      "Epoch 109: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4411 - acc: 0.7935 - val_loss: 0.4835 - val_acc: 0.7670\n",
      "Epoch 110/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4289 - acc: 0.7942\n",
      "Epoch 110: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4289 - acc: 0.7942 - val_loss: 0.4806 - val_acc: 0.7683\n",
      "Epoch 111/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.7982\n",
      "Epoch 111: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4266 - acc: 0.7981 - val_loss: 0.4747 - val_acc: 0.7815\n",
      "Epoch 112/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4291 - acc: 0.7931\n",
      "Epoch 112: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4296 - acc: 0.7923 - val_loss: 0.4793 - val_acc: 0.7678\n",
      "Epoch 113/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4412 - acc: 0.7927\n",
      "Epoch 113: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4417 - acc: 0.7923 - val_loss: 0.4874 - val_acc: 0.7704\n",
      "Epoch 114/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4427 - acc: 0.7865\n",
      "Epoch 114: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4414 - acc: 0.7864 - val_loss: 0.4905 - val_acc: 0.7670\n",
      "Epoch 115/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4244 - acc: 0.7949\n",
      "Epoch 115: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4240 - acc: 0.7949 - val_loss: 0.4760 - val_acc: 0.7854\n",
      "Epoch 116/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.7936\n",
      "Epoch 116: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4313 - acc: 0.7937 - val_loss: 0.4836 - val_acc: 0.7828\n",
      "Epoch 117/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.7897\n",
      "Epoch 117: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4328 - acc: 0.7900 - val_loss: 0.4786 - val_acc: 0.7726\n",
      "Epoch 118/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.7893\n",
      "Epoch 118: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4337 - acc: 0.7890 - val_loss: 0.4805 - val_acc: 0.7700\n",
      "Epoch 119/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.7951\n",
      "Epoch 119: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4262 - acc: 0.7952 - val_loss: 0.4771 - val_acc: 0.7781\n",
      "Epoch 120/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.7879\n",
      "Epoch 120: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4340 - acc: 0.7880 - val_loss: 0.4874 - val_acc: 0.7649\n",
      "Epoch 121/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.7949\n",
      "Epoch 121: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4237 - acc: 0.7949 - val_loss: 0.4855 - val_acc: 0.7798\n",
      "Epoch 122/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4332 - acc: 0.7920\n",
      "Epoch 122: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4332 - acc: 0.7920 - val_loss: 0.4861 - val_acc: 0.7768\n",
      "Epoch 123/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.7938\n",
      "Epoch 123: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4271 - acc: 0.7942 - val_loss: 0.4770 - val_acc: 0.7751\n",
      "Epoch 124/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.7897\n",
      "Epoch 124: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4349 - acc: 0.7898 - val_loss: 0.4906 - val_acc: 0.7751\n",
      "Epoch 125/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4268 - acc: 0.7942\n",
      "Epoch 125: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4266 - acc: 0.7942 - val_loss: 0.4853 - val_acc: 0.7691\n",
      "Epoch 126/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.7914\n",
      "Epoch 126: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4368 - acc: 0.7916 - val_loss: 0.4881 - val_acc: 0.7649\n",
      "Epoch 127/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4326 - acc: 0.7916\n",
      "Epoch 127: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4326 - acc: 0.7916 - val_loss: 0.4803 - val_acc: 0.7734\n",
      "Epoch 128/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.7956\n",
      "Epoch 128: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4287 - acc: 0.7956 - val_loss: 0.4778 - val_acc: 0.7708\n",
      "Epoch 129/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4256 - acc: 0.7961\n",
      "Epoch 129: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4251 - acc: 0.7963 - val_loss: 0.4699 - val_acc: 0.7850\n",
      "Epoch 130/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.7962\n",
      "Epoch 130: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4284 - acc: 0.7959 - val_loss: 0.4988 - val_acc: 0.7807\n",
      "Epoch 131/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.7926\n",
      "Epoch 131: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4362 - acc: 0.7927 - val_loss: 0.4882 - val_acc: 0.7708\n",
      "Epoch 132/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.7951\n",
      "Epoch 132: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4270 - acc: 0.7952 - val_loss: 0.4783 - val_acc: 0.7696\n",
      "Epoch 133/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.7934\n",
      "Epoch 133: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4849 - val_acc: 0.7704\n",
      "Epoch 134/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4268 - acc: 0.7967\n",
      "Epoch 134: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4268 - acc: 0.7967 - val_loss: 0.4817 - val_acc: 0.7696\n",
      "Epoch 135/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.7999\n",
      "Epoch 135: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4263 - acc: 0.7999 - val_loss: 0.5005 - val_acc: 0.7781\n",
      "Epoch 136/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.7978\n",
      "Epoch 136: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4272 - acc: 0.7973 - val_loss: 0.4803 - val_acc: 0.7640\n",
      "Epoch 137/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.7917\n",
      "Epoch 137: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4242 - acc: 0.7918 - val_loss: 0.4796 - val_acc: 0.7721\n",
      "Epoch 138/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4200 - acc: 0.7990\n",
      "Epoch 138: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4200 - acc: 0.7991 - val_loss: 0.4858 - val_acc: 0.7777\n",
      "Epoch 139/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4202 - acc: 0.7962\n",
      "Epoch 139: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4207 - acc: 0.7956 - val_loss: 0.4839 - val_acc: 0.7670\n",
      "Epoch 140/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.7948\n",
      "Epoch 140: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4329 - acc: 0.7950 - val_loss: 0.4801 - val_acc: 0.7811\n",
      "Epoch 141/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8023\n",
      "Epoch 141: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4195 - acc: 0.8022 - val_loss: 0.4850 - val_acc: 0.7614\n",
      "Epoch 142/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4237 - acc: 0.8008\n",
      "Epoch 142: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4268 - acc: 0.8001 - val_loss: 0.4841 - val_acc: 0.7640\n",
      "Epoch 143/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.7956\n",
      "Epoch 143: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4278 - acc: 0.7957 - val_loss: 0.4866 - val_acc: 0.7661\n",
      "Epoch 144/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.7971\n",
      "Epoch 144: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4219 - acc: 0.7971 - val_loss: 0.4821 - val_acc: 0.7743\n",
      "Epoch 145/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4198 - acc: 0.7982\n",
      "Epoch 145: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4204 - acc: 0.7983 - val_loss: 0.4834 - val_acc: 0.7768\n",
      "Epoch 146/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.7954\n",
      "Epoch 146: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4172 - acc: 0.7955 - val_loss: 0.4728 - val_acc: 0.7717\n",
      "Epoch 147/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.7956\n",
      "Epoch 147: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4267 - acc: 0.7950 - val_loss: 0.4819 - val_acc: 0.7734\n",
      "Epoch 148/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.7951\n",
      "Epoch 148: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4271 - acc: 0.7949 - val_loss: 0.4787 - val_acc: 0.7760\n",
      "Epoch 149/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4226 - acc: 0.7968\n",
      "Epoch 149: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4231 - acc: 0.7958 - val_loss: 0.4821 - val_acc: 0.7708\n",
      "Epoch 150/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.7962\n",
      "Epoch 150: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4315 - acc: 0.7970 - val_loss: 0.4959 - val_acc: 0.7824\n",
      "Epoch 151/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.7924\n",
      "Epoch 151: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4376 - acc: 0.7924 - val_loss: 0.4781 - val_acc: 0.7721\n",
      "Epoch 152/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4182 - acc: 0.7953\n",
      "Epoch 152: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4209 - acc: 0.7954 - val_loss: 0.4867 - val_acc: 0.7657\n",
      "Epoch 153/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8000\n",
      "Epoch 153: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4225 - acc: 0.8003 - val_loss: 0.4756 - val_acc: 0.7798\n",
      "Epoch 154/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8023\n",
      "Epoch 154: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4153 - acc: 0.8017 - val_loss: 0.4819 - val_acc: 0.7730\n",
      "Epoch 155/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4224 - acc: 0.7973\n",
      "Epoch 155: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4224 - acc: 0.7973 - val_loss: 0.4874 - val_acc: 0.7687\n",
      "Epoch 156/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.7986\n",
      "Epoch 156: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4231 - acc: 0.7983 - val_loss: 0.4886 - val_acc: 0.7653\n",
      "Epoch 157/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.7944\n",
      "Epoch 157: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4257 - acc: 0.7950 - val_loss: 0.4779 - val_acc: 0.7691\n",
      "Epoch 158/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8019\n",
      "Epoch 158: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4172 - acc: 0.8017 - val_loss: 0.4801 - val_acc: 0.7627\n",
      "Epoch 159/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4148 - acc: 0.7971\n",
      "Epoch 159: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4133 - acc: 0.7981 - val_loss: 0.4920 - val_acc: 0.7781\n",
      "Epoch 160/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.7997\n",
      "Epoch 160: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4182 - acc: 0.7997 - val_loss: 0.4844 - val_acc: 0.7785\n",
      "Epoch 161/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.7931\n",
      "Epoch 161: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4299 - acc: 0.7929 - val_loss: 0.4941 - val_acc: 0.7666\n",
      "Epoch 162/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.7960\n",
      "Epoch 162: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4305 - acc: 0.7959 - val_loss: 0.4839 - val_acc: 0.7717\n",
      "Epoch 163/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.7958\n",
      "Epoch 163: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4270 - acc: 0.7955 - val_loss: 0.4758 - val_acc: 0.7768\n",
      "Epoch 164/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.7964\n",
      "Epoch 164: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4228 - acc: 0.7966 - val_loss: 0.4788 - val_acc: 0.7832\n",
      "Epoch 165/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4234 - acc: 0.7964\n",
      "Epoch 165: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4239 - acc: 0.7957 - val_loss: 0.4834 - val_acc: 0.7636\n",
      "Epoch 166/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.7972\n",
      "Epoch 166: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4243 - acc: 0.7970 - val_loss: 0.4902 - val_acc: 0.7704\n",
      "Epoch 167/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8005\n",
      "Epoch 167: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4086 - acc: 0.8011 - val_loss: 0.4781 - val_acc: 0.7832\n",
      "Epoch 168/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4156 - acc: 0.8020\n",
      "Epoch 168: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4164 - acc: 0.8016 - val_loss: 0.4752 - val_acc: 0.7798\n",
      "Epoch 169/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8027\n",
      "Epoch 169: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4088 - acc: 0.8027 - val_loss: 0.4838 - val_acc: 0.7764\n",
      "Epoch 170/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8007\n",
      "Epoch 170: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4261 - acc: 0.8009 - val_loss: 0.4869 - val_acc: 0.7683\n",
      "Epoch 171/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4194 - acc: 0.7984\n",
      "Epoch 171: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4195 - acc: 0.7982 - val_loss: 0.4791 - val_acc: 0.7768\n",
      "Epoch 172/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.7990\n",
      "Epoch 172: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4239 - acc: 0.7986 - val_loss: 0.4719 - val_acc: 0.7708\n",
      "Epoch 173/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8016\n",
      "Epoch 173: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4214 - acc: 0.8012 - val_loss: 0.4729 - val_acc: 0.7794\n",
      "Epoch 174/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4162 - acc: 0.8022\n",
      "Epoch 174: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4164 - acc: 0.8024 - val_loss: 0.4770 - val_acc: 0.7717\n",
      "Epoch 175/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4075 - acc: 0.8031\n",
      "Epoch 175: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4077 - acc: 0.8033 - val_loss: 0.4713 - val_acc: 0.7824\n",
      "Epoch 176/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8025\n",
      "Epoch 176: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4074 - acc: 0.8025 - val_loss: 0.4789 - val_acc: 0.7867\n",
      "Epoch 177/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4158 - acc: 0.8054\n",
      "Epoch 177: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4169 - acc: 0.8041 - val_loss: 0.4804 - val_acc: 0.7845\n",
      "Epoch 178/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.7998\n",
      "Epoch 178: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4127 - acc: 0.8000 - val_loss: 0.4853 - val_acc: 0.7691\n",
      "Epoch 179/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4180 - acc: 0.7999\n",
      "Epoch 179: val_acc did not improve from 0.78709\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4180 - acc: 0.7994 - val_loss: 0.4934 - val_acc: 0.7777\n",
      "Epoch 180/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4076 - acc: 0.8037\n",
      "Epoch 180: val_acc improved from 0.78709 to 0.79179, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4076 - acc: 0.8037 - val_loss: 0.4706 - val_acc: 0.7918\n",
      "Epoch 181/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8027\n",
      "Epoch 181: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4188 - acc: 0.8027 - val_loss: 0.4746 - val_acc: 0.7871\n",
      "Epoch 182/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8000\n",
      "Epoch 182: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4103 - acc: 0.8003 - val_loss: 0.4804 - val_acc: 0.7760\n",
      "Epoch 183/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8021\n",
      "Epoch 183: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4177 - acc: 0.8022 - val_loss: 0.4747 - val_acc: 0.7773\n",
      "Epoch 184/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.7992\n",
      "Epoch 184: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4215 - acc: 0.7989 - val_loss: 0.4821 - val_acc: 0.7657\n",
      "Epoch 185/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.7994\n",
      "Epoch 185: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4180 - acc: 0.7988 - val_loss: 0.4861 - val_acc: 0.7606\n",
      "Epoch 186/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4137 - acc: 0.8045\n",
      "Epoch 186: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4158 - acc: 0.8042 - val_loss: 0.4791 - val_acc: 0.7760\n",
      "Epoch 187/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8028\n",
      "Epoch 187: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4121 - acc: 0.8028 - val_loss: 0.4886 - val_acc: 0.7820\n",
      "Epoch 188/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.7974\n",
      "Epoch 188: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4239 - acc: 0.7970 - val_loss: 0.4705 - val_acc: 0.7888\n",
      "Epoch 189/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8017\n",
      "Epoch 189: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4150 - acc: 0.8018 - val_loss: 0.4711 - val_acc: 0.7892\n",
      "Epoch 190/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4126 - acc: 0.8034\n",
      "Epoch 190: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4128 - acc: 0.8033 - val_loss: 0.4809 - val_acc: 0.7768\n",
      "Epoch 191/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4111 - acc: 0.8023\n",
      "Epoch 191: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4140 - acc: 0.8013 - val_loss: 0.4888 - val_acc: 0.7802\n",
      "Epoch 192/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8059\n",
      "Epoch 192: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4125 - acc: 0.8049 - val_loss: 0.4810 - val_acc: 0.7627\n",
      "Epoch 193/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8048\n",
      "Epoch 193: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4074 - acc: 0.8053 - val_loss: 0.4799 - val_acc: 0.7892\n",
      "Epoch 194/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.7968\n",
      "Epoch 194: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4154 - acc: 0.7970 - val_loss: 0.4853 - val_acc: 0.7631\n",
      "Epoch 195/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.7997\n",
      "Epoch 195: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4101 - acc: 0.7998 - val_loss: 0.4903 - val_acc: 0.7700\n",
      "Epoch 196/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8001\n",
      "Epoch 196: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4239 - acc: 0.8006 - val_loss: 0.5008 - val_acc: 0.7653\n",
      "Epoch 197/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8001\n",
      "Epoch 197: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4168 - acc: 0.8001 - val_loss: 0.4977 - val_acc: 0.7747\n",
      "Epoch 198/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4088 - acc: 0.7994\n",
      "Epoch 198: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4088 - acc: 0.7994 - val_loss: 0.4883 - val_acc: 0.7760\n",
      "Epoch 199/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4094 - acc: 0.8015\n",
      "Epoch 199: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4099 - acc: 0.8007 - val_loss: 0.4807 - val_acc: 0.7841\n",
      "Epoch 200/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8060\n",
      "Epoch 200: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4033 - acc: 0.8061 - val_loss: 0.4981 - val_acc: 0.7738\n",
      "Epoch 201/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8012\n",
      "Epoch 201: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4236 - acc: 0.8016 - val_loss: 0.4777 - val_acc: 0.7875\n",
      "Epoch 202/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8001\n",
      "Epoch 202: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4238 - acc: 0.7993 - val_loss: 0.4806 - val_acc: 0.7811\n",
      "Epoch 203/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4232 - acc: 0.8032\n",
      "Epoch 203: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4229 - acc: 0.8025 - val_loss: 0.4738 - val_acc: 0.7760\n",
      "Epoch 204/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4167 - acc: 0.8026\n",
      "Epoch 204: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4170 - acc: 0.8022 - val_loss: 0.4835 - val_acc: 0.7798\n",
      "Epoch 205/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8018\n",
      "Epoch 205: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4090 - acc: 0.8014 - val_loss: 0.4902 - val_acc: 0.7721\n",
      "Epoch 206/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4192 - acc: 0.8019\n",
      "Epoch 206: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4192 - acc: 0.8019 - val_loss: 0.4778 - val_acc: 0.7879\n",
      "Epoch 207/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4098 - acc: 0.8005\n",
      "Epoch 207: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4098 - acc: 0.8005 - val_loss: 0.4817 - val_acc: 0.7837\n",
      "Epoch 208/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4182 - acc: 0.8002\n",
      "Epoch 208: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4182 - acc: 0.8002 - val_loss: 0.4714 - val_acc: 0.7781\n",
      "Epoch 209/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8056\n",
      "Epoch 209: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4023 - acc: 0.8061 - val_loss: 0.4759 - val_acc: 0.7760\n",
      "Epoch 210/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4172 - acc: 0.8031\n",
      "Epoch 210: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4172 - acc: 0.8031 - val_loss: 0.4886 - val_acc: 0.7811\n",
      "Epoch 211/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4108 - acc: 0.8060\n",
      "Epoch 211: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4118 - acc: 0.8049 - val_loss: 0.4918 - val_acc: 0.7747\n",
      "Epoch 212/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4085 - acc: 0.8052\n",
      "Epoch 212: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4081 - acc: 0.8051 - val_loss: 0.4876 - val_acc: 0.7798\n",
      "Epoch 213/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8005\n",
      "Epoch 213: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4161 - acc: 0.8006 - val_loss: 0.4880 - val_acc: 0.7721\n",
      "Epoch 214/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8032\n",
      "Epoch 214: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4146 - acc: 0.8026 - val_loss: 0.4825 - val_acc: 0.7755\n",
      "Epoch 215/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.7988\n",
      "Epoch 215: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4082 - acc: 0.7989 - val_loss: 0.4768 - val_acc: 0.7760\n",
      "Epoch 216/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8038\n",
      "Epoch 216: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4081 - acc: 0.8036 - val_loss: 0.4765 - val_acc: 0.7794\n",
      "Epoch 217/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8025\n",
      "Epoch 217: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4128 - acc: 0.8025 - val_loss: 0.4773 - val_acc: 0.7845\n",
      "Epoch 218/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4023 - acc: 0.8057\n",
      "Epoch 218: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4030 - acc: 0.8050 - val_loss: 0.4824 - val_acc: 0.7670\n",
      "Epoch 219/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8080\n",
      "Epoch 219: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4046 - acc: 0.8078 - val_loss: 0.4739 - val_acc: 0.7841\n",
      "Epoch 220/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8030\n",
      "Epoch 220: val_acc did not improve from 0.79179\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4135 - acc: 0.8029 - val_loss: 0.4822 - val_acc: 0.7828\n",
      "Epoch 221/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8037\n",
      "Epoch 221: val_acc improved from 0.79179 to 0.79222, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4016 - acc: 0.8037 - val_loss: 0.4725 - val_acc: 0.7922\n",
      "Epoch 222/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.3921 - acc: 0.8112\n",
      "Epoch 222: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3946 - acc: 0.8099 - val_loss: 0.4862 - val_acc: 0.7708\n",
      "Epoch 223/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4018 - acc: 0.8063\n",
      "Epoch 223: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4014 - acc: 0.8060 - val_loss: 0.4874 - val_acc: 0.7837\n",
      "Epoch 224/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8079\n",
      "Epoch 224: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4041 - acc: 0.8081 - val_loss: 0.4860 - val_acc: 0.7785\n",
      "Epoch 225/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4098 - acc: 0.7962\n",
      "Epoch 225: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4098 - acc: 0.7962 - val_loss: 0.4847 - val_acc: 0.7862\n",
      "Epoch 226/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8044\n",
      "Epoch 226: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4236 - acc: 0.8043 - val_loss: 0.4901 - val_acc: 0.7751\n",
      "Epoch 227/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4106 - acc: 0.8101\n",
      "Epoch 227: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4121 - acc: 0.8088 - val_loss: 0.4740 - val_acc: 0.7871\n",
      "Epoch 228/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.7996\n",
      "Epoch 228: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4210 - acc: 0.7999 - val_loss: 0.4811 - val_acc: 0.7708\n",
      "Epoch 229/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4165 - acc: 0.8045\n",
      "Epoch 229: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4187 - acc: 0.8037 - val_loss: 0.4928 - val_acc: 0.7837\n",
      "Epoch 230/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8070\n",
      "Epoch 230: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4178 - acc: 0.8068 - val_loss: 0.4810 - val_acc: 0.7713\n",
      "Epoch 231/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.8065\n",
      "Epoch 231: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4067 - acc: 0.8066 - val_loss: 0.4849 - val_acc: 0.7760\n",
      "Epoch 232/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4109 - acc: 0.8036\n",
      "Epoch 232: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4109 - acc: 0.8036 - val_loss: 0.4812 - val_acc: 0.7841\n",
      "Epoch 233/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4011 - acc: 0.8084\n",
      "Epoch 233: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4018 - acc: 0.8075 - val_loss: 0.5109 - val_acc: 0.7820\n",
      "Epoch 234/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8049\n",
      "Epoch 234: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4064 - acc: 0.8057 - val_loss: 0.5058 - val_acc: 0.7884\n",
      "Epoch 235/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8035\n",
      "Epoch 235: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4094 - acc: 0.8036 - val_loss: 0.4887 - val_acc: 0.7755\n",
      "Epoch 236/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8112\n",
      "Epoch 236: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4041 - acc: 0.8109 - val_loss: 0.4929 - val_acc: 0.7897\n",
      "Epoch 237/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8132\n",
      "Epoch 237: val_acc did not improve from 0.79222\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3921 - acc: 0.8130 - val_loss: 0.4865 - val_acc: 0.7854\n",
      "Epoch 238/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4040 - acc: 0.8115\n",
      "Epoch 238: val_acc improved from 0.79222 to 0.79692, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\1\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4037 - acc: 0.8118 - val_loss: 0.4859 - val_acc: 0.7969\n",
      "Epoch 239/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8123\n",
      "Epoch 239: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3928 - acc: 0.8125 - val_loss: 0.4808 - val_acc: 0.7884\n",
      "Epoch 240/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8065\n",
      "Epoch 240: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4103 - acc: 0.8066 - val_loss: 0.4857 - val_acc: 0.7944\n",
      "Epoch 241/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4108 - acc: 0.8033\n",
      "Epoch 241: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4100 - acc: 0.8031 - val_loss: 0.5013 - val_acc: 0.7824\n",
      "Epoch 242/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4082 - acc: 0.8106\n",
      "Epoch 242: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4082 - acc: 0.8106 - val_loss: 0.4931 - val_acc: 0.7529\n",
      "Epoch 243/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.8091\n",
      "Epoch 243: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3978 - acc: 0.8092 - val_loss: 0.5022 - val_acc: 0.7909\n",
      "Epoch 244/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8134\n",
      "Epoch 244: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3942 - acc: 0.8138 - val_loss: 0.5009 - val_acc: 0.7918\n",
      "Epoch 245/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8062\n",
      "Epoch 245: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4106 - acc: 0.8063 - val_loss: 0.4981 - val_acc: 0.7802\n",
      "Epoch 246/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8039\n",
      "Epoch 246: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4168 - acc: 0.8043 - val_loss: 0.5343 - val_acc: 0.7892\n",
      "Epoch 247/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8048\n",
      "Epoch 247: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4186 - acc: 0.8040 - val_loss: 0.5067 - val_acc: 0.7858\n",
      "Epoch 248/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8028\n",
      "Epoch 248: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4155 - acc: 0.8027 - val_loss: 0.4987 - val_acc: 0.7773\n",
      "Epoch 249/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8082\n",
      "Epoch 249: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4094 - acc: 0.8082 - val_loss: 0.5001 - val_acc: 0.7777\n",
      "Epoch 250/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8075\n",
      "Epoch 250: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3963 - acc: 0.8077 - val_loss: 0.4805 - val_acc: 0.7755\n",
      "Epoch 251/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8061\n",
      "Epoch 251: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3997 - acc: 0.8061 - val_loss: 0.4978 - val_acc: 0.7850\n",
      "Epoch 252/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3867 - acc: 0.8138\n",
      "Epoch 252: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3867 - acc: 0.8138 - val_loss: 0.4876 - val_acc: 0.7811\n",
      "Epoch 253/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4010 - acc: 0.8093\n",
      "Epoch 253: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4013 - acc: 0.8088 - val_loss: 0.4924 - val_acc: 0.7841\n",
      "Epoch 254/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8076\n",
      "Epoch 254: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4014 - acc: 0.8075 - val_loss: 0.5010 - val_acc: 0.7815\n",
      "Epoch 255/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8078\n",
      "Epoch 255: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3990 - acc: 0.8083 - val_loss: 0.4960 - val_acc: 0.7850\n",
      "Epoch 256/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8071\n",
      "Epoch 256: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4046 - acc: 0.8074 - val_loss: 0.5206 - val_acc: 0.7790\n",
      "Epoch 257/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8110\n",
      "Epoch 257: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3952 - acc: 0.8111 - val_loss: 0.4998 - val_acc: 0.7760\n",
      "Epoch 258/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8125\n",
      "Epoch 258: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4008 - acc: 0.8124 - val_loss: 0.4829 - val_acc: 0.7905\n",
      "Epoch 259/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8032\n",
      "Epoch 259: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4051 - acc: 0.8036 - val_loss: 0.4845 - val_acc: 0.7845\n",
      "Epoch 260/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8072\n",
      "Epoch 260: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4063 - acc: 0.8071 - val_loss: 0.4868 - val_acc: 0.7948\n",
      "Epoch 261/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4083 - acc: 0.8032\n",
      "Epoch 261: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4082 - acc: 0.8030 - val_loss: 0.4948 - val_acc: 0.7815\n",
      "Epoch 262/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4030 - acc: 0.8053\n",
      "Epoch 262: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4028 - acc: 0.8055 - val_loss: 0.5350 - val_acc: 0.7850\n",
      "Epoch 263/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8082\n",
      "Epoch 263: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3998 - acc: 0.8081 - val_loss: 0.4950 - val_acc: 0.7747\n",
      "Epoch 264/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3948 - acc: 0.8096\n",
      "Epoch 264: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3948 - acc: 0.8096 - val_loss: 0.5153 - val_acc: 0.7811\n",
      "Epoch 265/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8101\n",
      "Epoch 265: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3907 - acc: 0.8102 - val_loss: 0.5203 - val_acc: 0.7785\n",
      "Epoch 266/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3993 - acc: 0.8112\n",
      "Epoch 266: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3993 - acc: 0.8112 - val_loss: 0.4825 - val_acc: 0.7888\n",
      "Epoch 267/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3992 - acc: 0.8124\n",
      "Epoch 267: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4004 - acc: 0.8121 - val_loss: 0.4952 - val_acc: 0.7854\n",
      "Epoch 268/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8136\n",
      "Epoch 268: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3911 - acc: 0.8128 - val_loss: 0.4867 - val_acc: 0.7935\n",
      "Epoch 269/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8126\n",
      "Epoch 269: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4095 - acc: 0.8128 - val_loss: 0.4875 - val_acc: 0.7858\n",
      "Epoch 270/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8096\n",
      "Epoch 270: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4108 - acc: 0.8092 - val_loss: 0.4803 - val_acc: 0.7790\n",
      "Epoch 271/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8062\n",
      "Epoch 271: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4145 - acc: 0.8066 - val_loss: 0.4869 - val_acc: 0.7845\n",
      "Epoch 272/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.3941 - acc: 0.8102\n",
      "Epoch 272: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3948 - acc: 0.8098 - val_loss: 0.5187 - val_acc: 0.7862\n",
      "Epoch 273/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8076\n",
      "Epoch 273: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4132 - acc: 0.8076 - val_loss: 0.5026 - val_acc: 0.7875\n",
      "Epoch 274/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4160 - acc: 0.8086\n",
      "Epoch 274: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4181 - acc: 0.8076 - val_loss: 0.4840 - val_acc: 0.7845\n",
      "Epoch 275/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8114\n",
      "Epoch 275: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3855 - acc: 0.8119 - val_loss: 0.5092 - val_acc: 0.7734\n",
      "Epoch 276/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4063 - acc: 0.8076\n",
      "Epoch 276: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4063 - acc: 0.8076 - val_loss: 0.5101 - val_acc: 0.7790\n",
      "Epoch 277/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8109\n",
      "Epoch 277: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3979 - acc: 0.8104 - val_loss: 0.5127 - val_acc: 0.7884\n",
      "Epoch 278/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8043\n",
      "Epoch 278: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4133 - acc: 0.8044 - val_loss: 0.5173 - val_acc: 0.7764\n",
      "Epoch 279/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8102\n",
      "Epoch 279: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3994 - acc: 0.8092 - val_loss: 0.5029 - val_acc: 0.7649\n",
      "Epoch 280/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8042\n",
      "Epoch 280: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4020 - acc: 0.8045 - val_loss: 0.5126 - val_acc: 0.7879\n",
      "Epoch 281/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4038 - acc: 0.8032\n",
      "Epoch 281: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4051 - acc: 0.8030 - val_loss: 0.5137 - val_acc: 0.7931\n",
      "Epoch 282/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8072\n",
      "Epoch 282: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4034 - acc: 0.8069 - val_loss: 0.5048 - val_acc: 0.7760\n",
      "Epoch 283/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8011\n",
      "Epoch 283: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4144 - acc: 0.8011 - val_loss: 0.4824 - val_acc: 0.7884\n",
      "Epoch 284/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8038\n",
      "Epoch 284: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4186 - acc: 0.8034 - val_loss: 0.5036 - val_acc: 0.7854\n",
      "Epoch 285/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8066\n",
      "Epoch 285: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4064 - acc: 0.8065 - val_loss: 0.4947 - val_acc: 0.7696\n",
      "Epoch 286/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8117\n",
      "Epoch 286: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3997 - acc: 0.8119 - val_loss: 0.5279 - val_acc: 0.7824\n",
      "Epoch 287/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8077\n",
      "Epoch 287: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4048 - acc: 0.8074 - val_loss: 0.4904 - val_acc: 0.7828\n",
      "Epoch 288/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8100\n",
      "Epoch 288: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4014 - acc: 0.8099 - val_loss: 0.4922 - val_acc: 0.7687\n",
      "Epoch 289/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8014\n",
      "Epoch 289: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4220 - acc: 0.8018 - val_loss: 0.4888 - val_acc: 0.7743\n",
      "Epoch 290/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4112 - acc: 0.8075\n",
      "Epoch 290: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4116 - acc: 0.8065 - val_loss: 0.4889 - val_acc: 0.7726\n",
      "Epoch 291/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8128\n",
      "Epoch 291: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3951 - acc: 0.8126 - val_loss: 0.4968 - val_acc: 0.7939\n",
      "Epoch 292/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8124\n",
      "Epoch 292: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3977 - acc: 0.8126 - val_loss: 0.4987 - val_acc: 0.7781\n",
      "Epoch 293/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4062 - acc: 0.8054\n",
      "Epoch 293: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4064 - acc: 0.8049 - val_loss: 0.4885 - val_acc: 0.7661\n",
      "Epoch 294/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8117\n",
      "Epoch 294: val_acc did not improve from 0.79692\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3936 - acc: 0.8115 - val_loss: 0.4942 - val_acc: 0.7751\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 378,881\n",
      "Trainable params: 378,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 1.5774 - acc: 0.5920\n",
      "Epoch 1: val_acc improved from -inf to 0.67251, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 1.5639 - acc: 0.5932 - val_loss: 0.6515 - val_acc: 0.6725\n",
      "Epoch 2/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6735 - acc: 0.6457\n",
      "Epoch 2: val_acc improved from 0.67251 to 0.67294, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6726 - acc: 0.6463 - val_loss: 0.6494 - val_acc: 0.6729\n",
      "Epoch 3/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6387 - acc: 0.6709\n",
      "Epoch 3: val_acc did not improve from 0.67294\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.6386 - acc: 0.6711 - val_loss: 0.6190 - val_acc: 0.6725\n",
      "Epoch 4/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6207 - acc: 0.6844\n",
      "Epoch 4: val_acc improved from 0.67294 to 0.67807, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6204 - acc: 0.6848 - val_loss: 0.6181 - val_acc: 0.6781\n",
      "Epoch 5/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6058 - acc: 0.6966\n",
      "Epoch 5: val_acc improved from 0.67807 to 0.70757, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6057 - acc: 0.6968 - val_loss: 0.5897 - val_acc: 0.7076\n",
      "Epoch 6/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.7040\n",
      "Epoch 6: val_acc improved from 0.70757 to 0.71013, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5937 - acc: 0.7045 - val_loss: 0.5910 - val_acc: 0.7101\n",
      "Epoch 7/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5933 - acc: 0.7097\n",
      "Epoch 7: val_acc improved from 0.71013 to 0.71484, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5929 - acc: 0.7102 - val_loss: 0.5838 - val_acc: 0.7148\n",
      "Epoch 8/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5840 - acc: 0.7108\n",
      "Epoch 8: val_acc improved from 0.71484 to 0.72253, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5830 - acc: 0.7115 - val_loss: 0.5846 - val_acc: 0.7225\n",
      "Epoch 9/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.7131\n",
      "Epoch 9: val_acc did not improve from 0.72253\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5780 - acc: 0.7141 - val_loss: 0.5709 - val_acc: 0.7183\n",
      "Epoch 10/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.7180\n",
      "Epoch 10: val_acc improved from 0.72253 to 0.72296, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5758 - acc: 0.7187 - val_loss: 0.5785 - val_acc: 0.7230\n",
      "Epoch 11/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.7204\n",
      "Epoch 11: val_acc improved from 0.72296 to 0.72381, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5684 - acc: 0.7206 - val_loss: 0.5689 - val_acc: 0.7238\n",
      "Epoch 12/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5680 - acc: 0.7234\n",
      "Epoch 12: val_acc improved from 0.72381 to 0.72681, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5688 - acc: 0.7234 - val_loss: 0.5681 - val_acc: 0.7268\n",
      "Epoch 13/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.7266\n",
      "Epoch 13: val_acc did not improve from 0.72681\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5628 - acc: 0.7268 - val_loss: 0.5608 - val_acc: 0.7260\n",
      "Epoch 14/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.5577 - acc: 0.7257\n",
      "Epoch 14: val_acc improved from 0.72681 to 0.73151, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5572 - acc: 0.7263 - val_loss: 0.5527 - val_acc: 0.7315\n",
      "Epoch 15/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.7276\n",
      "Epoch 15: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5543 - acc: 0.7281 - val_loss: 0.5578 - val_acc: 0.7311\n",
      "Epoch 16/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7311\n",
      "Epoch 16: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5484 - acc: 0.7319 - val_loss: 0.5529 - val_acc: 0.7277\n",
      "Epoch 17/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7364\n",
      "Epoch 17: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5477 - acc: 0.7364 - val_loss: 0.5501 - val_acc: 0.7307\n",
      "Epoch 18/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.7330\n",
      "Epoch 18: val_acc did not improve from 0.73151\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5418 - acc: 0.7334 - val_loss: 0.5410 - val_acc: 0.7260\n",
      "Epoch 19/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5350 - acc: 0.7382\n",
      "Epoch 19: val_acc improved from 0.73151 to 0.73536, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5344 - acc: 0.7384 - val_loss: 0.5412 - val_acc: 0.7354\n",
      "Epoch 20/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5368 - acc: 0.7390\n",
      "Epoch 20: val_acc did not improve from 0.73536\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5362 - acc: 0.7385 - val_loss: 0.5388 - val_acc: 0.7277\n",
      "Epoch 21/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.7359\n",
      "Epoch 21: val_acc did not improve from 0.73536\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5377 - acc: 0.7369 - val_loss: 0.5366 - val_acc: 0.7315\n",
      "Epoch 22/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7411\n",
      "Epoch 22: val_acc did not improve from 0.73536\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5307 - acc: 0.7416 - val_loss: 0.5351 - val_acc: 0.7255\n",
      "Epoch 23/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.7399\n",
      "Epoch 23: val_acc improved from 0.73536 to 0.74049, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5283 - acc: 0.7405 - val_loss: 0.5284 - val_acc: 0.7405\n",
      "Epoch 24/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.7423\n",
      "Epoch 24: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5242 - acc: 0.7431 - val_loss: 0.5197 - val_acc: 0.7371\n",
      "Epoch 25/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.7484\n",
      "Epoch 25: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5148 - acc: 0.7490 - val_loss: 0.5123 - val_acc: 0.7358\n",
      "Epoch 26/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.7489\n",
      "Epoch 26: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5075 - acc: 0.7492 - val_loss: 0.5102 - val_acc: 0.7383\n",
      "Epoch 27/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.7499\n",
      "Epoch 27: val_acc did not improve from 0.74049\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.5108 - acc: 0.7502 - val_loss: 0.5200 - val_acc: 0.7358\n",
      "Epoch 28/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5052 - acc: 0.7529\n",
      "Epoch 28: val_acc improved from 0.74049 to 0.74947, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5052 - acc: 0.7529 - val_loss: 0.5118 - val_acc: 0.7495\n",
      "Epoch 29/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.7501\n",
      "Epoch 29: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5077 - acc: 0.7505 - val_loss: 0.5089 - val_acc: 0.7452\n",
      "Epoch 30/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.7568\n",
      "Epoch 30: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4969 - acc: 0.7573 - val_loss: 0.5052 - val_acc: 0.7469\n",
      "Epoch 31/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.7569\n",
      "Epoch 31: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4966 - acc: 0.7570 - val_loss: 0.5056 - val_acc: 0.7452\n",
      "Epoch 32/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.7533\n",
      "Epoch 32: val_acc did not improve from 0.74947\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4984 - acc: 0.7538 - val_loss: 0.5150 - val_acc: 0.7482\n",
      "Epoch 33/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4988 - acc: 0.7594\n",
      "Epoch 33: val_acc improved from 0.74947 to 0.75160, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4983 - acc: 0.7602 - val_loss: 0.5100 - val_acc: 0.7516\n",
      "Epoch 34/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.7547\n",
      "Epoch 34: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5016 - acc: 0.7550 - val_loss: 0.5077 - val_acc: 0.7495\n",
      "Epoch 35/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4969 - acc: 0.7571\n",
      "Epoch 35: val_acc did not improve from 0.75160\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4969 - acc: 0.7571 - val_loss: 0.4994 - val_acc: 0.7409\n",
      "Epoch 36/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.7555\n",
      "Epoch 36: val_acc improved from 0.75160 to 0.75460, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4885 - acc: 0.7559 - val_loss: 0.5159 - val_acc: 0.7546\n",
      "Epoch 37/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.7611\n",
      "Epoch 37: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4793 - acc: 0.7612 - val_loss: 0.5056 - val_acc: 0.7512\n",
      "Epoch 38/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.7667\n",
      "Epoch 38: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4818 - acc: 0.7672 - val_loss: 0.5023 - val_acc: 0.7525\n",
      "Epoch 39/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.7622\n",
      "Epoch 39: val_acc did not improve from 0.75460\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4874 - acc: 0.7626 - val_loss: 0.4984 - val_acc: 0.7520\n",
      "Epoch 40/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4792 - acc: 0.7656\n",
      "Epoch 40: val_acc improved from 0.75460 to 0.75673, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4792 - acc: 0.7656 - val_loss: 0.4964 - val_acc: 0.7567\n",
      "Epoch 41/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4831 - acc: 0.7668\n",
      "Epoch 41: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4831 - acc: 0.7671 - val_loss: 0.5033 - val_acc: 0.7448\n",
      "Epoch 42/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.7644\n",
      "Epoch 42: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4801 - acc: 0.7652 - val_loss: 0.4986 - val_acc: 0.7507\n",
      "Epoch 43/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4782 - acc: 0.7677\n",
      "Epoch 43: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4782 - acc: 0.7671 - val_loss: 0.4943 - val_acc: 0.7435\n",
      "Epoch 44/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.7664\n",
      "Epoch 44: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4742 - acc: 0.7666 - val_loss: 0.4984 - val_acc: 0.7482\n",
      "Epoch 45/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.7713\n",
      "Epoch 45: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4702 - acc: 0.7718 - val_loss: 0.4946 - val_acc: 0.7495\n",
      "Epoch 46/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.7718\n",
      "Epoch 46: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4738 - acc: 0.7722 - val_loss: 0.5034 - val_acc: 0.7482\n",
      "Epoch 47/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.7644\n",
      "Epoch 47: val_acc did not improve from 0.75673\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4788 - acc: 0.7645 - val_loss: 0.5003 - val_acc: 0.7563\n",
      "Epoch 48/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4745 - acc: 0.7709\n",
      "Epoch 48: val_acc improved from 0.75673 to 0.75930, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4742 - acc: 0.7714 - val_loss: 0.4918 - val_acc: 0.7593\n",
      "Epoch 49/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.7758\n",
      "Epoch 49: val_acc improved from 0.75930 to 0.76015, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4689 - acc: 0.7758 - val_loss: 0.4841 - val_acc: 0.7602\n",
      "Epoch 50/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4730 - acc: 0.7704\n",
      "Epoch 50: val_acc improved from 0.76015 to 0.76357, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4727 - acc: 0.7706 - val_loss: 0.4821 - val_acc: 0.7636\n",
      "Epoch 51/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.7755\n",
      "Epoch 51: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4681 - acc: 0.7751 - val_loss: 0.4924 - val_acc: 0.7503\n",
      "Epoch 52/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.7746\n",
      "Epoch 52: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4667 - acc: 0.7738 - val_loss: 0.4900 - val_acc: 0.7627\n",
      "Epoch 53/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.7759\n",
      "Epoch 53: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4594 - acc: 0.7761 - val_loss: 0.4999 - val_acc: 0.7443\n",
      "Epoch 54/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4647 - acc: 0.7769\n",
      "Epoch 54: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4650 - acc: 0.7765 - val_loss: 0.4927 - val_acc: 0.7559\n",
      "Epoch 55/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4555 - acc: 0.7768\n",
      "Epoch 55: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4556 - acc: 0.7769 - val_loss: 0.4933 - val_acc: 0.7503\n",
      "Epoch 56/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4654 - acc: 0.7758\n",
      "Epoch 56: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4644 - acc: 0.7762 - val_loss: 0.4901 - val_acc: 0.7533\n",
      "Epoch 57/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.7762\n",
      "Epoch 57: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4570 - acc: 0.7765 - val_loss: 0.4868 - val_acc: 0.7503\n",
      "Epoch 58/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.7825\n",
      "Epoch 58: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4523 - acc: 0.7828 - val_loss: 0.4923 - val_acc: 0.7456\n",
      "Epoch 59/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.7803\n",
      "Epoch 59: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4523 - acc: 0.7802 - val_loss: 0.4946 - val_acc: 0.7537\n",
      "Epoch 60/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.7841\n",
      "Epoch 60: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4544 - acc: 0.7839 - val_loss: 0.4884 - val_acc: 0.7525\n",
      "Epoch 61/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.7809\n",
      "Epoch 61: val_acc improved from 0.76357 to 0.76999, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4590 - acc: 0.7810 - val_loss: 0.4871 - val_acc: 0.7700\n",
      "Epoch 62/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.7785\n",
      "Epoch 62: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4563 - acc: 0.7790 - val_loss: 0.5080 - val_acc: 0.7529\n",
      "Epoch 63/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4540 - acc: 0.7835\n",
      "Epoch 63: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4540 - acc: 0.7835 - val_loss: 0.5043 - val_acc: 0.7580\n",
      "Epoch 64/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4516 - acc: 0.7836\n",
      "Epoch 64: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4509 - acc: 0.7839 - val_loss: 0.4887 - val_acc: 0.7546\n",
      "Epoch 65/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.7827\n",
      "Epoch 65: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4504 - acc: 0.7828 - val_loss: 0.4908 - val_acc: 0.7580\n",
      "Epoch 66/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.7824\n",
      "Epoch 66: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.4461 - acc: 0.7832 - val_loss: 0.4894 - val_acc: 0.7687\n",
      "Epoch 67/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4534 - acc: 0.7821\n",
      "Epoch 67: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 3s 9ms/step - loss: 0.4524 - acc: 0.7827 - val_loss: 0.4802 - val_acc: 0.7563\n",
      "Epoch 68/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.7896\n",
      "Epoch 68: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4381 - acc: 0.7905 - val_loss: 0.4860 - val_acc: 0.7589\n",
      "Epoch 69/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.7864\n",
      "Epoch 69: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4511 - acc: 0.7863 - val_loss: 0.4775 - val_acc: 0.7636\n",
      "Epoch 70/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4501 - acc: 0.7841\n",
      "Epoch 70: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4493 - acc: 0.7842 - val_loss: 0.4878 - val_acc: 0.7529\n",
      "Epoch 71/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.7930\n",
      "Epoch 71: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4423 - acc: 0.7934 - val_loss: 0.4955 - val_acc: 0.7653\n",
      "Epoch 72/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.7836\n",
      "Epoch 72: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4435 - acc: 0.7841 - val_loss: 0.4845 - val_acc: 0.7542\n",
      "Epoch 73/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4459 - acc: 0.7878\n",
      "Epoch 73: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4459 - acc: 0.7878 - val_loss: 0.4909 - val_acc: 0.7653\n",
      "Epoch 74/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.7870\n",
      "Epoch 74: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4465 - acc: 0.7872 - val_loss: 0.4942 - val_acc: 0.7525\n",
      "Epoch 75/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.7883\n",
      "Epoch 75: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4466 - acc: 0.7886 - val_loss: 0.4915 - val_acc: 0.7563\n",
      "Epoch 76/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.7864\n",
      "Epoch 76: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4463 - acc: 0.7870 - val_loss: 0.5129 - val_acc: 0.7606\n",
      "Epoch 77/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4395 - acc: 0.7918\n",
      "Epoch 77: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4391 - acc: 0.7923 - val_loss: 0.4880 - val_acc: 0.7584\n",
      "Epoch 78/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4424 - acc: 0.7854\n",
      "Epoch 78: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4429 - acc: 0.7848 - val_loss: 0.4838 - val_acc: 0.7619\n",
      "Epoch 79/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4471 - acc: 0.7815\n",
      "Epoch 79: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4455 - acc: 0.7825 - val_loss: 0.5029 - val_acc: 0.7567\n",
      "Epoch 80/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4407 - acc: 0.7875\n",
      "Epoch 80: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4407 - acc: 0.7875 - val_loss: 0.4916 - val_acc: 0.7674\n",
      "Epoch 81/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.7850\n",
      "Epoch 81: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4405 - acc: 0.7850 - val_loss: 0.5059 - val_acc: 0.7597\n",
      "Epoch 82/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.7872\n",
      "Epoch 82: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4384 - acc: 0.7880 - val_loss: 0.4896 - val_acc: 0.7653\n",
      "Epoch 83/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.7886\n",
      "Epoch 83: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4348 - acc: 0.7883 - val_loss: 0.4836 - val_acc: 0.7525\n",
      "Epoch 84/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.7905\n",
      "Epoch 84: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4398 - acc: 0.7907 - val_loss: 0.4901 - val_acc: 0.7606\n",
      "Epoch 85/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.7943\n",
      "Epoch 85: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4341 - acc: 0.7947 - val_loss: 0.4814 - val_acc: 0.7666\n",
      "Epoch 86/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4433 - acc: 0.7884\n",
      "Epoch 86: val_acc did not improve from 0.76999\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4448 - acc: 0.7876 - val_loss: 0.4876 - val_acc: 0.7606\n",
      "Epoch 87/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4435 - acc: 0.7841\n",
      "Epoch 87: val_acc improved from 0.76999 to 0.77640, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4435 - acc: 0.7841 - val_loss: 0.5000 - val_acc: 0.7764\n",
      "Epoch 88/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4411 - acc: 0.7898\n",
      "Epoch 88: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4407 - acc: 0.7896 - val_loss: 0.4735 - val_acc: 0.7751\n",
      "Epoch 89/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4440 - acc: 0.7869\n",
      "Epoch 89: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4442 - acc: 0.7871 - val_loss: 0.5093 - val_acc: 0.7602\n",
      "Epoch 90/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.7918\n",
      "Epoch 90: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.5015 - val_acc: 0.7559\n",
      "Epoch 91/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4410 - acc: 0.7874\n",
      "Epoch 91: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4415 - acc: 0.7866 - val_loss: 0.4976 - val_acc: 0.7602\n",
      "Epoch 92/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.7897\n",
      "Epoch 92: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4349 - acc: 0.7904 - val_loss: 0.4948 - val_acc: 0.7636\n",
      "Epoch 93/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.7917\n",
      "Epoch 93: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4347 - acc: 0.7925 - val_loss: 0.4923 - val_acc: 0.7644\n",
      "Epoch 94/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.7905\n",
      "Epoch 94: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4319 - acc: 0.7904 - val_loss: 0.4942 - val_acc: 0.7691\n",
      "Epoch 95/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.7935\n",
      "Epoch 95: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4332 - acc: 0.7936 - val_loss: 0.4978 - val_acc: 0.7644\n",
      "Epoch 96/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4290 - acc: 0.7932\n",
      "Epoch 96: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4289 - acc: 0.7931 - val_loss: 0.4919 - val_acc: 0.7597\n",
      "Epoch 97/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.7965\n",
      "Epoch 97: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4276 - acc: 0.7972 - val_loss: 0.4915 - val_acc: 0.7649\n",
      "Epoch 98/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4245 - acc: 0.7991\n",
      "Epoch 98: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4253 - acc: 0.7988 - val_loss: 0.4861 - val_acc: 0.7734\n",
      "Epoch 99/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4279 - acc: 0.8005\n",
      "Epoch 99: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4273 - acc: 0.8009 - val_loss: 0.4950 - val_acc: 0.7537\n",
      "Epoch 100/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4377 - acc: 0.7860\n",
      "Epoch 100: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4369 - acc: 0.7864 - val_loss: 0.4787 - val_acc: 0.7610\n",
      "Epoch 101/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.7959\n",
      "Epoch 101: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4277 - acc: 0.7964 - val_loss: 0.4822 - val_acc: 0.7597\n",
      "Epoch 102/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4254 - acc: 0.7945\n",
      "Epoch 102: val_acc improved from 0.77640 to 0.77683, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4268 - acc: 0.7942 - val_loss: 0.4849 - val_acc: 0.7768\n",
      "Epoch 103/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.7969\n",
      "Epoch 103: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4314 - acc: 0.7967 - val_loss: 0.4873 - val_acc: 0.7610\n",
      "Epoch 104/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.7957\n",
      "Epoch 104: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4267 - acc: 0.7957 - val_loss: 0.4916 - val_acc: 0.7674\n",
      "Epoch 105/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.7963\n",
      "Epoch 105: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4281 - acc: 0.7964 - val_loss: 0.4944 - val_acc: 0.7661\n",
      "Epoch 106/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.7943\n",
      "Epoch 106: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4243 - acc: 0.7948 - val_loss: 0.4883 - val_acc: 0.7606\n",
      "Epoch 107/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4250 - acc: 0.7956\n",
      "Epoch 107: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4247 - acc: 0.7959 - val_loss: 0.4927 - val_acc: 0.7636\n",
      "Epoch 108/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.7929\n",
      "Epoch 108: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4235 - acc: 0.7931 - val_loss: 0.4853 - val_acc: 0.7627\n",
      "Epoch 109/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.7934\n",
      "Epoch 109: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4301 - acc: 0.7935 - val_loss: 0.4996 - val_acc: 0.7743\n",
      "Epoch 110/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4257 - acc: 0.7985\n",
      "Epoch 110: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4255 - acc: 0.7986 - val_loss: 0.4935 - val_acc: 0.7460\n",
      "Epoch 111/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.7944\n",
      "Epoch 111: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4313 - acc: 0.7944 - val_loss: 0.4923 - val_acc: 0.7597\n",
      "Epoch 112/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4244 - acc: 0.7969\n",
      "Epoch 112: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4247 - acc: 0.7973 - val_loss: 0.4896 - val_acc: 0.7683\n",
      "Epoch 113/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.7981\n",
      "Epoch 113: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4250 - acc: 0.7986 - val_loss: 0.4891 - val_acc: 0.7683\n",
      "Epoch 114/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4248 - acc: 0.7980\n",
      "Epoch 114: val_acc did not improve from 0.77683\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4254 - acc: 0.7978 - val_loss: 0.4927 - val_acc: 0.7597\n",
      "Epoch 115/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8003\n",
      "Epoch 115: val_acc improved from 0.77683 to 0.78239, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4249 - acc: 0.8006 - val_loss: 0.4976 - val_acc: 0.7824\n",
      "Epoch 116/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.7978\n",
      "Epoch 116: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4316 - acc: 0.7979 - val_loss: 0.4968 - val_acc: 0.7760\n",
      "Epoch 117/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.7942\n",
      "Epoch 117: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4251 - acc: 0.7947 - val_loss: 0.4955 - val_acc: 0.7644\n",
      "Epoch 118/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.7930\n",
      "Epoch 118: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4294 - acc: 0.7937 - val_loss: 0.5018 - val_acc: 0.7614\n",
      "Epoch 119/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.7946\n",
      "Epoch 119: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4273 - acc: 0.7944 - val_loss: 0.4807 - val_acc: 0.7683\n",
      "Epoch 120/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8013\n",
      "Epoch 120: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4175 - acc: 0.8015 - val_loss: 0.4862 - val_acc: 0.7700\n",
      "Epoch 121/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.7979\n",
      "Epoch 121: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4188 - acc: 0.7981 - val_loss: 0.4816 - val_acc: 0.7747\n",
      "Epoch 122/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.8003\n",
      "Epoch 122: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4253 - acc: 0.8006 - val_loss: 0.4888 - val_acc: 0.7516\n",
      "Epoch 123/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.7929\n",
      "Epoch 123: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4336 - acc: 0.7931 - val_loss: 0.4828 - val_acc: 0.7670\n",
      "Epoch 124/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4240 - acc: 0.7977\n",
      "Epoch 124: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4236 - acc: 0.7973 - val_loss: 0.4817 - val_acc: 0.7649\n",
      "Epoch 125/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.7994\n",
      "Epoch 125: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4169 - acc: 0.7998 - val_loss: 0.4912 - val_acc: 0.7653\n",
      "Epoch 126/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.7976\n",
      "Epoch 126: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4213 - acc: 0.7980 - val_loss: 0.4868 - val_acc: 0.7700\n",
      "Epoch 127/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8022\n",
      "Epoch 127: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4156 - acc: 0.8032 - val_loss: 0.4897 - val_acc: 0.7785\n",
      "Epoch 128/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4127 - acc: 0.8036\n",
      "Epoch 128: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4127 - acc: 0.8030 - val_loss: 0.4988 - val_acc: 0.7764\n",
      "Epoch 129/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.7992\n",
      "Epoch 129: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4274 - acc: 0.7987 - val_loss: 0.4911 - val_acc: 0.7751\n",
      "Epoch 130/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8005\n",
      "Epoch 130: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4201 - acc: 0.8005 - val_loss: 0.4840 - val_acc: 0.7777\n",
      "Epoch 131/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.7984\n",
      "Epoch 131: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4183 - acc: 0.7984 - val_loss: 0.4887 - val_acc: 0.7704\n",
      "Epoch 132/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8003\n",
      "Epoch 132: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4189 - acc: 0.8005 - val_loss: 0.4986 - val_acc: 0.7764\n",
      "Epoch 133/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.7989\n",
      "Epoch 133: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4273 - acc: 0.7989 - val_loss: 0.4985 - val_acc: 0.7691\n",
      "Epoch 134/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.7940\n",
      "Epoch 134: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4223 - acc: 0.7944 - val_loss: 0.4935 - val_acc: 0.7674\n",
      "Epoch 135/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4192 - acc: 0.8006\n",
      "Epoch 135: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4185 - acc: 0.8007 - val_loss: 0.4947 - val_acc: 0.7666\n",
      "Epoch 136/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4226 - acc: 0.7979\n",
      "Epoch 136: val_acc did not improve from 0.78239\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4225 - acc: 0.7980 - val_loss: 0.4986 - val_acc: 0.7661\n",
      "Epoch 137/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8017\n",
      "Epoch 137: val_acc improved from 0.78239 to 0.78410, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4179 - acc: 0.8019 - val_loss: 0.4921 - val_acc: 0.7841\n",
      "Epoch 138/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8029\n",
      "Epoch 138: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4186 - acc: 0.8030 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 139/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.7997\n",
      "Epoch 139: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4199 - acc: 0.8002 - val_loss: 0.4980 - val_acc: 0.7525\n",
      "Epoch 140/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4094 - acc: 0.8024\n",
      "Epoch 140: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4102 - acc: 0.8017 - val_loss: 0.4940 - val_acc: 0.7726\n",
      "Epoch 141/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4250 - acc: 0.7975\n",
      "Epoch 141: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4249 - acc: 0.7974 - val_loss: 0.5105 - val_acc: 0.7717\n",
      "Epoch 142/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4133 - acc: 0.8091\n",
      "Epoch 142: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4132 - acc: 0.8092 - val_loss: 0.4971 - val_acc: 0.7580\n",
      "Epoch 143/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4149 - acc: 0.8000\n",
      "Epoch 143: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4159 - acc: 0.7993 - val_loss: 0.4952 - val_acc: 0.7717\n",
      "Epoch 144/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4187 - acc: 0.8031\n",
      "Epoch 144: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4185 - acc: 0.8034 - val_loss: 0.4936 - val_acc: 0.7738\n",
      "Epoch 145/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4161 - acc: 0.8001\n",
      "Epoch 145: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4163 - acc: 0.8004 - val_loss: 0.4970 - val_acc: 0.7678\n",
      "Epoch 146/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8053\n",
      "Epoch 146: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4082 - acc: 0.8058 - val_loss: 0.5057 - val_acc: 0.7700\n",
      "Epoch 147/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8014\n",
      "Epoch 147: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4118 - acc: 0.8017 - val_loss: 0.4990 - val_acc: 0.7614\n",
      "Epoch 148/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8081\n",
      "Epoch 148: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4112 - acc: 0.8081 - val_loss: 0.4799 - val_acc: 0.7713\n",
      "Epoch 149/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.7983\n",
      "Epoch 149: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4179 - acc: 0.7984 - val_loss: 0.4870 - val_acc: 0.7704\n",
      "Epoch 150/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8010\n",
      "Epoch 150: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4080 - acc: 0.8006 - val_loss: 0.4861 - val_acc: 0.7738\n",
      "Epoch 151/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.7995\n",
      "Epoch 151: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4202 - acc: 0.7999 - val_loss: 0.4872 - val_acc: 0.7708\n",
      "Epoch 152/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.7991\n",
      "Epoch 152: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4310 - acc: 0.7991 - val_loss: 0.4944 - val_acc: 0.7614\n",
      "Epoch 153/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4215 - acc: 0.7967\n",
      "Epoch 153: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4200 - acc: 0.7971 - val_loss: 0.4891 - val_acc: 0.7666\n",
      "Epoch 154/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8023\n",
      "Epoch 154: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4184 - acc: 0.8029 - val_loss: 0.4952 - val_acc: 0.7448\n",
      "Epoch 155/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.7989\n",
      "Epoch 155: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4210 - acc: 0.7989 - val_loss: 0.5077 - val_acc: 0.7529\n",
      "Epoch 156/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8024\n",
      "Epoch 156: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4137 - acc: 0.8026 - val_loss: 0.4908 - val_acc: 0.7507\n",
      "Epoch 157/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.7977\n",
      "Epoch 157: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4336 - acc: 0.7978 - val_loss: 0.5043 - val_acc: 0.7572\n",
      "Epoch 158/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8015\n",
      "Epoch 158: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4211 - acc: 0.8018 - val_loss: 0.4864 - val_acc: 0.7602\n",
      "Epoch 159/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8045\n",
      "Epoch 159: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4113 - acc: 0.8046 - val_loss: 0.4837 - val_acc: 0.7670\n",
      "Epoch 160/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.7973\n",
      "Epoch 160: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4167 - acc: 0.7974 - val_loss: 0.4804 - val_acc: 0.7653\n",
      "Epoch 161/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8066\n",
      "Epoch 161: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4107 - acc: 0.8074 - val_loss: 0.4783 - val_acc: 0.7666\n",
      "Epoch 162/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.7990\n",
      "Epoch 162: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4159 - acc: 0.7995 - val_loss: 0.4872 - val_acc: 0.7640\n",
      "Epoch 163/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8037\n",
      "Epoch 163: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4129 - acc: 0.8038 - val_loss: 0.4855 - val_acc: 0.7696\n",
      "Epoch 164/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4082 - acc: 0.8076\n",
      "Epoch 164: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4087 - acc: 0.8076 - val_loss: 0.4799 - val_acc: 0.7666\n",
      "Epoch 165/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8039\n",
      "Epoch 165: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4170 - acc: 0.8038 - val_loss: 0.4867 - val_acc: 0.7631\n",
      "Epoch 166/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8008\n",
      "Epoch 166: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4124 - acc: 0.8013 - val_loss: 0.4855 - val_acc: 0.7777\n",
      "Epoch 167/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4057 - acc: 0.8062\n",
      "Epoch 167: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4064 - acc: 0.8058 - val_loss: 0.4864 - val_acc: 0.7644\n",
      "Epoch 168/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8101\n",
      "Epoch 168: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4057 - acc: 0.8104 - val_loss: 0.4885 - val_acc: 0.7657\n",
      "Epoch 169/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8022\n",
      "Epoch 169: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4137 - acc: 0.8024 - val_loss: 0.4826 - val_acc: 0.7644\n",
      "Epoch 170/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8090\n",
      "Epoch 170: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3996 - acc: 0.8091 - val_loss: 0.4805 - val_acc: 0.7644\n",
      "Epoch 171/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4074 - acc: 0.8092\n",
      "Epoch 171: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4072 - acc: 0.8089 - val_loss: 0.4802 - val_acc: 0.7653\n",
      "Epoch 172/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8007\n",
      "Epoch 172: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4110 - acc: 0.8007 - val_loss: 0.4950 - val_acc: 0.7653\n",
      "Epoch 173/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8012\n",
      "Epoch 173: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4158 - acc: 0.8015 - val_loss: 0.5040 - val_acc: 0.7738\n",
      "Epoch 174/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8099\n",
      "Epoch 174: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3998 - acc: 0.8100 - val_loss: 0.4962 - val_acc: 0.7678\n",
      "Epoch 175/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8102\n",
      "Epoch 175: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4018 - acc: 0.8104 - val_loss: 0.4816 - val_acc: 0.7683\n",
      "Epoch 176/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8049\n",
      "Epoch 176: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4065 - acc: 0.8053 - val_loss: 0.4809 - val_acc: 0.7734\n",
      "Epoch 177/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8106\n",
      "Epoch 177: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4046 - acc: 0.8107 - val_loss: 0.4874 - val_acc: 0.7678\n",
      "Epoch 178/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8104\n",
      "Epoch 178: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4013 - acc: 0.8105 - val_loss: 0.4860 - val_acc: 0.7781\n",
      "Epoch 179/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4114 - acc: 0.8015\n",
      "Epoch 179: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4123 - acc: 0.8013 - val_loss: 0.4878 - val_acc: 0.7824\n",
      "Epoch 180/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4102 - acc: 0.8075\n",
      "Epoch 180: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4101 - acc: 0.8072 - val_loss: 0.4827 - val_acc: 0.7602\n",
      "Epoch 181/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8043\n",
      "Epoch 181: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4012 - acc: 0.8050 - val_loss: 0.4897 - val_acc: 0.7721\n",
      "Epoch 182/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4062 - acc: 0.8102\n",
      "Epoch 182: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4055 - acc: 0.8103 - val_loss: 0.4877 - val_acc: 0.7644\n",
      "Epoch 183/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8081\n",
      "Epoch 183: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4065 - acc: 0.8088 - val_loss: 0.4751 - val_acc: 0.7661\n",
      "Epoch 184/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8079\n",
      "Epoch 184: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4042 - acc: 0.8080 - val_loss: 0.4786 - val_acc: 0.7661\n",
      "Epoch 185/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8094\n",
      "Epoch 185: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3980 - acc: 0.8098 - val_loss: 0.4915 - val_acc: 0.7713\n",
      "Epoch 186/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8079\n",
      "Epoch 186: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3967 - acc: 0.8080 - val_loss: 0.4886 - val_acc: 0.7704\n",
      "Epoch 187/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8059\n",
      "Epoch 187: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4073 - acc: 0.8062 - val_loss: 0.4873 - val_acc: 0.7563\n",
      "Epoch 188/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4106 - acc: 0.8032\n",
      "Epoch 188: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4093 - acc: 0.8035 - val_loss: 0.4980 - val_acc: 0.7760\n",
      "Epoch 189/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4124 - acc: 0.8035\n",
      "Epoch 189: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4124 - acc: 0.8035 - val_loss: 0.4899 - val_acc: 0.7802\n",
      "Epoch 190/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8055\n",
      "Epoch 190: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4100 - acc: 0.8057 - val_loss: 0.4835 - val_acc: 0.7760\n",
      "Epoch 191/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4267 - acc: 0.8027\n",
      "Epoch 191: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4259 - acc: 0.8029 - val_loss: 0.4993 - val_acc: 0.7687\n",
      "Epoch 192/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8008\n",
      "Epoch 192: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4165 - acc: 0.8014 - val_loss: 0.4961 - val_acc: 0.7584\n",
      "Epoch 193/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8060\n",
      "Epoch 193: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4034 - acc: 0.8058 - val_loss: 0.4876 - val_acc: 0.7576\n",
      "Epoch 194/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4017 - acc: 0.8077\n",
      "Epoch 194: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4017 - acc: 0.8077 - val_loss: 0.4903 - val_acc: 0.7674\n",
      "Epoch 195/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4076 - acc: 0.8061\n",
      "Epoch 195: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4071 - acc: 0.8068 - val_loss: 0.4928 - val_acc: 0.7726\n",
      "Epoch 196/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4079 - acc: 0.8032\n",
      "Epoch 196: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4065 - acc: 0.8033 - val_loss: 0.4947 - val_acc: 0.7670\n",
      "Epoch 197/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8045\n",
      "Epoch 197: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4028 - acc: 0.8050 - val_loss: 0.5032 - val_acc: 0.7602\n",
      "Epoch 198/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8053\n",
      "Epoch 198: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4150 - acc: 0.8055 - val_loss: 0.4951 - val_acc: 0.7567\n",
      "Epoch 199/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8072\n",
      "Epoch 199: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4053 - acc: 0.8072 - val_loss: 0.4941 - val_acc: 0.7657\n",
      "Epoch 200/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8094\n",
      "Epoch 200: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3961 - acc: 0.8092 - val_loss: 0.4897 - val_acc: 0.7619\n",
      "Epoch 201/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4199 - acc: 0.8049\n",
      "Epoch 201: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4187 - acc: 0.8049 - val_loss: 0.4933 - val_acc: 0.7567\n",
      "Epoch 202/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4108 - acc: 0.8074\n",
      "Epoch 202: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4100 - acc: 0.8074 - val_loss: 0.4995 - val_acc: 0.7555\n",
      "Epoch 203/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4034 - acc: 0.8113\n",
      "Epoch 203: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4045 - acc: 0.8102 - val_loss: 0.4902 - val_acc: 0.7738\n",
      "Epoch 204/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8094\n",
      "Epoch 204: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4036 - acc: 0.8100 - val_loss: 0.4799 - val_acc: 0.7704\n",
      "Epoch 205/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8011\n",
      "Epoch 205: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4087 - acc: 0.8013 - val_loss: 0.4858 - val_acc: 0.7597\n",
      "Epoch 206/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8084\n",
      "Epoch 206: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4040 - acc: 0.8086 - val_loss: 0.4825 - val_acc: 0.7768\n",
      "Epoch 207/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8111\n",
      "Epoch 207: val_acc improved from 0.78410 to 0.78837, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3981 - acc: 0.8111 - val_loss: 0.4660 - val_acc: 0.7884\n",
      "Epoch 208/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3997 - acc: 0.8105\n",
      "Epoch 208: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3997 - acc: 0.8097 - val_loss: 0.4782 - val_acc: 0.7811\n",
      "Epoch 209/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3970 - acc: 0.8145\n",
      "Epoch 209: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3970 - acc: 0.8145 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 210/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3993 - acc: 0.8133\n",
      "Epoch 210: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3997 - acc: 0.8115 - val_loss: 0.4924 - val_acc: 0.7572\n",
      "Epoch 211/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8075\n",
      "Epoch 211: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4023 - acc: 0.8078 - val_loss: 0.5048 - val_acc: 0.7619\n",
      "Epoch 212/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4054 - acc: 0.8033\n",
      "Epoch 212: val_acc did not improve from 0.78837\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4047 - acc: 0.8038 - val_loss: 0.4910 - val_acc: 0.7687\n",
      "Epoch 213/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8095\n",
      "Epoch 213: val_acc improved from 0.78837 to 0.79735, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\2\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4068 - acc: 0.8098 - val_loss: 0.4785 - val_acc: 0.7973\n",
      "Epoch 214/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8093\n",
      "Epoch 214: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4003 - acc: 0.8092 - val_loss: 0.4829 - val_acc: 0.7888\n",
      "Epoch 215/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8121\n",
      "Epoch 215: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3936 - acc: 0.8125 - val_loss: 0.4919 - val_acc: 0.7807\n",
      "Epoch 216/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8050\n",
      "Epoch 216: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4045 - acc: 0.8051 - val_loss: 0.5003 - val_acc: 0.7802\n",
      "Epoch 217/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8043\n",
      "Epoch 217: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4043 - acc: 0.8043 - val_loss: 0.5009 - val_acc: 0.7593\n",
      "Epoch 218/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8095\n",
      "Epoch 218: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.8094 - val_loss: 0.5074 - val_acc: 0.7790\n",
      "Epoch 219/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8083\n",
      "Epoch 219: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4029 - acc: 0.8090 - val_loss: 0.4920 - val_acc: 0.7734\n",
      "Epoch 220/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8081\n",
      "Epoch 220: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4038 - acc: 0.8081 - val_loss: 0.4820 - val_acc: 0.7888\n",
      "Epoch 221/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3962 - acc: 0.8104\n",
      "Epoch 221: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3957 - acc: 0.8107 - val_loss: 0.4892 - val_acc: 0.7657\n",
      "Epoch 222/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8111\n",
      "Epoch 222: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3977 - acc: 0.8113 - val_loss: 0.4989 - val_acc: 0.7678\n",
      "Epoch 223/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8057\n",
      "Epoch 223: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4002 - acc: 0.8057 - val_loss: 0.4979 - val_acc: 0.7670\n",
      "Epoch 224/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4072 - acc: 0.8062\n",
      "Epoch 224: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4072 - acc: 0.8062 - val_loss: 0.5055 - val_acc: 0.7683\n",
      "Epoch 225/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8113\n",
      "Epoch 225: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4049 - acc: 0.8120 - val_loss: 0.5039 - val_acc: 0.7820\n",
      "Epoch 226/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8103\n",
      "Epoch 226: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4144 - acc: 0.8104 - val_loss: 0.4868 - val_acc: 0.7644\n",
      "Epoch 227/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4041 - acc: 0.8102\n",
      "Epoch 227: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4037 - acc: 0.8099 - val_loss: 0.4821 - val_acc: 0.7584\n",
      "Epoch 228/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8127\n",
      "Epoch 228: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3899 - acc: 0.8127 - val_loss: 0.4959 - val_acc: 0.7674\n",
      "Epoch 229/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8108\n",
      "Epoch 229: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3968 - acc: 0.8109 - val_loss: 0.4882 - val_acc: 0.7580\n",
      "Epoch 230/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3980 - acc: 0.8072\n",
      "Epoch 230: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3980 - acc: 0.8072 - val_loss: 0.5008 - val_acc: 0.7525\n",
      "Epoch 231/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8086\n",
      "Epoch 231: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4062 - acc: 0.8090 - val_loss: 0.4907 - val_acc: 0.7721\n",
      "Epoch 232/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8065\n",
      "Epoch 232: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4209 - acc: 0.8066 - val_loss: 0.4875 - val_acc: 0.7542\n",
      "Epoch 233/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8135\n",
      "Epoch 233: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3908 - acc: 0.8138 - val_loss: 0.4914 - val_acc: 0.7674\n",
      "Epoch 234/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.3962 - acc: 0.8102\n",
      "Epoch 234: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3978 - acc: 0.8096 - val_loss: 0.4904 - val_acc: 0.7781\n",
      "Epoch 235/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4144 - acc: 0.8098\n",
      "Epoch 235: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4132 - acc: 0.8102 - val_loss: 0.4986 - val_acc: 0.7730\n",
      "Epoch 236/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8059\n",
      "Epoch 236: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4188 - acc: 0.8065 - val_loss: 0.4985 - val_acc: 0.7516\n",
      "Epoch 237/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8122\n",
      "Epoch 237: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4001 - acc: 0.8125 - val_loss: 0.4928 - val_acc: 0.7700\n",
      "Epoch 238/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3903 - acc: 0.8121\n",
      "Epoch 238: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3903 - acc: 0.8121 - val_loss: 0.5060 - val_acc: 0.7644\n",
      "Epoch 239/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8173\n",
      "Epoch 239: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4010 - acc: 0.8177 - val_loss: 0.4794 - val_acc: 0.7828\n",
      "Epoch 240/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8071\n",
      "Epoch 240: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3973 - acc: 0.8072 - val_loss: 0.4944 - val_acc: 0.7704\n",
      "Epoch 241/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8126\n",
      "Epoch 241: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3979 - acc: 0.8126 - val_loss: 0.4922 - val_acc: 0.7815\n",
      "Epoch 242/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8124\n",
      "Epoch 242: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3979 - acc: 0.8126 - val_loss: 0.5262 - val_acc: 0.7751\n",
      "Epoch 243/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8103\n",
      "Epoch 243: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4000 - acc: 0.8103 - val_loss: 0.4771 - val_acc: 0.7837\n",
      "Epoch 244/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4013 - acc: 0.8124\n",
      "Epoch 244: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4005 - acc: 0.8121 - val_loss: 0.4791 - val_acc: 0.7755\n",
      "Epoch 245/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8082\n",
      "Epoch 245: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3977 - acc: 0.8082 - val_loss: 0.4913 - val_acc: 0.7486\n",
      "Epoch 246/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8149\n",
      "Epoch 246: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3960 - acc: 0.8152 - val_loss: 0.4986 - val_acc: 0.7738\n",
      "Epoch 247/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8067\n",
      "Epoch 247: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4019 - acc: 0.8074 - val_loss: 0.4886 - val_acc: 0.7555\n",
      "Epoch 248/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8083\n",
      "Epoch 248: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3990 - acc: 0.8083 - val_loss: 0.4927 - val_acc: 0.7563\n",
      "Epoch 249/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3895 - acc: 0.8107\n",
      "Epoch 249: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3906 - acc: 0.8107 - val_loss: 0.4909 - val_acc: 0.7841\n",
      "Epoch 250/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4023 - acc: 0.8083\n",
      "Epoch 250: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4023 - acc: 0.8083 - val_loss: 0.4835 - val_acc: 0.7674\n",
      "Epoch 251/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8061\n",
      "Epoch 251: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4052 - acc: 0.8062 - val_loss: 0.4944 - val_acc: 0.7674\n",
      "Epoch 252/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3940 - acc: 0.8107\n",
      "Epoch 252: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3933 - acc: 0.8109 - val_loss: 0.4852 - val_acc: 0.7743\n",
      "Epoch 253/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.3925 - acc: 0.8125\n",
      "Epoch 253: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3934 - acc: 0.8123 - val_loss: 0.4974 - val_acc: 0.7790\n",
      "Epoch 254/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3888 - acc: 0.8169\n",
      "Epoch 254: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3883 - acc: 0.8168 - val_loss: 0.4776 - val_acc: 0.7815\n",
      "Epoch 255/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4019 - acc: 0.8097\n",
      "Epoch 255: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4019 - acc: 0.8094 - val_loss: 0.4993 - val_acc: 0.7495\n",
      "Epoch 256/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8107\n",
      "Epoch 256: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3946 - acc: 0.8106 - val_loss: 0.5085 - val_acc: 0.7674\n",
      "Epoch 257/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8086\n",
      "Epoch 257: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4133 - acc: 0.8090 - val_loss: 0.5554 - val_acc: 0.7666\n",
      "Epoch 258/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4049 - acc: 0.8048\n",
      "Epoch 258: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4049 - acc: 0.8048 - val_loss: 0.5008 - val_acc: 0.7721\n",
      "Epoch 259/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8073\n",
      "Epoch 259: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4047 - acc: 0.8077 - val_loss: 0.5255 - val_acc: 0.7610\n",
      "Epoch 260/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8121\n",
      "Epoch 260: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3935 - acc: 0.8120 - val_loss: 0.4932 - val_acc: 0.7717\n",
      "Epoch 261/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8128\n",
      "Epoch 261: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3972 - acc: 0.8130 - val_loss: 0.5079 - val_acc: 0.7717\n",
      "Epoch 262/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8120\n",
      "Epoch 262: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3968 - acc: 0.8126 - val_loss: 0.4962 - val_acc: 0.7755\n",
      "Epoch 263/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8113\n",
      "Epoch 263: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3959 - acc: 0.8117 - val_loss: 0.5064 - val_acc: 0.7631\n",
      "Epoch 264/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8080\n",
      "Epoch 264: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4034 - acc: 0.8083 - val_loss: 0.4879 - val_acc: 0.7580\n",
      "Epoch 265/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8053\n",
      "Epoch 265: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.8063 - val_loss: 0.4764 - val_acc: 0.7708\n",
      "Epoch 266/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8099\n",
      "Epoch 266: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4026 - acc: 0.8104 - val_loss: 0.4911 - val_acc: 0.7678\n",
      "Epoch 267/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8093\n",
      "Epoch 267: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3901 - acc: 0.8098 - val_loss: 0.4923 - val_acc: 0.7606\n",
      "Epoch 268/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8145\n",
      "Epoch 268: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3921 - acc: 0.8144 - val_loss: 0.4919 - val_acc: 0.7879\n",
      "Epoch 269/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4028 - acc: 0.8092\n",
      "Epoch 269: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4036 - acc: 0.8089 - val_loss: 0.5016 - val_acc: 0.7657\n",
      "Epoch 270/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8146\n",
      "Epoch 270: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3947 - acc: 0.8151 - val_loss: 0.4919 - val_acc: 0.7580\n",
      "Epoch 271/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8122\n",
      "Epoch 271: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4008 - acc: 0.8120 - val_loss: 0.4959 - val_acc: 0.7730\n",
      "Epoch 272/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3915 - acc: 0.8140\n",
      "Epoch 272: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3937 - acc: 0.8144 - val_loss: 0.4833 - val_acc: 0.7790\n",
      "Epoch 273/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8153\n",
      "Epoch 273: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3845 - acc: 0.8157 - val_loss: 0.4798 - val_acc: 0.7713\n",
      "Epoch 274/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3897 - acc: 0.8142\n",
      "Epoch 274: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3896 - acc: 0.8143 - val_loss: 0.4881 - val_acc: 0.7751\n",
      "Epoch 275/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8165\n",
      "Epoch 275: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3837 - acc: 0.8165 - val_loss: 0.5028 - val_acc: 0.7785\n",
      "Epoch 276/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8162\n",
      "Epoch 276: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3872 - acc: 0.8167 - val_loss: 0.4996 - val_acc: 0.7811\n",
      "Epoch 277/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8127\n",
      "Epoch 277: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3962 - acc: 0.8128 - val_loss: 0.5126 - val_acc: 0.7627\n",
      "Epoch 278/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8148\n",
      "Epoch 278: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3934 - acc: 0.8152 - val_loss: 0.5180 - val_acc: 0.7678\n",
      "Epoch 279/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.3888 - acc: 0.8168\n",
      "Epoch 279: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3899 - acc: 0.8164 - val_loss: 0.5125 - val_acc: 0.7747\n",
      "Epoch 280/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3963 - acc: 0.8129\n",
      "Epoch 280: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3952 - acc: 0.8135 - val_loss: 0.5142 - val_acc: 0.7807\n",
      "Epoch 281/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3987 - acc: 0.8125\n",
      "Epoch 281: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3987 - acc: 0.8125 - val_loss: 0.5088 - val_acc: 0.7627\n",
      "Epoch 282/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3925 - acc: 0.8085\n",
      "Epoch 282: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3936 - acc: 0.8078 - val_loss: 0.4995 - val_acc: 0.7790\n",
      "Epoch 283/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8151\n",
      "Epoch 283: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3903 - acc: 0.8157 - val_loss: 0.5119 - val_acc: 0.7751\n",
      "Epoch 284/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8146\n",
      "Epoch 284: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3891 - acc: 0.8148 - val_loss: 0.5056 - val_acc: 0.7747\n",
      "Epoch 285/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8128\n",
      "Epoch 285: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3853 - acc: 0.8126 - val_loss: 0.5180 - val_acc: 0.7850\n",
      "Epoch 286/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8105\n",
      "Epoch 286: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3923 - acc: 0.8106 - val_loss: 0.5272 - val_acc: 0.7726\n",
      "Epoch 287/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8144\n",
      "Epoch 287: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3998 - acc: 0.8150 - val_loss: 0.5204 - val_acc: 0.7820\n",
      "Epoch 288/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8122\n",
      "Epoch 288: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3925 - acc: 0.8127 - val_loss: 0.5498 - val_acc: 0.7593\n",
      "Epoch 289/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8150\n",
      "Epoch 289: val_acc did not improve from 0.79735\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4087 - acc: 0.8152 - val_loss: 0.5117 - val_acc: 0.7751\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 378,881\n",
      "Trainable params: 378,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.5437 - acc: 0.5922\n",
      "Epoch 1: val_acc improved from -inf to 0.67507, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 1.5437 - acc: 0.5922 - val_loss: 0.6595 - val_acc: 0.6751\n",
      "Epoch 2/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.6406\n",
      "Epoch 2: val_acc did not improve from 0.67507\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6689 - acc: 0.6428 - val_loss: 0.6438 - val_acc: 0.6712\n",
      "Epoch 3/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.6422 - acc: 0.6677\n",
      "Epoch 3: val_acc improved from 0.67507 to 0.67678, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6408 - acc: 0.6689 - val_loss: 0.6284 - val_acc: 0.6768\n",
      "Epoch 4/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.6169 - acc: 0.6879\n",
      "Epoch 4: val_acc improved from 0.67678 to 0.69944, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6162 - acc: 0.6892 - val_loss: 0.6219 - val_acc: 0.6994\n",
      "Epoch 5/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.6916\n",
      "Epoch 5: val_acc improved from 0.69944 to 0.70799, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6069 - acc: 0.6923 - val_loss: 0.6008 - val_acc: 0.7080\n",
      "Epoch 6/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.7045\n",
      "Epoch 6: val_acc improved from 0.70799 to 0.70928, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5941 - acc: 0.7053 - val_loss: 0.5926 - val_acc: 0.7093\n",
      "Epoch 7/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5831 - acc: 0.7123\n",
      "Epoch 7: val_acc improved from 0.70928 to 0.71441, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5828 - acc: 0.7128 - val_loss: 0.5720 - val_acc: 0.7144\n",
      "Epoch 8/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.7156\n",
      "Epoch 8: val_acc improved from 0.71441 to 0.71655, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5775 - acc: 0.7160 - val_loss: 0.5837 - val_acc: 0.7165\n",
      "Epoch 9/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5684 - acc: 0.7205\n",
      "Epoch 9: val_acc improved from 0.71655 to 0.71697, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5670 - acc: 0.7218 - val_loss: 0.5554 - val_acc: 0.7170\n",
      "Epoch 10/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5657 - acc: 0.7227\n",
      "Epoch 10: val_acc improved from 0.71697 to 0.71868, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5651 - acc: 0.7239 - val_loss: 0.5633 - val_acc: 0.7187\n",
      "Epoch 11/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.7316\n",
      "Epoch 11: val_acc improved from 0.71868 to 0.71954, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5549 - acc: 0.7316 - val_loss: 0.5546 - val_acc: 0.7195\n",
      "Epoch 12/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.7279\n",
      "Epoch 12: val_acc improved from 0.71954 to 0.72210, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5510 - acc: 0.7283 - val_loss: 0.5555 - val_acc: 0.7221\n",
      "Epoch 13/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7327\n",
      "Epoch 13: val_acc improved from 0.72210 to 0.72552, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5485 - acc: 0.7335 - val_loss: 0.5367 - val_acc: 0.7255\n",
      "Epoch 14/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5397 - acc: 0.7374\n",
      "Epoch 14: val_acc improved from 0.72552 to 0.72852, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5397 - acc: 0.7374 - val_loss: 0.5272 - val_acc: 0.7285\n",
      "Epoch 15/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.7365\n",
      "Epoch 15: val_acc improved from 0.72852 to 0.73621, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5370 - acc: 0.7374 - val_loss: 0.5246 - val_acc: 0.7362\n",
      "Epoch 16/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.7362\n",
      "Epoch 16: val_acc did not improve from 0.73621\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5358 - acc: 0.7368 - val_loss: 0.5250 - val_acc: 0.7307\n",
      "Epoch 17/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.7374\n",
      "Epoch 17: val_acc did not improve from 0.73621\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5326 - acc: 0.7379 - val_loss: 0.5273 - val_acc: 0.7272\n",
      "Epoch 18/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.7377\n",
      "Epoch 18: val_acc did not improve from 0.73621\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5286 - acc: 0.7378 - val_loss: 0.5218 - val_acc: 0.7302\n",
      "Epoch 19/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.7435\n",
      "Epoch 19: val_acc did not improve from 0.73621\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5187 - acc: 0.7438 - val_loss: 0.5261 - val_acc: 0.7362\n",
      "Epoch 20/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.7417\n",
      "Epoch 20: val_acc did not improve from 0.73621\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5216 - acc: 0.7427 - val_loss: 0.5269 - val_acc: 0.7268\n",
      "Epoch 21/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.7440\n",
      "Epoch 21: val_acc improved from 0.73621 to 0.73878, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5186 - acc: 0.7441 - val_loss: 0.5101 - val_acc: 0.7388\n",
      "Epoch 22/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5128 - acc: 0.7462\n",
      "Epoch 22: val_acc did not improve from 0.73878\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5129 - acc: 0.7469 - val_loss: 0.5132 - val_acc: 0.7383\n",
      "Epoch 23/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.7483\n",
      "Epoch 23: val_acc did not improve from 0.73878\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5123 - acc: 0.7488 - val_loss: 0.5041 - val_acc: 0.7375\n",
      "Epoch 24/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.7467\n",
      "Epoch 24: val_acc improved from 0.73878 to 0.74006, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5104 - acc: 0.7474 - val_loss: 0.5088 - val_acc: 0.7401\n",
      "Epoch 25/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5078 - acc: 0.7540\n",
      "Epoch 25: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5076 - acc: 0.7541 - val_loss: 0.4968 - val_acc: 0.7396\n",
      "Epoch 26/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.7552\n",
      "Epoch 26: val_acc did not improve from 0.74006\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5028 - acc: 0.7556 - val_loss: 0.5039 - val_acc: 0.7379\n",
      "Epoch 27/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5005 - acc: 0.7529\n",
      "Epoch 27: val_acc improved from 0.74006 to 0.74434, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5005 - acc: 0.7529 - val_loss: 0.4973 - val_acc: 0.7443\n",
      "Epoch 28/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.7559\n",
      "Epoch 28: val_acc did not improve from 0.74434\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4988 - acc: 0.7566 - val_loss: 0.5070 - val_acc: 0.7405\n",
      "Epoch 29/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.7577\n",
      "Epoch 29: val_acc did not improve from 0.74434\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4950 - acc: 0.7584 - val_loss: 0.4964 - val_acc: 0.7435\n",
      "Epoch 30/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.7573\n",
      "Epoch 30: val_acc did not improve from 0.74434\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4933 - acc: 0.7576 - val_loss: 0.5024 - val_acc: 0.7349\n",
      "Epoch 31/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4862 - acc: 0.7670\n",
      "Epoch 31: val_acc improved from 0.74434 to 0.74647, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4857 - acc: 0.7679 - val_loss: 0.5060 - val_acc: 0.7465\n",
      "Epoch 32/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4879 - acc: 0.7602\n",
      "Epoch 32: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4882 - acc: 0.7602 - val_loss: 0.5012 - val_acc: 0.7375\n",
      "Epoch 33/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.7563\n",
      "Epoch 33: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4946 - acc: 0.7567 - val_loss: 0.5135 - val_acc: 0.7388\n",
      "Epoch 34/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4820 - acc: 0.7650\n",
      "Epoch 34: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4820 - acc: 0.7650 - val_loss: 0.4931 - val_acc: 0.7456\n",
      "Epoch 35/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.7651\n",
      "Epoch 35: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4816 - acc: 0.7668 - val_loss: 0.4952 - val_acc: 0.7456\n",
      "Epoch 36/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4809 - acc: 0.7644\n",
      "Epoch 36: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4803 - acc: 0.7650 - val_loss: 0.4961 - val_acc: 0.7388\n",
      "Epoch 37/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.7679\n",
      "Epoch 37: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4792 - acc: 0.7687 - val_loss: 0.4891 - val_acc: 0.7418\n",
      "Epoch 38/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.7653\n",
      "Epoch 38: val_acc did not improve from 0.74647\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4793 - acc: 0.7658 - val_loss: 0.5086 - val_acc: 0.7396\n",
      "Epoch 39/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4852 - acc: 0.7642\n",
      "Epoch 39: val_acc improved from 0.74647 to 0.75502, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4836 - acc: 0.7648 - val_loss: 0.4861 - val_acc: 0.7550\n",
      "Epoch 40/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4752 - acc: 0.7690\n",
      "Epoch 40: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4752 - acc: 0.7690 - val_loss: 0.4836 - val_acc: 0.7443\n",
      "Epoch 41/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.7679\n",
      "Epoch 41: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4772 - acc: 0.7669 - val_loss: 0.5053 - val_acc: 0.7388\n",
      "Epoch 42/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4768 - acc: 0.7704\n",
      "Epoch 42: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4768 - acc: 0.7705 - val_loss: 0.4845 - val_acc: 0.7490\n",
      "Epoch 43/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4819 - acc: 0.7708\n",
      "Epoch 43: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4811 - acc: 0.7706 - val_loss: 0.4905 - val_acc: 0.7422\n",
      "Epoch 44/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4751 - acc: 0.7665\n",
      "Epoch 44: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4753 - acc: 0.7666 - val_loss: 0.4875 - val_acc: 0.7358\n",
      "Epoch 45/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.7708\n",
      "Epoch 45: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4692 - acc: 0.7706 - val_loss: 0.4762 - val_acc: 0.7426\n",
      "Epoch 46/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.7666\n",
      "Epoch 46: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4740 - acc: 0.7674 - val_loss: 0.4873 - val_acc: 0.7409\n",
      "Epoch 47/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4637 - acc: 0.7759\n",
      "Epoch 47: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4636 - acc: 0.7757 - val_loss: 0.4850 - val_acc: 0.7478\n",
      "Epoch 48/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4578 - acc: 0.7776\n",
      "Epoch 48: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4578 - acc: 0.7776 - val_loss: 0.4775 - val_acc: 0.7525\n",
      "Epoch 49/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.7735\n",
      "Epoch 49: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4658 - acc: 0.7743 - val_loss: 0.4907 - val_acc: 0.7375\n",
      "Epoch 50/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.7784\n",
      "Epoch 50: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4601 - acc: 0.7793 - val_loss: 0.4761 - val_acc: 0.7537\n",
      "Epoch 51/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4637 - acc: 0.7734\n",
      "Epoch 51: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4643 - acc: 0.7732 - val_loss: 0.4891 - val_acc: 0.7349\n",
      "Epoch 52/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4632 - acc: 0.7762\n",
      "Epoch 52: val_acc improved from 0.75502 to 0.76058, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4634 - acc: 0.7762 - val_loss: 0.4744 - val_acc: 0.7606\n",
      "Epoch 53/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.7767\n",
      "Epoch 53: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4643 - acc: 0.7770 - val_loss: 0.4817 - val_acc: 0.7460\n",
      "Epoch 54/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.7801\n",
      "Epoch 54: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4599 - acc: 0.7799 - val_loss: 0.4762 - val_acc: 0.7490\n",
      "Epoch 55/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4537 - acc: 0.7784\n",
      "Epoch 55: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4539 - acc: 0.7786 - val_loss: 0.4858 - val_acc: 0.7452\n",
      "Epoch 56/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.7819\n",
      "Epoch 56: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4527 - acc: 0.7825 - val_loss: 0.4784 - val_acc: 0.7413\n",
      "Epoch 57/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.7776\n",
      "Epoch 57: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4530 - acc: 0.7778 - val_loss: 0.4730 - val_acc: 0.7478\n",
      "Epoch 58/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4550 - acc: 0.7874\n",
      "Epoch 58: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4545 - acc: 0.7876 - val_loss: 0.4858 - val_acc: 0.7507\n",
      "Epoch 59/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.7793\n",
      "Epoch 59: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4590 - acc: 0.7793 - val_loss: 0.4746 - val_acc: 0.7482\n",
      "Epoch 60/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.7841\n",
      "Epoch 60: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4484 - acc: 0.7843 - val_loss: 0.4776 - val_acc: 0.7512\n",
      "Epoch 61/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4485 - acc: 0.7808\n",
      "Epoch 61: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4488 - acc: 0.7816 - val_loss: 0.4744 - val_acc: 0.7516\n",
      "Epoch 62/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.7831\n",
      "Epoch 62: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4524 - acc: 0.7833 - val_loss: 0.5010 - val_acc: 0.7409\n",
      "Epoch 63/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.7805\n",
      "Epoch 63: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4500 - acc: 0.7804 - val_loss: 0.4919 - val_acc: 0.7503\n",
      "Epoch 64/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.7782\n",
      "Epoch 64: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4559 - acc: 0.7788 - val_loss: 0.4729 - val_acc: 0.7593\n",
      "Epoch 65/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.7857\n",
      "Epoch 65: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4439 - acc: 0.7859 - val_loss: 0.4716 - val_acc: 0.7525\n",
      "Epoch 66/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.7858\n",
      "Epoch 66: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4459 - acc: 0.7859 - val_loss: 0.4745 - val_acc: 0.7439\n",
      "Epoch 67/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.7818\n",
      "Epoch 67: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4459 - acc: 0.7818 - val_loss: 0.4772 - val_acc: 0.7465\n",
      "Epoch 68/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.7848\n",
      "Epoch 68: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4444 - acc: 0.7850 - val_loss: 0.4627 - val_acc: 0.7559\n",
      "Epoch 69/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4447 - acc: 0.7884\n",
      "Epoch 69: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4438 - acc: 0.7893 - val_loss: 0.4699 - val_acc: 0.7550\n",
      "Epoch 70/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4483 - acc: 0.7875\n",
      "Epoch 70: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4481 - acc: 0.7877 - val_loss: 0.4778 - val_acc: 0.7482\n",
      "Epoch 71/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4407 - acc: 0.7863\n",
      "Epoch 71: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4406 - acc: 0.7869 - val_loss: 0.4743 - val_acc: 0.7550\n",
      "Epoch 72/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.7809\n",
      "Epoch 72: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4538 - acc: 0.7813 - val_loss: 0.4691 - val_acc: 0.7559\n",
      "Epoch 73/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.7865\n",
      "Epoch 73: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4461 - acc: 0.7865 - val_loss: 0.4708 - val_acc: 0.7456\n",
      "Epoch 74/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.7820\n",
      "Epoch 74: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4537 - acc: 0.7828 - val_loss: 0.4687 - val_acc: 0.7546\n",
      "Epoch 75/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4445 - acc: 0.7886\n",
      "Epoch 75: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4441 - acc: 0.7890 - val_loss: 0.4738 - val_acc: 0.7495\n",
      "Epoch 76/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.7833\n",
      "Epoch 76: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4468 - acc: 0.7833 - val_loss: 0.4823 - val_acc: 0.7576\n",
      "Epoch 77/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.7866\n",
      "Epoch 77: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4443 - acc: 0.7872 - val_loss: 0.4888 - val_acc: 0.7478\n",
      "Epoch 78/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.7861\n",
      "Epoch 78: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4430 - acc: 0.7865 - val_loss: 0.4687 - val_acc: 0.7555\n",
      "Epoch 79/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4415 - acc: 0.7895\n",
      "Epoch 79: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4415 - acc: 0.7898 - val_loss: 0.4696 - val_acc: 0.7559\n",
      "Epoch 80/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.7852\n",
      "Epoch 80: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4436 - acc: 0.7856 - val_loss: 0.4580 - val_acc: 0.7589\n",
      "Epoch 81/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4397 - acc: 0.7883\n",
      "Epoch 81: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4396 - acc: 0.7887 - val_loss: 0.4657 - val_acc: 0.7512\n",
      "Epoch 82/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7878\n",
      "Epoch 82: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4443 - acc: 0.7881 - val_loss: 0.4654 - val_acc: 0.7542\n",
      "Epoch 83/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.7912\n",
      "Epoch 83: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4302 - acc: 0.7918 - val_loss: 0.4687 - val_acc: 0.7516\n",
      "Epoch 84/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4357 - acc: 0.7900\n",
      "Epoch 84: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4356 - acc: 0.7903 - val_loss: 0.4851 - val_acc: 0.7418\n",
      "Epoch 85/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4366 - acc: 0.7882\n",
      "Epoch 85: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4366 - acc: 0.7882 - val_loss: 0.4777 - val_acc: 0.7503\n",
      "Epoch 86/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.7879\n",
      "Epoch 86: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4301 - acc: 0.7879 - val_loss: 0.4677 - val_acc: 0.7546\n",
      "Epoch 87/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.7968\n",
      "Epoch 87: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4215 - acc: 0.7972 - val_loss: 0.4683 - val_acc: 0.7563\n",
      "Epoch 88/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.7929\n",
      "Epoch 88: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4341 - acc: 0.7936 - val_loss: 0.4772 - val_acc: 0.7482\n",
      "Epoch 89/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4355 - acc: 0.7918\n",
      "Epoch 89: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4355 - acc: 0.7918 - val_loss: 0.4620 - val_acc: 0.7602\n",
      "Epoch 90/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4328 - acc: 0.7949\n",
      "Epoch 90: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4320 - acc: 0.7948 - val_loss: 0.4545 - val_acc: 0.7593\n",
      "Epoch 91/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4265 - acc: 0.7945\n",
      "Epoch 91: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4269 - acc: 0.7936 - val_loss: 0.4765 - val_acc: 0.7499\n",
      "Epoch 92/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.7901\n",
      "Epoch 92: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4376 - acc: 0.7907 - val_loss: 0.5003 - val_acc: 0.7341\n",
      "Epoch 93/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4346 - acc: 0.7928\n",
      "Epoch 93: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4347 - acc: 0.7923 - val_loss: 0.4806 - val_acc: 0.7520\n",
      "Epoch 94/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.7905\n",
      "Epoch 94: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4362 - acc: 0.7911 - val_loss: 0.4730 - val_acc: 0.7559\n",
      "Epoch 95/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.7933\n",
      "Epoch 95: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4228 - acc: 0.7939 - val_loss: 0.4652 - val_acc: 0.7576\n",
      "Epoch 96/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4276 - acc: 0.7944\n",
      "Epoch 96: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4276 - acc: 0.7944 - val_loss: 0.4565 - val_acc: 0.7567\n",
      "Epoch 97/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.7964\n",
      "Epoch 97: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4246 - acc: 0.7965 - val_loss: 0.4646 - val_acc: 0.7550\n",
      "Epoch 98/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4281 - acc: 0.7969\n",
      "Epoch 98: val_acc did not improve from 0.76058\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4280 - acc: 0.7968 - val_loss: 0.4766 - val_acc: 0.7567\n",
      "Epoch 99/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.7945\n",
      "Epoch 99: val_acc improved from 0.76058 to 0.76657, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4273 - acc: 0.7947 - val_loss: 0.4716 - val_acc: 0.7666\n",
      "Epoch 100/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4222 - acc: 0.7952\n",
      "Epoch 100: val_acc did not improve from 0.76657\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4247 - acc: 0.7941 - val_loss: 0.4938 - val_acc: 0.7473\n",
      "Epoch 101/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4291 - acc: 0.7965\n",
      "Epoch 101: val_acc did not improve from 0.76657\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4282 - acc: 0.7969 - val_loss: 0.4660 - val_acc: 0.7627\n",
      "Epoch 102/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.7948\n",
      "Epoch 102: val_acc did not improve from 0.76657\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4302 - acc: 0.7950 - val_loss: 0.4714 - val_acc: 0.7533\n",
      "Epoch 103/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4253 - acc: 0.7962\n",
      "Epoch 103: val_acc did not improve from 0.76657\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4251 - acc: 0.7962 - val_loss: 0.4614 - val_acc: 0.7559\n",
      "Epoch 104/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.7945\n",
      "Epoch 104: val_acc improved from 0.76657 to 0.77939, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4321 - acc: 0.7944 - val_loss: 0.4548 - val_acc: 0.7794\n",
      "Epoch 105/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.7956\n",
      "Epoch 105: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4308 - acc: 0.7962 - val_loss: 0.4648 - val_acc: 0.7456\n",
      "Epoch 106/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4163 - acc: 0.7976\n",
      "Epoch 106: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4151 - acc: 0.7986 - val_loss: 0.4503 - val_acc: 0.7666\n",
      "Epoch 107/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.7984\n",
      "Epoch 107: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4252 - acc: 0.7993 - val_loss: 0.4715 - val_acc: 0.7507\n",
      "Epoch 108/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8016\n",
      "Epoch 108: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4202 - acc: 0.8014 - val_loss: 0.4641 - val_acc: 0.7495\n",
      "Epoch 109/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4240 - acc: 0.7965\n",
      "Epoch 109: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4253 - acc: 0.7970 - val_loss: 0.4595 - val_acc: 0.7614\n",
      "Epoch 110/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4221 - acc: 0.7974\n",
      "Epoch 110: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4215 - acc: 0.7974 - val_loss: 0.4447 - val_acc: 0.7743\n",
      "Epoch 111/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.7977\n",
      "Epoch 111: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4196 - acc: 0.7981 - val_loss: 0.4599 - val_acc: 0.7619\n",
      "Epoch 112/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.7937\n",
      "Epoch 112: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4266 - acc: 0.7943 - val_loss: 0.4691 - val_acc: 0.7640\n",
      "Epoch 113/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.7911\n",
      "Epoch 113: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4268 - acc: 0.7919 - val_loss: 0.4608 - val_acc: 0.7567\n",
      "Epoch 114/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4167 - acc: 0.8069\n",
      "Epoch 114: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4169 - acc: 0.8069 - val_loss: 0.4669 - val_acc: 0.7516\n",
      "Epoch 115/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.7993\n",
      "Epoch 115: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4166 - acc: 0.7998 - val_loss: 0.4648 - val_acc: 0.7559\n",
      "Epoch 116/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4312 - acc: 0.7967\n",
      "Epoch 116: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4312 - acc: 0.7967 - val_loss: 0.4615 - val_acc: 0.7580\n",
      "Epoch 117/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4245 - acc: 0.7972\n",
      "Epoch 117: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4245 - acc: 0.7972 - val_loss: 0.4490 - val_acc: 0.7730\n",
      "Epoch 118/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8006\n",
      "Epoch 118: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4170 - acc: 0.8007 - val_loss: 0.4558 - val_acc: 0.7546\n",
      "Epoch 119/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.7961\n",
      "Epoch 119: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4225 - acc: 0.7973 - val_loss: 0.4725 - val_acc: 0.7537\n",
      "Epoch 120/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4290 - acc: 0.8008\n",
      "Epoch 120: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4273 - acc: 0.8017 - val_loss: 0.4641 - val_acc: 0.7597\n",
      "Epoch 121/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4318 - acc: 0.7999\n",
      "Epoch 121: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4310 - acc: 0.8006 - val_loss: 0.4658 - val_acc: 0.7512\n",
      "Epoch 122/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4191 - acc: 0.7990\n",
      "Epoch 122: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4177 - acc: 0.7999 - val_loss: 0.4569 - val_acc: 0.7657\n",
      "Epoch 123/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.7984\n",
      "Epoch 123: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4198 - acc: 0.7987 - val_loss: 0.4635 - val_acc: 0.7589\n",
      "Epoch 124/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.7995\n",
      "Epoch 124: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4203 - acc: 0.8001 - val_loss: 0.4572 - val_acc: 0.7606\n",
      "Epoch 125/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4233 - acc: 0.7983\n",
      "Epoch 125: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4233 - acc: 0.7983 - val_loss: 0.4631 - val_acc: 0.7503\n",
      "Epoch 126/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4237 - acc: 0.7980\n",
      "Epoch 126: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4237 - acc: 0.7981 - val_loss: 0.4653 - val_acc: 0.7495\n",
      "Epoch 127/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8001\n",
      "Epoch 127: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4167 - acc: 0.8004 - val_loss: 0.4756 - val_acc: 0.7537\n",
      "Epoch 128/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.7972\n",
      "Epoch 128: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4182 - acc: 0.7978 - val_loss: 0.4630 - val_acc: 0.7495\n",
      "Epoch 129/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8022\n",
      "Epoch 129: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4141 - acc: 0.8028 - val_loss: 0.4499 - val_acc: 0.7773\n",
      "Epoch 130/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4137 - acc: 0.8033\n",
      "Epoch 130: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4136 - acc: 0.8035 - val_loss: 0.4609 - val_acc: 0.7469\n",
      "Epoch 131/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8030\n",
      "Epoch 131: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4125 - acc: 0.8034 - val_loss: 0.4748 - val_acc: 0.7623\n",
      "Epoch 132/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8012\n",
      "Epoch 132: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4207 - acc: 0.8018 - val_loss: 0.4633 - val_acc: 0.7576\n",
      "Epoch 133/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.7992\n",
      "Epoch 133: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4175 - acc: 0.7997 - val_loss: 0.4585 - val_acc: 0.7542\n",
      "Epoch 134/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4115 - acc: 0.8008\n",
      "Epoch 134: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4127 - acc: 0.8016 - val_loss: 0.4644 - val_acc: 0.7627\n",
      "Epoch 135/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4095 - acc: 0.8001\n",
      "Epoch 135: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4097 - acc: 0.8000 - val_loss: 0.4604 - val_acc: 0.7516\n",
      "Epoch 136/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4123 - acc: 0.8049\n",
      "Epoch 136: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4123 - acc: 0.8049 - val_loss: 0.4522 - val_acc: 0.7572\n",
      "Epoch 137/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4090 - acc: 0.8026\n",
      "Epoch 137: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4086 - acc: 0.8032 - val_loss: 0.4707 - val_acc: 0.7580\n",
      "Epoch 138/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8050\n",
      "Epoch 138: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4173 - acc: 0.8056 - val_loss: 0.4666 - val_acc: 0.7478\n",
      "Epoch 139/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8030\n",
      "Epoch 139: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4128 - acc: 0.8030 - val_loss: 0.4608 - val_acc: 0.7593\n",
      "Epoch 140/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4142 - acc: 0.8032\n",
      "Epoch 140: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4142 - acc: 0.8032 - val_loss: 0.4516 - val_acc: 0.7623\n",
      "Epoch 141/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.8058\n",
      "Epoch 141: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3971 - acc: 0.8057 - val_loss: 0.4459 - val_acc: 0.7696\n",
      "Epoch 142/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8021\n",
      "Epoch 142: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4205 - acc: 0.8028 - val_loss: 0.4552 - val_acc: 0.7691\n",
      "Epoch 143/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8035\n",
      "Epoch 143: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4120 - acc: 0.8037 - val_loss: 0.4655 - val_acc: 0.7555\n",
      "Epoch 144/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8051\n",
      "Epoch 144: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4117 - acc: 0.8056 - val_loss: 0.4542 - val_acc: 0.7640\n",
      "Epoch 145/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8057\n",
      "Epoch 145: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4071 - acc: 0.8058 - val_loss: 0.4526 - val_acc: 0.7614\n",
      "Epoch 146/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4146 - acc: 0.8073\n",
      "Epoch 146: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4146 - acc: 0.8073 - val_loss: 0.4431 - val_acc: 0.7743\n",
      "Epoch 147/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8080\n",
      "Epoch 147: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4088 - acc: 0.8084 - val_loss: 0.4541 - val_acc: 0.7713\n",
      "Epoch 148/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4070 - acc: 0.8072\n",
      "Epoch 148: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4068 - acc: 0.8072 - val_loss: 0.4516 - val_acc: 0.7691\n",
      "Epoch 149/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8046\n",
      "Epoch 149: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4021 - acc: 0.8055 - val_loss: 0.4555 - val_acc: 0.7542\n",
      "Epoch 150/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4129 - acc: 0.8013\n",
      "Epoch 150: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4122 - acc: 0.8016 - val_loss: 0.4517 - val_acc: 0.7623\n",
      "Epoch 151/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8045\n",
      "Epoch 151: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4118 - acc: 0.8046 - val_loss: 0.4610 - val_acc: 0.7666\n",
      "Epoch 152/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.7987\n",
      "Epoch 152: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4260 - acc: 0.7990 - val_loss: 0.4567 - val_acc: 0.7593\n",
      "Epoch 153/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8069\n",
      "Epoch 153: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4072 - acc: 0.8068 - val_loss: 0.4663 - val_acc: 0.7516\n",
      "Epoch 154/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8096\n",
      "Epoch 154: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4052 - acc: 0.8095 - val_loss: 0.4613 - val_acc: 0.7636\n",
      "Epoch 155/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8057\n",
      "Epoch 155: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4066 - acc: 0.8058 - val_loss: 0.4552 - val_acc: 0.7563\n",
      "Epoch 156/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8047\n",
      "Epoch 156: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4002 - acc: 0.8053 - val_loss: 0.4466 - val_acc: 0.7657\n",
      "Epoch 157/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8077\n",
      "Epoch 157: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3991 - acc: 0.8069 - val_loss: 0.4472 - val_acc: 0.7696\n",
      "Epoch 158/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4148 - acc: 0.8038\n",
      "Epoch 158: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4143 - acc: 0.8037 - val_loss: 0.4670 - val_acc: 0.7563\n",
      "Epoch 159/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8020\n",
      "Epoch 159: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4182 - acc: 0.8021 - val_loss: 0.4574 - val_acc: 0.7567\n",
      "Epoch 160/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8037\n",
      "Epoch 160: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4091 - acc: 0.8035 - val_loss: 0.4688 - val_acc: 0.7469\n",
      "Epoch 161/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8085\n",
      "Epoch 161: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4076 - acc: 0.8087 - val_loss: 0.4650 - val_acc: 0.7422\n",
      "Epoch 162/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8079\n",
      "Epoch 162: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4074 - acc: 0.8081 - val_loss: 0.4491 - val_acc: 0.7584\n",
      "Epoch 163/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4006 - acc: 0.8079\n",
      "Epoch 163: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4007 - acc: 0.8083 - val_loss: 0.4747 - val_acc: 0.7418\n",
      "Epoch 164/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4023 - acc: 0.8120\n",
      "Epoch 164: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4023 - acc: 0.8120 - val_loss: 0.4573 - val_acc: 0.7589\n",
      "Epoch 165/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4017 - acc: 0.8107\n",
      "Epoch 165: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4016 - acc: 0.8106 - val_loss: 0.4657 - val_acc: 0.7435\n",
      "Epoch 166/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8100\n",
      "Epoch 166: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3977 - acc: 0.8099 - val_loss: 0.4609 - val_acc: 0.7482\n",
      "Epoch 167/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8083\n",
      "Epoch 167: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4039 - acc: 0.8086 - val_loss: 0.4534 - val_acc: 0.7537\n",
      "Epoch 168/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8108\n",
      "Epoch 168: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4019 - acc: 0.8110 - val_loss: 0.4459 - val_acc: 0.7691\n",
      "Epoch 169/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4033 - acc: 0.8083\n",
      "Epoch 169: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4022 - acc: 0.8087 - val_loss: 0.4561 - val_acc: 0.7580\n",
      "Epoch 170/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8043\n",
      "Epoch 170: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4053 - acc: 0.8045 - val_loss: 0.5003 - val_acc: 0.7225\n",
      "Epoch 171/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4129 - acc: 0.8067\n",
      "Epoch 171: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4129 - acc: 0.8067 - val_loss: 0.4597 - val_acc: 0.7512\n",
      "Epoch 172/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8065\n",
      "Epoch 172: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4103 - acc: 0.8066 - val_loss: 0.4677 - val_acc: 0.7499\n",
      "Epoch 173/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8053\n",
      "Epoch 173: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4071 - acc: 0.8060 - val_loss: 0.4555 - val_acc: 0.7550\n",
      "Epoch 174/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8073\n",
      "Epoch 174: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4056 - acc: 0.8081 - val_loss: 0.4498 - val_acc: 0.7674\n",
      "Epoch 175/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4152 - acc: 0.8058\n",
      "Epoch 175: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4152 - acc: 0.8058 - val_loss: 0.4530 - val_acc: 0.7623\n",
      "Epoch 176/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8058\n",
      "Epoch 176: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4044 - acc: 0.8058 - val_loss: 0.4585 - val_acc: 0.7478\n",
      "Epoch 177/2000\n",
      "279/293 [===========================>..] - ETA: 0s - loss: 0.4046 - acc: 0.8058\n",
      "Epoch 177: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4048 - acc: 0.8059 - val_loss: 0.4582 - val_acc: 0.7614\n",
      "Epoch 178/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8075\n",
      "Epoch 178: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4027 - acc: 0.8077 - val_loss: 0.4541 - val_acc: 0.7555\n",
      "Epoch 179/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3999 - acc: 0.8063\n",
      "Epoch 179: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3999 - acc: 0.8063 - val_loss: 0.4420 - val_acc: 0.7653\n",
      "Epoch 180/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8087\n",
      "Epoch 180: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4034 - acc: 0.8091 - val_loss: 0.4497 - val_acc: 0.7576\n",
      "Epoch 181/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8155\n",
      "Epoch 181: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3897 - acc: 0.8157 - val_loss: 0.4569 - val_acc: 0.7636\n",
      "Epoch 182/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.8054\n",
      "Epoch 182: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4171 - acc: 0.8055 - val_loss: 0.4558 - val_acc: 0.7563\n",
      "Epoch 183/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4054 - acc: 0.8048\n",
      "Epoch 183: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4053 - acc: 0.8050 - val_loss: 0.4588 - val_acc: 0.7533\n",
      "Epoch 184/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8059\n",
      "Epoch 184: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4059 - acc: 0.8060 - val_loss: 0.4613 - val_acc: 0.7580\n",
      "Epoch 185/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8071\n",
      "Epoch 185: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4131 - acc: 0.8077 - val_loss: 0.4545 - val_acc: 0.7627\n",
      "Epoch 186/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8044\n",
      "Epoch 186: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3974 - acc: 0.8041 - val_loss: 0.4388 - val_acc: 0.7704\n",
      "Epoch 187/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8057\n",
      "Epoch 187: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4010 - acc: 0.8062 - val_loss: 0.4544 - val_acc: 0.7499\n",
      "Epoch 188/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4010 - acc: 0.8087\n",
      "Epoch 188: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4017 - acc: 0.8083 - val_loss: 0.4657 - val_acc: 0.7469\n",
      "Epoch 189/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8094\n",
      "Epoch 189: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4139 - acc: 0.8094 - val_loss: 0.4544 - val_acc: 0.7589\n",
      "Epoch 190/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8095\n",
      "Epoch 190: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3929 - acc: 0.8102 - val_loss: 0.4511 - val_acc: 0.7602\n",
      "Epoch 191/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4049 - acc: 0.8078\n",
      "Epoch 191: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4039 - acc: 0.8082 - val_loss: 0.4612 - val_acc: 0.7576\n",
      "Epoch 192/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8102\n",
      "Epoch 192: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3994 - acc: 0.8107 - val_loss: 0.4456 - val_acc: 0.7644\n",
      "Epoch 193/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8097\n",
      "Epoch 193: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4108 - acc: 0.8097 - val_loss: 0.4717 - val_acc: 0.7512\n",
      "Epoch 194/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8106\n",
      "Epoch 194: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3979 - acc: 0.8107 - val_loss: 0.4597 - val_acc: 0.7465\n",
      "Epoch 195/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8053\n",
      "Epoch 195: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3962 - acc: 0.8058 - val_loss: 0.4424 - val_acc: 0.7636\n",
      "Epoch 196/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8104\n",
      "Epoch 196: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3997 - acc: 0.8106 - val_loss: 0.4493 - val_acc: 0.7623\n",
      "Epoch 197/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8065\n",
      "Epoch 197: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4025 - acc: 0.8072 - val_loss: 0.4530 - val_acc: 0.7589\n",
      "Epoch 198/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3962 - acc: 0.8101\n",
      "Epoch 198: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3963 - acc: 0.8096 - val_loss: 0.4491 - val_acc: 0.7593\n",
      "Epoch 199/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4030 - acc: 0.8136\n",
      "Epoch 199: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4022 - acc: 0.8138 - val_loss: 0.4630 - val_acc: 0.7507\n",
      "Epoch 200/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8113\n",
      "Epoch 200: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4010 - acc: 0.8117 - val_loss: 0.4616 - val_acc: 0.7555\n",
      "Epoch 201/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3997 - acc: 0.8063\n",
      "Epoch 201: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4002 - acc: 0.8062 - val_loss: 0.4554 - val_acc: 0.7542\n",
      "Epoch 202/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8050\n",
      "Epoch 202: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4041 - acc: 0.8051 - val_loss: 0.4640 - val_acc: 0.7546\n",
      "Epoch 203/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8078\n",
      "Epoch 203: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4083 - acc: 0.8079 - val_loss: 0.4617 - val_acc: 0.7490\n",
      "Epoch 204/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8069\n",
      "Epoch 204: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4010 - acc: 0.8080 - val_loss: 0.4518 - val_acc: 0.7631\n",
      "Epoch 205/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8036\n",
      "Epoch 205: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4034 - acc: 0.8037 - val_loss: 0.4707 - val_acc: 0.7507\n",
      "Epoch 206/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8078\n",
      "Epoch 206: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4140 - acc: 0.8086 - val_loss: 0.4621 - val_acc: 0.7559\n",
      "Epoch 207/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4152 - acc: 0.8079\n",
      "Epoch 207: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4152 - acc: 0.8079 - val_loss: 0.4552 - val_acc: 0.7666\n",
      "Epoch 208/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8084\n",
      "Epoch 208: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4104 - acc: 0.8083 - val_loss: 0.4465 - val_acc: 0.7657\n",
      "Epoch 209/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8073\n",
      "Epoch 209: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3999 - acc: 0.8080 - val_loss: 0.4561 - val_acc: 0.7606\n",
      "Epoch 210/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8001\n",
      "Epoch 210: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4167 - acc: 0.8005 - val_loss: 0.4602 - val_acc: 0.7563\n",
      "Epoch 211/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8067\n",
      "Epoch 211: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4022 - acc: 0.8073 - val_loss: 0.4709 - val_acc: 0.7503\n",
      "Epoch 212/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3944 - acc: 0.8098\n",
      "Epoch 212: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3942 - acc: 0.8100 - val_loss: 0.4505 - val_acc: 0.7653\n",
      "Epoch 213/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8080\n",
      "Epoch 213: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3933 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.7627\n",
      "Epoch 214/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8125\n",
      "Epoch 214: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4007 - acc: 0.8128 - val_loss: 0.4561 - val_acc: 0.7559\n",
      "Epoch 215/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8123\n",
      "Epoch 215: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3942 - acc: 0.8126 - val_loss: 0.4521 - val_acc: 0.7606\n",
      "Epoch 216/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3974 - acc: 0.8133\n",
      "Epoch 216: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3968 - acc: 0.8135 - val_loss: 0.4483 - val_acc: 0.7721\n",
      "Epoch 217/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8046\n",
      "Epoch 217: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4036 - acc: 0.8048 - val_loss: 0.4587 - val_acc: 0.7555\n",
      "Epoch 218/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8141\n",
      "Epoch 218: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3986 - acc: 0.8144 - val_loss: 0.4533 - val_acc: 0.7661\n",
      "Epoch 219/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3932 - acc: 0.8138\n",
      "Epoch 219: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3947 - acc: 0.8143 - val_loss: 0.4457 - val_acc: 0.7640\n",
      "Epoch 220/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8092\n",
      "Epoch 220: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3972 - acc: 0.8093 - val_loss: 0.4605 - val_acc: 0.7525\n",
      "Epoch 221/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8128\n",
      "Epoch 221: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3934 - acc: 0.8137 - val_loss: 0.4498 - val_acc: 0.7696\n",
      "Epoch 222/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8089\n",
      "Epoch 222: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4114 - acc: 0.8089 - val_loss: 0.4564 - val_acc: 0.7533\n",
      "Epoch 223/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8114\n",
      "Epoch 223: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3989 - acc: 0.8119 - val_loss: 0.4431 - val_acc: 0.7760\n",
      "Epoch 224/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8046\n",
      "Epoch 224: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4035 - acc: 0.8050 - val_loss: 0.4414 - val_acc: 0.7743\n",
      "Epoch 225/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8091\n",
      "Epoch 225: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4041 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.7593\n",
      "Epoch 226/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8121\n",
      "Epoch 226: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3878 - acc: 0.8119 - val_loss: 0.4498 - val_acc: 0.7730\n",
      "Epoch 227/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3891 - acc: 0.8176\n",
      "Epoch 227: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3898 - acc: 0.8170 - val_loss: 0.4593 - val_acc: 0.7559\n",
      "Epoch 228/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8025\n",
      "Epoch 228: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4085 - acc: 0.8028 - val_loss: 0.4555 - val_acc: 0.7597\n",
      "Epoch 229/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8097\n",
      "Epoch 229: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3955 - acc: 0.8103 - val_loss: 0.4646 - val_acc: 0.7507\n",
      "Epoch 230/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4013 - acc: 0.8112\n",
      "Epoch 230: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4013 - acc: 0.8112 - val_loss: 0.4563 - val_acc: 0.7516\n",
      "Epoch 231/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8123\n",
      "Epoch 231: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3923 - acc: 0.8124 - val_loss: 0.4583 - val_acc: 0.7610\n",
      "Epoch 232/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8132\n",
      "Epoch 232: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3990 - acc: 0.8135 - val_loss: 0.4520 - val_acc: 0.7636\n",
      "Epoch 233/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8136\n",
      "Epoch 233: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3873 - acc: 0.8139 - val_loss: 0.4568 - val_acc: 0.7584\n",
      "Epoch 234/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8113\n",
      "Epoch 234: val_acc did not improve from 0.77939\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3983 - acc: 0.8114 - val_loss: 0.4683 - val_acc: 0.7443\n",
      "Epoch 235/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8055\n",
      "Epoch 235: val_acc improved from 0.77939 to 0.78153, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4077 - acc: 0.8056 - val_loss: 0.4365 - val_acc: 0.7815\n",
      "Epoch 236/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8113\n",
      "Epoch 236: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3950 - acc: 0.8113 - val_loss: 0.4528 - val_acc: 0.7550\n",
      "Epoch 237/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8144\n",
      "Epoch 237: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3896 - acc: 0.8144 - val_loss: 0.4534 - val_acc: 0.7619\n",
      "Epoch 238/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8100\n",
      "Epoch 238: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3893 - acc: 0.8102 - val_loss: 0.4475 - val_acc: 0.7721\n",
      "Epoch 239/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3932 - acc: 0.8159\n",
      "Epoch 239: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3932 - acc: 0.8159 - val_loss: 0.4551 - val_acc: 0.7512\n",
      "Epoch 240/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3809 - acc: 0.8145\n",
      "Epoch 240: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3818 - acc: 0.8139 - val_loss: 0.4657 - val_acc: 0.7460\n",
      "Epoch 241/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3967 - acc: 0.8155\n",
      "Epoch 241: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3967 - acc: 0.8155 - val_loss: 0.4569 - val_acc: 0.7649\n",
      "Epoch 242/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3956 - acc: 0.8115\n",
      "Epoch 242: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3956 - acc: 0.8115 - val_loss: 0.4738 - val_acc: 0.7636\n",
      "Epoch 243/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8122\n",
      "Epoch 243: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3994 - acc: 0.8123 - val_loss: 0.4527 - val_acc: 0.7623\n",
      "Epoch 244/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8134\n",
      "Epoch 244: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4011 - acc: 0.8135 - val_loss: 0.4461 - val_acc: 0.7602\n",
      "Epoch 245/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8134\n",
      "Epoch 245: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3898 - acc: 0.8139 - val_loss: 0.4559 - val_acc: 0.7721\n",
      "Epoch 246/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8102\n",
      "Epoch 246: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4140 - acc: 0.8103 - val_loss: 0.4541 - val_acc: 0.7610\n",
      "Epoch 247/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3966 - acc: 0.8126\n",
      "Epoch 247: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3973 - acc: 0.8126 - val_loss: 0.4548 - val_acc: 0.7537\n",
      "Epoch 248/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8127\n",
      "Epoch 248: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3917 - acc: 0.8131 - val_loss: 0.4407 - val_acc: 0.7811\n",
      "Epoch 249/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4030 - acc: 0.8102\n",
      "Epoch 249: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4030 - acc: 0.8102 - val_loss: 0.4635 - val_acc: 0.7580\n",
      "Epoch 250/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8119\n",
      "Epoch 250: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3951 - acc: 0.8122 - val_loss: 0.4507 - val_acc: 0.7610\n",
      "Epoch 251/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8035\n",
      "Epoch 251: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4046 - acc: 0.8036 - val_loss: 0.4665 - val_acc: 0.7460\n",
      "Epoch 252/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8050\n",
      "Epoch 252: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4094 - acc: 0.8051 - val_loss: 0.4565 - val_acc: 0.7687\n",
      "Epoch 253/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3986 - acc: 0.8089\n",
      "Epoch 253: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3986 - acc: 0.8089 - val_loss: 0.4519 - val_acc: 0.7726\n",
      "Epoch 254/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3924 - acc: 0.8124\n",
      "Epoch 254: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3934 - acc: 0.8117 - val_loss: 0.4451 - val_acc: 0.7721\n",
      "Epoch 255/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8127\n",
      "Epoch 255: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3867 - acc: 0.8126 - val_loss: 0.4506 - val_acc: 0.7678\n",
      "Epoch 256/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3863 - acc: 0.8161\n",
      "Epoch 256: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3855 - acc: 0.8165 - val_loss: 0.4503 - val_acc: 0.7747\n",
      "Epoch 257/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3790 - acc: 0.8163\n",
      "Epoch 257: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3787 - acc: 0.8166 - val_loss: 0.4618 - val_acc: 0.7593\n",
      "Epoch 258/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8139\n",
      "Epoch 258: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3921 - acc: 0.8139 - val_loss: 0.4472 - val_acc: 0.7683\n",
      "Epoch 259/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3945 - acc: 0.8149\n",
      "Epoch 259: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3950 - acc: 0.8143 - val_loss: 0.4534 - val_acc: 0.7542\n",
      "Epoch 260/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8116\n",
      "Epoch 260: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3880 - acc: 0.8123 - val_loss: 0.4510 - val_acc: 0.7636\n",
      "Epoch 261/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8109\n",
      "Epoch 261: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4074 - acc: 0.8111 - val_loss: 0.5199 - val_acc: 0.7050\n",
      "Epoch 262/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4055 - acc: 0.8108\n",
      "Epoch 262: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.4041 - acc: 0.8111 - val_loss: 0.4496 - val_acc: 0.7691\n",
      "Epoch 263/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8142\n",
      "Epoch 263: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3888 - acc: 0.8146 - val_loss: 0.4441 - val_acc: 0.7717\n",
      "Epoch 264/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.3871 - acc: 0.8100\n",
      "Epoch 264: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3856 - acc: 0.8109 - val_loss: 0.4488 - val_acc: 0.7717\n",
      "Epoch 265/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3857 - acc: 0.8152\n",
      "Epoch 265: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3852 - acc: 0.8153 - val_loss: 0.4679 - val_acc: 0.7559\n",
      "Epoch 266/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8138\n",
      "Epoch 266: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3888 - acc: 0.8139 - val_loss: 0.4497 - val_acc: 0.7661\n",
      "Epoch 267/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8175\n",
      "Epoch 267: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3884 - acc: 0.8180 - val_loss: 0.4664 - val_acc: 0.7563\n",
      "Epoch 268/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3923 - acc: 0.8103\n",
      "Epoch 268: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3932 - acc: 0.8100 - val_loss: 0.4580 - val_acc: 0.7520\n",
      "Epoch 269/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8115\n",
      "Epoch 269: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4115 - acc: 0.8118 - val_loss: 0.4665 - val_acc: 0.7542\n",
      "Epoch 270/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8129\n",
      "Epoch 270: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3911 - acc: 0.8129 - val_loss: 0.4651 - val_acc: 0.7507\n",
      "Epoch 271/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3922 - acc: 0.8117\n",
      "Epoch 271: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3922 - acc: 0.8117 - val_loss: 0.4437 - val_acc: 0.7704\n",
      "Epoch 272/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8170\n",
      "Epoch 272: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3835 - acc: 0.8174 - val_loss: 0.4623 - val_acc: 0.7533\n",
      "Epoch 273/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8160\n",
      "Epoch 273: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3797 - acc: 0.8161 - val_loss: 0.4640 - val_acc: 0.7674\n",
      "Epoch 274/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.3902 - acc: 0.8131\n",
      "Epoch 274: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3893 - acc: 0.8131 - val_loss: 0.4471 - val_acc: 0.7726\n",
      "Epoch 275/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8095\n",
      "Epoch 275: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3894 - acc: 0.8098 - val_loss: 0.4571 - val_acc: 0.7606\n",
      "Epoch 276/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8127\n",
      "Epoch 276: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3916 - acc: 0.8131 - val_loss: 0.4460 - val_acc: 0.7683\n",
      "Epoch 277/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3964 - acc: 0.8117\n",
      "Epoch 277: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3964 - acc: 0.8117 - val_loss: 0.4581 - val_acc: 0.7546\n",
      "Epoch 278/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3818 - acc: 0.8171\n",
      "Epoch 278: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3818 - acc: 0.8171 - val_loss: 0.4472 - val_acc: 0.7666\n",
      "Epoch 279/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8180\n",
      "Epoch 279: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3831 - acc: 0.8184 - val_loss: 0.4577 - val_acc: 0.7584\n",
      "Epoch 280/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3886 - acc: 0.8175\n",
      "Epoch 280: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3886 - acc: 0.8175 - val_loss: 0.4561 - val_acc: 0.7606\n",
      "Epoch 281/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8201\n",
      "Epoch 281: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4122 - acc: 0.8201 - val_loss: 0.4569 - val_acc: 0.7572\n",
      "Epoch 282/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8110\n",
      "Epoch 282: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3929 - acc: 0.8113 - val_loss: 0.4380 - val_acc: 0.7781\n",
      "Epoch 283/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8166\n",
      "Epoch 283: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3808 - acc: 0.8172 - val_loss: 0.4474 - val_acc: 0.7640\n",
      "Epoch 284/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8211\n",
      "Epoch 284: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3847 - acc: 0.8212 - val_loss: 0.4439 - val_acc: 0.7747\n",
      "Epoch 285/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8084\n",
      "Epoch 285: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4121 - acc: 0.8088 - val_loss: 0.4520 - val_acc: 0.7726\n",
      "Epoch 286/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8096\n",
      "Epoch 286: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3909 - acc: 0.8099 - val_loss: 0.4541 - val_acc: 0.7721\n",
      "Epoch 287/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8157\n",
      "Epoch 287: val_acc did not improve from 0.78153\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3794 - acc: 0.8159 - val_loss: 0.4602 - val_acc: 0.7627\n",
      "Epoch 288/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3862 - acc: 0.8161\n",
      "Epoch 288: val_acc improved from 0.78153 to 0.78452, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3862 - acc: 0.8166 - val_loss: 0.4385 - val_acc: 0.7845\n",
      "Epoch 289/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3792 - acc: 0.8168\n",
      "Epoch 289: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3792 - acc: 0.8168 - val_loss: 0.4570 - val_acc: 0.7678\n",
      "Epoch 290/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8168\n",
      "Epoch 290: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3865 - acc: 0.8168 - val_loss: 0.4553 - val_acc: 0.7738\n",
      "Epoch 291/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8175\n",
      "Epoch 291: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3921 - acc: 0.8179 - val_loss: 0.4531 - val_acc: 0.7768\n",
      "Epoch 292/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8189\n",
      "Epoch 292: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3837 - acc: 0.8191 - val_loss: 0.4871 - val_acc: 0.7448\n",
      "Epoch 293/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8141\n",
      "Epoch 293: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3925 - acc: 0.8140 - val_loss: 0.4451 - val_acc: 0.7704\n",
      "Epoch 294/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3975 - acc: 0.8123\n",
      "Epoch 294: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3973 - acc: 0.8123 - val_loss: 0.4526 - val_acc: 0.7593\n",
      "Epoch 295/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8091\n",
      "Epoch 295: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3997 - acc: 0.8091 - val_loss: 0.4552 - val_acc: 0.7653\n",
      "Epoch 296/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.4007 - acc: 0.8105\n",
      "Epoch 296: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4019 - acc: 0.8099 - val_loss: 0.4679 - val_acc: 0.7499\n",
      "Epoch 297/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8093\n",
      "Epoch 297: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4000 - acc: 0.8095 - val_loss: 0.4561 - val_acc: 0.7670\n",
      "Epoch 298/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8169\n",
      "Epoch 298: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3850 - acc: 0.8171 - val_loss: 0.4663 - val_acc: 0.7653\n",
      "Epoch 299/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3898 - acc: 0.8163\n",
      "Epoch 299: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3906 - acc: 0.8159 - val_loss: 0.4432 - val_acc: 0.7726\n",
      "Epoch 300/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8145\n",
      "Epoch 300: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3851 - acc: 0.8146 - val_loss: 0.4533 - val_acc: 0.7636\n",
      "Epoch 301/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8126\n",
      "Epoch 301: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3998 - acc: 0.8126 - val_loss: 0.4611 - val_acc: 0.7649\n",
      "Epoch 302/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8148\n",
      "Epoch 302: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3854 - acc: 0.8142 - val_loss: 0.4519 - val_acc: 0.7751\n",
      "Epoch 303/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8186\n",
      "Epoch 303: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3893 - acc: 0.8188 - val_loss: 0.4516 - val_acc: 0.7764\n",
      "Epoch 304/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3940 - acc: 0.8094\n",
      "Epoch 304: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3928 - acc: 0.8100 - val_loss: 0.4671 - val_acc: 0.7631\n",
      "Epoch 305/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8097\n",
      "Epoch 305: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3954 - acc: 0.8102 - val_loss: 0.4669 - val_acc: 0.7537\n",
      "Epoch 306/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8170\n",
      "Epoch 306: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3841 - acc: 0.8171 - val_loss: 0.4704 - val_acc: 0.7687\n",
      "Epoch 307/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3958 - acc: 0.8140\n",
      "Epoch 307: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3955 - acc: 0.8138 - val_loss: 0.4601 - val_acc: 0.7674\n",
      "Epoch 308/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8147\n",
      "Epoch 308: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 4ms/step - loss: 0.3856 - acc: 0.8153 - val_loss: 0.4636 - val_acc: 0.7696\n",
      "Epoch 309/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8156\n",
      "Epoch 309: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4133 - acc: 0.8152 - val_loss: 0.4616 - val_acc: 0.7614\n",
      "Epoch 310/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8160\n",
      "Epoch 310: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3937 - acc: 0.8161 - val_loss: 0.4534 - val_acc: 0.7734\n",
      "Epoch 311/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8050\n",
      "Epoch 311: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4158 - acc: 0.8053 - val_loss: 0.4499 - val_acc: 0.7721\n",
      "Epoch 312/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3993 - acc: 0.8086\n",
      "Epoch 312: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3993 - acc: 0.8086 - val_loss: 0.4687 - val_acc: 0.7700\n",
      "Epoch 313/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8197\n",
      "Epoch 313: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3965 - acc: 0.8197 - val_loss: 0.4515 - val_acc: 0.7713\n",
      "Epoch 314/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8154\n",
      "Epoch 314: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3841 - acc: 0.8156 - val_loss: 0.4637 - val_acc: 0.7666\n",
      "Epoch 315/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8157\n",
      "Epoch 315: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3743 - acc: 0.8156 - val_loss: 0.4536 - val_acc: 0.7674\n",
      "Epoch 316/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8220\n",
      "Epoch 316: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3771 - acc: 0.8220 - val_loss: 0.4593 - val_acc: 0.7678\n",
      "Epoch 317/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8146\n",
      "Epoch 317: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3893 - acc: 0.8150 - val_loss: 0.4627 - val_acc: 0.7640\n",
      "Epoch 318/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8178\n",
      "Epoch 318: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3791 - acc: 0.8181 - val_loss: 0.4566 - val_acc: 0.7593\n",
      "Epoch 319/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3834 - acc: 0.8224\n",
      "Epoch 319: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3834 - acc: 0.8224 - val_loss: 0.4497 - val_acc: 0.7755\n",
      "Epoch 320/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3803 - acc: 0.8162\n",
      "Epoch 320: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3803 - acc: 0.8162 - val_loss: 0.4762 - val_acc: 0.7661\n",
      "Epoch 321/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8189\n",
      "Epoch 321: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3921 - acc: 0.8185 - val_loss: 0.4508 - val_acc: 0.7657\n",
      "Epoch 322/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8168\n",
      "Epoch 322: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3917 - acc: 0.8170 - val_loss: 0.4481 - val_acc: 0.7674\n",
      "Epoch 323/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8172\n",
      "Epoch 323: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3799 - acc: 0.8175 - val_loss: 0.4586 - val_acc: 0.7678\n",
      "Epoch 324/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8142\n",
      "Epoch 324: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3837 - acc: 0.8146 - val_loss: 0.4520 - val_acc: 0.7734\n",
      "Epoch 325/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8166\n",
      "Epoch 325: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3910 - acc: 0.8167 - val_loss: 0.4728 - val_acc: 0.7512\n",
      "Epoch 326/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8148\n",
      "Epoch 326: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3861 - acc: 0.8148 - val_loss: 0.4564 - val_acc: 0.7602\n",
      "Epoch 327/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8060\n",
      "Epoch 327: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4309 - acc: 0.8061 - val_loss: 0.4522 - val_acc: 0.7550\n",
      "Epoch 328/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4018 - acc: 0.8157\n",
      "Epoch 328: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4015 - acc: 0.8153 - val_loss: 0.4563 - val_acc: 0.7636\n",
      "Epoch 329/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8114\n",
      "Epoch 329: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3941 - acc: 0.8117 - val_loss: 0.4667 - val_acc: 0.7610\n",
      "Epoch 330/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.8188\n",
      "Epoch 330: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3831 - acc: 0.8193 - val_loss: 0.4529 - val_acc: 0.7653\n",
      "Epoch 331/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8178\n",
      "Epoch 331: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3903 - acc: 0.8177 - val_loss: 0.4545 - val_acc: 0.7559\n",
      "Epoch 332/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8147\n",
      "Epoch 332: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3900 - acc: 0.8146 - val_loss: 0.4571 - val_acc: 0.7649\n",
      "Epoch 333/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8111\n",
      "Epoch 333: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4146 - acc: 0.8118 - val_loss: 0.4594 - val_acc: 0.7661\n",
      "Epoch 334/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8098\n",
      "Epoch 334: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4079 - acc: 0.8108 - val_loss: 0.4596 - val_acc: 0.7678\n",
      "Epoch 335/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8061\n",
      "Epoch 335: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4335 - acc: 0.8061 - val_loss: 0.4470 - val_acc: 0.7678\n",
      "Epoch 336/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8171\n",
      "Epoch 336: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3880 - acc: 0.8168 - val_loss: 0.4481 - val_acc: 0.7670\n",
      "Epoch 337/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3867 - acc: 0.8196\n",
      "Epoch 337: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3873 - acc: 0.8189 - val_loss: 0.4560 - val_acc: 0.7550\n",
      "Epoch 338/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.3965 - acc: 0.8133\n",
      "Epoch 338: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3968 - acc: 0.8131 - val_loss: 0.4558 - val_acc: 0.7670\n",
      "Epoch 339/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.3918 - acc: 0.8153\n",
      "Epoch 339: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3917 - acc: 0.8148 - val_loss: 0.4566 - val_acc: 0.7734\n",
      "Epoch 340/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8215\n",
      "Epoch 340: val_acc improved from 0.78452 to 0.78794, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\3\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3752 - acc: 0.8218 - val_loss: 0.4383 - val_acc: 0.7879\n",
      "Epoch 341/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.3862 - acc: 0.8173\n",
      "Epoch 341: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3895 - acc: 0.8168 - val_loss: 0.4617 - val_acc: 0.7606\n",
      "Epoch 342/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8113\n",
      "Epoch 342: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4034 - acc: 0.8115 - val_loss: 0.4493 - val_acc: 0.7815\n",
      "Epoch 343/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8138\n",
      "Epoch 343: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3793 - acc: 0.8140 - val_loss: 0.4381 - val_acc: 0.7850\n",
      "Epoch 344/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8169\n",
      "Epoch 344: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3899 - acc: 0.8168 - val_loss: 0.4423 - val_acc: 0.7777\n",
      "Epoch 345/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8191\n",
      "Epoch 345: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3811 - acc: 0.8191 - val_loss: 0.4671 - val_acc: 0.7824\n",
      "Epoch 346/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8143\n",
      "Epoch 346: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3813 - acc: 0.8144 - val_loss: 0.4323 - val_acc: 0.7879\n",
      "Epoch 347/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8199\n",
      "Epoch 347: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3794 - acc: 0.8199 - val_loss: 0.4618 - val_acc: 0.7790\n",
      "Epoch 348/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8166\n",
      "Epoch 348: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3766 - acc: 0.8170 - val_loss: 0.4444 - val_acc: 0.7717\n",
      "Epoch 349/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8209\n",
      "Epoch 349: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3848 - acc: 0.8212 - val_loss: 0.4569 - val_acc: 0.7704\n",
      "Epoch 350/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8137\n",
      "Epoch 350: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3820 - acc: 0.8144 - val_loss: 0.4674 - val_acc: 0.7636\n",
      "Epoch 351/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3737 - acc: 0.8215\n",
      "Epoch 351: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3737 - acc: 0.8215 - val_loss: 0.4506 - val_acc: 0.7773\n",
      "Epoch 352/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4230 - acc: 0.8192\n",
      "Epoch 352: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4226 - acc: 0.8189 - val_loss: 0.4634 - val_acc: 0.7529\n",
      "Epoch 353/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8120\n",
      "Epoch 353: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4135 - acc: 0.8124 - val_loss: 0.4503 - val_acc: 0.7670\n",
      "Epoch 354/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8184\n",
      "Epoch 354: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3804 - acc: 0.8186 - val_loss: 0.4499 - val_acc: 0.7597\n",
      "Epoch 355/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8207\n",
      "Epoch 355: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4024 - acc: 0.8208 - val_loss: 0.4709 - val_acc: 0.7533\n",
      "Epoch 356/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8058\n",
      "Epoch 356: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4117 - acc: 0.8060 - val_loss: 0.4558 - val_acc: 0.7533\n",
      "Epoch 357/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8100\n",
      "Epoch 357: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3972 - acc: 0.8104 - val_loss: 0.4539 - val_acc: 0.7580\n",
      "Epoch 358/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8182\n",
      "Epoch 358: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.3936 - acc: 0.8182 - val_loss: 0.4529 - val_acc: 0.7713\n",
      "Epoch 359/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.3957 - acc: 0.8206\n",
      "Epoch 359: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3961 - acc: 0.8203 - val_loss: 0.4622 - val_acc: 0.7606\n",
      "Epoch 360/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8187\n",
      "Epoch 360: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3737 - acc: 0.8191 - val_loss: 0.4562 - val_acc: 0.7623\n",
      "Epoch 361/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8203\n",
      "Epoch 361: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3930 - acc: 0.8204 - val_loss: 0.4575 - val_acc: 0.7657\n",
      "Epoch 362/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8162\n",
      "Epoch 362: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3790 - acc: 0.8162 - val_loss: 0.4515 - val_acc: 0.7661\n",
      "Epoch 363/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8175\n",
      "Epoch 363: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3890 - acc: 0.8177 - val_loss: 0.4549 - val_acc: 0.7738\n",
      "Epoch 364/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8105\n",
      "Epoch 364: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4041 - acc: 0.8107 - val_loss: 0.4680 - val_acc: 0.7486\n",
      "Epoch 365/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4163 - acc: 0.8084\n",
      "Epoch 365: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4184 - acc: 0.8083 - val_loss: 0.4495 - val_acc: 0.7559\n",
      "Epoch 366/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.3981 - acc: 0.8165\n",
      "Epoch 366: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.3976 - acc: 0.8160 - val_loss: 0.4583 - val_acc: 0.7768\n",
      "Epoch 367/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4049 - acc: 0.8106\n",
      "Epoch 367: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4113 - acc: 0.8106 - val_loss: 0.5062 - val_acc: 0.7465\n",
      "Epoch 368/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8093\n",
      "Epoch 368: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4196 - acc: 0.8093 - val_loss: 0.4506 - val_acc: 0.7666\n",
      "Epoch 369/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8071\n",
      "Epoch 369: val_acc did not improve from 0.78794\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4093 - acc: 0.8074 - val_loss: 0.4531 - val_acc: 0.7738\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 378,881\n",
      "Trainable params: 378,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 1.4380 - acc: 0.5951\n",
      "Epoch 1: val_acc improved from -inf to 0.71740, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 1.4158 - acc: 0.5966 - val_loss: 0.6587 - val_acc: 0.7174\n",
      "Epoch 2/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.6692 - acc: 0.6462\n",
      "Epoch 2: val_acc did not improve from 0.71740\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6689 - acc: 0.6464 - val_loss: 0.6242 - val_acc: 0.7101\n",
      "Epoch 3/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6342 - acc: 0.6801\n",
      "Epoch 3: val_acc improved from 0.71740 to 0.71783, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6336 - acc: 0.6806 - val_loss: 0.5990 - val_acc: 0.7178\n",
      "Epoch 4/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.6952\n",
      "Epoch 4: val_acc improved from 0.71783 to 0.72424, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.6103 - acc: 0.6954 - val_loss: 0.5773 - val_acc: 0.7242\n",
      "Epoch 5/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5977 - acc: 0.7052\n",
      "Epoch 5: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.5977 - acc: 0.7052 - val_loss: 0.5841 - val_acc: 0.7234\n",
      "Epoch 6/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.7083\n",
      "Epoch 6: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5876 - acc: 0.7085 - val_loss: 0.5758 - val_acc: 0.7238\n",
      "Epoch 7/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.7098\n",
      "Epoch 7: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5902 - acc: 0.7102 - val_loss: 0.5789 - val_acc: 0.7217\n",
      "Epoch 8/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5828 - acc: 0.7106\n",
      "Epoch 8: val_acc did not improve from 0.72424\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5832 - acc: 0.7110 - val_loss: 0.5820 - val_acc: 0.7183\n",
      "Epoch 9/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.7177\n",
      "Epoch 9: val_acc improved from 0.72424 to 0.72809, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5744 - acc: 0.7179 - val_loss: 0.5755 - val_acc: 0.7281\n",
      "Epoch 10/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.7107\n",
      "Epoch 10: val_acc improved from 0.72809 to 0.72894, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5806 - acc: 0.7108 - val_loss: 0.5683 - val_acc: 0.7289\n",
      "Epoch 11/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5696 - acc: 0.7187\n",
      "Epoch 11: val_acc did not improve from 0.72894\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5696 - acc: 0.7187 - val_loss: 0.5648 - val_acc: 0.7277\n",
      "Epoch 12/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5675 - acc: 0.7226\n",
      "Epoch 12: val_acc improved from 0.72894 to 0.73065, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5670 - acc: 0.7231 - val_loss: 0.5596 - val_acc: 0.7307\n",
      "Epoch 13/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5659 - acc: 0.7211\n",
      "Epoch 13: val_acc improved from 0.73065 to 0.73493, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5659 - acc: 0.7211 - val_loss: 0.5566 - val_acc: 0.7349\n",
      "Epoch 14/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5626 - acc: 0.7243\n",
      "Epoch 14: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5631 - acc: 0.7237 - val_loss: 0.5672 - val_acc: 0.7277\n",
      "Epoch 15/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.7252\n",
      "Epoch 15: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5557 - acc: 0.7254 - val_loss: 0.5642 - val_acc: 0.7302\n",
      "Epoch 16/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7266\n",
      "Epoch 16: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5531 - acc: 0.7272 - val_loss: 0.5526 - val_acc: 0.7315\n",
      "Epoch 17/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7306\n",
      "Epoch 17: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5513 - acc: 0.7300 - val_loss: 0.5550 - val_acc: 0.7319\n",
      "Epoch 18/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.7269\n",
      "Epoch 18: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5465 - acc: 0.7271 - val_loss: 0.5479 - val_acc: 0.7332\n",
      "Epoch 19/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.7328\n",
      "Epoch 19: val_acc did not improve from 0.73493\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5439 - acc: 0.7329 - val_loss: 0.5608 - val_acc: 0.7298\n",
      "Epoch 20/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7325\n",
      "Epoch 20: val_acc improved from 0.73493 to 0.73963, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5405 - acc: 0.7330 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 21/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.7348\n",
      "Epoch 21: val_acc did not improve from 0.73963\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5351 - acc: 0.7348 - val_loss: 0.5477 - val_acc: 0.7319\n",
      "Epoch 22/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.7411\n",
      "Epoch 22: val_acc improved from 0.73963 to 0.74434, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5312 - acc: 0.7405 - val_loss: 0.5280 - val_acc: 0.7443\n",
      "Epoch 23/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5273 - acc: 0.7399\n",
      "Epoch 23: val_acc did not improve from 0.74434\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5273 - acc: 0.7399 - val_loss: 0.5136 - val_acc: 0.7405\n",
      "Epoch 24/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.7392\n",
      "Epoch 24: val_acc improved from 0.74434 to 0.74519, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5279 - acc: 0.7392 - val_loss: 0.5299 - val_acc: 0.7452\n",
      "Epoch 25/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.7387\n",
      "Epoch 25: val_acc improved from 0.74519 to 0.74818, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5253 - acc: 0.7389 - val_loss: 0.5244 - val_acc: 0.7482\n",
      "Epoch 26/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.7393\n",
      "Epoch 26: val_acc did not improve from 0.74818\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5233 - acc: 0.7391 - val_loss: 0.5501 - val_acc: 0.7401\n",
      "Epoch 27/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.7433\n",
      "Epoch 27: val_acc did not improve from 0.74818\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5224 - acc: 0.7425 - val_loss: 0.5260 - val_acc: 0.7478\n",
      "Epoch 28/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.7419\n",
      "Epoch 28: val_acc improved from 0.74818 to 0.75289, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5127 - acc: 0.7421 - val_loss: 0.5152 - val_acc: 0.7529\n",
      "Epoch 29/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7482\n",
      "Epoch 29: val_acc improved from 0.75289 to 0.75502, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5121 - acc: 0.7483 - val_loss: 0.5218 - val_acc: 0.7550\n",
      "Epoch 30/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.7474\n",
      "Epoch 30: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5063 - acc: 0.7473 - val_loss: 0.5192 - val_acc: 0.7490\n",
      "Epoch 31/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.7461\n",
      "Epoch 31: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5076 - acc: 0.7458 - val_loss: 0.5231 - val_acc: 0.7456\n",
      "Epoch 32/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.7515\n",
      "Epoch 32: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5038 - acc: 0.7514 - val_loss: 0.4969 - val_acc: 0.7516\n",
      "Epoch 33/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.7532\n",
      "Epoch 33: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4974 - acc: 0.7530 - val_loss: 0.5145 - val_acc: 0.7525\n",
      "Epoch 34/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4975 - acc: 0.7558\n",
      "Epoch 34: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4974 - acc: 0.7559 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 35/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.7531\n",
      "Epoch 35: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5071 - acc: 0.7531 - val_loss: 0.5075 - val_acc: 0.7516\n",
      "Epoch 36/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.7578\n",
      "Epoch 36: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4959 - acc: 0.7568 - val_loss: 0.5119 - val_acc: 0.7516\n",
      "Epoch 37/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4901 - acc: 0.7582\n",
      "Epoch 37: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4911 - acc: 0.7576 - val_loss: 0.5055 - val_acc: 0.7525\n",
      "Epoch 38/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4885 - acc: 0.7586\n",
      "Epoch 38: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4883 - acc: 0.7587 - val_loss: 0.5069 - val_acc: 0.7550\n",
      "Epoch 39/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.7565\n",
      "Epoch 39: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4944 - acc: 0.7568 - val_loss: 0.5234 - val_acc: 0.7503\n",
      "Epoch 40/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.7549\n",
      "Epoch 40: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4905 - acc: 0.7551 - val_loss: 0.5041 - val_acc: 0.7529\n",
      "Epoch 41/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.7606\n",
      "Epoch 41: val_acc did not improve from 0.75502\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4929 - acc: 0.7606 - val_loss: 0.5001 - val_acc: 0.7516\n",
      "Epoch 42/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.7625\n",
      "Epoch 42: val_acc improved from 0.75502 to 0.75673, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4843 - acc: 0.7618 - val_loss: 0.5091 - val_acc: 0.7567\n",
      "Epoch 43/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.7615\n",
      "Epoch 43: val_acc improved from 0.75673 to 0.75973, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4874 - acc: 0.7618 - val_loss: 0.5038 - val_acc: 0.7597\n",
      "Epoch 44/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.7658\n",
      "Epoch 44: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4849 - acc: 0.7652 - val_loss: 0.4996 - val_acc: 0.7546\n",
      "Epoch 45/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4790 - acc: 0.7612\n",
      "Epoch 45: val_acc did not improve from 0.75973\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4801 - acc: 0.7607 - val_loss: 0.4942 - val_acc: 0.7507\n",
      "Epoch 46/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.7641\n",
      "Epoch 46: val_acc improved from 0.75973 to 0.76357, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4760 - acc: 0.7641 - val_loss: 0.4911 - val_acc: 0.7636\n",
      "Epoch 47/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.7661\n",
      "Epoch 47: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4837 - acc: 0.7660 - val_loss: 0.4985 - val_acc: 0.7550\n",
      "Epoch 48/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4794 - acc: 0.7643\n",
      "Epoch 48: val_acc did not improve from 0.76357\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4801 - acc: 0.7642 - val_loss: 0.5101 - val_acc: 0.7520\n",
      "Epoch 49/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.7697\n",
      "Epoch 49: val_acc improved from 0.76357 to 0.76614, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4724 - acc: 0.7697 - val_loss: 0.4870 - val_acc: 0.7661\n",
      "Epoch 50/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.7702\n",
      "Epoch 50: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4797 - acc: 0.7704 - val_loss: 0.5023 - val_acc: 0.7589\n",
      "Epoch 51/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4715 - acc: 0.7694\n",
      "Epoch 51: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4736 - acc: 0.7681 - val_loss: 0.4817 - val_acc: 0.7610\n",
      "Epoch 52/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4743 - acc: 0.7683\n",
      "Epoch 52: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4742 - acc: 0.7685 - val_loss: 0.4940 - val_acc: 0.7499\n",
      "Epoch 53/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4749 - acc: 0.7699\n",
      "Epoch 53: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4745 - acc: 0.7693 - val_loss: 0.4789 - val_acc: 0.7602\n",
      "Epoch 54/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.7719\n",
      "Epoch 54: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4718 - acc: 0.7714 - val_loss: 0.4829 - val_acc: 0.7606\n",
      "Epoch 55/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.7707\n",
      "Epoch 55: val_acc did not improve from 0.76614\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4691 - acc: 0.7707 - val_loss: 0.4905 - val_acc: 0.7567\n",
      "Epoch 56/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.7698\n",
      "Epoch 56: val_acc improved from 0.76614 to 0.77426, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4635 - acc: 0.7699 - val_loss: 0.4619 - val_acc: 0.7743\n",
      "Epoch 57/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.7677\n",
      "Epoch 57: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4707 - acc: 0.7677 - val_loss: 0.4912 - val_acc: 0.7610\n",
      "Epoch 58/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.7742\n",
      "Epoch 58: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4634 - acc: 0.7738 - val_loss: 0.4914 - val_acc: 0.7516\n",
      "Epoch 59/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4708 - acc: 0.7701\n",
      "Epoch 59: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4708 - acc: 0.7701 - val_loss: 0.4918 - val_acc: 0.7542\n",
      "Epoch 60/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4602 - acc: 0.7741\n",
      "Epoch 60: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4607 - acc: 0.7742 - val_loss: 0.4756 - val_acc: 0.7610\n",
      "Epoch 61/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4623 - acc: 0.7769\n",
      "Epoch 61: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4649 - acc: 0.7757 - val_loss: 0.4935 - val_acc: 0.7537\n",
      "Epoch 62/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.7737\n",
      "Epoch 62: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4595 - acc: 0.7735 - val_loss: 0.4873 - val_acc: 0.7743\n",
      "Epoch 63/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4596 - acc: 0.7786\n",
      "Epoch 63: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4614 - acc: 0.7779 - val_loss: 0.4874 - val_acc: 0.7696\n",
      "Epoch 64/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.7796\n",
      "Epoch 64: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4555 - acc: 0.7790 - val_loss: 0.4843 - val_acc: 0.7657\n",
      "Epoch 65/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.7776\n",
      "Epoch 65: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4619 - acc: 0.7772 - val_loss: 0.4682 - val_acc: 0.7610\n",
      "Epoch 66/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.7796\n",
      "Epoch 66: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4525 - acc: 0.7790 - val_loss: 0.4708 - val_acc: 0.7619\n",
      "Epoch 67/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4555 - acc: 0.7785\n",
      "Epoch 67: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4560 - acc: 0.7783 - val_loss: 0.4789 - val_acc: 0.7666\n",
      "Epoch 68/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.7764\n",
      "Epoch 68: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4606 - acc: 0.7758 - val_loss: 0.4870 - val_acc: 0.7636\n",
      "Epoch 69/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.7778\n",
      "Epoch 69: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4608 - acc: 0.7781 - val_loss: 0.4801 - val_acc: 0.7631\n",
      "Epoch 70/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.7816\n",
      "Epoch 70: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4507 - acc: 0.7808 - val_loss: 0.4831 - val_acc: 0.7529\n",
      "Epoch 71/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.7822\n",
      "Epoch 71: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4551 - acc: 0.7815 - val_loss: 0.4782 - val_acc: 0.7606\n",
      "Epoch 72/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4582 - acc: 0.7781\n",
      "Epoch 72: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4581 - acc: 0.7783 - val_loss: 0.5032 - val_acc: 0.7555\n",
      "Epoch 73/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.7765\n",
      "Epoch 73: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4564 - acc: 0.7761 - val_loss: 0.4856 - val_acc: 0.7580\n",
      "Epoch 74/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.7777\n",
      "Epoch 74: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4533 - acc: 0.7771 - val_loss: 0.4928 - val_acc: 0.7589\n",
      "Epoch 75/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4584 - acc: 0.7747\n",
      "Epoch 75: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4584 - acc: 0.7747 - val_loss: 0.4904 - val_acc: 0.7597\n",
      "Epoch 76/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.7840\n",
      "Epoch 76: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4460 - acc: 0.7838 - val_loss: 0.4746 - val_acc: 0.7602\n",
      "Epoch 77/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.7815\n",
      "Epoch 77: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4489 - acc: 0.7815 - val_loss: 0.4831 - val_acc: 0.7636\n",
      "Epoch 78/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.7827\n",
      "Epoch 78: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4477 - acc: 0.7829 - val_loss: 0.4747 - val_acc: 0.7619\n",
      "Epoch 79/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4394 - acc: 0.7886\n",
      "Epoch 79: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4403 - acc: 0.7880 - val_loss: 0.4733 - val_acc: 0.7657\n",
      "Epoch 80/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.7837\n",
      "Epoch 80: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4546 - acc: 0.7831 - val_loss: 0.4935 - val_acc: 0.7563\n",
      "Epoch 81/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.7826\n",
      "Epoch 81: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4507 - acc: 0.7829 - val_loss: 0.4774 - val_acc: 0.7619\n",
      "Epoch 82/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.7845\n",
      "Epoch 82: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4462 - acc: 0.7849 - val_loss: 0.4955 - val_acc: 0.7610\n",
      "Epoch 83/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4521 - acc: 0.7854\n",
      "Epoch 83: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4528 - acc: 0.7848 - val_loss: 0.4920 - val_acc: 0.7520\n",
      "Epoch 84/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4559 - acc: 0.7781\n",
      "Epoch 84: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4565 - acc: 0.7779 - val_loss: 0.4764 - val_acc: 0.7661\n",
      "Epoch 85/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4471 - acc: 0.7820\n",
      "Epoch 85: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4477 - acc: 0.7817 - val_loss: 0.4796 - val_acc: 0.7730\n",
      "Epoch 86/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.7853\n",
      "Epoch 86: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4439 - acc: 0.7851 - val_loss: 0.4764 - val_acc: 0.7653\n",
      "Epoch 87/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.7853\n",
      "Epoch 87: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4480 - acc: 0.7852 - val_loss: 0.4685 - val_acc: 0.7666\n",
      "Epoch 88/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.7827\n",
      "Epoch 88: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4443 - acc: 0.7826 - val_loss: 0.4761 - val_acc: 0.7597\n",
      "Epoch 89/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.7852\n",
      "Epoch 89: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4399 - acc: 0.7851 - val_loss: 0.4798 - val_acc: 0.7631\n",
      "Epoch 90/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.7868\n",
      "Epoch 90: val_acc did not improve from 0.77426\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4404 - acc: 0.7873 - val_loss: 0.4748 - val_acc: 0.7649\n",
      "Epoch 91/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4350 - acc: 0.7927\n",
      "Epoch 91: val_acc improved from 0.77426 to 0.77597, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4350 - acc: 0.7927 - val_loss: 0.4659 - val_acc: 0.7760\n",
      "Epoch 92/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4392 - acc: 0.7857\n",
      "Epoch 92: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.4393 - acc: 0.7858 - val_loss: 0.4796 - val_acc: 0.7683\n",
      "Epoch 93/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4518 - acc: 0.7835\n",
      "Epoch 93: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4530 - acc: 0.7830 - val_loss: 0.4781 - val_acc: 0.7653\n",
      "Epoch 94/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.7903\n",
      "Epoch 94: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4400 - acc: 0.7903 - val_loss: 0.4605 - val_acc: 0.7683\n",
      "Epoch 95/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4392 - acc: 0.7881\n",
      "Epoch 95: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4393 - acc: 0.7879 - val_loss: 0.4793 - val_acc: 0.7597\n",
      "Epoch 96/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4390 - acc: 0.7856\n",
      "Epoch 96: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4386 - acc: 0.7858 - val_loss: 0.4894 - val_acc: 0.7520\n",
      "Epoch 97/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.7828\n",
      "Epoch 97: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4423 - acc: 0.7825 - val_loss: 0.4716 - val_acc: 0.7584\n",
      "Epoch 98/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.7898\n",
      "Epoch 98: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4356 - acc: 0.7900 - val_loss: 0.4844 - val_acc: 0.7572\n",
      "Epoch 99/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.7858\n",
      "Epoch 99: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4418 - acc: 0.7860 - val_loss: 0.4727 - val_acc: 0.7661\n",
      "Epoch 100/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.7909\n",
      "Epoch 100: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4295 - acc: 0.7904 - val_loss: 0.4726 - val_acc: 0.7644\n",
      "Epoch 101/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.7880\n",
      "Epoch 101: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4367 - acc: 0.7875 - val_loss: 0.4700 - val_acc: 0.7584\n",
      "Epoch 102/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.7870\n",
      "Epoch 102: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4354 - acc: 0.7863 - val_loss: 0.4754 - val_acc: 0.7542\n",
      "Epoch 103/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4354 - acc: 0.7898\n",
      "Epoch 103: val_acc did not improve from 0.77597\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4355 - acc: 0.7897 - val_loss: 0.4709 - val_acc: 0.7597\n",
      "Epoch 104/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4337 - acc: 0.7921\n",
      "Epoch 104: val_acc improved from 0.77597 to 0.77640, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4341 - acc: 0.7922 - val_loss: 0.4619 - val_acc: 0.7764\n",
      "Epoch 105/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4239 - acc: 0.7953\n",
      "Epoch 105: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4252 - acc: 0.7950 - val_loss: 0.4651 - val_acc: 0.7576\n",
      "Epoch 106/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4377 - acc: 0.7879\n",
      "Epoch 106: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4378 - acc: 0.7876 - val_loss: 0.4683 - val_acc: 0.7687\n",
      "Epoch 107/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.7891\n",
      "Epoch 107: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4360 - acc: 0.7887 - val_loss: 0.4729 - val_acc: 0.7687\n",
      "Epoch 108/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.7896\n",
      "Epoch 108: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4285 - acc: 0.7889 - val_loss: 0.4711 - val_acc: 0.7623\n",
      "Epoch 109/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.7907\n",
      "Epoch 109: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4341 - acc: 0.7908 - val_loss: 0.4692 - val_acc: 0.7614\n",
      "Epoch 110/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.7926\n",
      "Epoch 110: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4270 - acc: 0.7923 - val_loss: 0.4559 - val_acc: 0.7700\n",
      "Epoch 111/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.7930\n",
      "Epoch 111: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4274 - acc: 0.7925 - val_loss: 0.4780 - val_acc: 0.7704\n",
      "Epoch 112/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.7876\n",
      "Epoch 112: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4247 - acc: 0.7878 - val_loss: 0.4670 - val_acc: 0.7687\n",
      "Epoch 113/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.7940\n",
      "Epoch 113: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4246 - acc: 0.7941 - val_loss: 0.4613 - val_acc: 0.7687\n",
      "Epoch 114/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.7926\n",
      "Epoch 114: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4326 - acc: 0.7931 - val_loss: 0.4669 - val_acc: 0.7755\n",
      "Epoch 115/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4298 - acc: 0.7948\n",
      "Epoch 115: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4296 - acc: 0.7950 - val_loss: 0.4687 - val_acc: 0.7751\n",
      "Epoch 116/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.7884\n",
      "Epoch 116: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4338 - acc: 0.7880 - val_loss: 0.4741 - val_acc: 0.7606\n",
      "Epoch 117/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4241 - acc: 0.7934\n",
      "Epoch 117: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4240 - acc: 0.7935 - val_loss: 0.4635 - val_acc: 0.7666\n",
      "Epoch 118/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4290 - acc: 0.7952\n",
      "Epoch 118: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4303 - acc: 0.7945 - val_loss: 0.4610 - val_acc: 0.7619\n",
      "Epoch 119/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4210 - acc: 0.7933\n",
      "Epoch 119: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4214 - acc: 0.7931 - val_loss: 0.4712 - val_acc: 0.7683\n",
      "Epoch 120/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4336 - acc: 0.7873\n",
      "Epoch 120: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4335 - acc: 0.7873 - val_loss: 0.4473 - val_acc: 0.7670\n",
      "Epoch 121/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.7889\n",
      "Epoch 121: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4276 - acc: 0.7887 - val_loss: 0.4626 - val_acc: 0.7743\n",
      "Epoch 122/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.7953\n",
      "Epoch 122: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4292 - acc: 0.7955 - val_loss: 0.4590 - val_acc: 0.7657\n",
      "Epoch 123/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.7909\n",
      "Epoch 123: val_acc did not improve from 0.77640\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4223 - acc: 0.7909 - val_loss: 0.4577 - val_acc: 0.7704\n",
      "Epoch 124/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4241 - acc: 0.7979\n",
      "Epoch 124: val_acc improved from 0.77640 to 0.78367, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4240 - acc: 0.7980 - val_loss: 0.4464 - val_acc: 0.7837\n",
      "Epoch 125/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.7967\n",
      "Epoch 125: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4205 - acc: 0.7962 - val_loss: 0.4454 - val_acc: 0.7717\n",
      "Epoch 126/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.7902\n",
      "Epoch 126: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4241 - acc: 0.7903 - val_loss: 0.4625 - val_acc: 0.7657\n",
      "Epoch 127/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.7934\n",
      "Epoch 127: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4210 - acc: 0.7935 - val_loss: 0.4524 - val_acc: 0.7738\n",
      "Epoch 128/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.7944\n",
      "Epoch 128: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4218 - acc: 0.7947 - val_loss: 0.4694 - val_acc: 0.7683\n",
      "Epoch 129/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.7988\n",
      "Epoch 129: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4173 - acc: 0.7987 - val_loss: 0.4713 - val_acc: 0.7593\n",
      "Epoch 130/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.7930\n",
      "Epoch 130: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4248 - acc: 0.7929 - val_loss: 0.4667 - val_acc: 0.7713\n",
      "Epoch 131/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.7959\n",
      "Epoch 131: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4312 - acc: 0.7959 - val_loss: 0.4854 - val_acc: 0.7614\n",
      "Epoch 132/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.7972\n",
      "Epoch 132: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4268 - acc: 0.7968 - val_loss: 0.4752 - val_acc: 0.7666\n",
      "Epoch 133/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.7981\n",
      "Epoch 133: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4243 - acc: 0.7985 - val_loss: 0.4773 - val_acc: 0.7623\n",
      "Epoch 134/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.7957\n",
      "Epoch 134: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4185 - acc: 0.7957 - val_loss: 0.4804 - val_acc: 0.7649\n",
      "Epoch 135/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.7946\n",
      "Epoch 135: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4300 - acc: 0.7944 - val_loss: 0.4680 - val_acc: 0.7768\n",
      "Epoch 136/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4240 - acc: 0.7934\n",
      "Epoch 136: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4266 - acc: 0.7932 - val_loss: 0.4701 - val_acc: 0.7815\n",
      "Epoch 137/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4265 - acc: 0.7955\n",
      "Epoch 137: val_acc did not improve from 0.78367\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4281 - acc: 0.7947 - val_loss: 0.4715 - val_acc: 0.7704\n",
      "Epoch 138/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.7992\n",
      "Epoch 138: val_acc improved from 0.78367 to 0.78410, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4180 - acc: 0.7995 - val_loss: 0.4535 - val_acc: 0.7841\n",
      "Epoch 139/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.7993\n",
      "Epoch 139: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4242 - acc: 0.7982 - val_loss: 0.4842 - val_acc: 0.7422\n",
      "Epoch 140/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.7894\n",
      "Epoch 140: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4342 - acc: 0.7894 - val_loss: 0.4502 - val_acc: 0.7657\n",
      "Epoch 141/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.7991\n",
      "Epoch 141: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4185 - acc: 0.7994 - val_loss: 0.4789 - val_acc: 0.7649\n",
      "Epoch 142/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.7889\n",
      "Epoch 142: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4347 - acc: 0.7889 - val_loss: 0.4648 - val_acc: 0.7584\n",
      "Epoch 143/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.7921\n",
      "Epoch 143: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4281 - acc: 0.7921 - val_loss: 0.4545 - val_acc: 0.7708\n",
      "Epoch 144/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.7964\n",
      "Epoch 144: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4229 - acc: 0.7958 - val_loss: 0.4522 - val_acc: 0.7614\n",
      "Epoch 145/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4160 - acc: 0.8012\n",
      "Epoch 145: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4167 - acc: 0.8009 - val_loss: 0.4590 - val_acc: 0.7721\n",
      "Epoch 146/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.7931\n",
      "Epoch 146: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4210 - acc: 0.7933 - val_loss: 0.4604 - val_acc: 0.7657\n",
      "Epoch 147/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4171 - acc: 0.7984\n",
      "Epoch 147: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4186 - acc: 0.7974 - val_loss: 0.4604 - val_acc: 0.7696\n",
      "Epoch 148/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4229 - acc: 0.7929\n",
      "Epoch 148: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4229 - acc: 0.7929 - val_loss: 0.4533 - val_acc: 0.7704\n",
      "Epoch 149/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.7997\n",
      "Epoch 149: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4207 - acc: 0.7999 - val_loss: 0.4545 - val_acc: 0.7785\n",
      "Epoch 150/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.7989\n",
      "Epoch 150: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4184 - acc: 0.7986 - val_loss: 0.4663 - val_acc: 0.7666\n",
      "Epoch 151/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.7921\n",
      "Epoch 151: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4234 - acc: 0.7922 - val_loss: 0.4542 - val_acc: 0.7700\n",
      "Epoch 152/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.7971\n",
      "Epoch 152: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4209 - acc: 0.7969 - val_loss: 0.4580 - val_acc: 0.7768\n",
      "Epoch 153/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.7920\n",
      "Epoch 153: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4221 - acc: 0.7922 - val_loss: 0.4657 - val_acc: 0.7495\n",
      "Epoch 154/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.7974\n",
      "Epoch 154: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4160 - acc: 0.7966 - val_loss: 0.4492 - val_acc: 0.7687\n",
      "Epoch 155/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4154 - acc: 0.7965\n",
      "Epoch 155: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4156 - acc: 0.7957 - val_loss: 0.4590 - val_acc: 0.7623\n",
      "Epoch 156/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.7977\n",
      "Epoch 156: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4170 - acc: 0.7975 - val_loss: 0.4532 - val_acc: 0.7777\n",
      "Epoch 157/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.7999\n",
      "Epoch 157: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4108 - acc: 0.8000 - val_loss: 0.4462 - val_acc: 0.7815\n",
      "Epoch 158/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8002\n",
      "Epoch 158: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4151 - acc: 0.8001 - val_loss: 0.4561 - val_acc: 0.7683\n",
      "Epoch 159/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.7957\n",
      "Epoch 159: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4173 - acc: 0.7960 - val_loss: 0.4596 - val_acc: 0.7606\n",
      "Epoch 160/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.7949\n",
      "Epoch 160: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4152 - acc: 0.7951 - val_loss: 0.4772 - val_acc: 0.7584\n",
      "Epoch 161/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4133 - acc: 0.8022\n",
      "Epoch 161: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4150 - acc: 0.8020 - val_loss: 0.4722 - val_acc: 0.7773\n",
      "Epoch 162/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.7974\n",
      "Epoch 162: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4218 - acc: 0.7971 - val_loss: 0.4619 - val_acc: 0.7824\n",
      "Epoch 163/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.7945\n",
      "Epoch 163: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4286 - acc: 0.7948 - val_loss: 0.4707 - val_acc: 0.7644\n",
      "Epoch 164/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.7995\n",
      "Epoch 164: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4085 - acc: 0.7994 - val_loss: 0.4603 - val_acc: 0.7597\n",
      "Epoch 165/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4215 - acc: 0.7967\n",
      "Epoch 165: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4214 - acc: 0.7968 - val_loss: 0.4512 - val_acc: 0.7751\n",
      "Epoch 166/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.7979\n",
      "Epoch 166: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4172 - acc: 0.7979 - val_loss: 0.4767 - val_acc: 0.7614\n",
      "Epoch 167/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.7939\n",
      "Epoch 167: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4197 - acc: 0.7942 - val_loss: 0.4488 - val_acc: 0.7755\n",
      "Epoch 168/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.7980\n",
      "Epoch 168: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4247 - acc: 0.7974 - val_loss: 0.4552 - val_acc: 0.7627\n",
      "Epoch 169/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8031\n",
      "Epoch 169: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4095 - acc: 0.8020 - val_loss: 0.4584 - val_acc: 0.7657\n",
      "Epoch 170/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8012\n",
      "Epoch 170: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4066 - acc: 0.8006 - val_loss: 0.4684 - val_acc: 0.7546\n",
      "Epoch 171/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4227 - acc: 0.7973\n",
      "Epoch 171: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4227 - acc: 0.7973 - val_loss: 0.4684 - val_acc: 0.7512\n",
      "Epoch 172/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.7991\n",
      "Epoch 172: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4178 - acc: 0.7990 - val_loss: 0.4535 - val_acc: 0.7708\n",
      "Epoch 173/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8099\n",
      "Epoch 173: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4004 - acc: 0.8092 - val_loss: 0.4583 - val_acc: 0.7567\n",
      "Epoch 174/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.7993\n",
      "Epoch 174: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4187 - acc: 0.7988 - val_loss: 0.4618 - val_acc: 0.7678\n",
      "Epoch 175/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.7947\n",
      "Epoch 175: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4145 - acc: 0.7951 - val_loss: 0.4477 - val_acc: 0.7713\n",
      "Epoch 176/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8056\n",
      "Epoch 176: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4093 - acc: 0.8050 - val_loss: 0.4717 - val_acc: 0.7683\n",
      "Epoch 177/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.7990\n",
      "Epoch 177: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4297 - acc: 0.7983 - val_loss: 0.4795 - val_acc: 0.7636\n",
      "Epoch 178/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.7933\n",
      "Epoch 178: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4272 - acc: 0.7934 - val_loss: 0.4580 - val_acc: 0.7678\n",
      "Epoch 179/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8000\n",
      "Epoch 179: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4157 - acc: 0.8001 - val_loss: 0.4619 - val_acc: 0.7661\n",
      "Epoch 180/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4359 - acc: 0.7882\n",
      "Epoch 180: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4358 - acc: 0.7883 - val_loss: 0.4559 - val_acc: 0.7631\n",
      "Epoch 181/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.7981\n",
      "Epoch 181: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4155 - acc: 0.7974 - val_loss: 0.4387 - val_acc: 0.7696\n",
      "Epoch 182/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.7980\n",
      "Epoch 182: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4132 - acc: 0.7979 - val_loss: 0.4471 - val_acc: 0.7713\n",
      "Epoch 183/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8054\n",
      "Epoch 183: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4071 - acc: 0.8057 - val_loss: 0.4664 - val_acc: 0.7627\n",
      "Epoch 184/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.7994\n",
      "Epoch 184: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4091 - acc: 0.7995 - val_loss: 0.4651 - val_acc: 0.7512\n",
      "Epoch 185/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.7921\n",
      "Epoch 185: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4240 - acc: 0.7920 - val_loss: 0.4476 - val_acc: 0.7713\n",
      "Epoch 186/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8046\n",
      "Epoch 186: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4098 - acc: 0.8046 - val_loss: 0.4475 - val_acc: 0.7764\n",
      "Epoch 187/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.7959\n",
      "Epoch 187: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4145 - acc: 0.7959 - val_loss: 0.4533 - val_acc: 0.7764\n",
      "Epoch 188/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8044\n",
      "Epoch 188: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4065 - acc: 0.8045 - val_loss: 0.4470 - val_acc: 0.7751\n",
      "Epoch 189/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8002\n",
      "Epoch 189: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4128 - acc: 0.8002 - val_loss: 0.4622 - val_acc: 0.7627\n",
      "Epoch 190/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4210 - acc: 0.7972\n",
      "Epoch 190: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4221 - acc: 0.7965 - val_loss: 0.4528 - val_acc: 0.7798\n",
      "Epoch 191/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4072 - acc: 0.8045\n",
      "Epoch 191: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4079 - acc: 0.8043 - val_loss: 0.4610 - val_acc: 0.7760\n",
      "Epoch 192/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8015\n",
      "Epoch 192: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4060 - acc: 0.8016 - val_loss: 0.4504 - val_acc: 0.7700\n",
      "Epoch 193/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4102 - acc: 0.8040\n",
      "Epoch 193: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4103 - acc: 0.8034 - val_loss: 0.4491 - val_acc: 0.7785\n",
      "Epoch 194/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.7993\n",
      "Epoch 194: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4225 - acc: 0.7987 - val_loss: 0.4548 - val_acc: 0.7627\n",
      "Epoch 195/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.7961\n",
      "Epoch 195: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4058 - acc: 0.7962 - val_loss: 0.4437 - val_acc: 0.7798\n",
      "Epoch 196/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.7986\n",
      "Epoch 196: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4110 - acc: 0.7986 - val_loss: 0.4542 - val_acc: 0.7631\n",
      "Epoch 197/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.7967\n",
      "Epoch 197: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4109 - acc: 0.7967 - val_loss: 0.4536 - val_acc: 0.7687\n",
      "Epoch 198/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.7991\n",
      "Epoch 198: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4256 - acc: 0.7987 - val_loss: 0.4597 - val_acc: 0.7631\n",
      "Epoch 199/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4049 - acc: 0.8055\n",
      "Epoch 199: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4049 - acc: 0.8055 - val_loss: 0.4432 - val_acc: 0.7730\n",
      "Epoch 200/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4144 - acc: 0.8040\n",
      "Epoch 200: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4163 - acc: 0.8038 - val_loss: 0.4542 - val_acc: 0.7820\n",
      "Epoch 201/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8065\n",
      "Epoch 201: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4145 - acc: 0.8058 - val_loss: 0.4641 - val_acc: 0.7507\n",
      "Epoch 202/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8014\n",
      "Epoch 202: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4100 - acc: 0.8013 - val_loss: 0.4464 - val_acc: 0.7751\n",
      "Epoch 203/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4110 - acc: 0.8029\n",
      "Epoch 203: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4124 - acc: 0.8020 - val_loss: 0.4509 - val_acc: 0.7691\n",
      "Epoch 204/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8035\n",
      "Epoch 204: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4103 - acc: 0.8029 - val_loss: 0.4616 - val_acc: 0.7678\n",
      "Epoch 205/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4138 - acc: 0.7993\n",
      "Epoch 205: val_acc did not improve from 0.78410\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4138 - acc: 0.7993 - val_loss: 0.4460 - val_acc: 0.7794\n",
      "Epoch 206/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.7998\n",
      "Epoch 206: val_acc improved from 0.78410 to 0.78452, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4044 - acc: 0.7998 - val_loss: 0.4576 - val_acc: 0.7845\n",
      "Epoch 207/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8040\n",
      "Epoch 207: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4060 - acc: 0.8041 - val_loss: 0.4530 - val_acc: 0.7696\n",
      "Epoch 208/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8006\n",
      "Epoch 208: val_acc did not improve from 0.78452\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4154 - acc: 0.8004 - val_loss: 0.4568 - val_acc: 0.7717\n",
      "Epoch 209/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8061\n",
      "Epoch 209: val_acc improved from 0.78452 to 0.78923, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.3978 - acc: 0.8062 - val_loss: 0.4420 - val_acc: 0.7892\n",
      "Epoch 210/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4058 - acc: 0.8053\n",
      "Epoch 210: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4058 - acc: 0.8051 - val_loss: 0.4443 - val_acc: 0.7815\n",
      "Epoch 211/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8018\n",
      "Epoch 211: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4064 - acc: 0.8010 - val_loss: 0.4580 - val_acc: 0.7627\n",
      "Epoch 212/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8051\n",
      "Epoch 212: val_acc did not improve from 0.78923\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4007 - acc: 0.8047 - val_loss: 0.4386 - val_acc: 0.7734\n",
      "Epoch 213/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8047\n",
      "Epoch 213: val_acc improved from 0.78923 to 0.79949, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\4\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4011 - acc: 0.8046 - val_loss: 0.4274 - val_acc: 0.7995\n",
      "Epoch 214/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8067\n",
      "Epoch 214: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4105 - acc: 0.8059 - val_loss: 0.4416 - val_acc: 0.7764\n",
      "Epoch 215/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8031\n",
      "Epoch 215: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.3988 - acc: 0.8032 - val_loss: 0.4631 - val_acc: 0.7708\n",
      "Epoch 216/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8055\n",
      "Epoch 216: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4146 - acc: 0.8053 - val_loss: 0.4445 - val_acc: 0.7755\n",
      "Epoch 217/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8034\n",
      "Epoch 217: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4179 - acc: 0.8036 - val_loss: 0.4502 - val_acc: 0.7798\n",
      "Epoch 218/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4107 - acc: 0.8060\n",
      "Epoch 218: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4107 - acc: 0.8060 - val_loss: 0.4448 - val_acc: 0.7713\n",
      "Epoch 219/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8033\n",
      "Epoch 219: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4123 - acc: 0.8034 - val_loss: 0.4604 - val_acc: 0.7627\n",
      "Epoch 220/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8013\n",
      "Epoch 220: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4112 - acc: 0.8014 - val_loss: 0.4410 - val_acc: 0.7798\n",
      "Epoch 221/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8033\n",
      "Epoch 221: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4039 - acc: 0.8032 - val_loss: 0.4443 - val_acc: 0.7721\n",
      "Epoch 222/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8018\n",
      "Epoch 222: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4051 - acc: 0.8019 - val_loss: 0.4516 - val_acc: 0.7678\n",
      "Epoch 223/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8076\n",
      "Epoch 223: val_acc did not improve from 0.79949\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4016 - acc: 0.8075 - val_loss: 0.4415 - val_acc: 0.7850\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 378,881\n",
      "Trainable params: 378,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 1.4549 - acc: 0.5973\n",
      "Epoch 1: val_acc improved from -inf to 0.67579, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 1.4448 - acc: 0.5984 - val_loss: 0.6566 - val_acc: 0.6758\n",
      "Epoch 2/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.6731 - acc: 0.6521\n",
      "Epoch 2: val_acc did not improve from 0.67579\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6716 - acc: 0.6538 - val_loss: 0.6460 - val_acc: 0.6758\n",
      "Epoch 3/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.6477 - acc: 0.6700\n",
      "Epoch 3: val_acc did not improve from 0.67579\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6475 - acc: 0.6708 - val_loss: 0.6369 - val_acc: 0.6758\n",
      "Epoch 4/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.6294 - acc: 0.6778\n",
      "Epoch 4: val_acc improved from 0.67579 to 0.68007, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6289 - acc: 0.6784 - val_loss: 0.6149 - val_acc: 0.6801\n",
      "Epoch 5/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.6136 - acc: 0.6854\n",
      "Epoch 5: val_acc improved from 0.68007 to 0.71642, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.6132 - acc: 0.6861 - val_loss: 0.5888 - val_acc: 0.7164\n",
      "Epoch 6/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.6089 - acc: 0.6980\n",
      "Epoch 6: val_acc did not improve from 0.71642\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.6091 - acc: 0.6981 - val_loss: 0.5879 - val_acc: 0.7143\n",
      "Epoch 7/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5976 - acc: 0.7024\n",
      "Epoch 7: val_acc improved from 0.71642 to 0.72113, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5975 - acc: 0.7030 - val_loss: 0.5809 - val_acc: 0.7211\n",
      "Epoch 8/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5926 - acc: 0.7076\n",
      "Epoch 8: val_acc did not improve from 0.72113\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5922 - acc: 0.7086 - val_loss: 0.5832 - val_acc: 0.7143\n",
      "Epoch 9/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.7085\n",
      "Epoch 9: val_acc did not improve from 0.72113\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5884 - acc: 0.7090 - val_loss: 0.5868 - val_acc: 0.7121\n",
      "Epoch 10/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5910 - acc: 0.7068\n",
      "Epoch 10: val_acc improved from 0.72113 to 0.72284, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5910 - acc: 0.7068 - val_loss: 0.5740 - val_acc: 0.7228\n",
      "Epoch 11/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5815 - acc: 0.7120\n",
      "Epoch 11: val_acc improved from 0.72284 to 0.72754, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5816 - acc: 0.7126 - val_loss: 0.5679 - val_acc: 0.7275\n",
      "Epoch 12/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.7166\n",
      "Epoch 12: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5769 - acc: 0.7167 - val_loss: 0.5656 - val_acc: 0.7246\n",
      "Epoch 13/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.7164\n",
      "Epoch 13: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5725 - acc: 0.7165 - val_loss: 0.5698 - val_acc: 0.7254\n",
      "Epoch 14/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.7208\n",
      "Epoch 14: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5592 - acc: 0.7214 - val_loss: 0.5527 - val_acc: 0.7275\n",
      "Epoch 15/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.7276\n",
      "Epoch 15: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5625 - acc: 0.7278 - val_loss: 0.5650 - val_acc: 0.7241\n",
      "Epoch 16/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.7248\n",
      "Epoch 16: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5579 - acc: 0.7253 - val_loss: 0.5670 - val_acc: 0.7211\n",
      "Epoch 17/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7295\n",
      "Epoch 17: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5502 - acc: 0.7301 - val_loss: 0.5658 - val_acc: 0.7267\n",
      "Epoch 18/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.7299\n",
      "Epoch 18: val_acc did not improve from 0.72754\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5484 - acc: 0.7300 - val_loss: 0.5676 - val_acc: 0.7250\n",
      "Epoch 19/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5439 - acc: 0.7316\n",
      "Epoch 19: val_acc improved from 0.72754 to 0.73054, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5439 - acc: 0.7316 - val_loss: 0.5463 - val_acc: 0.7305\n",
      "Epoch 20/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.5421 - acc: 0.7296\n",
      "Epoch 20: val_acc improved from 0.73054 to 0.73439, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5422 - acc: 0.7291 - val_loss: 0.5322 - val_acc: 0.7344\n",
      "Epoch 21/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.7316\n",
      "Epoch 21: val_acc did not improve from 0.73439\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5420 - acc: 0.7326 - val_loss: 0.5470 - val_acc: 0.7340\n",
      "Epoch 22/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.7326\n",
      "Epoch 22: val_acc improved from 0.73439 to 0.73567, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5421 - acc: 0.7332 - val_loss: 0.5376 - val_acc: 0.7357\n",
      "Epoch 23/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.5321 - acc: 0.7403\n",
      "Epoch 23: val_acc improved from 0.73567 to 0.73867, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5336 - acc: 0.7401 - val_loss: 0.5317 - val_acc: 0.7387\n",
      "Epoch 24/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.7371\n",
      "Epoch 24: val_acc improved from 0.73867 to 0.74038, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.5304 - acc: 0.7373 - val_loss: 0.5203 - val_acc: 0.7404\n",
      "Epoch 25/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.5177 - acc: 0.7437\n",
      "Epoch 25: val_acc did not improve from 0.74038\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5186 - acc: 0.7440 - val_loss: 0.5217 - val_acc: 0.7395\n",
      "Epoch 26/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7404\n",
      "Epoch 26: val_acc improved from 0.74038 to 0.74251, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5254 - acc: 0.7412 - val_loss: 0.5245 - val_acc: 0.7425\n",
      "Epoch 27/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.7445\n",
      "Epoch 27: val_acc improved from 0.74251 to 0.74594, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5187 - acc: 0.7448 - val_loss: 0.5010 - val_acc: 0.7459\n",
      "Epoch 28/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.7468\n",
      "Epoch 28: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5079 - acc: 0.7476 - val_loss: 0.5044 - val_acc: 0.7455\n",
      "Epoch 29/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.7500\n",
      "Epoch 29: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5143 - acc: 0.7502 - val_loss: 0.5258 - val_acc: 0.7429\n",
      "Epoch 30/2000\n",
      "280/293 [===========================>..] - ETA: 0s - loss: 0.5081 - acc: 0.7500\n",
      "Epoch 30: val_acc did not improve from 0.74594\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.5091 - acc: 0.7499 - val_loss: 0.5030 - val_acc: 0.7455\n",
      "Epoch 31/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.7519\n",
      "Epoch 31: val_acc improved from 0.74594 to 0.74893, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5066 - acc: 0.7522 - val_loss: 0.5067 - val_acc: 0.7489\n",
      "Epoch 32/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.7488\n",
      "Epoch 32: val_acc did not improve from 0.74893\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5080 - acc: 0.7500 - val_loss: 0.5209 - val_acc: 0.7395\n",
      "Epoch 33/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.7496\n",
      "Epoch 33: val_acc did not improve from 0.74893\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.5037 - acc: 0.7502 - val_loss: 0.5042 - val_acc: 0.7459\n",
      "Epoch 34/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4985 - acc: 0.7512\n",
      "Epoch 34: val_acc did not improve from 0.74893\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4994 - acc: 0.7515 - val_loss: 0.5038 - val_acc: 0.7387\n",
      "Epoch 35/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4959 - acc: 0.7542\n",
      "Epoch 35: val_acc did not improve from 0.74893\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4959 - acc: 0.7542 - val_loss: 0.4944 - val_acc: 0.7472\n",
      "Epoch 36/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4892 - acc: 0.7620\n",
      "Epoch 36: val_acc did not improve from 0.74893\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4900 - acc: 0.7621 - val_loss: 0.4979 - val_acc: 0.7468\n",
      "Epoch 37/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4878 - acc: 0.7636\n",
      "Epoch 37: val_acc did not improve from 0.74893\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4877 - acc: 0.7636 - val_loss: 0.4930 - val_acc: 0.7476\n",
      "Epoch 38/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4898 - acc: 0.7574\n",
      "Epoch 38: val_acc improved from 0.74893 to 0.75021, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4906 - acc: 0.7576 - val_loss: 0.4964 - val_acc: 0.7502\n",
      "Epoch 39/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.7610\n",
      "Epoch 39: val_acc improved from 0.75021 to 0.75278, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4897 - acc: 0.7613 - val_loss: 0.5008 - val_acc: 0.7528\n",
      "Epoch 40/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.7626\n",
      "Epoch 40: val_acc improved from 0.75278 to 0.75492, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4827 - acc: 0.7629 - val_loss: 0.4918 - val_acc: 0.7549\n",
      "Epoch 41/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.7626\n",
      "Epoch 41: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4876 - acc: 0.7628 - val_loss: 0.4955 - val_acc: 0.7447\n",
      "Epoch 42/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.7607\n",
      "Epoch 42: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4849 - acc: 0.7614 - val_loss: 0.4827 - val_acc: 0.7481\n",
      "Epoch 43/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.7668\n",
      "Epoch 43: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4794 - acc: 0.7671 - val_loss: 0.4748 - val_acc: 0.7515\n",
      "Epoch 44/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.7684\n",
      "Epoch 44: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4829 - acc: 0.7681 - val_loss: 0.4895 - val_acc: 0.7519\n",
      "Epoch 45/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.7684\n",
      "Epoch 45: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4765 - acc: 0.7686 - val_loss: 0.4889 - val_acc: 0.7506\n",
      "Epoch 46/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.7639\n",
      "Epoch 46: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4799 - acc: 0.7647 - val_loss: 0.4926 - val_acc: 0.7506\n",
      "Epoch 47/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4745 - acc: 0.7691\n",
      "Epoch 47: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4753 - acc: 0.7687 - val_loss: 0.5030 - val_acc: 0.7536\n",
      "Epoch 48/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.7642\n",
      "Epoch 48: val_acc did not improve from 0.75492\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4763 - acc: 0.7646 - val_loss: 0.4870 - val_acc: 0.7472\n",
      "Epoch 49/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.7707\n",
      "Epoch 49: val_acc improved from 0.75492 to 0.75535, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4755 - acc: 0.7708 - val_loss: 0.4766 - val_acc: 0.7553\n",
      "Epoch 50/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.7721\n",
      "Epoch 50: val_acc improved from 0.75535 to 0.75577, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4718 - acc: 0.7726 - val_loss: 0.4793 - val_acc: 0.7558\n",
      "Epoch 51/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.7743\n",
      "Epoch 51: val_acc did not improve from 0.75577\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4662 - acc: 0.7749 - val_loss: 0.4838 - val_acc: 0.7485\n",
      "Epoch 52/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4755 - acc: 0.7703\n",
      "Epoch 52: val_acc did not improve from 0.75577\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4755 - acc: 0.7706 - val_loss: 0.4890 - val_acc: 0.7549\n",
      "Epoch 53/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.7760\n",
      "Epoch 53: val_acc did not improve from 0.75577\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4630 - acc: 0.7765 - val_loss: 0.4791 - val_acc: 0.7511\n",
      "Epoch 54/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4748 - acc: 0.7673\n",
      "Epoch 54: val_acc did not improve from 0.75577\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4760 - acc: 0.7673 - val_loss: 0.4845 - val_acc: 0.7519\n",
      "Epoch 55/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.7711\n",
      "Epoch 55: val_acc improved from 0.75577 to 0.75620, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4707 - acc: 0.7709 - val_loss: 0.4745 - val_acc: 0.7562\n",
      "Epoch 56/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4655 - acc: 0.7668\n",
      "Epoch 56: val_acc did not improve from 0.75620\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4661 - acc: 0.7666 - val_loss: 0.4944 - val_acc: 0.7489\n",
      "Epoch 57/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.7741\n",
      "Epoch 57: val_acc improved from 0.75620 to 0.76133, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4644 - acc: 0.7744 - val_loss: 0.4764 - val_acc: 0.7613\n",
      "Epoch 58/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.7788\n",
      "Epoch 58: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4613 - acc: 0.7792 - val_loss: 0.4780 - val_acc: 0.7605\n",
      "Epoch 59/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.7750\n",
      "Epoch 59: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4609 - acc: 0.7754 - val_loss: 0.4859 - val_acc: 0.7575\n",
      "Epoch 60/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.7719\n",
      "Epoch 60: val_acc did not improve from 0.76133\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4645 - acc: 0.7726 - val_loss: 0.4794 - val_acc: 0.7601\n",
      "Epoch 61/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.7774\n",
      "Epoch 61: val_acc improved from 0.76133 to 0.76561, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4669 - acc: 0.7776 - val_loss: 0.4730 - val_acc: 0.7656\n",
      "Epoch 62/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4590 - acc: 0.7770\n",
      "Epoch 62: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4586 - acc: 0.7780 - val_loss: 0.4862 - val_acc: 0.7558\n",
      "Epoch 63/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4638 - acc: 0.7773\n",
      "Epoch 63: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4636 - acc: 0.7781 - val_loss: 0.4830 - val_acc: 0.7618\n",
      "Epoch 64/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4582 - acc: 0.7774\n",
      "Epoch 64: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4590 - acc: 0.7771 - val_loss: 0.4803 - val_acc: 0.7643\n",
      "Epoch 65/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4666 - acc: 0.7755\n",
      "Epoch 65: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4666 - acc: 0.7755 - val_loss: 0.4749 - val_acc: 0.7648\n",
      "Epoch 66/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.7807\n",
      "Epoch 66: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4529 - acc: 0.7810 - val_loss: 0.4750 - val_acc: 0.7601\n",
      "Epoch 67/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4652 - acc: 0.7737\n",
      "Epoch 67: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4647 - acc: 0.7740 - val_loss: 0.4799 - val_acc: 0.7511\n",
      "Epoch 68/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.7711\n",
      "Epoch 68: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4658 - acc: 0.7718 - val_loss: 0.4754 - val_acc: 0.7635\n",
      "Epoch 69/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4589 - acc: 0.7810\n",
      "Epoch 69: val_acc did not improve from 0.76561\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4591 - acc: 0.7810 - val_loss: 0.4733 - val_acc: 0.7618\n",
      "Epoch 70/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4519 - acc: 0.7834\n",
      "Epoch 70: val_acc improved from 0.76561 to 0.76647, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4522 - acc: 0.7837 - val_loss: 0.4912 - val_acc: 0.7665\n",
      "Epoch 71/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.7810\n",
      "Epoch 71: val_acc did not improve from 0.76647\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4527 - acc: 0.7812 - val_loss: 0.4762 - val_acc: 0.7583\n",
      "Epoch 72/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.7802\n",
      "Epoch 72: val_acc did not improve from 0.76647\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4536 - acc: 0.7806 - val_loss: 0.4758 - val_acc: 0.7532\n",
      "Epoch 73/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.7780\n",
      "Epoch 73: val_acc did not improve from 0.76647\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4537 - acc: 0.7785 - val_loss: 0.4774 - val_acc: 0.7596\n",
      "Epoch 74/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.7764\n",
      "Epoch 74: val_acc did not improve from 0.76647\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4583 - acc: 0.7764 - val_loss: 0.4750 - val_acc: 0.7575\n",
      "Epoch 75/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.7794\n",
      "Epoch 75: val_acc did not improve from 0.76647\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4503 - acc: 0.7795 - val_loss: 0.4783 - val_acc: 0.7575\n",
      "Epoch 76/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.7817\n",
      "Epoch 76: val_acc did not improve from 0.76647\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4490 - acc: 0.7820 - val_loss: 0.4796 - val_acc: 0.7519\n",
      "Epoch 77/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.7867\n",
      "Epoch 77: val_acc did not improve from 0.76647\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4445 - acc: 0.7868 - val_loss: 0.4863 - val_acc: 0.7583\n",
      "Epoch 78/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.7878\n",
      "Epoch 78: val_acc improved from 0.76647 to 0.76818, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4473 - acc: 0.7878 - val_loss: 0.4722 - val_acc: 0.7682\n",
      "Epoch 79/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.7839\n",
      "Epoch 79: val_acc did not improve from 0.76818\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4506 - acc: 0.7845 - val_loss: 0.4765 - val_acc: 0.7536\n",
      "Epoch 80/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.7752\n",
      "Epoch 80: val_acc did not improve from 0.76818\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4552 - acc: 0.7757 - val_loss: 0.4793 - val_acc: 0.7549\n",
      "Epoch 81/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.7830\n",
      "Epoch 81: val_acc did not improve from 0.76818\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4465 - acc: 0.7833 - val_loss: 0.4736 - val_acc: 0.7639\n",
      "Epoch 82/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4446 - acc: 0.7868\n",
      "Epoch 82: val_acc did not improve from 0.76818\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4444 - acc: 0.7871 - val_loss: 0.4728 - val_acc: 0.7579\n",
      "Epoch 83/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4571 - acc: 0.7789\n",
      "Epoch 83: val_acc did not improve from 0.76818\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4571 - acc: 0.7789 - val_loss: 0.4811 - val_acc: 0.7596\n",
      "Epoch 84/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.7802\n",
      "Epoch 84: val_acc did not improve from 0.76818\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4500 - acc: 0.7806 - val_loss: 0.4646 - val_acc: 0.7609\n",
      "Epoch 85/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.7862\n",
      "Epoch 85: val_acc improved from 0.76818 to 0.77160, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4431 - acc: 0.7872 - val_loss: 0.4723 - val_acc: 0.7716\n",
      "Epoch 86/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4449 - acc: 0.7814\n",
      "Epoch 86: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4455 - acc: 0.7817 - val_loss: 0.4731 - val_acc: 0.7635\n",
      "Epoch 87/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.7863\n",
      "Epoch 87: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4435 - acc: 0.7870 - val_loss: 0.4678 - val_acc: 0.7656\n",
      "Epoch 88/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.7825\n",
      "Epoch 88: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4474 - acc: 0.7830 - val_loss: 0.4734 - val_acc: 0.7545\n",
      "Epoch 89/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.7858\n",
      "Epoch 89: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4400 - acc: 0.7859 - val_loss: 0.4740 - val_acc: 0.7669\n",
      "Epoch 90/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.7874\n",
      "Epoch 90: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4384 - acc: 0.7878 - val_loss: 0.4757 - val_acc: 0.7549\n",
      "Epoch 91/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.7831\n",
      "Epoch 91: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 7ms/step - loss: 0.4372 - acc: 0.7842 - val_loss: 0.4709 - val_acc: 0.7678\n",
      "Epoch 92/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.7884\n",
      "Epoch 92: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4407 - acc: 0.7889 - val_loss: 0.4716 - val_acc: 0.7648\n",
      "Epoch 93/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4467 - acc: 0.7886\n",
      "Epoch 93: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4467 - acc: 0.7886 - val_loss: 0.4668 - val_acc: 0.7609\n",
      "Epoch 94/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.7994\n",
      "Epoch 94: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4295 - acc: 0.8003 - val_loss: 0.4807 - val_acc: 0.7502\n",
      "Epoch 95/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4478 - acc: 0.7838\n",
      "Epoch 95: val_acc did not improve from 0.77160\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4482 - acc: 0.7840 - val_loss: 0.4724 - val_acc: 0.7549\n",
      "Epoch 96/2000\n",
      "281/293 [===========================>..] - ETA: 0s - loss: 0.4314 - acc: 0.7917\n",
      "Epoch 96: val_acc improved from 0.77160 to 0.77288, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4328 - acc: 0.7921 - val_loss: 0.4633 - val_acc: 0.7729\n",
      "Epoch 97/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.7909\n",
      "Epoch 97: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4351 - acc: 0.7915 - val_loss: 0.4673 - val_acc: 0.7571\n",
      "Epoch 98/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.7885\n",
      "Epoch 98: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 6ms/step - loss: 0.4320 - acc: 0.7886 - val_loss: 0.4732 - val_acc: 0.7575\n",
      "Epoch 99/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.7884\n",
      "Epoch 99: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4417 - acc: 0.7890 - val_loss: 0.4861 - val_acc: 0.7485\n",
      "Epoch 100/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.7889\n",
      "Epoch 100: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4399 - acc: 0.7891 - val_loss: 0.4777 - val_acc: 0.7494\n",
      "Epoch 101/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4468 - acc: 0.7817\n",
      "Epoch 101: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4455 - acc: 0.7838 - val_loss: 0.4624 - val_acc: 0.7703\n",
      "Epoch 102/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4340 - acc: 0.7902\n",
      "Epoch 102: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4340 - acc: 0.7902 - val_loss: 0.4711 - val_acc: 0.7622\n",
      "Epoch 103/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.7897\n",
      "Epoch 103: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4351 - acc: 0.7898 - val_loss: 0.4880 - val_acc: 0.7374\n",
      "Epoch 104/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.7880\n",
      "Epoch 104: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4392 - acc: 0.7883 - val_loss: 0.4793 - val_acc: 0.7378\n",
      "Epoch 105/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.7890\n",
      "Epoch 105: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4410 - acc: 0.7890 - val_loss: 0.4659 - val_acc: 0.7588\n",
      "Epoch 106/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.7854\n",
      "Epoch 106: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4435 - acc: 0.7861 - val_loss: 0.4725 - val_acc: 0.7558\n",
      "Epoch 107/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.7896\n",
      "Epoch 107: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4316 - acc: 0.7895 - val_loss: 0.4724 - val_acc: 0.7596\n",
      "Epoch 108/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.7952\n",
      "Epoch 108: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4343 - acc: 0.7953 - val_loss: 0.4626 - val_acc: 0.7626\n",
      "Epoch 109/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.7950\n",
      "Epoch 109: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4258 - acc: 0.7954 - val_loss: 0.4694 - val_acc: 0.7532\n",
      "Epoch 110/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.7962\n",
      "Epoch 110: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4209 - acc: 0.7969 - val_loss: 0.4673 - val_acc: 0.7635\n",
      "Epoch 111/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.7894\n",
      "Epoch 111: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4314 - acc: 0.7897 - val_loss: 0.4818 - val_acc: 0.7635\n",
      "Epoch 112/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.7890\n",
      "Epoch 112: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4294 - acc: 0.7892 - val_loss: 0.4697 - val_acc: 0.7601\n",
      "Epoch 113/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.7887\n",
      "Epoch 113: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4321 - acc: 0.7891 - val_loss: 0.4650 - val_acc: 0.7558\n",
      "Epoch 114/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.7916\n",
      "Epoch 114: val_acc did not improve from 0.77288\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4279 - acc: 0.7919 - val_loss: 0.4600 - val_acc: 0.7707\n",
      "Epoch 115/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.7950\n",
      "Epoch 115: val_acc improved from 0.77288 to 0.78358, saving model to train_logs/logs7/RWB_ANN_512_256_256_512_Adam\\5\\256\\best_model.h5\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4303 - acc: 0.7956 - val_loss: 0.4422 - val_acc: 0.7836\n",
      "Epoch 116/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.7946\n",
      "Epoch 116: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4280 - acc: 0.7955 - val_loss: 0.4616 - val_acc: 0.7635\n",
      "Epoch 117/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.7948\n",
      "Epoch 117: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4316 - acc: 0.7951 - val_loss: 0.4802 - val_acc: 0.7519\n",
      "Epoch 118/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4253 - acc: 0.7917\n",
      "Epoch 118: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4262 - acc: 0.7922 - val_loss: 0.4697 - val_acc: 0.7609\n",
      "Epoch 119/2000\n",
      "288/293 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.7975\n",
      "Epoch 119: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4267 - acc: 0.7978 - val_loss: 0.4772 - val_acc: 0.7605\n",
      "Epoch 120/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.7944\n",
      "Epoch 120: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4305 - acc: 0.7947 - val_loss: 0.4635 - val_acc: 0.7605\n",
      "Epoch 121/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4229 - acc: 0.7931\n",
      "Epoch 121: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4229 - acc: 0.7931 - val_loss: 0.4751 - val_acc: 0.7622\n",
      "Epoch 122/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4303 - acc: 0.7884\n",
      "Epoch 122: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4303 - acc: 0.7884 - val_loss: 0.4644 - val_acc: 0.7622\n",
      "Epoch 123/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.7887\n",
      "Epoch 123: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4360 - acc: 0.7891 - val_loss: 0.4618 - val_acc: 0.7699\n",
      "Epoch 124/2000\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.7962\n",
      "Epoch 124: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4308 - acc: 0.7963 - val_loss: 0.4845 - val_acc: 0.7562\n",
      "Epoch 125/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.7908\n",
      "Epoch 125: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4306 - acc: 0.7917 - val_loss: 0.4642 - val_acc: 0.7690\n",
      "Epoch 126/2000\n",
      "286/293 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.7918\n",
      "Epoch 126: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4265 - acc: 0.7924 - val_loss: 0.4625 - val_acc: 0.7690\n",
      "Epoch 127/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.7983\n",
      "Epoch 127: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4261 - acc: 0.7988 - val_loss: 0.4716 - val_acc: 0.7630\n",
      "Epoch 128/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.7963\n",
      "Epoch 128: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4211 - acc: 0.7967 - val_loss: 0.4678 - val_acc: 0.7737\n",
      "Epoch 129/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.7958\n",
      "Epoch 129: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4222 - acc: 0.7964 - val_loss: 0.4668 - val_acc: 0.7613\n",
      "Epoch 130/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.7994\n",
      "Epoch 130: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4245 - acc: 0.7999 - val_loss: 0.4665 - val_acc: 0.7592\n",
      "Epoch 131/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.7968\n",
      "Epoch 131: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4196 - acc: 0.7972 - val_loss: 0.4704 - val_acc: 0.7712\n",
      "Epoch 132/2000\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.7989\n",
      "Epoch 132: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4173 - acc: 0.7991 - val_loss: 0.4649 - val_acc: 0.7635\n",
      "Epoch 133/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.7914\n",
      "Epoch 133: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4316 - acc: 0.7919 - val_loss: 0.4738 - val_acc: 0.7588\n",
      "Epoch 134/2000\n",
      "285/293 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.7897\n",
      "Epoch 134: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4289 - acc: 0.7897 - val_loss: 0.4715 - val_acc: 0.7669\n",
      "Epoch 135/2000\n",
      "290/293 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.7923\n",
      "Epoch 135: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4278 - acc: 0.7930 - val_loss: 0.4601 - val_acc: 0.7690\n",
      "Epoch 136/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.7949\n",
      "Epoch 136: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4245 - acc: 0.7956 - val_loss: 0.4648 - val_acc: 0.7622\n",
      "Epoch 137/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4230 - acc: 0.7951\n",
      "Epoch 137: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4224 - acc: 0.7959 - val_loss: 0.4515 - val_acc: 0.7802\n",
      "Epoch 138/2000\n",
      "287/293 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.7970\n",
      "Epoch 138: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4194 - acc: 0.7975 - val_loss: 0.4793 - val_acc: 0.7639\n",
      "Epoch 139/2000\n",
      "284/293 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.7915\n",
      "Epoch 139: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4331 - acc: 0.7919 - val_loss: 0.4678 - val_acc: 0.7613\n",
      "Epoch 140/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.7980\n",
      "Epoch 140: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4244 - acc: 0.7987 - val_loss: 0.4603 - val_acc: 0.7682\n",
      "Epoch 141/2000\n",
      "282/293 [===========================>..] - ETA: 0s - loss: 0.4272 - acc: 0.7941\n",
      "Epoch 141: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4281 - acc: 0.7940 - val_loss: 0.4758 - val_acc: 0.7601\n",
      "Epoch 142/2000\n",
      "289/293 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.7903\n",
      "Epoch 142: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4318 - acc: 0.7913 - val_loss: 0.4574 - val_acc: 0.7742\n",
      "Epoch 143/2000\n",
      "283/293 [===========================>..] - ETA: 0s - loss: 0.4275 - acc: 0.7969\n",
      "Epoch 143: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 1s 5ms/step - loss: 0.4282 - acc: 0.7963 - val_loss: 0.4742 - val_acc: 0.7571\n",
      "Epoch 144/2000\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4181 - acc: 0.7981\n",
      "Epoch 144: val_acc did not improve from 0.78358\n",
      "293/293 [==============================] - 2s 5ms/step - loss: 0.4181 - acc: 0.7981 - val_loss: 0.4671 - val_acc: 0.7793\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.4593 - acc: 0.7917\n"
     ]
    }
   ],
   "source": [
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=range(1, 6))\n",
    "    training_time = pd.DataFrame(index=lags, columns=[f'CPU_Time_{i}' for i in range(1, 6)] + [f'Wall_Time_{i}' for i in range(1, 6)])\n",
    "\n",
    "    for lag in lags:\n",
    "        train_temp_dir = train_dir + '_' + str(lag)\n",
    "        train = tf.data.Dataset.load(train_temp_dir)\n",
    "        flattened_train = train.unbatch()\n",
    "\n",
    "        train_data = list(flattened_train.as_numpy_iterator())\n",
    "        train_size = len(list(train_data))\n",
    "\n",
    "        kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_data), 1):\n",
    "            train_fold_data = ([train_data[i][0] for i in train_index], [train_data[i][1] for i in train_index])\n",
    "            val_fold_data = ([train_data[i][0] for i in val_index], [train_data[i][1] for i in val_index])\n",
    "\n",
    "            train_fold = tf.data.Dataset.from_tensor_slices(train_fold_data).batch(32)\n",
    "            val_fold = tf.data.Dataset.from_tensor_slices(val_fold_data).batch(32)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_fold, epochs=epochs, validation_data=val_fold, callbacks=myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "\n",
    "            training_time.loc[lag, f'CPU_Time_{fold}'] = cpu_time\n",
    "            training_time.loc[lag, f'Wall_Time_{fold}'] = wall_time\n",
    "\n",
    "            recap.loc[lag, fold] = history.history['acc'][-1]\n",
    "\n",
    "\n",
    "    # Evaluate on the test dataset after cross-validation\n",
    "    test_temp_dir = test_dir + '_' + str(lag)\n",
    "    test_ds = tf.data.Dataset.load(test_temp_dir)\n",
    "    results = model.evaluate(test_ds, callbacks=myCallbacks(log_path))\n",
    "\n",
    "    recap[f'test_{lag}'] = results[1]\n",
    "\n",
    "    log_recap_dir = os.path.join(log_dir, 'Recap')\n",
    "    if not os.path.exists(log_recap_dir):\n",
    "        os.makedirs(log_recap_dir)\n",
    "\n",
    "    recap.to_csv(os.path.join(log_recap_dir, 'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_recap_dir, 'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "LAG = 256\n",
    "\n",
    "test_dir = f\"datasets/tf_batch/RWB/segment_1 seconds/test_{LAG}\"\n",
    "test_ds = tf.data.Dataset.load(test_dir)\n",
    "model_dir = [f\"train_logs/logs7/RWB_ANN_512_256_256_512_Adam/{i}/{LAG}/best_model.h5\" for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_test = test_ds.unbatch()\n",
    "test_data = list(flattened_test.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_value = np.array([test_data[i][0] for i in range(len(test_data))])\n",
    "test_data_label = np.array([test_data[i][1] for i in range(len(test_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 96)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_value.reshape(test_data_value.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4348 - acc: 0.7972\n",
      "0.4348430037498474 0.7971956133842468\n",
      "92/92 [==============================] - 0s 1ms/step\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4380 - acc: 0.8006\n",
      "0.4379970133304596 0.8006156086921692\n",
      "92/92 [==============================] - 0s 1ms/step\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4525 - acc: 0.8044\n",
      "0.4524590075016022 0.804377555847168\n",
      "92/92 [==============================] - 0s 1ms/step\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.4352 - acc: 0.7934\n",
      "0.4351736903190613 0.793433666229248\n",
      "92/92 [==============================] - 0s 2ms/step\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.4300 - acc: 0.7986\n",
      "0.42995381355285645 0.7985635995864868\n",
      "92/92 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, model_path in enumerate(model_dir):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(loss, acc)\n",
    "    pred = model.predict(test_data_value.reshape(test_data_value.shape[0], -1))\n",
    "    results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_data_label.astype(bool), pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'autism'])\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Result: RWB ANN (512,256,256,512) lag 256 for model in fold {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: RWB ANN (512,256,256,512)(Adam) lag 256 for model in fold 1\n",
      "Precision: 0.7788423153692615\n",
      "Recall: 0.9804020100502513\n",
      "Accuracy: 0.7971956224350205\n",
      "F1-score: 0.868075639599555\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256,256,512)(Adam) lag 256 for model in fold 2\n",
      "Precision: 0.7858594067452255\n",
      "Recall: 0.9718592964824121\n",
      "Accuracy: 0.8006155950752394\n",
      "F1-score: 0.8690181981577173\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256,256,512)(Adam) lag 256 for model in fold 3\n",
      "Precision: 0.7861178369652946\n",
      "Recall: 0.978894472361809\n",
      "Accuracy: 0.8043775649794802\n",
      "F1-score: 0.8719785138764549\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256,256,512)(Adam) lag 256 for model in fold 4\n",
      "Precision: 0.773696682464455\n",
      "Recall: 0.9844221105527639\n",
      "Accuracy: 0.7934336525307798\n",
      "F1-score: 0.8664307828394515\n",
      "\n",
      "\n",
      "Result: RWB ANN (512,256,256,512)(Adam) lag 256 for model in fold 5\n",
      "Precision: 0.7807615230460921\n",
      "Recall: 0.978894472361809\n",
      "Accuracy: 0.7985636114911081\n",
      "F1-score: 0.8686733556298774\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    test_data_label = test_data_label.astype(bool)  \n",
    "\n",
    "    print(f\"Result: RWB ANN (512,256,256,512)(Adam) lag 256 for model in fold {i}\")\n",
    "\n",
    "    TP = np.sum((test_data_label == True) & (pred == True))\n",
    "    FP = np.sum((test_data_label == False) & (pred == True))\n",
    "    TN = np.sum((test_data_label == False) & (pred == False))\n",
    "    FN = np.sum((test_data_label == True) & (pred == False))  \n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
