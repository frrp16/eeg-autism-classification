{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farra\\anaconda3\\envs\\eeg_env\\lib\\site-packages\\stingray\\utils.py:54: UserWarning: Numba not installed. Faking it\n",
      "  warnings.warn(\"Numba not installed. Faking it\")\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from scipy import signal\n",
    "from stingray import lightcurve\n",
    "import sys\n",
    "from stingray import Bispectrum\n",
    "import warnings\n",
    "import csv\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file that will be skipped because of information loss\n",
    "# zeros_test = ['co2a0000368_91.csv', 'co2c0000341_26.csv']\n",
    "# zeros_train = ['co2a0000368_0.csv', 'co2a0000368_1.csv', 'co2a0000368_2.csv', 'co2a0000368_3.csv', 'co2a0000368_4.csv', 'co2a0000368_5.csv', 'co2c0000341_27.csv']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghitung Matriks Cumulant orde ke-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCumulantOrde3(df_data, t, lag):\n",
    "    # Compute the bispectrum of the signal\n",
    "    lc = lightcurve.Lightcurve(t,df_data)\n",
    "    bs = Bispectrum(lc, maxlag=lag)\n",
    "    return bs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melakukan dekomposisi wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcWaveletDec(bs):\n",
    "    # Select wavelet and decomposition level\n",
    "    wavelet = 'db4'\n",
    "    level = 5\n",
    "\n",
    "    # Deecompose signal\n",
    "    coeffs = pywt.wavedec2(bs.cum3, wavelet, level=level)\n",
    "    \n",
    "    # cA5 = coeffs[0][np.triu(np.ones_like(coeffs[0], dtype=bool))]\n",
    "    # cD5 = np.ravel([coeffs[1][0], coeffs[1][1], coeffs[1][2]])\n",
    "    # cD4 = np.ravel([coeffs[2][0], coeffs[2][1], coeffs[2][2]])\n",
    "    # cD3 = np.ravel([coeffs[3][0], coeffs[3][1], coeffs[3][2]])\n",
    "    # cD2 = np.ravel([coeffs[4][0], coeffs[4][1], coeffs[4][2]])\n",
    "    # cD1 = np.ravel([coeffs[5][0], coeffs[5][1], coeffs[5][2]])\n",
    "    \n",
    "    # coeff = [cA5,cD5,cD4,cD3,cD2,cD1]\n",
    "    # # fig, axs = plt.subplots(6)\n",
    "    \n",
    "    \n",
    "    # # axs[0].plot(cA5)\n",
    "    # # axs[0].set_title(f'Approximation - Level 5')\n",
    "    # # axs[1].plot(cD5)\n",
    "    # # axs[1].set_title(f'Detail - Level 5')\n",
    "    # # axs[2].plot(cD4)\n",
    "    # # axs[2].set_title(f'Detail - Level 4')\n",
    "    # # axs[3].plot(cD3)\n",
    "    # # axs[3].set_title(f'Detail - Level 3')\n",
    "    # # axs[4].plot(cD2)\n",
    "    # # axs[4].set_title(f'Detail - Level 2')\n",
    "    # # axs[5].plot(cD1)\n",
    "    # # axs[5].set_title(f'Detail - Level 1')\n",
    "    # # plt.show()\n",
    "\n",
    "    return coeffs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghitung energi relatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRelativeEnergy(coeffs, df_data):\n",
    "    # Calculate relative wavelet energy\n",
    "    energies = []\n",
    "    for c in coeffs:\n",
    "        energies.append(np.sum(np.square(c)))\n",
    "\n",
    "    energies[1:6] = energies[-1:-6:-1]\n",
    "\n",
    "    total_energy = np.sum(energies)\n",
    "    relative_energies = [(e / total_energy) * 100 for e in energies]\n",
    "\n",
    "    return energies, relative_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persiapan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sampling frequency\n",
    "fs = 256\n",
    "t = np.arange(0, 1, 1/(fs*10))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_csv_EEG(filename):\n",
    "    # Load data from CSV\n",
    "    df_data = pd.read_csv(filename)\n",
    "    return df_data, df_data.columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perhitungan RWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(directory, lag, destination, segment_time=1):\n",
    "    fs = 256\n",
    "    t = np.arange(0, 1, 1/(fs * segment_time))\n",
    "\n",
    "    recap = pd.DataFrame(columns=['Wall Time', 'CPU Time'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            des_dir = os.path.join(destination +\"_\" + str(lag), foldername).lower()\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                # print(f\"Extract feature from {filename}...\")\n",
    "                cpu_start = time.process_time()\n",
    "                wt_start = time.time()\n",
    "                # if filename in zeros_train or filename in zeros_test:\n",
    "                #     continue\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                # if 'metadata' in filename.lower():\n",
    "                #     continue\n",
    "                # trial_number = filename.split('.')[0].split('_')[1]\n",
    "                df_data, channel_name = get_csv_EEG(rel_path)\n",
    "                des_file = foldername +'_'+ filename + '_bispectrum' +'.npy'\n",
    "                if not os.path.exists(des_dir):\n",
    "                    os.makedirs(des_dir)\n",
    "                des_path = os.path.join(des_dir, des_file)\n",
    "                if os.path.exists(des_path):\n",
    "                    continue\n",
    "                RWB = []\n",
    "                for channel in channel_name:\n",
    "                    y = df_data[channel]; # sinyal per channel\n",
    "                    # N = len(y);\n",
    "                    # z = y - np.mean(y);\n",
    "                    # nsamp = len (y[0])\n",
    "                    energies, relative_energies = calcRelativeEnergy(calcWaveletDec(calcCumulantOrde3(y, t, lag)), y)\n",
    "                    for x in relative_energies:\n",
    "                        RWB.append(x)\n",
    "                RWB = np.array(RWB)\n",
    "                np.save(des_path, RWB)\n",
    "                wt_end = time.time()\n",
    "                cpu_end = time.process_time()\n",
    "                wall_time = wt_end - wt_start\n",
    "                cpu_time = cpu_end - cpu_start\n",
    "                recap_temp = pd.DataFrame([[wall_time, cpu_time]],columns=recap.columns)\n",
    "                recap = pd.concat([recap, recap_temp], ignore_index=True)\n",
    "                # print(f\"CPU Time: {cpu_time}, Wall Time: {wall_time}\")\n",
    "                # pd.DataFrame(RWB.T).to_csv(des_path, index=False)\n",
    "    recap_dir = os.path.join('./logs/Execution',directory.split('/')[1])\n",
    "    if not os.path.exists(recap_dir):\n",
    "        os.makedirs(recap_dir)\n",
    "    recap_path = os.path.join(recap_dir,'recap_rwb'+str(lag)+'.csv')\n",
    "    recap.to_csv(recap_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mengecek nilai nol pada data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zeros(directory):\n",
    "    contain_zero = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                # if 'metadata' in filename.lower():\n",
    "                #     continue\n",
    "                df_data, channel_name = get_csv_EEG(rel_path)\n",
    "                for channel in channel_name:\n",
    "                    if (df_data[channel]== 0).all():\n",
    "                        contain_zero.append(filename)\n",
    "                        break\n",
    "    return contain_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_zeros(os.path.join(directory_segmented, \"autism\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy(directory, lag):\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            # des_dir = os.path.join(directory.replace('CSV', 'FEATURE')+\"_\" + str(lag),foldername).lower()\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                # if 'metadata' in filename.lower():\n",
    "                #     continue\n",
    "                # trial_number = filename.split('.')[0].split('_')[1]\n",
    "                print(rel_path)\n",
    "                df_data, channel_name = get_csv_EEG(rel_path)\n",
    "                RWB = []\n",
    "                return df_data[channel_name[0]]\n",
    "                # pd.DataFrame(RWB.T).to_csv(des_path, index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/segmented_10 seconds/autism\\Bader\\segment_1.csv\n",
      "(513, 513)\n",
      "(513, 513)\n",
      "(513, 513)\n",
      "(6,)\n",
      "([112107909129063.05, 32392153.511949405, 605719914.025452, 6014831714.317251, 154661772562.2268, 1248157569376.144], [98.75836434195911, 2.853497244936536e-05, 0.0005335922186331983, 0.00529859977331537, 0.13624501431820848, 1.0995299167582784])\n"
     ]
    }
   ],
   "source": [
    "lag = 256\n",
    "\n",
    "dummy = get_dummy(\"datasets/segmented_10 seconds/autism\", lag)\n",
    "bs = calcCumulantOrde3(dummy, t, lag)\n",
    "print(bs.bispec_mag.shape)\n",
    "print(bs.cum3.shape)\n",
    "print(bs.bispec.shape)\n",
    "\n",
    "wavelet = calcWaveletDec(bs)\n",
    "print(np.array(wavelet).shape)\n",
    "relative_energies = calcRelativeEnergy(wavelet, dummy)\n",
    "print(relative_energies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(relative_energies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_TIME = 1\n",
    "\n",
    "directory_segmented = f\"datasets/segmented_{SEGMENT_TIME} seconds\"\n",
    "directory_feature = f\"datasets/features/RWB/segment_{SEGMENT_TIME} seconds\"\n",
    "directory_logs = f\"logs/Execution/segmented_{SEGMENT_TIME} seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "lags = [128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in lags:\n",
    "    extract_feature(os.path.join(directory_segmented, \"autism\"), lag, os.path.join(directory_feature, \"autism\"), segment_time=SEGMENT_TIME)\n",
    "    extract_feature(os.path.join(directory_segmented, \"normal\"), lag, os.path.join(directory_feature, \"normal\"), segment_time=SEGMENT_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 256:\n",
      " Wall Time    3776.337031\n",
      "CPU Time     6545.921875\n",
      "dtype: float64 \n",
      "\n",
      "Lag 128:\n",
      " Wall Time    4054.345197\n",
      "CPU Time     6908.421875\n",
      "dtype: float64 \n",
      "\n",
      "Lag 64:\n",
      " Wall Time    1536.385925\n",
      "CPU Time     2219.437500\n",
      "dtype: float64 \n",
      "\n",
      "Lag 32:\n",
      " Wall Time    1071.020538\n",
      "CPU Time     1263.968750\n",
      "dtype: float64 \n",
      "\n",
      "Lag 16:\n",
      " Wall Time    565.037749\n",
      "CPU Time     682.375000\n",
      "dtype: float64 \n",
      "\n",
      "Lag 8:\n",
      " Wall Time    691.267961\n",
      "CPU Time     881.984375\n",
      "dtype: float64 \n",
      "\n",
      "Lag 4:\n",
      " Wall Time    556.042306\n",
      "CPU Time     132.484375\n",
      "dtype: float64 \n",
      "\n",
      "Lag 2:\n",
      " Wall Time    136.555223\n",
      "CPU Time      63.453125\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lags = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "\n",
    "SEGMENT_TIME = 1\n",
    "\n",
    "for lag in lags:\n",
    "    df = pd.DataFrame()\n",
    "    for folder in os.listdir(directory_logs):\n",
    "        temp = pd.read_csv(os.path.join(directory_logs, folder, f\"recap_rwb{lag}.csv\"))\n",
    "        df = pd.concat([df, temp])\n",
    "    df = df.drop('Unnamed: 0', axis=1)   \n",
    "    print(f\"Lag {lag}:\\n\", df.sum(), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7899ebb7610f36207965d38f68f9fcf69ead4646d08358e8b81fddd5bb8ec13e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
