{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, excluded_name=[]):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):        \n",
    "        folder = os.path.join(directory, foldername)\n",
    "        # print(folder)\n",
    "        # print(os.listdir(folder))\n",
    "        for name in os.listdir(folder):\n",
    "            if name in excluded_name:\n",
    "                # print(name)\n",
    "                continue\n",
    "            filename = os.path.join(folder, name)\n",
    "            # print(filename)\n",
    "            for files in os.listdir(filename):\n",
    "                rel_path = os.path.join(filename, files)\n",
    "                # print(rel_path)\n",
    "                temp_label = folder\n",
    "                if \"autism\" in temp_label:\n",
    "                    label = 'autism'\n",
    "                else:\n",
    "                    label = 'normal'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64).reshape(-1,1)\n",
    "                                \n",
    "                temp_data.loc[0, \"data\"] = rwb\n",
    "                temp_data['label'] = label\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"autism\": 1, \"normal\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_value(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "    missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "    clean_data = data.drop(index=data.index[missing_indices])\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "\n",
    "    epsilon = 1e-9  # To avoid log(0) issues\n",
    "    log_transformed_series = np.log(series_list + epsilon)\n",
    "\n",
    "    log_transformed_list = [arr for arr in log_transformed_series]\n",
    "    data[\"data\"] = log_transformed_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "# rwb = np.load(\"datasets/features/rwb/segment_1 seconds/autism_256/bader/Bader_segment_100.csv_bispectrum.npy\")\n",
    "# rwb.astype(np.float64).reshape(-1,1)\n",
    "# temp_data.loc[0, \"data\"] = rwb\n",
    "# temp_data['label'] = \"autism\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data, des_path):\n",
    "    if not os.path.exists(des_path):\n",
    "        os.makedirs(des_path)\n",
    "    data.save(des_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, train_split: float):\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "        data['data'],\n",
    "        data[['label', 'label_map']],\n",
    "        train_size=train_split,\n",
    "        stratify=data['label_map']\n",
    "    )\n",
    "\n",
    "    train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "    test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "    train_df[\"data\"] = train_x\n",
    "    train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "    test_df[\"data\"] = test_x\n",
    "    test_df[['label', 'label_map']] = test_y\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    # loading extracted feature & label\n",
    "    # x = get_dataset(path, lag, excluded_name)\n",
    "\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = data[\"label_map\"].values\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list))\n",
    "\n",
    "    # train_size = int(train_split * len(labels_list))  \n",
    "    # test_size = len(labels_list) - train_size  \n",
    "\n",
    "    # train_dataset = dataset.take(train_size)\n",
    "    # test_dataset = dataset.skip(train_size)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "#         data['data'],\n",
    "#         data[['label', 'label_map']],\n",
    "#         train_size=0.8,\n",
    "#         stratify=data['label_map']\n",
    "#     )\n",
    "\n",
    "# train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "# test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "# train_df[\"data\"] = train_x\n",
    "# train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "# test_df[\"data\"] = test_x\n",
    "# test_df[['label', 'label_map']] = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = [\"zyad\"]\n",
    "# data = get_dataset(data_dir, 256, excluded_name=excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = data[\"data\"].values\n",
    "# series_list = np.vstack(temp)\n",
    "# series_list = series_list.reshape(-1, 96, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/features/dwt_relative/segment_1 seconds\"\n",
    "\n",
    "train_dir = \"datasets/tf_batch/dwt_relative/segment_1 seconds/train\"\n",
    "test_dir = \"datasets/tf_batch/dwt_relative/segment_1 seconds/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15434, 3)\n",
      "(15434, 3)\n",
      "train: (12347, 3)\n",
      "test: (3087, 3)\n"
     ]
    }
   ],
   "source": [
    "excluded = [\"zyad\"]\n",
    "train_split = 0.8\n",
    "# LAG = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "\n",
    "# for lag in LAG:\n",
    "data = get_dataset(data_dir, excluded)\n",
    "print(data.shape)\n",
    "data = remove_missing_value(data)\n",
    "print(data.shape)\n",
    "# data = log_transform(data)\n",
    "# print(data.shape)\n",
    "train_data, test_data = get_train_test(data, train_split)\n",
    "print(\"train:\", train_data.shape)\n",
    "print(\"test:\", test_data.shape)\n",
    "\n",
    "train_batch = get_batch(train_data)\n",
    "test_batch = get_batch(test_data)\n",
    "tf.data.Dataset.save(train_batch, f\"{train_dir}\")\n",
    "tf.data.Dataset.save(test_batch, f\"{test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load(\"datasets/features/dwt_relative/segment_1 seconds/normal_/amer/Amer_segment_1003.csv_dwt.npy\")\n",
    "\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "print(signal.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(96,)))\n",
    "    model.add(layers.Reshape((96, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags = [256]\n",
    "folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000\n",
    "log_dirs = ['train_logs/logs7/DWT_Relative_ANN_512_256_Adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_835/2764964901.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(list(train_data.as_numpy_iterator())).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(386, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.load(train_dir)\n",
    "np.array(list(train_data.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 1.4697 - acc: 0.6326\n",
      "Epoch 1: val_acc improved from -inf to 0.69798, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 5s 6ms/step - loss: 1.4643 - acc: 0.6328 - val_loss: 0.5993 - val_acc: 0.6980\n",
      "Epoch 2/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.6276 - acc: 0.6844\n",
      "Epoch 2: val_acc improved from 0.69798 to 0.71134, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.6273 - acc: 0.6844 - val_loss: 0.5786 - val_acc: 0.7113\n",
      "Epoch 3/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.7025\n",
      "Epoch 3: val_acc improved from 0.71134 to 0.73522, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.6052 - acc: 0.7025 - val_loss: 0.5481 - val_acc: 0.7352\n",
      "Epoch 4/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.7098\n",
      "Epoch 4: val_acc did not improve from 0.73522\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5944 - acc: 0.7100 - val_loss: 0.5774 - val_acc: 0.7117\n",
      "Epoch 5/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5853 - acc: 0.7176\n",
      "Epoch 5: val_acc improved from 0.73522 to 0.73603, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5853 - acc: 0.7175 - val_loss: 0.5434 - val_acc: 0.7360\n",
      "Epoch 6/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5766 - acc: 0.7165\n",
      "Epoch 6: val_acc improved from 0.73603 to 0.73927, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5766 - acc: 0.7165 - val_loss: 0.5419 - val_acc: 0.7393\n",
      "Epoch 7/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.7261\n",
      "Epoch 7: val_acc improved from 0.73927 to 0.74737, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5665 - acc: 0.7262 - val_loss: 0.5354 - val_acc: 0.7474\n",
      "Epoch 8/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.7264\n",
      "Epoch 8: val_acc did not improve from 0.74737\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5656 - acc: 0.7265 - val_loss: 0.5403 - val_acc: 0.7308\n",
      "Epoch 9/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.7320\n",
      "Epoch 9: val_acc did not improve from 0.74737\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5560 - acc: 0.7324 - val_loss: 0.5317 - val_acc: 0.7385\n",
      "Epoch 10/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.7281\n",
      "Epoch 10: val_acc did not improve from 0.74737\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5557 - acc: 0.7283 - val_loss: 0.5319 - val_acc: 0.7312\n",
      "Epoch 11/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7307\n",
      "Epoch 11: val_acc did not improve from 0.74737\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.5509 - acc: 0.7306 - val_loss: 0.5150 - val_acc: 0.7429\n",
      "Epoch 12/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.7318\n",
      "Epoch 12: val_acc did not improve from 0.74737\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5467 - acc: 0.7318 - val_loss: 0.5156 - val_acc: 0.7372\n",
      "Epoch 13/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.7358\n",
      "Epoch 13: val_acc did not improve from 0.74737\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.5428 - acc: 0.7359 - val_loss: 0.5432 - val_acc: 0.7300\n",
      "Epoch 14/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.7385\n",
      "Epoch 14: val_acc improved from 0.74737 to 0.75789, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.5376 - acc: 0.7384 - val_loss: 0.5075 - val_acc: 0.7579\n",
      "Epoch 15/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7452\n",
      "Epoch 15: val_acc improved from 0.75789 to 0.76275, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5318 - acc: 0.7454 - val_loss: 0.5077 - val_acc: 0.7628\n",
      "Epoch 16/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7479\n",
      "Epoch 16: val_acc did not improve from 0.76275\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5240 - acc: 0.7480 - val_loss: 0.4935 - val_acc: 0.7579\n",
      "Epoch 17/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5277 - acc: 0.7450\n",
      "Epoch 17: val_acc improved from 0.76275 to 0.77045, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5277 - acc: 0.7450 - val_loss: 0.4788 - val_acc: 0.7704\n",
      "Epoch 18/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5248 - acc: 0.7474\n",
      "Epoch 18: val_acc did not improve from 0.77045\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5248 - acc: 0.7474 - val_loss: 0.4861 - val_acc: 0.7587\n",
      "Epoch 19/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7603\n",
      "Epoch 19: val_acc did not improve from 0.77045\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.5076 - acc: 0.7603 - val_loss: 0.5068 - val_acc: 0.7534\n",
      "Epoch 20/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.7549\n",
      "Epoch 20: val_acc did not improve from 0.77045\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.5148 - acc: 0.7553 - val_loss: 0.4997 - val_acc: 0.7543\n",
      "Epoch 21/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.7561\n",
      "Epoch 21: val_acc did not improve from 0.77045\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.5097 - acc: 0.7563 - val_loss: 0.4895 - val_acc: 0.7559\n",
      "Epoch 22/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.7578\n",
      "Epoch 22: val_acc improved from 0.77045 to 0.78016, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5103 - acc: 0.7580 - val_loss: 0.4740 - val_acc: 0.7802\n",
      "Epoch 23/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.7591\n",
      "Epoch 23: val_acc did not improve from 0.78016\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5011 - acc: 0.7594 - val_loss: 0.4620 - val_acc: 0.7676\n",
      "Epoch 24/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5038 - acc: 0.7600\n",
      "Epoch 24: val_acc improved from 0.78016 to 0.78826, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5034 - acc: 0.7603 - val_loss: 0.4436 - val_acc: 0.7883\n",
      "Epoch 25/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.7586\n",
      "Epoch 25: val_acc did not improve from 0.78826\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5058 - acc: 0.7588 - val_loss: 0.4809 - val_acc: 0.7729\n",
      "Epoch 26/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4975 - acc: 0.7668\n",
      "Epoch 26: val_acc did not improve from 0.78826\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4971 - acc: 0.7669 - val_loss: 0.4571 - val_acc: 0.7830\n",
      "Epoch 27/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4990 - acc: 0.7621\n",
      "Epoch 27: val_acc did not improve from 0.78826\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4988 - acc: 0.7621 - val_loss: 0.4482 - val_acc: 0.7838\n",
      "Epoch 28/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.7623\n",
      "Epoch 28: val_acc improved from 0.78826 to 0.79352, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5035 - acc: 0.7622 - val_loss: 0.4480 - val_acc: 0.7935\n",
      "Epoch 29/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4866 - acc: 0.7686\n",
      "Epoch 29: val_acc did not improve from 0.79352\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4858 - acc: 0.7690 - val_loss: 0.4674 - val_acc: 0.7648\n",
      "Epoch 30/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4892 - acc: 0.7686\n",
      "Epoch 30: val_acc did not improve from 0.79352\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4888 - acc: 0.7687 - val_loss: 0.4440 - val_acc: 0.7923\n",
      "Epoch 31/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4810 - acc: 0.7721\n",
      "Epoch 31: val_acc did not improve from 0.79352\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4804 - acc: 0.7723 - val_loss: 0.4456 - val_acc: 0.7818\n",
      "Epoch 32/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4846 - acc: 0.7685\n",
      "Epoch 32: val_acc did not improve from 0.79352\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4846 - acc: 0.7685 - val_loss: 0.4440 - val_acc: 0.7919\n",
      "Epoch 33/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.7716\n",
      "Epoch 33: val_acc improved from 0.79352 to 0.79919, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4855 - acc: 0.7719 - val_loss: 0.4362 - val_acc: 0.7992\n",
      "Epoch 34/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.7737\n",
      "Epoch 34: val_acc did not improve from 0.79919\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4833 - acc: 0.7735 - val_loss: 0.4512 - val_acc: 0.7854\n",
      "Epoch 35/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.7747\n",
      "Epoch 35: val_acc did not improve from 0.79919\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4838 - acc: 0.7747 - val_loss: 0.4401 - val_acc: 0.7810\n",
      "Epoch 36/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.7759\n",
      "Epoch 36: val_acc did not improve from 0.79919\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4780 - acc: 0.7760 - val_loss: 0.4466 - val_acc: 0.7879\n",
      "Epoch 37/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.7754\n",
      "Epoch 37: val_acc improved from 0.79919 to 0.80445, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4703 - acc: 0.7758 - val_loss: 0.4403 - val_acc: 0.8045\n",
      "Epoch 38/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.7797\n",
      "Epoch 38: val_acc improved from 0.80445 to 0.80486, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4710 - acc: 0.7800 - val_loss: 0.4245 - val_acc: 0.8049\n",
      "Epoch 39/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4743 - acc: 0.7742\n",
      "Epoch 39: val_acc did not improve from 0.80486\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4739 - acc: 0.7745 - val_loss: 0.4653 - val_acc: 0.7652\n",
      "Epoch 40/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.7750\n",
      "Epoch 40: val_acc did not improve from 0.80486\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4739 - acc: 0.7752 - val_loss: 0.4341 - val_acc: 0.7858\n",
      "Epoch 41/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4699 - acc: 0.7829\n",
      "Epoch 41: val_acc did not improve from 0.80486\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4690 - acc: 0.7836 - val_loss: 0.4247 - val_acc: 0.8036\n",
      "Epoch 42/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4773 - acc: 0.7755\n",
      "Epoch 42: val_acc did not improve from 0.80486\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4768 - acc: 0.7759 - val_loss: 0.4478 - val_acc: 0.7947\n",
      "Epoch 43/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.7779\n",
      "Epoch 43: val_acc did not improve from 0.80486\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4643 - acc: 0.7782 - val_loss: 0.4285 - val_acc: 0.7899\n",
      "Epoch 44/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.7833\n",
      "Epoch 44: val_acc improved from 0.80486 to 0.81053, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4626 - acc: 0.7836 - val_loss: 0.4195 - val_acc: 0.8105\n",
      "Epoch 45/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.7825\n",
      "Epoch 45: val_acc did not improve from 0.81053\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4591 - acc: 0.7830 - val_loss: 0.4167 - val_acc: 0.7964\n",
      "Epoch 46/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.7770\n",
      "Epoch 46: val_acc did not improve from 0.81053\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4726 - acc: 0.7771 - val_loss: 0.4146 - val_acc: 0.8077\n",
      "Epoch 47/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4699 - acc: 0.7778\n",
      "Epoch 47: val_acc improved from 0.81053 to 0.81336, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4690 - acc: 0.7786 - val_loss: 0.4176 - val_acc: 0.8134\n",
      "Epoch 48/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4561 - acc: 0.7876\n",
      "Epoch 48: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4549 - acc: 0.7882 - val_loss: 0.4182 - val_acc: 0.8065\n",
      "Epoch 49/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4591 - acc: 0.7837\n",
      "Epoch 49: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4589 - acc: 0.7840 - val_loss: 0.4257 - val_acc: 0.7992\n",
      "Epoch 50/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4536 - acc: 0.7872\n",
      "Epoch 50: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4536 - acc: 0.7872 - val_loss: 0.3998 - val_acc: 0.8121\n",
      "Epoch 51/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.7855\n",
      "Epoch 51: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4561 - acc: 0.7859 - val_loss: 0.4176 - val_acc: 0.8109\n",
      "Epoch 52/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.7845\n",
      "Epoch 52: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4575 - acc: 0.7844 - val_loss: 0.4123 - val_acc: 0.8065\n",
      "Epoch 53/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.7819\n",
      "Epoch 53: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4564 - acc: 0.7819 - val_loss: 0.4124 - val_acc: 0.7939\n",
      "Epoch 54/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4596 - acc: 0.7808\n",
      "Epoch 54: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4596 - acc: 0.7808 - val_loss: 0.4233 - val_acc: 0.7866\n",
      "Epoch 55/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.7875\n",
      "Epoch 55: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4498 - acc: 0.7879 - val_loss: 0.4077 - val_acc: 0.8053\n",
      "Epoch 56/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.7823\n",
      "Epoch 56: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4510 - acc: 0.7825 - val_loss: 0.4219 - val_acc: 0.7923\n",
      "Epoch 57/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4534 - acc: 0.7862\n",
      "Epoch 57: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4534 - acc: 0.7862 - val_loss: 0.4022 - val_acc: 0.8113\n",
      "Epoch 58/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.7852\n",
      "Epoch 58: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4556 - acc: 0.7857 - val_loss: 0.4080 - val_acc: 0.8126\n",
      "Epoch 59/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4498 - acc: 0.7907\n",
      "Epoch 59: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4498 - acc: 0.7907 - val_loss: 0.4193 - val_acc: 0.7903\n",
      "Epoch 60/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.7825\n",
      "Epoch 60: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4530 - acc: 0.7825 - val_loss: 0.4230 - val_acc: 0.8073\n",
      "Epoch 61/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.7884\n",
      "Epoch 61: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4507 - acc: 0.7884 - val_loss: 0.4049 - val_acc: 0.8073\n",
      "Epoch 62/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.7887\n",
      "Epoch 62: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4504 - acc: 0.7890 - val_loss: 0.4105 - val_acc: 0.7972\n",
      "Epoch 63/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4479 - acc: 0.7884\n",
      "Epoch 63: val_acc did not improve from 0.81336\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4473 - acc: 0.7888 - val_loss: 0.4012 - val_acc: 0.8097\n",
      "Epoch 64/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.7908\n",
      "Epoch 64: val_acc improved from 0.81336 to 0.82470, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4420 - acc: 0.7911 - val_loss: 0.3947 - val_acc: 0.8247\n",
      "Epoch 65/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.7886\n",
      "Epoch 65: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4400 - acc: 0.7891 - val_loss: 0.4101 - val_acc: 0.8146\n",
      "Epoch 66/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.7895\n",
      "Epoch 66: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4490 - acc: 0.7898 - val_loss: 0.4073 - val_acc: 0.7976\n",
      "Epoch 67/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4432 - acc: 0.7915\n",
      "Epoch 67: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4426 - acc: 0.7918 - val_loss: 0.3989 - val_acc: 0.8162\n",
      "Epoch 68/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.7923\n",
      "Epoch 68: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4429 - acc: 0.7933 - val_loss: 0.3893 - val_acc: 0.8243\n",
      "Epoch 69/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.7952\n",
      "Epoch 69: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4370 - acc: 0.7952 - val_loss: 0.4118 - val_acc: 0.8004\n",
      "Epoch 70/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.7861\n",
      "Epoch 70: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4448 - acc: 0.7870 - val_loss: 0.4164 - val_acc: 0.8138\n",
      "Epoch 71/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4406 - acc: 0.7885\n",
      "Epoch 71: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.4406 - acc: 0.7885 - val_loss: 0.4122 - val_acc: 0.7911\n",
      "Epoch 72/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.7883\n",
      "Epoch 72: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4506 - acc: 0.7885 - val_loss: 0.3969 - val_acc: 0.8126\n",
      "Epoch 73/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.7884\n",
      "Epoch 73: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4420 - acc: 0.7886 - val_loss: 0.4139 - val_acc: 0.8016\n",
      "Epoch 74/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.7901\n",
      "Epoch 74: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4432 - acc: 0.7906 - val_loss: 0.3958 - val_acc: 0.8243\n",
      "Epoch 75/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.7955\n",
      "Epoch 75: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4304 - acc: 0.7957 - val_loss: 0.3925 - val_acc: 0.8186\n",
      "Epoch 76/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4395 - acc: 0.7910\n",
      "Epoch 76: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4386 - acc: 0.7915 - val_loss: 0.3918 - val_acc: 0.8170\n",
      "Epoch 77/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.7909\n",
      "Epoch 77: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4431 - acc: 0.7907 - val_loss: 0.4130 - val_acc: 0.7955\n",
      "Epoch 78/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.7942\n",
      "Epoch 78: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4408 - acc: 0.7946 - val_loss: 0.3967 - val_acc: 0.8186\n",
      "Epoch 79/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.7970\n",
      "Epoch 79: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4335 - acc: 0.7972 - val_loss: 0.3985 - val_acc: 0.8085\n",
      "Epoch 80/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.7963\n",
      "Epoch 80: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4310 - acc: 0.7967 - val_loss: 0.4111 - val_acc: 0.7996\n",
      "Epoch 81/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.7869\n",
      "Epoch 81: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4459 - acc: 0.7872 - val_loss: 0.3819 - val_acc: 0.8206\n",
      "Epoch 82/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4378 - acc: 0.7937\n",
      "Epoch 82: val_acc improved from 0.82470 to 0.82713, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4378 - acc: 0.7937 - val_loss: 0.3913 - val_acc: 0.8271\n",
      "Epoch 83/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.7924\n",
      "Epoch 83: val_acc did not improve from 0.82713\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4362 - acc: 0.7929 - val_loss: 0.3936 - val_acc: 0.8130\n",
      "Epoch 84/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.7946\n",
      "Epoch 84: val_acc improved from 0.82713 to 0.83360, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4281 - acc: 0.7950 - val_loss: 0.4123 - val_acc: 0.8336\n",
      "Epoch 85/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.7944\n",
      "Epoch 85: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4414 - acc: 0.7946 - val_loss: 0.4097 - val_acc: 0.8012\n",
      "Epoch 86/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4329 - acc: 0.7958\n",
      "Epoch 86: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4321 - acc: 0.7962 - val_loss: 0.4112 - val_acc: 0.8130\n",
      "Epoch 87/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.7950\n",
      "Epoch 87: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4309 - acc: 0.7950 - val_loss: 0.4141 - val_acc: 0.8089\n",
      "Epoch 88/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8023\n",
      "Epoch 88: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4239 - acc: 0.8027 - val_loss: 0.3841 - val_acc: 0.8223\n",
      "Epoch 89/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.7988\n",
      "Epoch 89: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4318 - acc: 0.7989 - val_loss: 0.4004 - val_acc: 0.8081\n",
      "Epoch 90/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4379 - acc: 0.7925\n",
      "Epoch 90: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4371 - acc: 0.7931 - val_loss: 0.3897 - val_acc: 0.8138\n",
      "Epoch 91/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.7933\n",
      "Epoch 91: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4317 - acc: 0.7938 - val_loss: 0.4007 - val_acc: 0.8182\n",
      "Epoch 92/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.7942\n",
      "Epoch 92: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4353 - acc: 0.7942 - val_loss: 0.3957 - val_acc: 0.8040\n",
      "Epoch 93/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.7954\n",
      "Epoch 93: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4317 - acc: 0.7953 - val_loss: 0.3867 - val_acc: 0.8231\n",
      "Epoch 94/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4298 - acc: 0.7955\n",
      "Epoch 94: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4291 - acc: 0.7958 - val_loss: 0.4078 - val_acc: 0.7911\n",
      "Epoch 95/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4318 - acc: 0.7937\n",
      "Epoch 95: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4309 - acc: 0.7943 - val_loss: 0.3965 - val_acc: 0.8040\n",
      "Epoch 96/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4314 - acc: 0.8007\n",
      "Epoch 96: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4314 - acc: 0.8007 - val_loss: 0.3848 - val_acc: 0.8194\n",
      "Epoch 97/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.7938\n",
      "Epoch 97: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4302 - acc: 0.7941 - val_loss: 0.3965 - val_acc: 0.8077\n",
      "Epoch 98/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4291 - acc: 0.7986\n",
      "Epoch 98: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4284 - acc: 0.7989 - val_loss: 0.3941 - val_acc: 0.8101\n",
      "Epoch 99/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.7927\n",
      "Epoch 99: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4325 - acc: 0.7926 - val_loss: 0.3972 - val_acc: 0.8215\n",
      "Epoch 100/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.7906\n",
      "Epoch 100: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4364 - acc: 0.7911 - val_loss: 0.3896 - val_acc: 0.8243\n",
      "Epoch 101/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.7976\n",
      "Epoch 101: val_acc did not improve from 0.83360\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4238 - acc: 0.7977 - val_loss: 0.3839 - val_acc: 0.8320\n",
      "Epoch 102/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.7926\n",
      "Epoch 102: val_acc improved from 0.83360 to 0.83765, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4275 - acc: 0.7928 - val_loss: 0.3803 - val_acc: 0.8377\n",
      "Epoch 103/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.8003\n",
      "Epoch 103: val_acc did not improve from 0.83765\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4304 - acc: 0.8007 - val_loss: 0.4004 - val_acc: 0.8032\n",
      "Epoch 104/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.8002\n",
      "Epoch 104: val_acc did not improve from 0.83765\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4189 - acc: 0.8006 - val_loss: 0.4004 - val_acc: 0.8146\n",
      "Epoch 105/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4200 - acc: 0.8012\n",
      "Epoch 105: val_acc did not improve from 0.83765\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4200 - acc: 0.8012 - val_loss: 0.3917 - val_acc: 0.8336\n",
      "Epoch 106/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.8033\n",
      "Epoch 106: val_acc did not improve from 0.83765\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4225 - acc: 0.8033 - val_loss: 0.4152 - val_acc: 0.8105\n",
      "Epoch 107/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8014\n",
      "Epoch 107: val_acc improved from 0.83765 to 0.84089, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4221 - acc: 0.8014 - val_loss: 0.4054 - val_acc: 0.8409\n",
      "Epoch 108/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.8002\n",
      "Epoch 108: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4196 - acc: 0.8009 - val_loss: 0.3928 - val_acc: 0.8121\n",
      "Epoch 109/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.7983\n",
      "Epoch 109: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4258 - acc: 0.7983 - val_loss: 0.4154 - val_acc: 0.8069\n",
      "Epoch 110/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.7971\n",
      "Epoch 110: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4275 - acc: 0.7978 - val_loss: 0.3797 - val_acc: 0.8198\n",
      "Epoch 111/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.7985\n",
      "Epoch 111: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4246 - acc: 0.7984 - val_loss: 0.4038 - val_acc: 0.8182\n",
      "Epoch 112/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8044\n",
      "Epoch 112: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4136 - acc: 0.8045 - val_loss: 0.3929 - val_acc: 0.8182\n",
      "Epoch 113/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4143 - acc: 0.7999\n",
      "Epoch 113: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4143 - acc: 0.7999 - val_loss: 0.3852 - val_acc: 0.8138\n",
      "Epoch 114/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.7998\n",
      "Epoch 114: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4184 - acc: 0.8000 - val_loss: 0.3909 - val_acc: 0.8142\n",
      "Epoch 115/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8031\n",
      "Epoch 115: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4154 - acc: 0.8036 - val_loss: 0.3984 - val_acc: 0.8211\n",
      "Epoch 116/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.7996\n",
      "Epoch 116: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4217 - acc: 0.8002 - val_loss: 0.3925 - val_acc: 0.8243\n",
      "Epoch 117/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8024\n",
      "Epoch 117: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4149 - acc: 0.8027 - val_loss: 0.3896 - val_acc: 0.8211\n",
      "Epoch 118/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.7936\n",
      "Epoch 118: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4246 - acc: 0.7943 - val_loss: 0.3945 - val_acc: 0.8085\n",
      "Epoch 119/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.7997\n",
      "Epoch 119: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4214 - acc: 0.7996 - val_loss: 0.3852 - val_acc: 0.8239\n",
      "Epoch 120/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8017\n",
      "Epoch 120: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4137 - acc: 0.8021 - val_loss: 0.4149 - val_acc: 0.8085\n",
      "Epoch 121/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8024\n",
      "Epoch 121: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4211 - acc: 0.8030 - val_loss: 0.3973 - val_acc: 0.8263\n",
      "Epoch 122/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.7988\n",
      "Epoch 122: val_acc did not improve from 0.84089\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4208 - acc: 0.7991 - val_loss: 0.3983 - val_acc: 0.8271\n",
      "Epoch 123/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8023\n",
      "Epoch 123: val_acc improved from 0.84089 to 0.84170, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4144 - acc: 0.8025 - val_loss: 0.3825 - val_acc: 0.8417\n",
      "Epoch 124/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4163 - acc: 0.8049\n",
      "Epoch 124: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4153 - acc: 0.8052 - val_loss: 0.3857 - val_acc: 0.8255\n",
      "Epoch 125/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8000\n",
      "Epoch 125: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4180 - acc: 0.8005 - val_loss: 0.3925 - val_acc: 0.8142\n",
      "Epoch 126/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8060\n",
      "Epoch 126: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4131 - acc: 0.8061 - val_loss: 0.4066 - val_acc: 0.8227\n",
      "Epoch 127/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4213 - acc: 0.7987\n",
      "Epoch 127: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4213 - acc: 0.7987 - val_loss: 0.3708 - val_acc: 0.8417\n",
      "Epoch 128/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4102 - acc: 0.8046\n",
      "Epoch 128: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4102 - acc: 0.8046 - val_loss: 0.3897 - val_acc: 0.8356\n",
      "Epoch 129/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8024\n",
      "Epoch 129: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4111 - acc: 0.8026 - val_loss: 0.3719 - val_acc: 0.8352\n",
      "Epoch 130/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.7996\n",
      "Epoch 130: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4096 - acc: 0.8000 - val_loss: 0.3933 - val_acc: 0.8206\n",
      "Epoch 131/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8008\n",
      "Epoch 131: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4197 - acc: 0.8015 - val_loss: 0.3818 - val_acc: 0.8259\n",
      "Epoch 132/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.7988\n",
      "Epoch 132: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4228 - acc: 0.7990 - val_loss: 0.3863 - val_acc: 0.8142\n",
      "Epoch 133/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8009\n",
      "Epoch 133: val_acc did not improve from 0.84170\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4171 - acc: 0.8011 - val_loss: 0.3789 - val_acc: 0.8352\n",
      "Epoch 134/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8024\n",
      "Epoch 134: val_acc improved from 0.84170 to 0.84372, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4150 - acc: 0.8033 - val_loss: 0.3754 - val_acc: 0.8437\n",
      "Epoch 135/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8051\n",
      "Epoch 135: val_acc did not improve from 0.84372\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4119 - acc: 0.8051 - val_loss: 0.3817 - val_acc: 0.8231\n",
      "Epoch 136/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.7998\n",
      "Epoch 136: val_acc did not improve from 0.84372\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4168 - acc: 0.8003 - val_loss: 0.3716 - val_acc: 0.8239\n",
      "Epoch 137/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8046\n",
      "Epoch 137: val_acc did not improve from 0.84372\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4212 - acc: 0.8046 - val_loss: 0.3850 - val_acc: 0.8166\n",
      "Epoch 138/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8017\n",
      "Epoch 138: val_acc improved from 0.84372 to 0.84534, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4148 - acc: 0.8016 - val_loss: 0.3639 - val_acc: 0.8453\n",
      "Epoch 139/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8044\n",
      "Epoch 139: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4099 - acc: 0.8047 - val_loss: 0.3955 - val_acc: 0.8332\n",
      "Epoch 140/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8038\n",
      "Epoch 140: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4084 - acc: 0.8046 - val_loss: 0.3602 - val_acc: 0.8445\n",
      "Epoch 141/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4061 - acc: 0.8044\n",
      "Epoch 141: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4061 - acc: 0.8044 - val_loss: 0.4000 - val_acc: 0.8057\n",
      "Epoch 142/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8002\n",
      "Epoch 142: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4180 - acc: 0.8003 - val_loss: 0.3735 - val_acc: 0.8385\n",
      "Epoch 143/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8002\n",
      "Epoch 143: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4211 - acc: 0.8003 - val_loss: 0.3956 - val_acc: 0.8287\n",
      "Epoch 144/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8034\n",
      "Epoch 144: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4109 - acc: 0.8042 - val_loss: 0.3773 - val_acc: 0.8312\n",
      "Epoch 145/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8085\n",
      "Epoch 145: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4051 - acc: 0.8093 - val_loss: 0.3825 - val_acc: 0.8368\n",
      "Epoch 146/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4034 - acc: 0.8095\n",
      "Epoch 146: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4034 - acc: 0.8095 - val_loss: 0.3701 - val_acc: 0.8267\n",
      "Epoch 147/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8051\n",
      "Epoch 147: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4082 - acc: 0.8056 - val_loss: 0.3912 - val_acc: 0.8275\n",
      "Epoch 148/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8035\n",
      "Epoch 148: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4132 - acc: 0.8037 - val_loss: 0.3797 - val_acc: 0.8215\n",
      "Epoch 149/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3987 - acc: 0.8127\n",
      "Epoch 149: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3987 - acc: 0.8127 - val_loss: 0.3739 - val_acc: 0.8360\n",
      "Epoch 150/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8028\n",
      "Epoch 150: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4078 - acc: 0.8031 - val_loss: 0.3676 - val_acc: 0.8328\n",
      "Epoch 151/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8032\n",
      "Epoch 151: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4090 - acc: 0.8034 - val_loss: 0.3988 - val_acc: 0.8158\n",
      "Epoch 152/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8075\n",
      "Epoch 152: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4056 - acc: 0.8079 - val_loss: 0.3806 - val_acc: 0.8215\n",
      "Epoch 153/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8057\n",
      "Epoch 153: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4096 - acc: 0.8057 - val_loss: 0.3809 - val_acc: 0.8300\n",
      "Epoch 154/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8106\n",
      "Epoch 154: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4057 - acc: 0.8106 - val_loss: 0.3808 - val_acc: 0.8247\n",
      "Epoch 155/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8090\n",
      "Epoch 155: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4047 - acc: 0.8092 - val_loss: 0.3899 - val_acc: 0.8393\n",
      "Epoch 156/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8074\n",
      "Epoch 156: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4084 - acc: 0.8076 - val_loss: 0.4045 - val_acc: 0.8198\n",
      "Epoch 157/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8059\n",
      "Epoch 157: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4037 - acc: 0.8060 - val_loss: 0.3827 - val_acc: 0.8275\n",
      "Epoch 158/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8108\n",
      "Epoch 158: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3984 - acc: 0.8109 - val_loss: 0.3674 - val_acc: 0.8279\n",
      "Epoch 159/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8068\n",
      "Epoch 159: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4006 - acc: 0.8069 - val_loss: 0.3764 - val_acc: 0.8328\n",
      "Epoch 160/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8069\n",
      "Epoch 160: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4085 - acc: 0.8068 - val_loss: 0.3913 - val_acc: 0.8101\n",
      "Epoch 161/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8124\n",
      "Epoch 161: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4027 - acc: 0.8126 - val_loss: 0.3819 - val_acc: 0.8409\n",
      "Epoch 162/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8074\n",
      "Epoch 162: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4022 - acc: 0.8079 - val_loss: 0.3795 - val_acc: 0.8397\n",
      "Epoch 163/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8083\n",
      "Epoch 163: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4053 - acc: 0.8087 - val_loss: 0.3740 - val_acc: 0.8413\n",
      "Epoch 164/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8076\n",
      "Epoch 164: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4038 - acc: 0.8077 - val_loss: 0.3598 - val_acc: 0.8405\n",
      "Epoch 165/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8092\n",
      "Epoch 165: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4028 - acc: 0.8097 - val_loss: 0.3668 - val_acc: 0.8368\n",
      "Epoch 166/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8068\n",
      "Epoch 166: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4008 - acc: 0.8069 - val_loss: 0.3697 - val_acc: 0.8287\n",
      "Epoch 167/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8091\n",
      "Epoch 167: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4018 - acc: 0.8096 - val_loss: 0.3815 - val_acc: 0.8360\n",
      "Epoch 168/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8110\n",
      "Epoch 168: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4068 - acc: 0.8110 - val_loss: 0.3779 - val_acc: 0.8239\n",
      "Epoch 169/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4017 - acc: 0.8066\n",
      "Epoch 169: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4017 - acc: 0.8066 - val_loss: 0.3849 - val_acc: 0.8231\n",
      "Epoch 170/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8080\n",
      "Epoch 170: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4122 - acc: 0.8080 - val_loss: 0.3707 - val_acc: 0.8405\n",
      "Epoch 171/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8100\n",
      "Epoch 171: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3992 - acc: 0.8103 - val_loss: 0.3746 - val_acc: 0.8308\n",
      "Epoch 172/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.8114\n",
      "Epoch 172: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4009 - acc: 0.8116 - val_loss: 0.3778 - val_acc: 0.8328\n",
      "Epoch 173/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4037 - acc: 0.8072\n",
      "Epoch 173: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4037 - acc: 0.8072 - val_loss: 0.4004 - val_acc: 0.8158\n",
      "Epoch 174/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8120\n",
      "Epoch 174: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4023 - acc: 0.8125 - val_loss: 0.3636 - val_acc: 0.8437\n",
      "Epoch 175/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4043 - acc: 0.8063\n",
      "Epoch 175: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4043 - acc: 0.8063 - val_loss: 0.3620 - val_acc: 0.8425\n",
      "Epoch 176/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8117\n",
      "Epoch 176: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3954 - acc: 0.8122 - val_loss: 0.3985 - val_acc: 0.8409\n",
      "Epoch 177/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8125\n",
      "Epoch 177: val_acc did not improve from 0.84534\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3954 - acc: 0.8129 - val_loss: 0.3794 - val_acc: 0.8267\n",
      "Epoch 178/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8075\n",
      "Epoch 178: val_acc improved from 0.84534 to 0.84615, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4031 - acc: 0.8084 - val_loss: 0.3935 - val_acc: 0.8462\n",
      "Epoch 179/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8082\n",
      "Epoch 179: val_acc improved from 0.84615 to 0.85425, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4021 - acc: 0.8083 - val_loss: 0.3597 - val_acc: 0.8543\n",
      "Epoch 180/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.8081\n",
      "Epoch 180: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3977 - acc: 0.8082 - val_loss: 0.3776 - val_acc: 0.8287\n",
      "Epoch 181/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8073\n",
      "Epoch 181: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4043 - acc: 0.8077 - val_loss: 0.3722 - val_acc: 0.8401\n",
      "Epoch 182/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8096\n",
      "Epoch 182: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4051 - acc: 0.8098 - val_loss: 0.3546 - val_acc: 0.8437\n",
      "Epoch 183/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4021 - acc: 0.8092\n",
      "Epoch 183: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4021 - acc: 0.8092 - val_loss: 0.3505 - val_acc: 0.8486\n",
      "Epoch 184/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8094\n",
      "Epoch 184: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4107 - acc: 0.8098 - val_loss: 0.3582 - val_acc: 0.8385\n",
      "Epoch 185/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8152\n",
      "Epoch 185: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3901 - acc: 0.8159 - val_loss: 0.3578 - val_acc: 0.8474\n",
      "Epoch 186/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8093\n",
      "Epoch 186: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4040 - acc: 0.8094 - val_loss: 0.3688 - val_acc: 0.8267\n",
      "Epoch 187/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8071\n",
      "Epoch 187: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4015 - acc: 0.8075 - val_loss: 0.3729 - val_acc: 0.8441\n",
      "Epoch 188/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8056\n",
      "Epoch 188: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4003 - acc: 0.8060 - val_loss: 0.3727 - val_acc: 0.8385\n",
      "Epoch 189/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3914 - acc: 0.8087\n",
      "Epoch 189: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3914 - acc: 0.8087 - val_loss: 0.3734 - val_acc: 0.8413\n",
      "Epoch 190/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8138\n",
      "Epoch 190: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.3888 - acc: 0.8142 - val_loss: 0.3545 - val_acc: 0.8389\n",
      "Epoch 191/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3963 - acc: 0.8064\n",
      "Epoch 191: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3963 - acc: 0.8064 - val_loss: 0.3704 - val_acc: 0.8381\n",
      "Epoch 192/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8141\n",
      "Epoch 192: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3910 - acc: 0.8141 - val_loss: 0.3701 - val_acc: 0.8360\n",
      "Epoch 193/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8084\n",
      "Epoch 193: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3973 - acc: 0.8090 - val_loss: 0.3578 - val_acc: 0.8393\n",
      "Epoch 194/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8157\n",
      "Epoch 194: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3948 - acc: 0.8158 - val_loss: 0.3631 - val_acc: 0.8445\n",
      "Epoch 195/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8112\n",
      "Epoch 195: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3941 - acc: 0.8118 - val_loss: 0.3639 - val_acc: 0.8466\n",
      "Epoch 196/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8084\n",
      "Epoch 196: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3963 - acc: 0.8083 - val_loss: 0.3755 - val_acc: 0.8279\n",
      "Epoch 197/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.8103\n",
      "Epoch 197: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3982 - acc: 0.8103 - val_loss: 0.3565 - val_acc: 0.8498\n",
      "Epoch 198/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3994 - acc: 0.8094\n",
      "Epoch 198: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3994 - acc: 0.8094 - val_loss: 0.3749 - val_acc: 0.8166\n",
      "Epoch 199/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8117\n",
      "Epoch 199: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3960 - acc: 0.8122 - val_loss: 0.3526 - val_acc: 0.8377\n",
      "Epoch 200/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8070\n",
      "Epoch 200: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4029 - acc: 0.8071 - val_loss: 0.3418 - val_acc: 0.8506\n",
      "Epoch 201/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8163\n",
      "Epoch 201: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3924 - acc: 0.8167 - val_loss: 0.3664 - val_acc: 0.8154\n",
      "Epoch 202/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8083\n",
      "Epoch 202: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4013 - acc: 0.8082 - val_loss: 0.3735 - val_acc: 0.8413\n",
      "Epoch 203/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8182\n",
      "Epoch 203: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3942 - acc: 0.8184 - val_loss: 0.3636 - val_acc: 0.8393\n",
      "Epoch 204/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8125\n",
      "Epoch 204: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3945 - acc: 0.8127 - val_loss: 0.3561 - val_acc: 0.8243\n",
      "Epoch 205/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8185\n",
      "Epoch 205: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3909 - acc: 0.8187 - val_loss: 0.3748 - val_acc: 0.8364\n",
      "Epoch 206/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8141\n",
      "Epoch 206: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3937 - acc: 0.8141 - val_loss: 0.3702 - val_acc: 0.8401\n",
      "Epoch 207/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3922 - acc: 0.8157\n",
      "Epoch 207: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3922 - acc: 0.8157 - val_loss: 0.3503 - val_acc: 0.8462\n",
      "Epoch 208/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8186\n",
      "Epoch 208: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3897 - acc: 0.8188 - val_loss: 0.3608 - val_acc: 0.8401\n",
      "Epoch 209/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8154\n",
      "Epoch 209: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3950 - acc: 0.8158 - val_loss: 0.3469 - val_acc: 0.8377\n",
      "Epoch 210/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8162\n",
      "Epoch 210: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3954 - acc: 0.8160 - val_loss: 0.3797 - val_acc: 0.8425\n",
      "Epoch 211/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8126\n",
      "Epoch 211: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3931 - acc: 0.8127 - val_loss: 0.3666 - val_acc: 0.8255\n",
      "Epoch 212/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8096\n",
      "Epoch 212: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3973 - acc: 0.8100 - val_loss: 0.3731 - val_acc: 0.8312\n",
      "Epoch 213/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3953 - acc: 0.8091\n",
      "Epoch 213: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3953 - acc: 0.8091 - val_loss: 0.3771 - val_acc: 0.8397\n",
      "Epoch 214/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8147\n",
      "Epoch 214: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3918 - acc: 0.8155 - val_loss: 0.3669 - val_acc: 0.8360\n",
      "Epoch 215/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8147\n",
      "Epoch 215: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3865 - acc: 0.8146 - val_loss: 0.3721 - val_acc: 0.8275\n",
      "Epoch 216/2000\n",
      "607/618 [============================>.] - ETA: 0s - loss: 0.3897 - acc: 0.8219\n",
      "Epoch 216: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3887 - acc: 0.8225 - val_loss: 0.3662 - val_acc: 0.8526\n",
      "Epoch 217/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8115\n",
      "Epoch 217: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3910 - acc: 0.8121 - val_loss: 0.3775 - val_acc: 0.8352\n",
      "Epoch 218/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8125\n",
      "Epoch 218: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3863 - acc: 0.8127 - val_loss: 0.3887 - val_acc: 0.8308\n",
      "Epoch 219/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8142\n",
      "Epoch 219: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3796 - acc: 0.8147 - val_loss: 0.3605 - val_acc: 0.8506\n",
      "Epoch 220/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8144\n",
      "Epoch 220: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3896 - acc: 0.8147 - val_loss: 0.3585 - val_acc: 0.8372\n",
      "Epoch 221/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8144\n",
      "Epoch 221: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3920 - acc: 0.8149 - val_loss: 0.3716 - val_acc: 0.8466\n",
      "Epoch 222/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8155\n",
      "Epoch 222: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3895 - acc: 0.8153 - val_loss: 0.3653 - val_acc: 0.8211\n",
      "Epoch 223/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3875 - acc: 0.8119\n",
      "Epoch 223: val_acc improved from 0.85425 to 0.85992, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3875 - acc: 0.8119 - val_loss: 0.3605 - val_acc: 0.8599\n",
      "Epoch 224/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8155\n",
      "Epoch 224: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3884 - acc: 0.8156 - val_loss: 0.3717 - val_acc: 0.8559\n",
      "Epoch 225/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8115\n",
      "Epoch 225: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3941 - acc: 0.8122 - val_loss: 0.3723 - val_acc: 0.8453\n",
      "Epoch 226/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8110\n",
      "Epoch 226: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3908 - acc: 0.8111 - val_loss: 0.3771 - val_acc: 0.8243\n",
      "Epoch 227/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3885 - acc: 0.8181\n",
      "Epoch 227: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3885 - acc: 0.8181 - val_loss: 0.3694 - val_acc: 0.8397\n",
      "Epoch 228/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8055\n",
      "Epoch 228: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4024 - acc: 0.8057 - val_loss: 0.3730 - val_acc: 0.8364\n",
      "Epoch 229/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8127\n",
      "Epoch 229: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3938 - acc: 0.8132 - val_loss: 0.3792 - val_acc: 0.8332\n",
      "Epoch 230/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8121\n",
      "Epoch 230: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3949 - acc: 0.8121 - val_loss: 0.3624 - val_acc: 0.8271\n",
      "Epoch 231/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8113\n",
      "Epoch 231: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3944 - acc: 0.8116 - val_loss: 0.3891 - val_acc: 0.8283\n",
      "Epoch 232/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8100\n",
      "Epoch 232: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3910 - acc: 0.8104 - val_loss: 0.3715 - val_acc: 0.8344\n",
      "Epoch 233/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8070\n",
      "Epoch 233: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3954 - acc: 0.8074 - val_loss: 0.3779 - val_acc: 0.8449\n",
      "Epoch 234/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8147\n",
      "Epoch 234: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3943 - acc: 0.8146 - val_loss: 0.3806 - val_acc: 0.8186\n",
      "Epoch 235/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8165\n",
      "Epoch 235: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3857 - acc: 0.8173 - val_loss: 0.3481 - val_acc: 0.8457\n",
      "Epoch 236/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3815 - acc: 0.8191\n",
      "Epoch 236: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3815 - acc: 0.8191 - val_loss: 0.3590 - val_acc: 0.8243\n",
      "Epoch 237/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8093\n",
      "Epoch 237: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3925 - acc: 0.8095 - val_loss: 0.3576 - val_acc: 0.8409\n",
      "Epoch 238/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3932 - acc: 0.8116\n",
      "Epoch 238: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3932 - acc: 0.8116 - val_loss: 0.4317 - val_acc: 0.8178\n",
      "Epoch 239/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8141\n",
      "Epoch 239: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3852 - acc: 0.8141 - val_loss: 0.3816 - val_acc: 0.8117\n",
      "Epoch 240/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8133\n",
      "Epoch 240: val_acc did not improve from 0.85992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3932 - acc: 0.8136 - val_loss: 0.3635 - val_acc: 0.8530\n",
      "Epoch 241/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8173\n",
      "Epoch 241: val_acc improved from 0.85992 to 0.87126, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/1/best_model.h5\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3867 - acc: 0.8178 - val_loss: 0.3516 - val_acc: 0.8713\n",
      "Epoch 242/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8108\n",
      "Epoch 242: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3943 - acc: 0.8108 - val_loss: 0.3730 - val_acc: 0.8429\n",
      "Epoch 243/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8078\n",
      "Epoch 243: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3938 - acc: 0.8078 - val_loss: 0.3659 - val_acc: 0.8397\n",
      "Epoch 244/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8131\n",
      "Epoch 244: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3884 - acc: 0.8134 - val_loss: 0.3888 - val_acc: 0.8178\n",
      "Epoch 245/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8120\n",
      "Epoch 245: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3881 - acc: 0.8126 - val_loss: 0.3681 - val_acc: 0.8543\n",
      "Epoch 246/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8051\n",
      "Epoch 246: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4055 - acc: 0.8052 - val_loss: 0.3541 - val_acc: 0.8462\n",
      "Epoch 247/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8102\n",
      "Epoch 247: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3964 - acc: 0.8102 - val_loss: 0.3727 - val_acc: 0.8271\n",
      "Epoch 248/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8094\n",
      "Epoch 248: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3935 - acc: 0.8100 - val_loss: 0.3593 - val_acc: 0.8372\n",
      "Epoch 249/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3920 - acc: 0.8196\n",
      "Epoch 249: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3917 - acc: 0.8195 - val_loss: 0.3712 - val_acc: 0.8279\n",
      "Epoch 250/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8172\n",
      "Epoch 250: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3875 - acc: 0.8174 - val_loss: 0.3532 - val_acc: 0.8457\n",
      "Epoch 251/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8200\n",
      "Epoch 251: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3818 - acc: 0.8204 - val_loss: 0.3606 - val_acc: 0.8393\n",
      "Epoch 252/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8200\n",
      "Epoch 252: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3875 - acc: 0.8203 - val_loss: 0.3491 - val_acc: 0.8696\n",
      "Epoch 253/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8121\n",
      "Epoch 253: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3853 - acc: 0.8128 - val_loss: 0.3558 - val_acc: 0.8494\n",
      "Epoch 254/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3824 - acc: 0.8172\n",
      "Epoch 254: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3824 - acc: 0.8172 - val_loss: 0.3647 - val_acc: 0.8275\n",
      "Epoch 255/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8204\n",
      "Epoch 255: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3815 - acc: 0.8206 - val_loss: 0.3752 - val_acc: 0.8453\n",
      "Epoch 256/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8148\n",
      "Epoch 256: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3824 - acc: 0.8152 - val_loss: 0.3682 - val_acc: 0.8668\n",
      "Epoch 257/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8195\n",
      "Epoch 257: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3916 - acc: 0.8203 - val_loss: 0.3670 - val_acc: 0.8437\n",
      "Epoch 258/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3832 - acc: 0.8193\n",
      "Epoch 258: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3832 - acc: 0.8193 - val_loss: 0.3898 - val_acc: 0.8348\n",
      "Epoch 259/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8123\n",
      "Epoch 259: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3868 - acc: 0.8131 - val_loss: 0.3722 - val_acc: 0.8692\n",
      "Epoch 260/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8126\n",
      "Epoch 260: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3881 - acc: 0.8131 - val_loss: 0.3537 - val_acc: 0.8482\n",
      "Epoch 261/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8182\n",
      "Epoch 261: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3799 - acc: 0.8185 - val_loss: 0.3844 - val_acc: 0.8247\n",
      "Epoch 262/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8115\n",
      "Epoch 262: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3871 - acc: 0.8114 - val_loss: 0.3754 - val_acc: 0.8413\n",
      "Epoch 263/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8161\n",
      "Epoch 263: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3840 - acc: 0.8156 - val_loss: 0.3625 - val_acc: 0.8421\n",
      "Epoch 264/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8188\n",
      "Epoch 264: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3794 - acc: 0.8189 - val_loss: 0.3694 - val_acc: 0.8393\n",
      "Epoch 265/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8184\n",
      "Epoch 265: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3796 - acc: 0.8190 - val_loss: 0.3856 - val_acc: 0.8441\n",
      "Epoch 266/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8166\n",
      "Epoch 266: val_acc did not improve from 0.87126\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3842 - acc: 0.8167 - val_loss: 0.3658 - val_acc: 0.8348\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 1.5449 - acc: 0.6285\n",
      "Epoch 1: val_acc improved from -inf to 0.69069, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1.5396 - acc: 0.6292 - val_loss: 0.6050 - val_acc: 0.6907\n",
      "Epoch 2/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.6309 - acc: 0.6810\n",
      "Epoch 2: val_acc improved from 0.69069 to 0.71660, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.6295 - acc: 0.6821 - val_loss: 0.5781 - val_acc: 0.7166\n",
      "Epoch 3/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.7043\n",
      "Epoch 3: val_acc did not improve from 0.71660\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.6024 - acc: 0.7048 - val_loss: 0.5722 - val_acc: 0.7146\n",
      "Epoch 4/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5927 - acc: 0.7097\n",
      "Epoch 4: val_acc improved from 0.71660 to 0.71943, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5924 - acc: 0.7099 - val_loss: 0.5664 - val_acc: 0.7194\n",
      "Epoch 5/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5814 - acc: 0.7176\n",
      "Epoch 5: val_acc did not improve from 0.71943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5813 - acc: 0.7176 - val_loss: 0.5654 - val_acc: 0.7174\n",
      "Epoch 6/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.7248\n",
      "Epoch 6: val_acc improved from 0.71943 to 0.72551, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5686 - acc: 0.7248 - val_loss: 0.5497 - val_acc: 0.7255\n",
      "Epoch 7/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.7227\n",
      "Epoch 7: val_acc improved from 0.72551 to 0.73036, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5719 - acc: 0.7229 - val_loss: 0.5392 - val_acc: 0.7304\n",
      "Epoch 8/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.7293\n",
      "Epoch 8: val_acc improved from 0.73036 to 0.73360, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5589 - acc: 0.7301 - val_loss: 0.5337 - val_acc: 0.7336\n",
      "Epoch 9/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.7268\n",
      "Epoch 9: val_acc improved from 0.73360 to 0.74089, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5589 - acc: 0.7273 - val_loss: 0.5306 - val_acc: 0.7409\n",
      "Epoch 10/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.7315\n",
      "Epoch 10: val_acc did not improve from 0.74089\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5506 - acc: 0.7317 - val_loss: 0.5260 - val_acc: 0.7389\n",
      "Epoch 11/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7371\n",
      "Epoch 11: val_acc improved from 0.74089 to 0.74211, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5451 - acc: 0.7380 - val_loss: 0.5267 - val_acc: 0.7421\n",
      "Epoch 12/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.7431\n",
      "Epoch 12: val_acc improved from 0.74211 to 0.74656, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5392 - acc: 0.7433 - val_loss: 0.5113 - val_acc: 0.7466\n",
      "Epoch 13/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7439\n",
      "Epoch 13: val_acc did not improve from 0.74656\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5348 - acc: 0.7446 - val_loss: 0.5266 - val_acc: 0.7364\n",
      "Epoch 14/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5296 - acc: 0.7464\n",
      "Epoch 14: val_acc did not improve from 0.74656\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5289 - acc: 0.7468 - val_loss: 0.5291 - val_acc: 0.7356\n",
      "Epoch 15/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.7498\n",
      "Epoch 15: val_acc did not improve from 0.74656\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5247 - acc: 0.7501 - val_loss: 0.5129 - val_acc: 0.7413\n",
      "Epoch 16/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7519\n",
      "Epoch 16: val_acc did not improve from 0.74656\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.5194 - acc: 0.7528 - val_loss: 0.5057 - val_acc: 0.7449\n",
      "Epoch 17/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.7512\n",
      "Epoch 17: val_acc improved from 0.74656 to 0.75425, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5138 - acc: 0.7512 - val_loss: 0.4956 - val_acc: 0.7543\n",
      "Epoch 18/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.7523\n",
      "Epoch 18: val_acc improved from 0.75425 to 0.75992, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5142 - acc: 0.7528 - val_loss: 0.5008 - val_acc: 0.7599\n",
      "Epoch 19/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.7558\n",
      "Epoch 19: val_acc did not improve from 0.75992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.5097 - acc: 0.7561 - val_loss: 0.4880 - val_acc: 0.7559\n",
      "Epoch 20/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7561\n",
      "Epoch 20: val_acc did not improve from 0.75992\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5095 - acc: 0.7570 - val_loss: 0.4979 - val_acc: 0.7498\n",
      "Epoch 21/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.7558\n",
      "Epoch 21: val_acc did not improve from 0.75992\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.5062 - acc: 0.7561 - val_loss: 0.5031 - val_acc: 0.7457\n",
      "Epoch 22/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.7635\n",
      "Epoch 22: val_acc improved from 0.75992 to 0.76073, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5003 - acc: 0.7641 - val_loss: 0.4837 - val_acc: 0.7607\n",
      "Epoch 23/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5032 - acc: 0.7625\n",
      "Epoch 23: val_acc did not improve from 0.76073\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5032 - acc: 0.7625 - val_loss: 0.4890 - val_acc: 0.7579\n",
      "Epoch 24/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.7565\n",
      "Epoch 24: val_acc did not improve from 0.76073\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.5083 - acc: 0.7569 - val_loss: 0.4840 - val_acc: 0.7563\n",
      "Epoch 25/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.7595\n",
      "Epoch 25: val_acc did not improve from 0.76073\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.5016 - acc: 0.7595 - val_loss: 0.4942 - val_acc: 0.7490\n",
      "Epoch 26/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.7627\n",
      "Epoch 26: val_acc improved from 0.76073 to 0.77368, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4939 - acc: 0.7637 - val_loss: 0.4535 - val_acc: 0.7737\n",
      "Epoch 27/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4971 - acc: 0.7662\n",
      "Epoch 27: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4960 - acc: 0.7671 - val_loss: 0.4912 - val_acc: 0.7575\n",
      "Epoch 28/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4901 - acc: 0.7673\n",
      "Epoch 28: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4891 - acc: 0.7680 - val_loss: 0.5108 - val_acc: 0.7587\n",
      "Epoch 29/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4892 - acc: 0.7672\n",
      "Epoch 29: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4885 - acc: 0.7680 - val_loss: 0.4756 - val_acc: 0.7510\n",
      "Epoch 30/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.7718\n",
      "Epoch 30: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4829 - acc: 0.7722 - val_loss: 0.4709 - val_acc: 0.7615\n",
      "Epoch 31/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4831 - acc: 0.7689\n",
      "Epoch 31: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4824 - acc: 0.7691 - val_loss: 0.4975 - val_acc: 0.7607\n",
      "Epoch 32/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4840 - acc: 0.7697\n",
      "Epoch 32: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4841 - acc: 0.7697 - val_loss: 0.4608 - val_acc: 0.7636\n",
      "Epoch 33/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4797 - acc: 0.7713\n",
      "Epoch 33: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4797 - acc: 0.7713 - val_loss: 0.4532 - val_acc: 0.7680\n",
      "Epoch 34/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.7710\n",
      "Epoch 34: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4772 - acc: 0.7714 - val_loss: 0.4656 - val_acc: 0.7628\n",
      "Epoch 35/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.7763\n",
      "Epoch 35: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4707 - acc: 0.7772 - val_loss: 0.4752 - val_acc: 0.7644\n",
      "Epoch 36/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4781 - acc: 0.7709\n",
      "Epoch 36: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4775 - acc: 0.7711 - val_loss: 0.4638 - val_acc: 0.7636\n",
      "Epoch 37/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.7736\n",
      "Epoch 37: val_acc did not improve from 0.77368\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4751 - acc: 0.7739 - val_loss: 0.4608 - val_acc: 0.7709\n",
      "Epoch 38/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.7795\n",
      "Epoch 38: val_acc improved from 0.77368 to 0.77814, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4657 - acc: 0.7798 - val_loss: 0.4486 - val_acc: 0.7781\n",
      "Epoch 39/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4675 - acc: 0.7742\n",
      "Epoch 39: val_acc did not improve from 0.77814\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4666 - acc: 0.7754 - val_loss: 0.4497 - val_acc: 0.7692\n",
      "Epoch 40/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.7769\n",
      "Epoch 40: val_acc did not improve from 0.77814\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4645 - acc: 0.7777 - val_loss: 0.4579 - val_acc: 0.7640\n",
      "Epoch 41/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4640 - acc: 0.7790\n",
      "Epoch 41: val_acc did not improve from 0.77814\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4639 - acc: 0.7790 - val_loss: 0.4652 - val_acc: 0.7664\n",
      "Epoch 42/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4637 - acc: 0.7783\n",
      "Epoch 42: val_acc did not improve from 0.77814\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4634 - acc: 0.7785 - val_loss: 0.4660 - val_acc: 0.7575\n",
      "Epoch 43/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.7838\n",
      "Epoch 43: val_acc did not improve from 0.77814\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4580 - acc: 0.7841 - val_loss: 0.4455 - val_acc: 0.7761\n",
      "Epoch 44/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.7834\n",
      "Epoch 44: val_acc did not improve from 0.77814\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4548 - acc: 0.7836 - val_loss: 0.4574 - val_acc: 0.7700\n",
      "Epoch 45/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.7795\n",
      "Epoch 45: val_acc improved from 0.77814 to 0.78988, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4585 - acc: 0.7806 - val_loss: 0.4483 - val_acc: 0.7899\n",
      "Epoch 46/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4551 - acc: 0.7815\n",
      "Epoch 46: val_acc did not improve from 0.78988\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4548 - acc: 0.7820 - val_loss: 0.4359 - val_acc: 0.7838\n",
      "Epoch 47/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.7814\n",
      "Epoch 47: val_acc did not improve from 0.78988\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4579 - acc: 0.7816 - val_loss: 0.4507 - val_acc: 0.7725\n",
      "Epoch 48/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.7854\n",
      "Epoch 48: val_acc did not improve from 0.78988\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4510 - acc: 0.7856 - val_loss: 0.4552 - val_acc: 0.7688\n",
      "Epoch 49/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.7871\n",
      "Epoch 49: val_acc did not improve from 0.78988\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4501 - acc: 0.7875 - val_loss: 0.4317 - val_acc: 0.7798\n",
      "Epoch 50/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.7881\n",
      "Epoch 50: val_acc did not improve from 0.78988\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4430 - acc: 0.7881 - val_loss: 0.4291 - val_acc: 0.7834\n",
      "Epoch 51/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.7840\n",
      "Epoch 51: val_acc did not improve from 0.78988\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4523 - acc: 0.7850 - val_loss: 0.4303 - val_acc: 0.7870\n",
      "Epoch 52/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7878\n",
      "Epoch 52: val_acc did not improve from 0.78988\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4438 - acc: 0.7884 - val_loss: 0.4502 - val_acc: 0.7696\n",
      "Epoch 53/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.7899\n",
      "Epoch 53: val_acc improved from 0.78988 to 0.79433, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4433 - acc: 0.7902 - val_loss: 0.4219 - val_acc: 0.7943\n",
      "Epoch 54/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7880\n",
      "Epoch 54: val_acc did not improve from 0.79433\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4442 - acc: 0.7884 - val_loss: 0.4374 - val_acc: 0.7785\n",
      "Epoch 55/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.7844\n",
      "Epoch 55: val_acc did not improve from 0.79433\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4457 - acc: 0.7847 - val_loss: 0.4342 - val_acc: 0.7927\n",
      "Epoch 56/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.7864\n",
      "Epoch 56: val_acc did not improve from 0.79433\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4436 - acc: 0.7868 - val_loss: 0.4369 - val_acc: 0.7757\n",
      "Epoch 57/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.7911\n",
      "Epoch 57: val_acc improved from 0.79433 to 0.79555, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4453 - acc: 0.7915 - val_loss: 0.4212 - val_acc: 0.7955\n",
      "Epoch 58/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.7893\n",
      "Epoch 58: val_acc did not improve from 0.79555\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4420 - acc: 0.7896 - val_loss: 0.4220 - val_acc: 0.7862\n",
      "Epoch 59/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.7910\n",
      "Epoch 59: val_acc improved from 0.79555 to 0.79595, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4396 - acc: 0.7914 - val_loss: 0.4149 - val_acc: 0.7960\n",
      "Epoch 60/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.7904\n",
      "Epoch 60: val_acc did not improve from 0.79595\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4418 - acc: 0.7908 - val_loss: 0.4162 - val_acc: 0.7927\n",
      "Epoch 61/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.7889\n",
      "Epoch 61: val_acc did not improve from 0.79595\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4437 - acc: 0.7892 - val_loss: 0.4203 - val_acc: 0.7846\n",
      "Epoch 62/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.7949\n",
      "Epoch 62: val_acc improved from 0.79595 to 0.80243, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4311 - acc: 0.7951 - val_loss: 0.4215 - val_acc: 0.8024\n",
      "Epoch 63/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.7925\n",
      "Epoch 63: val_acc improved from 0.80243 to 0.80648, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4358 - acc: 0.7931 - val_loss: 0.4196 - val_acc: 0.8065\n",
      "Epoch 64/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.7909\n",
      "Epoch 64: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4394 - acc: 0.7920 - val_loss: 0.4304 - val_acc: 0.7806\n",
      "Epoch 65/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.7908\n",
      "Epoch 65: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4378 - acc: 0.7912 - val_loss: 0.4458 - val_acc: 0.7700\n",
      "Epoch 66/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4378 - acc: 0.7947\n",
      "Epoch 66: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4378 - acc: 0.7947 - val_loss: 0.4332 - val_acc: 0.7870\n",
      "Epoch 67/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8005\n",
      "Epoch 67: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4224 - acc: 0.8005 - val_loss: 0.4395 - val_acc: 0.7826\n",
      "Epoch 68/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.7917\n",
      "Epoch 68: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4396 - acc: 0.7921 - val_loss: 0.4603 - val_acc: 0.7745\n",
      "Epoch 69/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4394 - acc: 0.7948\n",
      "Epoch 69: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4390 - acc: 0.7952 - val_loss: 0.4243 - val_acc: 0.7777\n",
      "Epoch 70/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8008\n",
      "Epoch 70: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4268 - acc: 0.8015 - val_loss: 0.4185 - val_acc: 0.7870\n",
      "Epoch 71/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.7986\n",
      "Epoch 71: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4278 - acc: 0.7989 - val_loss: 0.4151 - val_acc: 0.7980\n",
      "Epoch 72/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.7890\n",
      "Epoch 72: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4350 - acc: 0.7894 - val_loss: 0.4324 - val_acc: 0.7713\n",
      "Epoch 73/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.7961\n",
      "Epoch 73: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4287 - acc: 0.7961 - val_loss: 0.4160 - val_acc: 0.7968\n",
      "Epoch 74/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.7979\n",
      "Epoch 74: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4284 - acc: 0.7982 - val_loss: 0.4053 - val_acc: 0.7972\n",
      "Epoch 75/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4266 - acc: 0.8041\n",
      "Epoch 75: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4266 - acc: 0.8041 - val_loss: 0.4413 - val_acc: 0.7644\n",
      "Epoch 76/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8041\n",
      "Epoch 76: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4278 - acc: 0.8042 - val_loss: 0.4337 - val_acc: 0.7810\n",
      "Epoch 77/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.8017\n",
      "Epoch 77: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4299 - acc: 0.8018 - val_loss: 0.4300 - val_acc: 0.7822\n",
      "Epoch 78/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.8009\n",
      "Epoch 78: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4259 - acc: 0.8013 - val_loss: 0.4057 - val_acc: 0.7980\n",
      "Epoch 79/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4196 - acc: 0.8017\n",
      "Epoch 79: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4196 - acc: 0.8017 - val_loss: 0.4302 - val_acc: 0.7623\n",
      "Epoch 80/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4323 - acc: 0.7996\n",
      "Epoch 80: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4323 - acc: 0.7996 - val_loss: 0.4232 - val_acc: 0.7830\n",
      "Epoch 81/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.7981\n",
      "Epoch 81: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4303 - acc: 0.7983 - val_loss: 0.4145 - val_acc: 0.7842\n",
      "Epoch 82/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.7983\n",
      "Epoch 82: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4191 - acc: 0.7992 - val_loss: 0.4056 - val_acc: 0.8053\n",
      "Epoch 83/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8079\n",
      "Epoch 83: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4144 - acc: 0.8078 - val_loss: 0.4152 - val_acc: 0.7935\n",
      "Epoch 84/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8033\n",
      "Epoch 84: val_acc did not improve from 0.80648\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4193 - acc: 0.8036 - val_loss: 0.4214 - val_acc: 0.7879\n",
      "Epoch 85/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8040\n",
      "Epoch 85: val_acc improved from 0.80648 to 0.81943, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4197 - acc: 0.8043 - val_loss: 0.4117 - val_acc: 0.8194\n",
      "Epoch 86/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8069\n",
      "Epoch 86: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4129 - acc: 0.8071 - val_loss: 0.4233 - val_acc: 0.7903\n",
      "Epoch 87/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8006\n",
      "Epoch 87: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4210 - acc: 0.8014 - val_loss: 0.4129 - val_acc: 0.8016\n",
      "Epoch 88/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4191 - acc: 0.8042\n",
      "Epoch 88: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4181 - acc: 0.8051 - val_loss: 0.3983 - val_acc: 0.8081\n",
      "Epoch 89/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.7941\n",
      "Epoch 89: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4335 - acc: 0.7945 - val_loss: 0.3998 - val_acc: 0.7972\n",
      "Epoch 90/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8076\n",
      "Epoch 90: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4054 - acc: 0.8082 - val_loss: 0.4253 - val_acc: 0.8113\n",
      "Epoch 91/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4208 - acc: 0.7993\n",
      "Epoch 91: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4208 - acc: 0.7993 - val_loss: 0.4047 - val_acc: 0.7879\n",
      "Epoch 92/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8032\n",
      "Epoch 92: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4160 - acc: 0.8035 - val_loss: 0.3897 - val_acc: 0.8158\n",
      "Epoch 93/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8092\n",
      "Epoch 93: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4105 - acc: 0.8100 - val_loss: 0.3896 - val_acc: 0.8097\n",
      "Epoch 94/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8100\n",
      "Epoch 94: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4067 - acc: 0.8105 - val_loss: 0.3999 - val_acc: 0.8020\n",
      "Epoch 95/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.8049\n",
      "Epoch 95: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4075 - acc: 0.8055 - val_loss: 0.3979 - val_acc: 0.8057\n",
      "Epoch 96/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8097\n",
      "Epoch 96: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4071 - acc: 0.8100 - val_loss: 0.4146 - val_acc: 0.7874\n",
      "Epoch 97/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8059\n",
      "Epoch 97: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4047 - acc: 0.8062 - val_loss: 0.4050 - val_acc: 0.8020\n",
      "Epoch 98/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8125\n",
      "Epoch 98: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4039 - acc: 0.8128 - val_loss: 0.3995 - val_acc: 0.7972\n",
      "Epoch 99/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8094\n",
      "Epoch 99: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4093 - acc: 0.8099 - val_loss: 0.3958 - val_acc: 0.8093\n",
      "Epoch 100/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8118\n",
      "Epoch 100: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4120 - acc: 0.8122 - val_loss: 0.4277 - val_acc: 0.7968\n",
      "Epoch 101/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4117 - acc: 0.8075\n",
      "Epoch 101: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4107 - acc: 0.8082 - val_loss: 0.3909 - val_acc: 0.8109\n",
      "Epoch 102/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8092\n",
      "Epoch 102: val_acc did not improve from 0.81943\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4033 - acc: 0.8094 - val_loss: 0.4279 - val_acc: 0.7834\n",
      "Epoch 103/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8125\n",
      "Epoch 103: val_acc improved from 0.81943 to 0.82470, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4027 - acc: 0.8125 - val_loss: 0.3858 - val_acc: 0.8247\n",
      "Epoch 104/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8055\n",
      "Epoch 104: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4116 - acc: 0.8058 - val_loss: 0.4057 - val_acc: 0.7903\n",
      "Epoch 105/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8124\n",
      "Epoch 105: val_acc did not improve from 0.82470\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4048 - acc: 0.8128 - val_loss: 0.4040 - val_acc: 0.7951\n",
      "Epoch 106/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8037\n",
      "Epoch 106: val_acc improved from 0.82470 to 0.82510, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4116 - acc: 0.8043 - val_loss: 0.3884 - val_acc: 0.8251\n",
      "Epoch 107/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8102\n",
      "Epoch 107: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3987 - acc: 0.8104 - val_loss: 0.3930 - val_acc: 0.8162\n",
      "Epoch 108/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8099\n",
      "Epoch 108: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4074 - acc: 0.8102 - val_loss: 0.3805 - val_acc: 0.8113\n",
      "Epoch 109/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8106\n",
      "Epoch 109: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4010 - acc: 0.8106 - val_loss: 0.3827 - val_acc: 0.8045\n",
      "Epoch 110/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8085\n",
      "Epoch 110: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4027 - acc: 0.8092 - val_loss: 0.3958 - val_acc: 0.7947\n",
      "Epoch 111/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8084\n",
      "Epoch 111: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4070 - acc: 0.8083 - val_loss: 0.3860 - val_acc: 0.8097\n",
      "Epoch 112/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8053\n",
      "Epoch 112: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4121 - acc: 0.8056 - val_loss: 0.4073 - val_acc: 0.7968\n",
      "Epoch 113/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8105\n",
      "Epoch 113: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4066 - acc: 0.8108 - val_loss: 0.4033 - val_acc: 0.7964\n",
      "Epoch 114/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8110\n",
      "Epoch 114: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3993 - acc: 0.8111 - val_loss: 0.4083 - val_acc: 0.7862\n",
      "Epoch 115/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8065\n",
      "Epoch 115: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4104 - acc: 0.8071 - val_loss: 0.4051 - val_acc: 0.7935\n",
      "Epoch 116/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8092\n",
      "Epoch 116: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3996 - acc: 0.8099 - val_loss: 0.4192 - val_acc: 0.7919\n",
      "Epoch 117/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8181\n",
      "Epoch 117: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3934 - acc: 0.8186 - val_loss: 0.3860 - val_acc: 0.8146\n",
      "Epoch 118/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8113\n",
      "Epoch 118: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4052 - acc: 0.8115 - val_loss: 0.4073 - val_acc: 0.7915\n",
      "Epoch 119/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8098\n",
      "Epoch 119: val_acc did not improve from 0.82510\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3970 - acc: 0.8101 - val_loss: 0.3821 - val_acc: 0.8040\n",
      "Epoch 120/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3965 - acc: 0.8134\n",
      "Epoch 120: val_acc improved from 0.82510 to 0.82753, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3965 - acc: 0.8134 - val_loss: 0.3813 - val_acc: 0.8275\n",
      "Epoch 121/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.8129\n",
      "Epoch 121: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3934 - acc: 0.8131 - val_loss: 0.4132 - val_acc: 0.7891\n",
      "Epoch 122/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8130\n",
      "Epoch 122: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4053 - acc: 0.8133 - val_loss: 0.3859 - val_acc: 0.8113\n",
      "Epoch 123/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8105\n",
      "Epoch 123: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3962 - acc: 0.8108 - val_loss: 0.4102 - val_acc: 0.7866\n",
      "Epoch 124/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8110\n",
      "Epoch 124: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4075 - acc: 0.8113 - val_loss: 0.4065 - val_acc: 0.7980\n",
      "Epoch 125/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8153\n",
      "Epoch 125: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3953 - acc: 0.8156 - val_loss: 0.3907 - val_acc: 0.8219\n",
      "Epoch 126/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8139\n",
      "Epoch 126: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4003 - acc: 0.8140 - val_loss: 0.4053 - val_acc: 0.7951\n",
      "Epoch 127/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3956 - acc: 0.8162\n",
      "Epoch 127: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3956 - acc: 0.8162 - val_loss: 0.4028 - val_acc: 0.7947\n",
      "Epoch 128/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8097\n",
      "Epoch 128: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4032 - acc: 0.8107 - val_loss: 0.3888 - val_acc: 0.8194\n",
      "Epoch 129/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8127\n",
      "Epoch 129: val_acc did not improve from 0.82753\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3981 - acc: 0.8132 - val_loss: 0.3921 - val_acc: 0.8121\n",
      "Epoch 130/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8182\n",
      "Epoch 130: val_acc improved from 0.82753 to 0.83198, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3940 - acc: 0.8185 - val_loss: 0.3861 - val_acc: 0.8320\n",
      "Epoch 131/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8082\n",
      "Epoch 131: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.4078 - acc: 0.8085 - val_loss: 0.3885 - val_acc: 0.8113\n",
      "Epoch 132/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8149\n",
      "Epoch 132: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3934 - acc: 0.8154 - val_loss: 0.3934 - val_acc: 0.8004\n",
      "Epoch 133/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8129\n",
      "Epoch 133: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4026 - acc: 0.8134 - val_loss: 0.3888 - val_acc: 0.8186\n",
      "Epoch 134/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8197\n",
      "Epoch 134: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3907 - acc: 0.8197 - val_loss: 0.4311 - val_acc: 0.7887\n",
      "Epoch 135/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8132\n",
      "Epoch 135: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4002 - acc: 0.8136 - val_loss: 0.3787 - val_acc: 0.8194\n",
      "Epoch 136/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8225\n",
      "Epoch 136: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3800 - acc: 0.8225 - val_loss: 0.3798 - val_acc: 0.8219\n",
      "Epoch 137/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8181\n",
      "Epoch 137: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3887 - acc: 0.8187 - val_loss: 0.3981 - val_acc: 0.8040\n",
      "Epoch 138/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8163\n",
      "Epoch 138: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4003 - acc: 0.8166 - val_loss: 0.3846 - val_acc: 0.8121\n",
      "Epoch 139/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8166\n",
      "Epoch 139: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3958 - acc: 0.8168 - val_loss: 0.3766 - val_acc: 0.8190\n",
      "Epoch 140/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8159\n",
      "Epoch 140: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3925 - acc: 0.8162 - val_loss: 0.3911 - val_acc: 0.8097\n",
      "Epoch 141/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8151\n",
      "Epoch 141: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3959 - acc: 0.8154 - val_loss: 0.3953 - val_acc: 0.8178\n",
      "Epoch 142/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8174\n",
      "Epoch 142: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3890 - acc: 0.8176 - val_loss: 0.3867 - val_acc: 0.8085\n",
      "Epoch 143/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8176\n",
      "Epoch 143: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3892 - acc: 0.8179 - val_loss: 0.3894 - val_acc: 0.8061\n",
      "Epoch 144/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8171\n",
      "Epoch 144: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3935 - acc: 0.8177 - val_loss: 0.3764 - val_acc: 0.8271\n",
      "Epoch 145/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8191\n",
      "Epoch 145: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3868 - acc: 0.8194 - val_loss: 0.3776 - val_acc: 0.8231\n",
      "Epoch 146/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8124\n",
      "Epoch 146: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3921 - acc: 0.8133 - val_loss: 0.4034 - val_acc: 0.8126\n",
      "Epoch 147/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8153\n",
      "Epoch 147: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3866 - acc: 0.8154 - val_loss: 0.3688 - val_acc: 0.8247\n",
      "Epoch 148/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8144\n",
      "Epoch 148: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3871 - acc: 0.8146 - val_loss: 0.3964 - val_acc: 0.8117\n",
      "Epoch 149/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8223\n",
      "Epoch 149: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3836 - acc: 0.8226 - val_loss: 0.3925 - val_acc: 0.8243\n",
      "Epoch 150/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8204\n",
      "Epoch 150: val_acc did not improve from 0.83198\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3900 - acc: 0.8215 - val_loss: 0.4013 - val_acc: 0.7879\n",
      "Epoch 151/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.8209\n",
      "Epoch 151: val_acc improved from 0.83198 to 0.84130, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3842 - acc: 0.8209 - val_loss: 0.3672 - val_acc: 0.8413\n",
      "Epoch 152/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8199\n",
      "Epoch 152: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3866 - acc: 0.8200 - val_loss: 0.3857 - val_acc: 0.8198\n",
      "Epoch 153/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8160\n",
      "Epoch 153: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3820 - acc: 0.8164 - val_loss: 0.3850 - val_acc: 0.8231\n",
      "Epoch 154/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8186\n",
      "Epoch 154: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3904 - acc: 0.8187 - val_loss: 0.4008 - val_acc: 0.7943\n",
      "Epoch 155/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8178\n",
      "Epoch 155: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3922 - acc: 0.8184 - val_loss: 0.3690 - val_acc: 0.8320\n",
      "Epoch 156/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8157\n",
      "Epoch 156: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3897 - acc: 0.8162 - val_loss: 0.3808 - val_acc: 0.8300\n",
      "Epoch 157/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8116\n",
      "Epoch 157: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.4002 - acc: 0.8124 - val_loss: 0.3750 - val_acc: 0.8271\n",
      "Epoch 158/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8177\n",
      "Epoch 158: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3865 - acc: 0.8178 - val_loss: 0.3998 - val_acc: 0.8121\n",
      "Epoch 159/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8246\n",
      "Epoch 159: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.3788 - acc: 0.8249 - val_loss: 0.3817 - val_acc: 0.8275\n",
      "Epoch 160/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8211\n",
      "Epoch 160: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3860 - acc: 0.8212 - val_loss: 0.3806 - val_acc: 0.8194\n",
      "Epoch 161/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8237\n",
      "Epoch 161: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3783 - acc: 0.8239 - val_loss: 0.3835 - val_acc: 0.8158\n",
      "Epoch 162/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8186\n",
      "Epoch 162: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3859 - acc: 0.8191 - val_loss: 0.3756 - val_acc: 0.8393\n",
      "Epoch 163/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8197\n",
      "Epoch 163: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3817 - acc: 0.8203 - val_loss: 0.3843 - val_acc: 0.8275\n",
      "Epoch 164/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8194\n",
      "Epoch 164: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3878 - acc: 0.8197 - val_loss: 0.3713 - val_acc: 0.8194\n",
      "Epoch 165/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8232\n",
      "Epoch 165: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3871 - acc: 0.8233 - val_loss: 0.3782 - val_acc: 0.8235\n",
      "Epoch 166/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8252\n",
      "Epoch 166: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3803 - acc: 0.8259 - val_loss: 0.3767 - val_acc: 0.8340\n",
      "Epoch 167/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8250\n",
      "Epoch 167: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3799 - acc: 0.8251 - val_loss: 0.3883 - val_acc: 0.8004\n",
      "Epoch 168/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3719 - acc: 0.8248\n",
      "Epoch 168: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3719 - acc: 0.8248 - val_loss: 0.3943 - val_acc: 0.8032\n",
      "Epoch 169/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8213\n",
      "Epoch 169: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3832 - acc: 0.8216 - val_loss: 0.3657 - val_acc: 0.8300\n",
      "Epoch 170/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8213\n",
      "Epoch 170: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3842 - acc: 0.8221 - val_loss: 0.3787 - val_acc: 0.8170\n",
      "Epoch 171/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8206\n",
      "Epoch 171: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3800 - acc: 0.8211 - val_loss: 0.3887 - val_acc: 0.8138\n",
      "Epoch 172/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8194\n",
      "Epoch 172: val_acc did not improve from 0.84130\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3916 - acc: 0.8197 - val_loss: 0.3947 - val_acc: 0.8036\n",
      "Epoch 173/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8214\n",
      "Epoch 173: val_acc improved from 0.84130 to 0.84696, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3826 - acc: 0.8220 - val_loss: 0.3671 - val_acc: 0.8470\n",
      "Epoch 174/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3808 - acc: 0.8243\n",
      "Epoch 174: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3808 - acc: 0.8243 - val_loss: 0.3704 - val_acc: 0.8397\n",
      "Epoch 175/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8216\n",
      "Epoch 175: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3752 - acc: 0.8224 - val_loss: 0.3816 - val_acc: 0.8223\n",
      "Epoch 176/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8252\n",
      "Epoch 176: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3763 - acc: 0.8255 - val_loss: 0.4063 - val_acc: 0.8105\n",
      "Epoch 177/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8195\n",
      "Epoch 177: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3815 - acc: 0.8199 - val_loss: 0.3543 - val_acc: 0.8445\n",
      "Epoch 178/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8245\n",
      "Epoch 178: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3780 - acc: 0.8247 - val_loss: 0.3843 - val_acc: 0.8178\n",
      "Epoch 179/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8223\n",
      "Epoch 179: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3794 - acc: 0.8223 - val_loss: 0.3879 - val_acc: 0.8162\n",
      "Epoch 180/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8202\n",
      "Epoch 180: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3844 - acc: 0.8204 - val_loss: 0.3655 - val_acc: 0.8328\n",
      "Epoch 181/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8197\n",
      "Epoch 181: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3788 - acc: 0.8206 - val_loss: 0.3696 - val_acc: 0.8243\n",
      "Epoch 182/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8260\n",
      "Epoch 182: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3796 - acc: 0.8266 - val_loss: 0.3915 - val_acc: 0.8211\n",
      "Epoch 183/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8252\n",
      "Epoch 183: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3892 - acc: 0.8254 - val_loss: 0.3685 - val_acc: 0.8344\n",
      "Epoch 184/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3800 - acc: 0.8246\n",
      "Epoch 184: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3800 - acc: 0.8246 - val_loss: 0.3617 - val_acc: 0.8437\n",
      "Epoch 185/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8316\n",
      "Epoch 185: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3708 - acc: 0.8319 - val_loss: 0.3791 - val_acc: 0.8243\n",
      "Epoch 186/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8255\n",
      "Epoch 186: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3823 - acc: 0.8256 - val_loss: 0.3772 - val_acc: 0.8206\n",
      "Epoch 187/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8228\n",
      "Epoch 187: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3722 - acc: 0.8229 - val_loss: 0.3810 - val_acc: 0.8069\n",
      "Epoch 188/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8166\n",
      "Epoch 188: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3927 - acc: 0.8174 - val_loss: 0.3821 - val_acc: 0.8206\n",
      "Epoch 189/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8203\n",
      "Epoch 189: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3812 - acc: 0.8206 - val_loss: 0.3685 - val_acc: 0.8344\n",
      "Epoch 190/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8260\n",
      "Epoch 190: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3708 - acc: 0.8261 - val_loss: 0.3916 - val_acc: 0.8360\n",
      "Epoch 191/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8276\n",
      "Epoch 191: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.3788 - acc: 0.8277 - val_loss: 0.3724 - val_acc: 0.8312\n",
      "Epoch 192/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8232\n",
      "Epoch 192: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3795 - acc: 0.8239 - val_loss: 0.4045 - val_acc: 0.8004\n",
      "Epoch 193/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8257\n",
      "Epoch 193: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3792 - acc: 0.8258 - val_loss: 0.3490 - val_acc: 0.8462\n",
      "Epoch 194/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8261\n",
      "Epoch 194: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3827 - acc: 0.8261 - val_loss: 0.3658 - val_acc: 0.8360\n",
      "Epoch 195/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8250\n",
      "Epoch 195: val_acc did not improve from 0.84696\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3818 - acc: 0.8250 - val_loss: 0.3704 - val_acc: 0.8300\n",
      "Epoch 196/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8308\n",
      "Epoch 196: val_acc improved from 0.84696 to 0.85425, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3680 - acc: 0.8314 - val_loss: 0.3638 - val_acc: 0.8543\n",
      "Epoch 197/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8275\n",
      "Epoch 197: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3765 - acc: 0.8277 - val_loss: 0.3709 - val_acc: 0.8320\n",
      "Epoch 198/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8208\n",
      "Epoch 198: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3793 - acc: 0.8211 - val_loss: 0.3719 - val_acc: 0.8287\n",
      "Epoch 199/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8173\n",
      "Epoch 199: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3832 - acc: 0.8173 - val_loss: 0.3843 - val_acc: 0.8045\n",
      "Epoch 200/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8179\n",
      "Epoch 200: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3924 - acc: 0.8180 - val_loss: 0.4067 - val_acc: 0.7955\n",
      "Epoch 201/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8227\n",
      "Epoch 201: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3864 - acc: 0.8230 - val_loss: 0.3730 - val_acc: 0.8202\n",
      "Epoch 202/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8151\n",
      "Epoch 202: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3854 - acc: 0.8155 - val_loss: 0.3552 - val_acc: 0.8413\n",
      "Epoch 203/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8292\n",
      "Epoch 203: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3710 - acc: 0.8294 - val_loss: 0.3906 - val_acc: 0.8045\n",
      "Epoch 204/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8284\n",
      "Epoch 204: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3782 - acc: 0.8286 - val_loss: 0.3753 - val_acc: 0.8247\n",
      "Epoch 205/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8232\n",
      "Epoch 205: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3769 - acc: 0.8239 - val_loss: 0.3725 - val_acc: 0.8348\n",
      "Epoch 206/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8282\n",
      "Epoch 206: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3764 - acc: 0.8282 - val_loss: 0.3697 - val_acc: 0.8498\n",
      "Epoch 207/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3742 - acc: 0.8198\n",
      "Epoch 207: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3742 - acc: 0.8198 - val_loss: 0.3839 - val_acc: 0.8267\n",
      "Epoch 208/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8240\n",
      "Epoch 208: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3756 - acc: 0.8239 - val_loss: 0.4139 - val_acc: 0.8109\n",
      "Epoch 209/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8264\n",
      "Epoch 209: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3773 - acc: 0.8268 - val_loss: 0.3756 - val_acc: 0.8287\n",
      "Epoch 210/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8320\n",
      "Epoch 210: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3698 - acc: 0.8324 - val_loss: 0.3723 - val_acc: 0.8202\n",
      "Epoch 211/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8235\n",
      "Epoch 211: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3781 - acc: 0.8237 - val_loss: 0.3933 - val_acc: 0.8081\n",
      "Epoch 212/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8248\n",
      "Epoch 212: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3771 - acc: 0.8251 - val_loss: 0.3666 - val_acc: 0.8348\n",
      "Epoch 213/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8281\n",
      "Epoch 213: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3703 - acc: 0.8287 - val_loss: 0.3826 - val_acc: 0.8381\n",
      "Epoch 214/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8205\n",
      "Epoch 214: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3747 - acc: 0.8213 - val_loss: 0.3882 - val_acc: 0.8441\n",
      "Epoch 215/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3654 - acc: 0.8304\n",
      "Epoch 215: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3655 - acc: 0.8304 - val_loss: 0.3978 - val_acc: 0.8287\n",
      "Epoch 216/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8260\n",
      "Epoch 216: val_acc did not improve from 0.85425\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3765 - acc: 0.8264 - val_loss: 0.3846 - val_acc: 0.8049\n",
      "Epoch 217/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.8286\n",
      "Epoch 217: val_acc improved from 0.85425 to 0.86437, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3680 - acc: 0.8294 - val_loss: 0.3723 - val_acc: 0.8644\n",
      "Epoch 218/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8201\n",
      "Epoch 218: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3768 - acc: 0.8201 - val_loss: 0.3700 - val_acc: 0.8231\n",
      "Epoch 219/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8317\n",
      "Epoch 219: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3706 - acc: 0.8320 - val_loss: 0.4121 - val_acc: 0.8117\n",
      "Epoch 220/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8185\n",
      "Epoch 220: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3726 - acc: 0.8188 - val_loss: 0.3960 - val_acc: 0.8081\n",
      "Epoch 221/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8298\n",
      "Epoch 221: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3704 - acc: 0.8301 - val_loss: 0.3642 - val_acc: 0.8482\n",
      "Epoch 222/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8273\n",
      "Epoch 222: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3724 - acc: 0.8276 - val_loss: 0.3625 - val_acc: 0.8364\n",
      "Epoch 223/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8298\n",
      "Epoch 223: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3672 - acc: 0.8302 - val_loss: 0.3536 - val_acc: 0.8445\n",
      "Epoch 224/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8313\n",
      "Epoch 224: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3708 - acc: 0.8313 - val_loss: 0.4049 - val_acc: 0.8105\n",
      "Epoch 225/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8252\n",
      "Epoch 225: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3810 - acc: 0.8257 - val_loss: 0.3696 - val_acc: 0.8275\n",
      "Epoch 226/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3695 - acc: 0.8248\n",
      "Epoch 226: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3695 - acc: 0.8247 - val_loss: 0.3485 - val_acc: 0.8575\n",
      "Epoch 227/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3695 - acc: 0.8250\n",
      "Epoch 227: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3695 - acc: 0.8250 - val_loss: 0.3748 - val_acc: 0.8498\n",
      "Epoch 228/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8283\n",
      "Epoch 228: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3770 - acc: 0.8285 - val_loss: 0.3624 - val_acc: 0.8170\n",
      "Epoch 229/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8225\n",
      "Epoch 229: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3720 - acc: 0.8234 - val_loss: 0.3508 - val_acc: 0.8526\n",
      "Epoch 230/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8321\n",
      "Epoch 230: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3681 - acc: 0.8328 - val_loss: 0.3526 - val_acc: 0.8510\n",
      "Epoch 231/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8237\n",
      "Epoch 231: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3713 - acc: 0.8235 - val_loss: 0.3514 - val_acc: 0.8607\n",
      "Epoch 232/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8297\n",
      "Epoch 232: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3708 - acc: 0.8299 - val_loss: 0.3872 - val_acc: 0.8198\n",
      "Epoch 233/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8326\n",
      "Epoch 233: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3667 - acc: 0.8329 - val_loss: 0.3601 - val_acc: 0.8437\n",
      "Epoch 234/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8301\n",
      "Epoch 234: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3643 - acc: 0.8301 - val_loss: 0.3699 - val_acc: 0.8453\n",
      "Epoch 235/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8289\n",
      "Epoch 235: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3663 - acc: 0.8289 - val_loss: 0.3900 - val_acc: 0.8251\n",
      "Epoch 236/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8298\n",
      "Epoch 236: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3646 - acc: 0.8300 - val_loss: 0.3910 - val_acc: 0.8016\n",
      "Epoch 237/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8289\n",
      "Epoch 237: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3708 - acc: 0.8292 - val_loss: 0.3736 - val_acc: 0.8223\n",
      "Epoch 238/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8309\n",
      "Epoch 238: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3685 - acc: 0.8310 - val_loss: 0.3827 - val_acc: 0.8308\n",
      "Epoch 239/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8296\n",
      "Epoch 239: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3612 - acc: 0.8302 - val_loss: 0.3655 - val_acc: 0.8490\n",
      "Epoch 240/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8310\n",
      "Epoch 240: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3678 - acc: 0.8313 - val_loss: 0.3800 - val_acc: 0.8255\n",
      "Epoch 241/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8298\n",
      "Epoch 241: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3767 - acc: 0.8301 - val_loss: 0.3672 - val_acc: 0.8405\n",
      "Epoch 242/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8277\n",
      "Epoch 242: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3758 - acc: 0.8285 - val_loss: 0.3933 - val_acc: 0.8316\n",
      "Epoch 243/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8299\n",
      "Epoch 243: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3619 - acc: 0.8305 - val_loss: 0.3852 - val_acc: 0.8356\n",
      "Epoch 244/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8229\n",
      "Epoch 244: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3670 - acc: 0.8236 - val_loss: 0.4015 - val_acc: 0.8093\n",
      "Epoch 245/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8258\n",
      "Epoch 245: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3677 - acc: 0.8260 - val_loss: 0.4036 - val_acc: 0.8275\n",
      "Epoch 246/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8353\n",
      "Epoch 246: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3589 - acc: 0.8359 - val_loss: 0.3975 - val_acc: 0.8206\n",
      "Epoch 247/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8321\n",
      "Epoch 247: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3623 - acc: 0.8323 - val_loss: 0.3706 - val_acc: 0.8259\n",
      "Epoch 248/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8336\n",
      "Epoch 248: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3597 - acc: 0.8341 - val_loss: 0.3712 - val_acc: 0.8425\n",
      "Epoch 249/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.8246\n",
      "Epoch 249: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3729 - acc: 0.8248 - val_loss: 0.3900 - val_acc: 0.8198\n",
      "Epoch 250/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8318\n",
      "Epoch 250: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3679 - acc: 0.8322 - val_loss: 0.3649 - val_acc: 0.8267\n",
      "Epoch 251/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8359\n",
      "Epoch 251: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3543 - acc: 0.8362 - val_loss: 0.3734 - val_acc: 0.8267\n",
      "Epoch 252/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8295\n",
      "Epoch 252: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3699 - acc: 0.8295 - val_loss: 0.3780 - val_acc: 0.8251\n",
      "Epoch 253/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8311\n",
      "Epoch 253: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3645 - acc: 0.8312 - val_loss: 0.3761 - val_acc: 0.8223\n",
      "Epoch 254/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8216\n",
      "Epoch 254: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3853 - acc: 0.8218 - val_loss: 0.4120 - val_acc: 0.8101\n",
      "Epoch 255/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8352\n",
      "Epoch 255: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3583 - acc: 0.8355 - val_loss: 0.3903 - val_acc: 0.8211\n",
      "Epoch 256/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8344\n",
      "Epoch 256: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3577 - acc: 0.8345 - val_loss: 0.3613 - val_acc: 0.8579\n",
      "Epoch 257/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8282\n",
      "Epoch 257: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3690 - acc: 0.8284 - val_loss: 0.3528 - val_acc: 0.8449\n",
      "Epoch 258/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8364\n",
      "Epoch 258: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3564 - acc: 0.8366 - val_loss: 0.3537 - val_acc: 0.8510\n",
      "Epoch 259/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8300\n",
      "Epoch 259: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3646 - acc: 0.8304 - val_loss: 0.3527 - val_acc: 0.8401\n",
      "Epoch 260/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8322\n",
      "Epoch 260: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3642 - acc: 0.8322 - val_loss: 0.3679 - val_acc: 0.8332\n",
      "Epoch 261/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8235\n",
      "Epoch 261: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3762 - acc: 0.8237 - val_loss: 0.3713 - val_acc: 0.8352\n",
      "Epoch 262/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8248\n",
      "Epoch 262: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3686 - acc: 0.8247 - val_loss: 0.3718 - val_acc: 0.8316\n",
      "Epoch 263/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8319\n",
      "Epoch 263: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3557 - acc: 0.8323 - val_loss: 0.4011 - val_acc: 0.8162\n",
      "Epoch 264/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8330\n",
      "Epoch 264: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3565 - acc: 0.8334 - val_loss: 0.4048 - val_acc: 0.8057\n",
      "Epoch 265/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8330\n",
      "Epoch 265: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3656 - acc: 0.8331 - val_loss: 0.3646 - val_acc: 0.8389\n",
      "Epoch 266/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8318\n",
      "Epoch 266: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3617 - acc: 0.8318 - val_loss: 0.3699 - val_acc: 0.8364\n",
      "Epoch 267/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3670 - acc: 0.8304\n",
      "Epoch 267: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3670 - acc: 0.8304 - val_loss: 0.3592 - val_acc: 0.8425\n",
      "Epoch 268/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8324\n",
      "Epoch 268: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3671 - acc: 0.8326 - val_loss: 0.3668 - val_acc: 0.8462\n",
      "Epoch 269/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3572 - acc: 0.8335\n",
      "Epoch 269: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3572 - acc: 0.8335 - val_loss: 0.3997 - val_acc: 0.8045\n",
      "Epoch 270/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3549 - acc: 0.8360\n",
      "Epoch 270: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3549 - acc: 0.8360 - val_loss: 0.4456 - val_acc: 0.7822\n",
      "Epoch 271/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8256\n",
      "Epoch 271: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3654 - acc: 0.8262 - val_loss: 0.3724 - val_acc: 0.8287\n",
      "Epoch 272/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3609 - acc: 0.8279\n",
      "Epoch 272: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3609 - acc: 0.8279 - val_loss: 0.3774 - val_acc: 0.8227\n",
      "Epoch 273/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8317\n",
      "Epoch 273: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3581 - acc: 0.8320 - val_loss: 0.3664 - val_acc: 0.8603\n",
      "Epoch 274/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8319\n",
      "Epoch 274: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3578 - acc: 0.8319 - val_loss: 0.3710 - val_acc: 0.8247\n",
      "Epoch 275/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8339\n",
      "Epoch 275: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3649 - acc: 0.8342 - val_loss: 0.3860 - val_acc: 0.8227\n",
      "Epoch 276/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8317\n",
      "Epoch 276: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3666 - acc: 0.8318 - val_loss: 0.3790 - val_acc: 0.8198\n",
      "Epoch 277/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8317\n",
      "Epoch 277: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3635 - acc: 0.8319 - val_loss: 0.3677 - val_acc: 0.8312\n",
      "Epoch 278/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8311\n",
      "Epoch 278: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3684 - acc: 0.8317 - val_loss: 0.3698 - val_acc: 0.8441\n",
      "Epoch 279/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8340\n",
      "Epoch 279: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3559 - acc: 0.8341 - val_loss: 0.3917 - val_acc: 0.8308\n",
      "Epoch 280/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8354\n",
      "Epoch 280: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3551 - acc: 0.8357 - val_loss: 0.4070 - val_acc: 0.8101\n",
      "Epoch 281/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8345\n",
      "Epoch 281: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3613 - acc: 0.8348 - val_loss: 0.3755 - val_acc: 0.8389\n",
      "Epoch 282/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8311\n",
      "Epoch 282: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3601 - acc: 0.8316 - val_loss: 0.3789 - val_acc: 0.8332\n",
      "Epoch 283/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8345\n",
      "Epoch 283: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3565 - acc: 0.8347 - val_loss: 0.3845 - val_acc: 0.8421\n",
      "Epoch 284/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8367\n",
      "Epoch 284: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3537 - acc: 0.8375 - val_loss: 0.4014 - val_acc: 0.8146\n",
      "Epoch 285/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3601 - acc: 0.8301\n",
      "Epoch 285: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3601 - acc: 0.8301 - val_loss: 0.3888 - val_acc: 0.8324\n",
      "Epoch 286/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8347\n",
      "Epoch 286: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3646 - acc: 0.8346 - val_loss: 0.3737 - val_acc: 0.8514\n",
      "Epoch 287/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8218\n",
      "Epoch 287: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3745 - acc: 0.8225 - val_loss: 0.3806 - val_acc: 0.8073\n",
      "Epoch 288/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8344\n",
      "Epoch 288: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3584 - acc: 0.8347 - val_loss: 0.3616 - val_acc: 0.8429\n",
      "Epoch 289/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8332\n",
      "Epoch 289: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3649 - acc: 0.8336 - val_loss: 0.3696 - val_acc: 0.8433\n",
      "Epoch 290/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3578 - acc: 0.8340\n",
      "Epoch 290: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3578 - acc: 0.8340 - val_loss: 0.3950 - val_acc: 0.8154\n",
      "Epoch 291/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8360\n",
      "Epoch 291: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3537 - acc: 0.8358 - val_loss: 0.3750 - val_acc: 0.8186\n",
      "Epoch 292/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8345\n",
      "Epoch 292: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3491 - acc: 0.8345 - val_loss: 0.3872 - val_acc: 0.8150\n",
      "Epoch 293/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8373\n",
      "Epoch 293: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3599 - acc: 0.8377 - val_loss: 0.3506 - val_acc: 0.8397\n",
      "Epoch 294/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8289\n",
      "Epoch 294: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3622 - acc: 0.8289 - val_loss: 0.3720 - val_acc: 0.8364\n",
      "Epoch 295/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8310\n",
      "Epoch 295: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3607 - acc: 0.8312 - val_loss: 0.3739 - val_acc: 0.8312\n",
      "Epoch 296/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8315\n",
      "Epoch 296: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3585 - acc: 0.8318 - val_loss: 0.3858 - val_acc: 0.8296\n",
      "Epoch 297/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3574 - acc: 0.8352\n",
      "Epoch 297: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3574 - acc: 0.8352 - val_loss: 0.3671 - val_acc: 0.8421\n",
      "Epoch 298/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8316\n",
      "Epoch 298: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3590 - acc: 0.8319 - val_loss: 0.3717 - val_acc: 0.8518\n",
      "Epoch 299/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3505 - acc: 0.8379\n",
      "Epoch 299: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3505 - acc: 0.8379 - val_loss: 0.3810 - val_acc: 0.8579\n",
      "Epoch 300/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8297\n",
      "Epoch 300: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3651 - acc: 0.8298 - val_loss: 0.3845 - val_acc: 0.8154\n",
      "Epoch 301/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8324\n",
      "Epoch 301: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3588 - acc: 0.8324 - val_loss: 0.3777 - val_acc: 0.8360\n",
      "Epoch 302/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8319\n",
      "Epoch 302: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3612 - acc: 0.8324 - val_loss: 0.3889 - val_acc: 0.8219\n",
      "Epoch 303/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8313\n",
      "Epoch 303: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3639 - acc: 0.8316 - val_loss: 0.3831 - val_acc: 0.8202\n",
      "Epoch 304/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8308\n",
      "Epoch 304: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3620 - acc: 0.8312 - val_loss: 0.3459 - val_acc: 0.8494\n",
      "Epoch 305/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8352\n",
      "Epoch 305: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3561 - acc: 0.8357 - val_loss: 0.3715 - val_acc: 0.8453\n",
      "Epoch 306/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8313\n",
      "Epoch 306: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3642 - acc: 0.8315 - val_loss: 0.3518 - val_acc: 0.8409\n",
      "Epoch 307/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8342\n",
      "Epoch 307: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3615 - acc: 0.8348 - val_loss: 0.3492 - val_acc: 0.8595\n",
      "Epoch 308/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8361\n",
      "Epoch 308: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3561 - acc: 0.8364 - val_loss: 0.3568 - val_acc: 0.8433\n",
      "Epoch 309/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8299\n",
      "Epoch 309: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3651 - acc: 0.8301 - val_loss: 0.3469 - val_acc: 0.8437\n",
      "Epoch 310/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8341\n",
      "Epoch 310: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3559 - acc: 0.8348 - val_loss: 0.3678 - val_acc: 0.8377\n",
      "Epoch 311/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8351\n",
      "Epoch 311: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3623 - acc: 0.8351 - val_loss: 0.3592 - val_acc: 0.8304\n",
      "Epoch 312/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8368\n",
      "Epoch 312: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3584 - acc: 0.8375 - val_loss: 0.3646 - val_acc: 0.8555\n",
      "Epoch 313/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8357\n",
      "Epoch 313: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3566 - acc: 0.8362 - val_loss: 0.3492 - val_acc: 0.8591\n",
      "Epoch 314/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8388\n",
      "Epoch 314: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3548 - acc: 0.8393 - val_loss: 0.3617 - val_acc: 0.8401\n",
      "Epoch 315/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8414\n",
      "Epoch 315: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3527 - acc: 0.8417 - val_loss: 0.3517 - val_acc: 0.8514\n",
      "Epoch 316/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8333\n",
      "Epoch 316: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3714 - acc: 0.8340 - val_loss: 0.3572 - val_acc: 0.8543\n",
      "Epoch 317/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8382\n",
      "Epoch 317: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3597 - acc: 0.8385 - val_loss: 0.3635 - val_acc: 0.8381\n",
      "Epoch 318/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8357\n",
      "Epoch 318: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3573 - acc: 0.8362 - val_loss: 0.3710 - val_acc: 0.8393\n",
      "Epoch 319/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8379\n",
      "Epoch 319: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3538 - acc: 0.8383 - val_loss: 0.3600 - val_acc: 0.8478\n",
      "Epoch 320/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8384\n",
      "Epoch 320: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3544 - acc: 0.8385 - val_loss: 0.3639 - val_acc: 0.8555\n",
      "Epoch 321/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8316\n",
      "Epoch 321: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3667 - acc: 0.8319 - val_loss: 0.3615 - val_acc: 0.8259\n",
      "Epoch 322/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8430\n",
      "Epoch 322: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3480 - acc: 0.8434 - val_loss: 0.3672 - val_acc: 0.8445\n",
      "Epoch 323/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8339\n",
      "Epoch 323: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3566 - acc: 0.8339 - val_loss: 0.3545 - val_acc: 0.8413\n",
      "Epoch 324/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3497 - acc: 0.8360\n",
      "Epoch 324: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3497 - acc: 0.8360 - val_loss: 0.3725 - val_acc: 0.8466\n",
      "Epoch 325/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8345\n",
      "Epoch 325: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3553 - acc: 0.8347 - val_loss: 0.3547 - val_acc: 0.8462\n",
      "Epoch 326/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8388\n",
      "Epoch 326: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3516 - acc: 0.8388 - val_loss: 0.3549 - val_acc: 0.8474\n",
      "Epoch 327/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8347\n",
      "Epoch 327: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3552 - acc: 0.8350 - val_loss: 0.3865 - val_acc: 0.8287\n",
      "Epoch 328/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8400\n",
      "Epoch 328: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3572 - acc: 0.8402 - val_loss: 0.3586 - val_acc: 0.8518\n",
      "Epoch 329/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8348\n",
      "Epoch 329: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3509 - acc: 0.8355 - val_loss: 0.3682 - val_acc: 0.8397\n",
      "Epoch 330/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8329\n",
      "Epoch 330: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3543 - acc: 0.8332 - val_loss: 0.3653 - val_acc: 0.8417\n",
      "Epoch 331/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8347\n",
      "Epoch 331: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3629 - acc: 0.8351 - val_loss: 0.3594 - val_acc: 0.8441\n",
      "Epoch 332/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8382\n",
      "Epoch 332: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3544 - acc: 0.8384 - val_loss: 0.4486 - val_acc: 0.8364\n",
      "Epoch 333/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3540 - acc: 0.8400\n",
      "Epoch 333: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3540 - acc: 0.8400 - val_loss: 0.3597 - val_acc: 0.8385\n",
      "Epoch 334/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8398\n",
      "Epoch 334: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3509 - acc: 0.8402 - val_loss: 0.3767 - val_acc: 0.8182\n",
      "Epoch 335/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8388\n",
      "Epoch 335: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3509 - acc: 0.8389 - val_loss: 0.3622 - val_acc: 0.8320\n",
      "Epoch 336/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8410\n",
      "Epoch 336: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3477 - acc: 0.8416 - val_loss: 0.3944 - val_acc: 0.8571\n",
      "Epoch 337/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8316\n",
      "Epoch 337: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3655 - acc: 0.8318 - val_loss: 0.3429 - val_acc: 0.8466\n",
      "Epoch 338/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8352\n",
      "Epoch 338: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3538 - acc: 0.8355 - val_loss: 0.3668 - val_acc: 0.8364\n",
      "Epoch 339/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8285\n",
      "Epoch 339: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3669 - acc: 0.8290 - val_loss: 0.3740 - val_acc: 0.8555\n",
      "Epoch 340/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8383\n",
      "Epoch 340: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3517 - acc: 0.8386 - val_loss: 0.3542 - val_acc: 0.8514\n",
      "Epoch 341/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8396\n",
      "Epoch 341: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3514 - acc: 0.8397 - val_loss: 0.3620 - val_acc: 0.8287\n",
      "Epoch 342/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8357\n",
      "Epoch 342: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3557 - acc: 0.8360 - val_loss: 0.3593 - val_acc: 0.8413\n",
      "Epoch 343/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8424\n",
      "Epoch 343: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3482 - acc: 0.8426 - val_loss: 0.3658 - val_acc: 0.8413\n",
      "Epoch 344/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3514 - acc: 0.8384\n",
      "Epoch 344: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3514 - acc: 0.8384 - val_loss: 0.3464 - val_acc: 0.8526\n",
      "Epoch 345/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3497 - acc: 0.8388\n",
      "Epoch 345: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3497 - acc: 0.8388 - val_loss: 0.3870 - val_acc: 0.8142\n",
      "Epoch 346/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3542 - acc: 0.8380\n",
      "Epoch 346: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3542 - acc: 0.8380 - val_loss: 0.3633 - val_acc: 0.8291\n",
      "Epoch 347/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8381\n",
      "Epoch 347: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3451 - acc: 0.8387 - val_loss: 0.3638 - val_acc: 0.8377\n",
      "Epoch 348/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8337\n",
      "Epoch 348: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3580 - acc: 0.8341 - val_loss: 0.3703 - val_acc: 0.8514\n",
      "Epoch 349/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8376\n",
      "Epoch 349: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3567 - acc: 0.8380 - val_loss: 0.3684 - val_acc: 0.8328\n",
      "Epoch 350/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8431\n",
      "Epoch 350: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3443 - acc: 0.8434 - val_loss: 0.3596 - val_acc: 0.8494\n",
      "Epoch 351/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8427\n",
      "Epoch 351: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3450 - acc: 0.8433 - val_loss: 0.3644 - val_acc: 0.8441\n",
      "Epoch 352/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8395\n",
      "Epoch 352: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3516 - acc: 0.8400 - val_loss: 0.3696 - val_acc: 0.8247\n",
      "Epoch 353/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8387\n",
      "Epoch 353: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3467 - acc: 0.8391 - val_loss: 0.3756 - val_acc: 0.8206\n",
      "Epoch 354/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.8385\n",
      "Epoch 354: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3464 - acc: 0.8385 - val_loss: 0.3649 - val_acc: 0.8401\n",
      "Epoch 355/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8451\n",
      "Epoch 355: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3390 - acc: 0.8456 - val_loss: 0.3700 - val_acc: 0.8356\n",
      "Epoch 356/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8434\n",
      "Epoch 356: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3425 - acc: 0.8440 - val_loss: 0.3675 - val_acc: 0.8453\n",
      "Epoch 357/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8377\n",
      "Epoch 357: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3506 - acc: 0.8383 - val_loss: 0.3655 - val_acc: 0.8372\n",
      "Epoch 358/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8441\n",
      "Epoch 358: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3375 - acc: 0.8443 - val_loss: 0.3810 - val_acc: 0.8352\n",
      "Epoch 359/2000\n",
      "607/618 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8431\n",
      "Epoch 359: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3402 - acc: 0.8436 - val_loss: 0.3555 - val_acc: 0.8445\n",
      "Epoch 360/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8378\n",
      "Epoch 360: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3597 - acc: 0.8377 - val_loss: 0.3778 - val_acc: 0.8486\n",
      "Epoch 361/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8382\n",
      "Epoch 361: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3505 - acc: 0.8385 - val_loss: 0.3977 - val_acc: 0.8486\n",
      "Epoch 362/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8415\n",
      "Epoch 362: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3443 - acc: 0.8422 - val_loss: 0.3876 - val_acc: 0.8231\n",
      "Epoch 363/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3493 - acc: 0.8426\n",
      "Epoch 363: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3493 - acc: 0.8426 - val_loss: 0.3791 - val_acc: 0.8437\n",
      "Epoch 364/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8440\n",
      "Epoch 364: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3453 - acc: 0.8440 - val_loss: 0.3782 - val_acc: 0.8304\n",
      "Epoch 365/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8411\n",
      "Epoch 365: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3446 - acc: 0.8414 - val_loss: 0.3557 - val_acc: 0.8502\n",
      "Epoch 366/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.8392\n",
      "Epoch 366: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3430 - acc: 0.8397 - val_loss: 0.3954 - val_acc: 0.8344\n",
      "Epoch 367/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8396\n",
      "Epoch 367: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3514 - acc: 0.8397 - val_loss: 0.3807 - val_acc: 0.8243\n",
      "Epoch 368/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8337\n",
      "Epoch 368: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3537 - acc: 0.8342 - val_loss: 0.4000 - val_acc: 0.8555\n",
      "Epoch 369/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8420\n",
      "Epoch 369: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3454 - acc: 0.8422 - val_loss: 0.3420 - val_acc: 0.8571\n",
      "Epoch 370/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8379\n",
      "Epoch 370: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3536 - acc: 0.8382 - val_loss: 0.3901 - val_acc: 0.8328\n",
      "Epoch 371/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8343\n",
      "Epoch 371: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3603 - acc: 0.8348 - val_loss: 0.3569 - val_acc: 0.8340\n",
      "Epoch 372/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8350\n",
      "Epoch 372: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3536 - acc: 0.8351 - val_loss: 0.3893 - val_acc: 0.8206\n",
      "Epoch 373/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8413\n",
      "Epoch 373: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3477 - acc: 0.8414 - val_loss: 0.3827 - val_acc: 0.8433\n",
      "Epoch 374/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8414\n",
      "Epoch 374: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3483 - acc: 0.8418 - val_loss: 0.3760 - val_acc: 0.8567\n",
      "Epoch 375/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.8373\n",
      "Epoch 375: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3461 - acc: 0.8375 - val_loss: 0.3659 - val_acc: 0.8579\n",
      "Epoch 376/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8406\n",
      "Epoch 376: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3400 - acc: 0.8409 - val_loss: 0.3613 - val_acc: 0.8583\n",
      "Epoch 377/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8393\n",
      "Epoch 377: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3493 - acc: 0.8396 - val_loss: 0.3947 - val_acc: 0.8360\n",
      "Epoch 378/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8424\n",
      "Epoch 378: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3489 - acc: 0.8425 - val_loss: 0.3709 - val_acc: 0.8385\n",
      "Epoch 379/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8374\n",
      "Epoch 379: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3498 - acc: 0.8380 - val_loss: 0.3747 - val_acc: 0.8457\n",
      "Epoch 380/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3702 - acc: 0.8341\n",
      "Epoch 380: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3702 - acc: 0.8341 - val_loss: 0.3840 - val_acc: 0.8433\n",
      "Epoch 381/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8420\n",
      "Epoch 381: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3531 - acc: 0.8420 - val_loss: 0.3718 - val_acc: 0.8389\n",
      "Epoch 382/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8450\n",
      "Epoch 382: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3428 - acc: 0.8452 - val_loss: 0.3597 - val_acc: 0.8453\n",
      "Epoch 383/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8312\n",
      "Epoch 383: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3506 - acc: 0.8313 - val_loss: 0.3551 - val_acc: 0.8579\n",
      "Epoch 384/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8373\n",
      "Epoch 384: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3517 - acc: 0.8376 - val_loss: 0.3864 - val_acc: 0.8368\n",
      "Epoch 385/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8412\n",
      "Epoch 385: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3458 - acc: 0.8420 - val_loss: 0.3900 - val_acc: 0.8490\n",
      "Epoch 386/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8360\n",
      "Epoch 386: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3536 - acc: 0.8368 - val_loss: 0.3697 - val_acc: 0.8324\n",
      "Epoch 387/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8443\n",
      "Epoch 387: val_acc did not improve from 0.86437\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3468 - acc: 0.8446 - val_loss: 0.3677 - val_acc: 0.8316\n",
      "Epoch 388/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8424\n",
      "Epoch 388: val_acc improved from 0.86437 to 0.87085, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3474 - acc: 0.8426 - val_loss: 0.3565 - val_acc: 0.8709\n",
      "Epoch 389/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8368\n",
      "Epoch 389: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3521 - acc: 0.8370 - val_loss: 0.4116 - val_acc: 0.8077\n",
      "Epoch 390/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8372\n",
      "Epoch 390: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3502 - acc: 0.8372 - val_loss: 0.3732 - val_acc: 0.8304\n",
      "Epoch 391/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8420\n",
      "Epoch 391: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3413 - acc: 0.8422 - val_loss: 0.3665 - val_acc: 0.8433\n",
      "Epoch 392/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8415\n",
      "Epoch 392: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3439 - acc: 0.8419 - val_loss: 0.3511 - val_acc: 0.8413\n",
      "Epoch 393/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8400\n",
      "Epoch 393: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3482 - acc: 0.8401 - val_loss: 0.3739 - val_acc: 0.8409\n",
      "Epoch 394/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8404\n",
      "Epoch 394: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3484 - acc: 0.8411 - val_loss: 0.3642 - val_acc: 0.8563\n",
      "Epoch 395/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.8430\n",
      "Epoch 395: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3412 - acc: 0.8430 - val_loss: 0.3612 - val_acc: 0.8413\n",
      "Epoch 396/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8420\n",
      "Epoch 396: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3529 - acc: 0.8424 - val_loss: 0.3651 - val_acc: 0.8502\n",
      "Epoch 397/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8434\n",
      "Epoch 397: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3379 - acc: 0.8437 - val_loss: 0.3463 - val_acc: 0.8611\n",
      "Epoch 398/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8397\n",
      "Epoch 398: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3464 - acc: 0.8400 - val_loss: 0.3431 - val_acc: 0.8514\n",
      "Epoch 399/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8474\n",
      "Epoch 399: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3387 - acc: 0.8478 - val_loss: 0.3476 - val_acc: 0.8636\n",
      "Epoch 400/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8430\n",
      "Epoch 400: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3419 - acc: 0.8437 - val_loss: 0.3514 - val_acc: 0.8543\n",
      "Epoch 401/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8427\n",
      "Epoch 401: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3470 - acc: 0.8429 - val_loss: 0.3515 - val_acc: 0.8640\n",
      "Epoch 402/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8368\n",
      "Epoch 402: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3518 - acc: 0.8371 - val_loss: 0.3589 - val_acc: 0.8530\n",
      "Epoch 403/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8413\n",
      "Epoch 403: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3433 - acc: 0.8414 - val_loss: 0.3730 - val_acc: 0.8575\n",
      "Epoch 404/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8408\n",
      "Epoch 404: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3469 - acc: 0.8409 - val_loss: 0.3711 - val_acc: 0.8348\n",
      "Epoch 405/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3353 - acc: 0.8459\n",
      "Epoch 405: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3353 - acc: 0.8459 - val_loss: 0.3821 - val_acc: 0.8547\n",
      "Epoch 406/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8388\n",
      "Epoch 406: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3544 - acc: 0.8389 - val_loss: 0.3629 - val_acc: 0.8364\n",
      "Epoch 407/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8364\n",
      "Epoch 407: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 0.3445 - acc: 0.8367 - val_loss: 0.3818 - val_acc: 0.8405\n",
      "Epoch 408/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8395\n",
      "Epoch 408: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3502 - acc: 0.8405 - val_loss: 0.3667 - val_acc: 0.8498\n",
      "Epoch 409/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8446\n",
      "Epoch 409: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3387 - acc: 0.8454 - val_loss: 0.3737 - val_acc: 0.8437\n",
      "Epoch 410/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8436\n",
      "Epoch 410: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3472 - acc: 0.8442 - val_loss: 0.3521 - val_acc: 0.8555\n",
      "Epoch 411/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8469\n",
      "Epoch 411: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3352 - acc: 0.8475 - val_loss: 0.3605 - val_acc: 0.8462\n",
      "Epoch 412/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.8384\n",
      "Epoch 412: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3528 - acc: 0.8386 - val_loss: 0.3808 - val_acc: 0.8453\n",
      "Epoch 413/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8421\n",
      "Epoch 413: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3489 - acc: 0.8423 - val_loss: 0.3431 - val_acc: 0.8688\n",
      "Epoch 414/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.8464\n",
      "Epoch 414: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3370 - acc: 0.8466 - val_loss: 0.3717 - val_acc: 0.8534\n",
      "Epoch 415/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8434\n",
      "Epoch 415: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3386 - acc: 0.8434 - val_loss: 0.3542 - val_acc: 0.8688\n",
      "Epoch 416/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8407\n",
      "Epoch 416: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3486 - acc: 0.8416 - val_loss: 0.3646 - val_acc: 0.8441\n",
      "Epoch 417/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8378\n",
      "Epoch 417: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3541 - acc: 0.8380 - val_loss: 0.3667 - val_acc: 0.8498\n",
      "Epoch 418/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3488 - acc: 0.8390\n",
      "Epoch 418: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3484 - acc: 0.8392 - val_loss: 0.3726 - val_acc: 0.8494\n",
      "Epoch 419/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8433\n",
      "Epoch 419: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3453 - acc: 0.8434 - val_loss: 0.3803 - val_acc: 0.8372\n",
      "Epoch 420/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8440\n",
      "Epoch 420: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3375 - acc: 0.8438 - val_loss: 0.3668 - val_acc: 0.8640\n",
      "Epoch 421/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8429\n",
      "Epoch 421: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3407 - acc: 0.8429 - val_loss: 0.3689 - val_acc: 0.8498\n",
      "Epoch 422/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3379 - acc: 0.8472\n",
      "Epoch 422: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3379 - acc: 0.8472 - val_loss: 0.3736 - val_acc: 0.8599\n",
      "Epoch 423/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.8439\n",
      "Epoch 423: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3440 - acc: 0.8439 - val_loss: 0.3692 - val_acc: 0.8405\n",
      "Epoch 424/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8436\n",
      "Epoch 424: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3507 - acc: 0.8440 - val_loss: 0.3676 - val_acc: 0.8275\n",
      "Epoch 425/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8411\n",
      "Epoch 425: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3671 - acc: 0.8413 - val_loss: 0.3696 - val_acc: 0.8599\n",
      "Epoch 426/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8486\n",
      "Epoch 426: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3355 - acc: 0.8488 - val_loss: 0.3912 - val_acc: 0.8470\n",
      "Epoch 427/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8403\n",
      "Epoch 427: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3432 - acc: 0.8406 - val_loss: 0.3521 - val_acc: 0.8506\n",
      "Epoch 428/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8423\n",
      "Epoch 428: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3520 - acc: 0.8423 - val_loss: 0.3566 - val_acc: 0.8304\n",
      "Epoch 429/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8409\n",
      "Epoch 429: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3449 - acc: 0.8416 - val_loss: 0.3770 - val_acc: 0.8494\n",
      "Epoch 430/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8434\n",
      "Epoch 430: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3407 - acc: 0.8442 - val_loss: 0.3579 - val_acc: 0.8571\n",
      "Epoch 431/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8493\n",
      "Epoch 431: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3289 - acc: 0.8495 - val_loss: 0.3354 - val_acc: 0.8688\n",
      "Epoch 432/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8435\n",
      "Epoch 432: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3390 - acc: 0.8435 - val_loss: 0.3314 - val_acc: 0.8599\n",
      "Epoch 433/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8479\n",
      "Epoch 433: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3394 - acc: 0.8479 - val_loss: 0.3617 - val_acc: 0.8494\n",
      "Epoch 434/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8435\n",
      "Epoch 434: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3337 - acc: 0.8437 - val_loss: 0.3551 - val_acc: 0.8603\n",
      "Epoch 435/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8467\n",
      "Epoch 435: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3434 - acc: 0.8470 - val_loss: 0.3547 - val_acc: 0.8421\n",
      "Epoch 436/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8456\n",
      "Epoch 436: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3399 - acc: 0.8456 - val_loss: 0.3553 - val_acc: 0.8551\n",
      "Epoch 437/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8410\n",
      "Epoch 437: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3507 - acc: 0.8413 - val_loss: 0.3688 - val_acc: 0.8433\n",
      "Epoch 438/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8436\n",
      "Epoch 438: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3446 - acc: 0.8436 - val_loss: 0.3768 - val_acc: 0.8332\n",
      "Epoch 439/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8495\n",
      "Epoch 439: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3332 - acc: 0.8495 - val_loss: 0.3308 - val_acc: 0.8680\n",
      "Epoch 440/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8439\n",
      "Epoch 440: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3445 - acc: 0.8439 - val_loss: 0.3519 - val_acc: 0.8664\n",
      "Epoch 441/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8410\n",
      "Epoch 441: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3458 - acc: 0.8409 - val_loss: 0.3557 - val_acc: 0.8551\n",
      "Epoch 442/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8391\n",
      "Epoch 442: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3522 - acc: 0.8393 - val_loss: 0.3719 - val_acc: 0.8551\n",
      "Epoch 443/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8419\n",
      "Epoch 443: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3487 - acc: 0.8425 - val_loss: 0.3448 - val_acc: 0.8628\n",
      "Epoch 444/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8428\n",
      "Epoch 444: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3321 - acc: 0.8434 - val_loss: 0.3486 - val_acc: 0.8587\n",
      "Epoch 445/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8434\n",
      "Epoch 445: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3383 - acc: 0.8439 - val_loss: 0.3635 - val_acc: 0.8538\n",
      "Epoch 446/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8438\n",
      "Epoch 446: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3330 - acc: 0.8441 - val_loss: 0.3369 - val_acc: 0.8676\n",
      "Epoch 447/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8458\n",
      "Epoch 447: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3435 - acc: 0.8463 - val_loss: 0.3710 - val_acc: 0.8619\n",
      "Epoch 448/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8433\n",
      "Epoch 448: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3471 - acc: 0.8437 - val_loss: 0.3616 - val_acc: 0.8486\n",
      "Epoch 449/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3473 - acc: 0.8475\n",
      "Epoch 449: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3473 - acc: 0.8475 - val_loss: 0.3245 - val_acc: 0.8619\n",
      "Epoch 450/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8389\n",
      "Epoch 450: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3469 - acc: 0.8389 - val_loss: 0.3699 - val_acc: 0.8219\n",
      "Epoch 451/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8387\n",
      "Epoch 451: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3428 - acc: 0.8393 - val_loss: 0.3615 - val_acc: 0.8571\n",
      "Epoch 452/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8505\n",
      "Epoch 452: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3396 - acc: 0.8510 - val_loss: 0.3343 - val_acc: 0.8534\n",
      "Epoch 453/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8458\n",
      "Epoch 453: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3368 - acc: 0.8460 - val_loss: 0.3405 - val_acc: 0.8603\n",
      "Epoch 454/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8449\n",
      "Epoch 454: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3382 - acc: 0.8449 - val_loss: 0.3495 - val_acc: 0.8401\n",
      "Epoch 455/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8476\n",
      "Epoch 455: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3399 - acc: 0.8478 - val_loss: 0.3378 - val_acc: 0.8563\n",
      "Epoch 456/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8434\n",
      "Epoch 456: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3446 - acc: 0.8442 - val_loss: 0.3438 - val_acc: 0.8510\n",
      "Epoch 457/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3375 - acc: 0.8470\n",
      "Epoch 457: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3365 - acc: 0.8476 - val_loss: 0.3268 - val_acc: 0.8648\n",
      "Epoch 458/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8473\n",
      "Epoch 458: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3463 - acc: 0.8476 - val_loss: 0.3403 - val_acc: 0.8534\n",
      "Epoch 459/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8464\n",
      "Epoch 459: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3421 - acc: 0.8465 - val_loss: 0.3616 - val_acc: 0.8393\n",
      "Epoch 460/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8464\n",
      "Epoch 460: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3428 - acc: 0.8464 - val_loss: 0.3403 - val_acc: 0.8595\n",
      "Epoch 461/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8426\n",
      "Epoch 461: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3370 - acc: 0.8432 - val_loss: 0.3623 - val_acc: 0.8413\n",
      "Epoch 462/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.8473\n",
      "Epoch 462: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3378 - acc: 0.8479 - val_loss: 0.3528 - val_acc: 0.8522\n",
      "Epoch 463/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8468\n",
      "Epoch 463: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3463 - acc: 0.8469 - val_loss: 0.3531 - val_acc: 0.8555\n",
      "Epoch 464/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8405\n",
      "Epoch 464: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3440 - acc: 0.8411 - val_loss: 0.3616 - val_acc: 0.8486\n",
      "Epoch 465/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3391 - acc: 0.8442\n",
      "Epoch 465: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3391 - acc: 0.8442 - val_loss: 0.3317 - val_acc: 0.8575\n",
      "Epoch 466/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8465\n",
      "Epoch 466: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3330 - acc: 0.8472 - val_loss: 0.3651 - val_acc: 0.8547\n",
      "Epoch 467/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3425 - acc: 0.8401\n",
      "Epoch 467: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3425 - acc: 0.8401 - val_loss: 0.3592 - val_acc: 0.8409\n",
      "Epoch 468/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8467\n",
      "Epoch 468: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3355 - acc: 0.8468 - val_loss: 0.3748 - val_acc: 0.8486\n",
      "Epoch 469/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8473\n",
      "Epoch 469: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3384 - acc: 0.8474 - val_loss: 0.3465 - val_acc: 0.8259\n",
      "Epoch 470/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8459\n",
      "Epoch 470: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3380 - acc: 0.8462 - val_loss: 0.3591 - val_acc: 0.8571\n",
      "Epoch 471/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8472\n",
      "Epoch 471: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3340 - acc: 0.8475 - val_loss: 0.3595 - val_acc: 0.8287\n",
      "Epoch 472/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3316 - acc: 0.8473\n",
      "Epoch 472: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3316 - acc: 0.8473 - val_loss: 0.3382 - val_acc: 0.8607\n",
      "Epoch 473/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8444\n",
      "Epoch 473: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3341 - acc: 0.8446 - val_loss: 0.3615 - val_acc: 0.8352\n",
      "Epoch 474/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8422\n",
      "Epoch 474: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3395 - acc: 0.8425 - val_loss: 0.3807 - val_acc: 0.8453\n",
      "Epoch 475/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8472\n",
      "Epoch 475: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3363 - acc: 0.8473 - val_loss: 0.3355 - val_acc: 0.8595\n",
      "Epoch 476/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8401\n",
      "Epoch 476: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3430 - acc: 0.8401 - val_loss: 0.3520 - val_acc: 0.8640\n",
      "Epoch 477/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8443\n",
      "Epoch 477: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3393 - acc: 0.8445 - val_loss: 0.3500 - val_acc: 0.8632\n",
      "Epoch 478/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8404\n",
      "Epoch 478: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3442 - acc: 0.8408 - val_loss: 0.3797 - val_acc: 0.8324\n",
      "Epoch 479/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8456\n",
      "Epoch 479: val_acc did not improve from 0.87085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3350 - acc: 0.8459 - val_loss: 0.3542 - val_acc: 0.8389\n",
      "Epoch 480/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8476\n",
      "Epoch 480: val_acc improved from 0.87085 to 0.87530, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/2/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3363 - acc: 0.8476 - val_loss: 0.3377 - val_acc: 0.8753\n",
      "Epoch 481/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8435\n",
      "Epoch 481: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3364 - acc: 0.8439 - val_loss: 0.3780 - val_acc: 0.8449\n",
      "Epoch 482/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8427\n",
      "Epoch 482: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3493 - acc: 0.8434 - val_loss: 0.3342 - val_acc: 0.8680\n",
      "Epoch 483/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8443\n",
      "Epoch 483: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3388 - acc: 0.8447 - val_loss: 0.3712 - val_acc: 0.8263\n",
      "Epoch 484/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8417\n",
      "Epoch 484: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3406 - acc: 0.8424 - val_loss: 0.3582 - val_acc: 0.8526\n",
      "Epoch 485/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8423\n",
      "Epoch 485: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3491 - acc: 0.8429 - val_loss: 0.3658 - val_acc: 0.8235\n",
      "Epoch 486/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3473 - acc: 0.8455\n",
      "Epoch 486: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3473 - acc: 0.8455 - val_loss: 0.3510 - val_acc: 0.8543\n",
      "Epoch 487/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.8479\n",
      "Epoch 487: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3335 - acc: 0.8480 - val_loss: 0.3443 - val_acc: 0.8551\n",
      "Epoch 488/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.8434\n",
      "Epoch 488: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3405 - acc: 0.8439 - val_loss: 0.3555 - val_acc: 0.8591\n",
      "Epoch 489/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8450\n",
      "Epoch 489: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3431 - acc: 0.8450 - val_loss: 0.3773 - val_acc: 0.8291\n",
      "Epoch 490/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3462 - acc: 0.8372\n",
      "Epoch 490: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3462 - acc: 0.8372 - val_loss: 0.3630 - val_acc: 0.8425\n",
      "Epoch 491/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8490\n",
      "Epoch 491: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3320 - acc: 0.8491 - val_loss: 0.3504 - val_acc: 0.8526\n",
      "Epoch 492/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8471\n",
      "Epoch 492: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3382 - acc: 0.8476 - val_loss: 0.3195 - val_acc: 0.8725\n",
      "Epoch 493/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.8487\n",
      "Epoch 493: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3332 - acc: 0.8487 - val_loss: 0.3268 - val_acc: 0.8599\n",
      "Epoch 494/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8484\n",
      "Epoch 494: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3368 - acc: 0.8487 - val_loss: 0.3469 - val_acc: 0.8563\n",
      "Epoch 495/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8400\n",
      "Epoch 495: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3413 - acc: 0.8402 - val_loss: 0.3810 - val_acc: 0.8368\n",
      "Epoch 496/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8433\n",
      "Epoch 496: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3478 - acc: 0.8436 - val_loss: 0.3595 - val_acc: 0.8466\n",
      "Epoch 497/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8409\n",
      "Epoch 497: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3386 - acc: 0.8412 - val_loss: 0.3589 - val_acc: 0.8672\n",
      "Epoch 498/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8458\n",
      "Epoch 498: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3349 - acc: 0.8460 - val_loss: 0.3502 - val_acc: 0.8510\n",
      "Epoch 499/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8462\n",
      "Epoch 499: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3313 - acc: 0.8466 - val_loss: 0.3484 - val_acc: 0.8514\n",
      "Epoch 500/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8478\n",
      "Epoch 500: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3402 - acc: 0.8479 - val_loss: 0.3597 - val_acc: 0.8433\n",
      "Epoch 501/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8456\n",
      "Epoch 501: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3392 - acc: 0.8459 - val_loss: 0.3508 - val_acc: 0.8603\n",
      "Epoch 502/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8470\n",
      "Epoch 502: val_acc did not improve from 0.87530\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3296 - acc: 0.8471 - val_loss: 0.3617 - val_acc: 0.8381\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 1.4126 - acc: 0.6360\n",
      "Epoch 1: val_acc improved from -inf to 0.67720, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1.4126 - acc: 0.6360 - val_loss: 0.6102 - val_acc: 0.6772\n",
      "Epoch 2/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.6235 - acc: 0.6909\n",
      "Epoch 2: val_acc improved from 0.67720 to 0.71284, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.6230 - acc: 0.6914 - val_loss: 0.5769 - val_acc: 0.7128\n",
      "Epoch 3/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.6008 - acc: 0.7077\n",
      "Epoch 3: val_acc improved from 0.71284 to 0.72215, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.6004 - acc: 0.7079 - val_loss: 0.5721 - val_acc: 0.7222\n",
      "Epoch 4/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.7144\n",
      "Epoch 4: val_acc did not improve from 0.72215\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5886 - acc: 0.7147 - val_loss: 0.5696 - val_acc: 0.7092\n",
      "Epoch 5/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.7203\n",
      "Epoch 5: val_acc improved from 0.72215 to 0.72823, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5800 - acc: 0.7203 - val_loss: 0.5597 - val_acc: 0.7282\n",
      "Epoch 6/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.7175\n",
      "Epoch 6: val_acc did not improve from 0.72823\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5783 - acc: 0.7177 - val_loss: 0.5789 - val_acc: 0.7096\n",
      "Epoch 7/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.7260\n",
      "Epoch 7: val_acc did not improve from 0.72823\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5686 - acc: 0.7259 - val_loss: 0.5642 - val_acc: 0.7120\n",
      "Epoch 8/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5599 - acc: 0.7314\n",
      "Epoch 8: val_acc improved from 0.72823 to 0.73876, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5600 - acc: 0.7313 - val_loss: 0.5396 - val_acc: 0.7388\n",
      "Epoch 9/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.7323\n",
      "Epoch 9: val_acc did not improve from 0.73876\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5537 - acc: 0.7322 - val_loss: 0.5400 - val_acc: 0.7319\n",
      "Epoch 10/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7315\n",
      "Epoch 10: val_acc did not improve from 0.73876\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5525 - acc: 0.7320 - val_loss: 0.5364 - val_acc: 0.7355\n",
      "Epoch 11/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5443 - acc: 0.7358\n",
      "Epoch 11: val_acc did not improve from 0.73876\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5443 - acc: 0.7358 - val_loss: 0.5414 - val_acc: 0.7217\n",
      "Epoch 12/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7386\n",
      "Epoch 12: val_acc improved from 0.73876 to 0.73917, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5358 - acc: 0.7394 - val_loss: 0.5201 - val_acc: 0.7392\n",
      "Epoch 13/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.7401\n",
      "Epoch 13: val_acc improved from 0.73917 to 0.74403, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5326 - acc: 0.7403 - val_loss: 0.5189 - val_acc: 0.7440\n",
      "Epoch 14/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.7470\n",
      "Epoch 14: val_acc did not improve from 0.74403\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5293 - acc: 0.7469 - val_loss: 0.5271 - val_acc: 0.7299\n",
      "Epoch 15/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.7502\n",
      "Epoch 15: val_acc improved from 0.74403 to 0.75051, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5232 - acc: 0.7504 - val_loss: 0.5098 - val_acc: 0.7505\n",
      "Epoch 16/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5233 - acc: 0.7549\n",
      "Epoch 16: val_acc did not improve from 0.75051\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5233 - acc: 0.7549 - val_loss: 0.5171 - val_acc: 0.7388\n",
      "Epoch 17/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.7596\n",
      "Epoch 17: val_acc improved from 0.75051 to 0.75456, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5104 - acc: 0.7595 - val_loss: 0.4939 - val_acc: 0.7546\n",
      "Epoch 18/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.7600\n",
      "Epoch 18: val_acc did not improve from 0.75456\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5114 - acc: 0.7603 - val_loss: 0.5018 - val_acc: 0.7465\n",
      "Epoch 19/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.7582\n",
      "Epoch 19: val_acc improved from 0.75456 to 0.75658, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5101 - acc: 0.7585 - val_loss: 0.4853 - val_acc: 0.7566\n",
      "Epoch 20/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.7567\n",
      "Epoch 20: val_acc did not improve from 0.75658\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5134 - acc: 0.7572 - val_loss: 0.4931 - val_acc: 0.7469\n",
      "Epoch 21/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.7606\n",
      "Epoch 21: val_acc did not improve from 0.75658\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5007 - acc: 0.7605 - val_loss: 0.5165 - val_acc: 0.7367\n",
      "Epoch 22/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.7634\n",
      "Epoch 22: val_acc improved from 0.75658 to 0.75820, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5002 - acc: 0.7634 - val_loss: 0.4815 - val_acc: 0.7582\n",
      "Epoch 23/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.7687\n",
      "Epoch 23: val_acc improved from 0.75820 to 0.76023, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4916 - acc: 0.7686 - val_loss: 0.4943 - val_acc: 0.7602\n",
      "Epoch 24/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.7695\n",
      "Epoch 24: val_acc did not improve from 0.76023\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4980 - acc: 0.7696 - val_loss: 0.4908 - val_acc: 0.7513\n",
      "Epoch 25/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.7671\n",
      "Epoch 25: val_acc did not improve from 0.76023\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4921 - acc: 0.7676 - val_loss: 0.4846 - val_acc: 0.7517\n",
      "Epoch 26/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.7730\n",
      "Epoch 26: val_acc improved from 0.76023 to 0.77116, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4821 - acc: 0.7728 - val_loss: 0.4663 - val_acc: 0.7712\n",
      "Epoch 27/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.7656\n",
      "Epoch 27: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4939 - acc: 0.7661 - val_loss: 0.5135 - val_acc: 0.7339\n",
      "Epoch 28/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.7737\n",
      "Epoch 28: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4846 - acc: 0.7742 - val_loss: 0.4721 - val_acc: 0.7598\n",
      "Epoch 29/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4897 - acc: 0.7692\n",
      "Epoch 29: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4897 - acc: 0.7693 - val_loss: 0.4684 - val_acc: 0.7667\n",
      "Epoch 30/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4802 - acc: 0.7723\n",
      "Epoch 30: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4796 - acc: 0.7727 - val_loss: 0.4595 - val_acc: 0.7659\n",
      "Epoch 31/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.7758\n",
      "Epoch 31: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4785 - acc: 0.7759 - val_loss: 0.4567 - val_acc: 0.7704\n",
      "Epoch 32/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.7757\n",
      "Epoch 32: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4664 - acc: 0.7760 - val_loss: 0.4770 - val_acc: 0.7550\n",
      "Epoch 33/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.7816\n",
      "Epoch 33: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4716 - acc: 0.7819 - val_loss: 0.4638 - val_acc: 0.7566\n",
      "Epoch 34/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4785 - acc: 0.7772\n",
      "Epoch 34: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4785 - acc: 0.7772 - val_loss: 0.4701 - val_acc: 0.7602\n",
      "Epoch 35/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.7816\n",
      "Epoch 35: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4645 - acc: 0.7819 - val_loss: 0.4598 - val_acc: 0.7606\n",
      "Epoch 36/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.7792\n",
      "Epoch 36: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4742 - acc: 0.7793 - val_loss: 0.4560 - val_acc: 0.7655\n",
      "Epoch 37/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4672 - acc: 0.7795\n",
      "Epoch 37: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4672 - acc: 0.7795 - val_loss: 0.4625 - val_acc: 0.7618\n",
      "Epoch 38/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.7818\n",
      "Epoch 38: val_acc did not improve from 0.77116\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4635 - acc: 0.7820 - val_loss: 0.4858 - val_acc: 0.7643\n",
      "Epoch 39/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.7795\n",
      "Epoch 39: val_acc improved from 0.77116 to 0.77238, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4691 - acc: 0.7797 - val_loss: 0.4608 - val_acc: 0.7724\n",
      "Epoch 40/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4651 - acc: 0.7868\n",
      "Epoch 40: val_acc improved from 0.77238 to 0.77886, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4651 - acc: 0.7868 - val_loss: 0.4403 - val_acc: 0.7789\n",
      "Epoch 41/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4654 - acc: 0.7835\n",
      "Epoch 41: val_acc did not improve from 0.77886\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4645 - acc: 0.7840 - val_loss: 0.4499 - val_acc: 0.7785\n",
      "Epoch 42/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4636 - acc: 0.7819\n",
      "Epoch 42: val_acc did not improve from 0.77886\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4636 - acc: 0.7819 - val_loss: 0.4714 - val_acc: 0.7606\n",
      "Epoch 43/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.7823\n",
      "Epoch 43: val_acc did not improve from 0.77886\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4660 - acc: 0.7823 - val_loss: 0.4610 - val_acc: 0.7631\n",
      "Epoch 44/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.7869\n",
      "Epoch 44: val_acc improved from 0.77886 to 0.78129, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4590 - acc: 0.7871 - val_loss: 0.4400 - val_acc: 0.7813\n",
      "Epoch 45/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.7869\n",
      "Epoch 45: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4602 - acc: 0.7873 - val_loss: 0.4517 - val_acc: 0.7712\n",
      "Epoch 46/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.7872\n",
      "Epoch 46: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4549 - acc: 0.7879 - val_loss: 0.4355 - val_acc: 0.7772\n",
      "Epoch 47/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.7908\n",
      "Epoch 47: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4531 - acc: 0.7908 - val_loss: 0.4455 - val_acc: 0.7768\n",
      "Epoch 48/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.7896\n",
      "Epoch 48: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4555 - acc: 0.7898 - val_loss: 0.4518 - val_acc: 0.7704\n",
      "Epoch 49/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.7915\n",
      "Epoch 49: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4496 - acc: 0.7916 - val_loss: 0.4609 - val_acc: 0.7586\n",
      "Epoch 50/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.7858\n",
      "Epoch 50: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4578 - acc: 0.7859 - val_loss: 0.4390 - val_acc: 0.7772\n",
      "Epoch 51/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7956\n",
      "Epoch 51: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4444 - acc: 0.7959 - val_loss: 0.4453 - val_acc: 0.7683\n",
      "Epoch 52/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.7931\n",
      "Epoch 52: val_acc did not improve from 0.78129\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4451 - acc: 0.7934 - val_loss: 0.4435 - val_acc: 0.7813\n",
      "Epoch 53/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.7914\n",
      "Epoch 53: val_acc improved from 0.78129 to 0.78453, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4467 - acc: 0.7915 - val_loss: 0.4282 - val_acc: 0.7845\n",
      "Epoch 54/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.7916\n",
      "Epoch 54: val_acc did not improve from 0.78453\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4453 - acc: 0.7917 - val_loss: 0.4339 - val_acc: 0.7756\n",
      "Epoch 55/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.7914\n",
      "Epoch 55: val_acc improved from 0.78453 to 0.78979, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4471 - acc: 0.7915 - val_loss: 0.4275 - val_acc: 0.7898\n",
      "Epoch 56/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.7962\n",
      "Epoch 56: val_acc did not improve from 0.78979\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4435 - acc: 0.7966 - val_loss: 0.4402 - val_acc: 0.7724\n",
      "Epoch 57/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8016\n",
      "Epoch 57: val_acc did not improve from 0.78979\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4344 - acc: 0.8017 - val_loss: 0.4369 - val_acc: 0.7712\n",
      "Epoch 58/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.8030\n",
      "Epoch 58: val_acc did not improve from 0.78979\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4362 - acc: 0.8039 - val_loss: 0.4372 - val_acc: 0.7797\n",
      "Epoch 59/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.7991\n",
      "Epoch 59: val_acc improved from 0.78979 to 0.79911, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4340 - acc: 0.7999 - val_loss: 0.4178 - val_acc: 0.7991\n",
      "Epoch 60/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.7948\n",
      "Epoch 60: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4381 - acc: 0.7952 - val_loss: 0.4329 - val_acc: 0.7748\n",
      "Epoch 61/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.7934\n",
      "Epoch 61: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4378 - acc: 0.7934 - val_loss: 0.4202 - val_acc: 0.7821\n",
      "Epoch 62/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4387 - acc: 0.7978\n",
      "Epoch 62: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4383 - acc: 0.7981 - val_loss: 0.4116 - val_acc: 0.7902\n",
      "Epoch 63/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.8006\n",
      "Epoch 63: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4404 - acc: 0.8010 - val_loss: 0.4322 - val_acc: 0.7785\n",
      "Epoch 64/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.7959\n",
      "Epoch 64: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4320 - acc: 0.7967 - val_loss: 0.4070 - val_acc: 0.7906\n",
      "Epoch 65/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8005\n",
      "Epoch 65: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4312 - acc: 0.8009 - val_loss: 0.4197 - val_acc: 0.7789\n",
      "Epoch 66/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8017\n",
      "Epoch 66: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4382 - acc: 0.8017 - val_loss: 0.4316 - val_acc: 0.7704\n",
      "Epoch 67/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4345 - acc: 0.7956\n",
      "Epoch 67: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4340 - acc: 0.7961 - val_loss: 0.4274 - val_acc: 0.7785\n",
      "Epoch 68/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.7997\n",
      "Epoch 68: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4307 - acc: 0.8001 - val_loss: 0.4234 - val_acc: 0.7987\n",
      "Epoch 69/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.7991\n",
      "Epoch 69: val_acc did not improve from 0.79911\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4338 - acc: 0.7991 - val_loss: 0.4309 - val_acc: 0.7724\n",
      "Epoch 70/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.8042\n",
      "Epoch 70: val_acc improved from 0.79911 to 0.80761, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4260 - acc: 0.8042 - val_loss: 0.4177 - val_acc: 0.8076\n",
      "Epoch 71/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8004\n",
      "Epoch 71: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4286 - acc: 0.8006 - val_loss: 0.4171 - val_acc: 0.7878\n",
      "Epoch 72/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.7995\n",
      "Epoch 72: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4307 - acc: 0.7998 - val_loss: 0.4455 - val_acc: 0.7720\n",
      "Epoch 73/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.7947\n",
      "Epoch 73: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4407 - acc: 0.7946 - val_loss: 0.4287 - val_acc: 0.7894\n",
      "Epoch 74/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.7972\n",
      "Epoch 74: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4345 - acc: 0.7977 - val_loss: 0.4220 - val_acc: 0.7870\n",
      "Epoch 75/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4322 - acc: 0.8053\n",
      "Epoch 75: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4317 - acc: 0.8058 - val_loss: 0.4128 - val_acc: 0.8019\n",
      "Epoch 76/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.7989\n",
      "Epoch 76: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4377 - acc: 0.7990 - val_loss: 0.4295 - val_acc: 0.7797\n",
      "Epoch 77/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4256 - acc: 0.8052\n",
      "Epoch 77: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4256 - acc: 0.8052 - val_loss: 0.4181 - val_acc: 0.7870\n",
      "Epoch 78/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8000\n",
      "Epoch 78: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4360 - acc: 0.8002 - val_loss: 0.4186 - val_acc: 0.7756\n",
      "Epoch 79/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.8033\n",
      "Epoch 79: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4270 - acc: 0.8038 - val_loss: 0.4224 - val_acc: 0.7849\n",
      "Epoch 80/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8067\n",
      "Epoch 80: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4236 - acc: 0.8071 - val_loss: 0.4160 - val_acc: 0.7821\n",
      "Epoch 81/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8028\n",
      "Epoch 81: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4281 - acc: 0.8031 - val_loss: 0.4335 - val_acc: 0.7699\n",
      "Epoch 82/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4250 - acc: 0.8033\n",
      "Epoch 82: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4244 - acc: 0.8037 - val_loss: 0.4100 - val_acc: 0.7942\n",
      "Epoch 83/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8023\n",
      "Epoch 83: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4258 - acc: 0.8027 - val_loss: 0.4079 - val_acc: 0.8064\n",
      "Epoch 84/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8022\n",
      "Epoch 84: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4340 - acc: 0.8027 - val_loss: 0.4283 - val_acc: 0.7772\n",
      "Epoch 85/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8072\n",
      "Epoch 85: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4280 - acc: 0.8071 - val_loss: 0.4267 - val_acc: 0.7882\n",
      "Epoch 86/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8094\n",
      "Epoch 86: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4182 - acc: 0.8100 - val_loss: 0.4046 - val_acc: 0.8040\n",
      "Epoch 87/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8099\n",
      "Epoch 87: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4132 - acc: 0.8102 - val_loss: 0.4126 - val_acc: 0.7902\n",
      "Epoch 88/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8011\n",
      "Epoch 88: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4189 - acc: 0.8017 - val_loss: 0.4101 - val_acc: 0.7930\n",
      "Epoch 89/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8102\n",
      "Epoch 89: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4143 - acc: 0.8102 - val_loss: 0.4108 - val_acc: 0.7914\n",
      "Epoch 90/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4238 - acc: 0.8052\n",
      "Epoch 90: val_acc improved from 0.80761 to 0.81045, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4232 - acc: 0.8054 - val_loss: 0.3902 - val_acc: 0.8104\n",
      "Epoch 91/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4179 - acc: 0.8050\n",
      "Epoch 91: val_acc improved from 0.81045 to 0.82625, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4179 - acc: 0.8050 - val_loss: 0.3976 - val_acc: 0.8262\n",
      "Epoch 92/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8045\n",
      "Epoch 92: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4260 - acc: 0.8045 - val_loss: 0.4169 - val_acc: 0.7914\n",
      "Epoch 93/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8068\n",
      "Epoch 93: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4173 - acc: 0.8071 - val_loss: 0.4040 - val_acc: 0.7959\n",
      "Epoch 94/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4167 - acc: 0.8086\n",
      "Epoch 94: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4167 - acc: 0.8086 - val_loss: 0.4219 - val_acc: 0.7801\n",
      "Epoch 95/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8087\n",
      "Epoch 95: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4136 - acc: 0.8088 - val_loss: 0.4085 - val_acc: 0.7942\n",
      "Epoch 96/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4207 - acc: 0.8063\n",
      "Epoch 96: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4207 - acc: 0.8063 - val_loss: 0.3924 - val_acc: 0.8011\n",
      "Epoch 97/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8075\n",
      "Epoch 97: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4177 - acc: 0.8079 - val_loss: 0.3943 - val_acc: 0.8125\n",
      "Epoch 98/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8068\n",
      "Epoch 98: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4150 - acc: 0.8075 - val_loss: 0.4110 - val_acc: 0.7955\n",
      "Epoch 99/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8099\n",
      "Epoch 99: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4089 - acc: 0.8099 - val_loss: 0.3856 - val_acc: 0.8141\n",
      "Epoch 100/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8108\n",
      "Epoch 100: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4095 - acc: 0.8111 - val_loss: 0.4104 - val_acc: 0.7938\n",
      "Epoch 101/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8096\n",
      "Epoch 101: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4102 - acc: 0.8098 - val_loss: 0.3980 - val_acc: 0.8084\n",
      "Epoch 102/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8095\n",
      "Epoch 102: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4120 - acc: 0.8097 - val_loss: 0.4095 - val_acc: 0.8060\n",
      "Epoch 103/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.8079\n",
      "Epoch 103: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4085 - acc: 0.8081 - val_loss: 0.4066 - val_acc: 0.8092\n",
      "Epoch 104/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8135\n",
      "Epoch 104: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.4152 - acc: 0.8138 - val_loss: 0.4119 - val_acc: 0.7963\n",
      "Epoch 105/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8118\n",
      "Epoch 105: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4188 - acc: 0.8119 - val_loss: 0.3832 - val_acc: 0.8202\n",
      "Epoch 106/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8129\n",
      "Epoch 106: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4083 - acc: 0.8136 - val_loss: 0.3975 - val_acc: 0.8096\n",
      "Epoch 107/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8120\n",
      "Epoch 107: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4108 - acc: 0.8120 - val_loss: 0.4122 - val_acc: 0.7930\n",
      "Epoch 108/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8104\n",
      "Epoch 108: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4099 - acc: 0.8106 - val_loss: 0.4076 - val_acc: 0.7955\n",
      "Epoch 109/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8110\n",
      "Epoch 109: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4124 - acc: 0.8112 - val_loss: 0.3892 - val_acc: 0.8084\n",
      "Epoch 110/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8162\n",
      "Epoch 110: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4101 - acc: 0.8161 - val_loss: 0.4002 - val_acc: 0.7959\n",
      "Epoch 111/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8127\n",
      "Epoch 111: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4156 - acc: 0.8129 - val_loss: 0.4053 - val_acc: 0.7898\n",
      "Epoch 112/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8176\n",
      "Epoch 112: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4053 - acc: 0.8177 - val_loss: 0.3995 - val_acc: 0.8011\n",
      "Epoch 113/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8121\n",
      "Epoch 113: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4077 - acc: 0.8125 - val_loss: 0.3983 - val_acc: 0.7963\n",
      "Epoch 114/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8119\n",
      "Epoch 114: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4092 - acc: 0.8120 - val_loss: 0.3819 - val_acc: 0.8125\n",
      "Epoch 115/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4058 - acc: 0.8097\n",
      "Epoch 115: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4058 - acc: 0.8097 - val_loss: 0.4247 - val_acc: 0.7853\n",
      "Epoch 116/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8175\n",
      "Epoch 116: val_acc did not improve from 0.82625\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4074 - acc: 0.8176 - val_loss: 0.4252 - val_acc: 0.7866\n",
      "Epoch 117/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8120\n",
      "Epoch 117: val_acc improved from 0.82625 to 0.83840, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4041 - acc: 0.8124 - val_loss: 0.3920 - val_acc: 0.8384\n",
      "Epoch 118/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8179\n",
      "Epoch 118: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3969 - acc: 0.8179 - val_loss: 0.3930 - val_acc: 0.8117\n",
      "Epoch 119/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8141\n",
      "Epoch 119: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3995 - acc: 0.8149 - val_loss: 0.3921 - val_acc: 0.8222\n",
      "Epoch 120/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8208\n",
      "Epoch 120: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3949 - acc: 0.8209 - val_loss: 0.4181 - val_acc: 0.7926\n",
      "Epoch 121/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8195\n",
      "Epoch 121: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3971 - acc: 0.8200 - val_loss: 0.3887 - val_acc: 0.7995\n",
      "Epoch 122/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8126\n",
      "Epoch 122: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3964 - acc: 0.8127 - val_loss: 0.3917 - val_acc: 0.8088\n",
      "Epoch 123/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8138\n",
      "Epoch 123: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4039 - acc: 0.8144 - val_loss: 0.4068 - val_acc: 0.8133\n",
      "Epoch 124/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8154\n",
      "Epoch 124: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4022 - acc: 0.8158 - val_loss: 0.3844 - val_acc: 0.8133\n",
      "Epoch 125/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8161\n",
      "Epoch 125: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3990 - acc: 0.8170 - val_loss: 0.3939 - val_acc: 0.8032\n",
      "Epoch 126/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8131\n",
      "Epoch 126: val_acc did not improve from 0.83840\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4028 - acc: 0.8139 - val_loss: 0.3871 - val_acc: 0.8198\n",
      "Epoch 127/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8170\n",
      "Epoch 127: val_acc improved from 0.83840 to 0.84123, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4016 - acc: 0.8182 - val_loss: 0.3767 - val_acc: 0.8412\n",
      "Epoch 128/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8167\n",
      "Epoch 128: val_acc did not improve from 0.84123\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3998 - acc: 0.8167 - val_loss: 0.3833 - val_acc: 0.8271\n",
      "Epoch 129/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8179\n",
      "Epoch 129: val_acc did not improve from 0.84123\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4047 - acc: 0.8180 - val_loss: 0.3903 - val_acc: 0.8113\n",
      "Epoch 130/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8170\n",
      "Epoch 130: val_acc did not improve from 0.84123\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4023 - acc: 0.8174 - val_loss: 0.4023 - val_acc: 0.7930\n",
      "Epoch 131/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8263\n",
      "Epoch 131: val_acc did not improve from 0.84123\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3957 - acc: 0.8263 - val_loss: 0.3950 - val_acc: 0.8214\n",
      "Epoch 132/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8213\n",
      "Epoch 132: val_acc improved from 0.84123 to 0.84650, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3951 - acc: 0.8215 - val_loss: 0.3690 - val_acc: 0.8465\n",
      "Epoch 133/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8138\n",
      "Epoch 133: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4026 - acc: 0.8143 - val_loss: 0.3812 - val_acc: 0.8271\n",
      "Epoch 134/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.8154\n",
      "Epoch 134: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3976 - acc: 0.8158 - val_loss: 0.3807 - val_acc: 0.8194\n",
      "Epoch 135/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4020 - acc: 0.8141\n",
      "Epoch 135: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4020 - acc: 0.8141 - val_loss: 0.3715 - val_acc: 0.8441\n",
      "Epoch 136/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8200\n",
      "Epoch 136: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3997 - acc: 0.8205 - val_loss: 0.4034 - val_acc: 0.7979\n",
      "Epoch 137/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8185\n",
      "Epoch 137: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3985 - acc: 0.8185 - val_loss: 0.3811 - val_acc: 0.8214\n",
      "Epoch 138/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8207\n",
      "Epoch 138: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3932 - acc: 0.8211 - val_loss: 0.3687 - val_acc: 0.8416\n",
      "Epoch 139/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8193\n",
      "Epoch 139: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3897 - acc: 0.8196 - val_loss: 0.3877 - val_acc: 0.8121\n",
      "Epoch 140/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8227\n",
      "Epoch 140: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3882 - acc: 0.8229 - val_loss: 0.4129 - val_acc: 0.8169\n",
      "Epoch 141/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8190\n",
      "Epoch 141: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4016 - acc: 0.8195 - val_loss: 0.3960 - val_acc: 0.8056\n",
      "Epoch 142/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8107\n",
      "Epoch 142: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4004 - acc: 0.8111 - val_loss: 0.3857 - val_acc: 0.8271\n",
      "Epoch 143/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4043 - acc: 0.8172\n",
      "Epoch 143: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4043 - acc: 0.8172 - val_loss: 0.3814 - val_acc: 0.8206\n",
      "Epoch 144/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8167\n",
      "Epoch 144: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4040 - acc: 0.8173 - val_loss: 0.4088 - val_acc: 0.8060\n",
      "Epoch 145/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8157\n",
      "Epoch 145: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3957 - acc: 0.8163 - val_loss: 0.3995 - val_acc: 0.7975\n",
      "Epoch 146/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8184\n",
      "Epoch 146: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3960 - acc: 0.8187 - val_loss: 0.3949 - val_acc: 0.8064\n",
      "Epoch 147/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8192\n",
      "Epoch 147: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3964 - acc: 0.8196 - val_loss: 0.3914 - val_acc: 0.8080\n",
      "Epoch 148/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8199\n",
      "Epoch 148: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3896 - acc: 0.8200 - val_loss: 0.3835 - val_acc: 0.8096\n",
      "Epoch 149/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8218\n",
      "Epoch 149: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3926 - acc: 0.8218 - val_loss: 0.3935 - val_acc: 0.8420\n",
      "Epoch 150/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8197\n",
      "Epoch 150: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3901 - acc: 0.8197 - val_loss: 0.3980 - val_acc: 0.8044\n",
      "Epoch 151/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8222\n",
      "Epoch 151: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3915 - acc: 0.8223 - val_loss: 0.3796 - val_acc: 0.8149\n",
      "Epoch 152/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8192\n",
      "Epoch 152: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3956 - acc: 0.8196 - val_loss: 0.3991 - val_acc: 0.8088\n",
      "Epoch 153/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8165\n",
      "Epoch 153: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3979 - acc: 0.8169 - val_loss: 0.4046 - val_acc: 0.8165\n",
      "Epoch 154/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8194\n",
      "Epoch 154: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3908 - acc: 0.8196 - val_loss: 0.3718 - val_acc: 0.8267\n",
      "Epoch 155/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8227\n",
      "Epoch 155: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3876 - acc: 0.8228 - val_loss: 0.3873 - val_acc: 0.8007\n",
      "Epoch 156/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8210\n",
      "Epoch 156: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3825 - acc: 0.8211 - val_loss: 0.3879 - val_acc: 0.8157\n",
      "Epoch 157/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8165\n",
      "Epoch 157: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3965 - acc: 0.8164 - val_loss: 0.3946 - val_acc: 0.8084\n",
      "Epoch 158/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8193\n",
      "Epoch 158: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3921 - acc: 0.8196 - val_loss: 0.3827 - val_acc: 0.8076\n",
      "Epoch 159/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8235\n",
      "Epoch 159: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3895 - acc: 0.8236 - val_loss: 0.3893 - val_acc: 0.8048\n",
      "Epoch 160/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8170\n",
      "Epoch 160: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3942 - acc: 0.8175 - val_loss: 0.3848 - val_acc: 0.8023\n",
      "Epoch 161/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3895 - acc: 0.8198\n",
      "Epoch 161: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3895 - acc: 0.8198 - val_loss: 0.3790 - val_acc: 0.8109\n",
      "Epoch 162/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8199\n",
      "Epoch 162: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3979 - acc: 0.8203 - val_loss: 0.3805 - val_acc: 0.8153\n",
      "Epoch 163/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8218\n",
      "Epoch 163: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3864 - acc: 0.8225 - val_loss: 0.3903 - val_acc: 0.8113\n",
      "Epoch 164/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8254\n",
      "Epoch 164: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3889 - acc: 0.8255 - val_loss: 0.3785 - val_acc: 0.8218\n",
      "Epoch 165/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8232\n",
      "Epoch 165: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3899 - acc: 0.8233 - val_loss: 0.3843 - val_acc: 0.8133\n",
      "Epoch 166/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8251\n",
      "Epoch 166: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3874 - acc: 0.8255 - val_loss: 0.3829 - val_acc: 0.8198\n",
      "Epoch 167/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8252\n",
      "Epoch 167: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3825 - acc: 0.8254 - val_loss: 0.3774 - val_acc: 0.8348\n",
      "Epoch 168/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8231\n",
      "Epoch 168: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3859 - acc: 0.8231 - val_loss: 0.4139 - val_acc: 0.7861\n",
      "Epoch 169/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3883 - acc: 0.8236\n",
      "Epoch 169: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3883 - acc: 0.8236 - val_loss: 0.3918 - val_acc: 0.8104\n",
      "Epoch 170/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3855 - acc: 0.8269\n",
      "Epoch 170: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3855 - acc: 0.8269 - val_loss: 0.3805 - val_acc: 0.8190\n",
      "Epoch 171/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8224\n",
      "Epoch 171: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3874 - acc: 0.8225 - val_loss: 0.3845 - val_acc: 0.8121\n",
      "Epoch 172/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8234\n",
      "Epoch 172: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3859 - acc: 0.8236 - val_loss: 0.3882 - val_acc: 0.8177\n",
      "Epoch 173/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8253\n",
      "Epoch 173: val_acc did not improve from 0.84650\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3878 - acc: 0.8253 - val_loss: 0.3898 - val_acc: 0.8040\n",
      "Epoch 174/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8248\n",
      "Epoch 174: val_acc improved from 0.84650 to 0.85176, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3866 - acc: 0.8257 - val_loss: 0.3602 - val_acc: 0.8518\n",
      "Epoch 175/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8208\n",
      "Epoch 175: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3941 - acc: 0.8212 - val_loss: 0.3994 - val_acc: 0.7983\n",
      "Epoch 176/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8286\n",
      "Epoch 176: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3751 - acc: 0.8288 - val_loss: 0.3964 - val_acc: 0.7987\n",
      "Epoch 177/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8238\n",
      "Epoch 177: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3823 - acc: 0.8247 - val_loss: 0.3745 - val_acc: 0.8392\n",
      "Epoch 178/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8197\n",
      "Epoch 178: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3853 - acc: 0.8198 - val_loss: 0.3827 - val_acc: 0.8169\n",
      "Epoch 179/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8274\n",
      "Epoch 179: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3798 - acc: 0.8276 - val_loss: 0.3933 - val_acc: 0.8044\n",
      "Epoch 180/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8276\n",
      "Epoch 180: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3807 - acc: 0.8277 - val_loss: 0.3850 - val_acc: 0.8129\n",
      "Epoch 181/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3847 - acc: 0.8262\n",
      "Epoch 181: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3847 - acc: 0.8262 - val_loss: 0.3946 - val_acc: 0.8149\n",
      "Epoch 182/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8268\n",
      "Epoch 182: val_acc did not improve from 0.85176\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3894 - acc: 0.8269 - val_loss: 0.3853 - val_acc: 0.8303\n",
      "Epoch 183/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8240\n",
      "Epoch 183: val_acc improved from 0.85176 to 0.85338, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3822 - acc: 0.8246 - val_loss: 0.3539 - val_acc: 0.8534\n",
      "Epoch 184/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8275\n",
      "Epoch 184: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3838 - acc: 0.8279 - val_loss: 0.3855 - val_acc: 0.8234\n",
      "Epoch 185/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8220\n",
      "Epoch 185: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3904 - acc: 0.8224 - val_loss: 0.3694 - val_acc: 0.8291\n",
      "Epoch 186/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8249\n",
      "Epoch 186: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3881 - acc: 0.8255 - val_loss: 0.3804 - val_acc: 0.8141\n",
      "Epoch 187/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8246\n",
      "Epoch 187: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3851 - acc: 0.8245 - val_loss: 0.3678 - val_acc: 0.8453\n",
      "Epoch 188/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8251\n",
      "Epoch 188: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3845 - acc: 0.8261 - val_loss: 0.3936 - val_acc: 0.8023\n",
      "Epoch 189/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8227\n",
      "Epoch 189: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3844 - acc: 0.8228 - val_loss: 0.4263 - val_acc: 0.8072\n",
      "Epoch 190/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3836 - acc: 0.8235\n",
      "Epoch 190: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3836 - acc: 0.8235 - val_loss: 0.3725 - val_acc: 0.8275\n",
      "Epoch 191/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8293\n",
      "Epoch 191: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3762 - acc: 0.8292 - val_loss: 0.3787 - val_acc: 0.8283\n",
      "Epoch 192/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8220\n",
      "Epoch 192: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3904 - acc: 0.8220 - val_loss: 0.3742 - val_acc: 0.8194\n",
      "Epoch 193/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8203\n",
      "Epoch 193: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3832 - acc: 0.8203 - val_loss: 0.3735 - val_acc: 0.8230\n",
      "Epoch 194/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8272\n",
      "Epoch 194: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3781 - acc: 0.8275 - val_loss: 0.3800 - val_acc: 0.8104\n",
      "Epoch 195/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3834 - acc: 0.8256\n",
      "Epoch 195: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3829 - acc: 0.8260 - val_loss: 0.3748 - val_acc: 0.8335\n",
      "Epoch 196/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8301\n",
      "Epoch 196: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3782 - acc: 0.8303 - val_loss: 0.3853 - val_acc: 0.8194\n",
      "Epoch 197/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8247\n",
      "Epoch 197: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3851 - acc: 0.8244 - val_loss: 0.4092 - val_acc: 0.8109\n",
      "Epoch 198/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8221\n",
      "Epoch 198: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3902 - acc: 0.8229 - val_loss: 0.3831 - val_acc: 0.8109\n",
      "Epoch 199/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8256\n",
      "Epoch 199: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3788 - acc: 0.8256 - val_loss: 0.3861 - val_acc: 0.8060\n",
      "Epoch 200/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3869 - acc: 0.8225\n",
      "Epoch 200: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3869 - acc: 0.8225 - val_loss: 0.3906 - val_acc: 0.8165\n",
      "Epoch 201/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8234\n",
      "Epoch 201: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3818 - acc: 0.8233 - val_loss: 0.4012 - val_acc: 0.8145\n",
      "Epoch 202/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8253\n",
      "Epoch 202: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3840 - acc: 0.8254 - val_loss: 0.3699 - val_acc: 0.8254\n",
      "Epoch 203/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8214\n",
      "Epoch 203: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3871 - acc: 0.8219 - val_loss: 0.3899 - val_acc: 0.8185\n",
      "Epoch 204/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3757 - acc: 0.8319\n",
      "Epoch 204: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3757 - acc: 0.8319 - val_loss: 0.3777 - val_acc: 0.8165\n",
      "Epoch 205/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8277\n",
      "Epoch 205: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3758 - acc: 0.8284 - val_loss: 0.3754 - val_acc: 0.8165\n",
      "Epoch 206/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8276\n",
      "Epoch 206: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3786 - acc: 0.8276 - val_loss: 0.3739 - val_acc: 0.8335\n",
      "Epoch 207/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8298\n",
      "Epoch 207: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3789 - acc: 0.8299 - val_loss: 0.3995 - val_acc: 0.8185\n",
      "Epoch 208/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8252\n",
      "Epoch 208: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3834 - acc: 0.8259 - val_loss: 0.3617 - val_acc: 0.8299\n",
      "Epoch 209/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8242\n",
      "Epoch 209: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3903 - acc: 0.8244 - val_loss: 0.3920 - val_acc: 0.7975\n",
      "Epoch 210/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8256\n",
      "Epoch 210: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3801 - acc: 0.8256 - val_loss: 0.3839 - val_acc: 0.8072\n",
      "Epoch 211/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8294\n",
      "Epoch 211: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3786 - acc: 0.8294 - val_loss: 0.3694 - val_acc: 0.8262\n",
      "Epoch 212/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8316\n",
      "Epoch 212: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3745 - acc: 0.8322 - val_loss: 0.3673 - val_acc: 0.8420\n",
      "Epoch 213/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8304\n",
      "Epoch 213: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3755 - acc: 0.8308 - val_loss: 0.3784 - val_acc: 0.8234\n",
      "Epoch 214/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8271\n",
      "Epoch 214: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3764 - acc: 0.8275 - val_loss: 0.3706 - val_acc: 0.8246\n",
      "Epoch 215/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8245\n",
      "Epoch 215: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3810 - acc: 0.8251 - val_loss: 0.3550 - val_acc: 0.8404\n",
      "Epoch 216/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8245\n",
      "Epoch 216: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3783 - acc: 0.8248 - val_loss: 0.3849 - val_acc: 0.8096\n",
      "Epoch 217/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8271\n",
      "Epoch 217: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3792 - acc: 0.8275 - val_loss: 0.3620 - val_acc: 0.8404\n",
      "Epoch 218/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8293\n",
      "Epoch 218: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3770 - acc: 0.8298 - val_loss: 0.3511 - val_acc: 0.8408\n",
      "Epoch 219/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8296\n",
      "Epoch 219: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3715 - acc: 0.8296 - val_loss: 0.3720 - val_acc: 0.8218\n",
      "Epoch 220/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8242\n",
      "Epoch 220: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3841 - acc: 0.8246 - val_loss: 0.3696 - val_acc: 0.8303\n",
      "Epoch 221/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8344\n",
      "Epoch 221: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3644 - acc: 0.8347 - val_loss: 0.3698 - val_acc: 0.8246\n",
      "Epoch 222/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3701 - acc: 0.8356\n",
      "Epoch 222: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3701 - acc: 0.8356 - val_loss: 0.3887 - val_acc: 0.8088\n",
      "Epoch 223/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8317\n",
      "Epoch 223: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3740 - acc: 0.8319 - val_loss: 0.3703 - val_acc: 0.8181\n",
      "Epoch 224/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8317\n",
      "Epoch 224: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3704 - acc: 0.8318 - val_loss: 0.3675 - val_acc: 0.8258\n",
      "Epoch 225/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8279\n",
      "Epoch 225: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3739 - acc: 0.8282 - val_loss: 0.3746 - val_acc: 0.8303\n",
      "Epoch 226/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.8316\n",
      "Epoch 226: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3735 - acc: 0.8314 - val_loss: 0.3818 - val_acc: 0.8044\n",
      "Epoch 227/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8289\n",
      "Epoch 227: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3783 - acc: 0.8291 - val_loss: 0.3710 - val_acc: 0.8404\n",
      "Epoch 228/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8302\n",
      "Epoch 228: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3747 - acc: 0.8304 - val_loss: 0.3861 - val_acc: 0.8068\n",
      "Epoch 229/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8272\n",
      "Epoch 229: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3753 - acc: 0.8275 - val_loss: 0.3709 - val_acc: 0.8198\n",
      "Epoch 230/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8306\n",
      "Epoch 230: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3631 - acc: 0.8306 - val_loss: 0.3578 - val_acc: 0.8416\n",
      "Epoch 231/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8214\n",
      "Epoch 231: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3846 - acc: 0.8221 - val_loss: 0.3622 - val_acc: 0.8319\n",
      "Epoch 232/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8327\n",
      "Epoch 232: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3751 - acc: 0.8334 - val_loss: 0.3705 - val_acc: 0.8424\n",
      "Epoch 233/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8307\n",
      "Epoch 233: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3749 - acc: 0.8307 - val_loss: 0.3810 - val_acc: 0.8068\n",
      "Epoch 234/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8271\n",
      "Epoch 234: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3798 - acc: 0.8277 - val_loss: 0.3904 - val_acc: 0.8032\n",
      "Epoch 235/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3737 - acc: 0.8307\n",
      "Epoch 235: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3737 - acc: 0.8307 - val_loss: 0.3780 - val_acc: 0.8295\n",
      "Epoch 236/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8301\n",
      "Epoch 236: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3694 - acc: 0.8304 - val_loss: 0.3544 - val_acc: 0.8457\n",
      "Epoch 237/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8333\n",
      "Epoch 237: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3677 - acc: 0.8341 - val_loss: 0.3495 - val_acc: 0.8420\n",
      "Epoch 238/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8288\n",
      "Epoch 238: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3706 - acc: 0.8293 - val_loss: 0.3710 - val_acc: 0.8206\n",
      "Epoch 239/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8273\n",
      "Epoch 239: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3729 - acc: 0.8279 - val_loss: 0.3638 - val_acc: 0.8222\n",
      "Epoch 240/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8291\n",
      "Epoch 240: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3695 - acc: 0.8291 - val_loss: 0.3629 - val_acc: 0.8372\n",
      "Epoch 241/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8290\n",
      "Epoch 241: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3796 - acc: 0.8295 - val_loss: 0.3696 - val_acc: 0.8408\n",
      "Epoch 242/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8268\n",
      "Epoch 242: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3734 - acc: 0.8272 - val_loss: 0.3840 - val_acc: 0.8161\n",
      "Epoch 243/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8333\n",
      "Epoch 243: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3705 - acc: 0.8340 - val_loss: 0.3677 - val_acc: 0.8238\n",
      "Epoch 244/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8304\n",
      "Epoch 244: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3726 - acc: 0.8303 - val_loss: 0.3632 - val_acc: 0.8198\n",
      "Epoch 245/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3734 - acc: 0.8275\n",
      "Epoch 245: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3734 - acc: 0.8275 - val_loss: 0.3698 - val_acc: 0.8157\n",
      "Epoch 246/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8265\n",
      "Epoch 246: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3723 - acc: 0.8271 - val_loss: 0.3765 - val_acc: 0.8133\n",
      "Epoch 247/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3725 - acc: 0.8290\n",
      "Epoch 247: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3725 - acc: 0.8290 - val_loss: 0.3653 - val_acc: 0.8262\n",
      "Epoch 248/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8225\n",
      "Epoch 248: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3808 - acc: 0.8226 - val_loss: 0.3685 - val_acc: 0.8267\n",
      "Epoch 249/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8329\n",
      "Epoch 249: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3670 - acc: 0.8334 - val_loss: 0.3769 - val_acc: 0.8218\n",
      "Epoch 250/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8374\n",
      "Epoch 250: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3666 - acc: 0.8376 - val_loss: 0.3611 - val_acc: 0.8279\n",
      "Epoch 251/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8305\n",
      "Epoch 251: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3784 - acc: 0.8306 - val_loss: 0.3732 - val_acc: 0.8364\n",
      "Epoch 252/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8366\n",
      "Epoch 252: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3691 - acc: 0.8373 - val_loss: 0.3794 - val_acc: 0.8315\n",
      "Epoch 253/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.8340\n",
      "Epoch 253: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3681 - acc: 0.8344 - val_loss: 0.3633 - val_acc: 0.8348\n",
      "Epoch 254/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8336\n",
      "Epoch 254: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3803 - acc: 0.8338 - val_loss: 0.3680 - val_acc: 0.8343\n",
      "Epoch 255/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3686 - acc: 0.8357\n",
      "Epoch 255: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3682 - acc: 0.8360 - val_loss: 0.3554 - val_acc: 0.8416\n",
      "Epoch 256/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8309\n",
      "Epoch 256: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3730 - acc: 0.8312 - val_loss: 0.3746 - val_acc: 0.8222\n",
      "Epoch 257/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8328\n",
      "Epoch 257: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3698 - acc: 0.8331 - val_loss: 0.3724 - val_acc: 0.8190\n",
      "Epoch 258/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8322\n",
      "Epoch 258: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3720 - acc: 0.8326 - val_loss: 0.3854 - val_acc: 0.8380\n",
      "Epoch 259/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8385\n",
      "Epoch 259: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3618 - acc: 0.8386 - val_loss: 0.3766 - val_acc: 0.8275\n",
      "Epoch 260/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8283\n",
      "Epoch 260: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3730 - acc: 0.8285 - val_loss: 0.3715 - val_acc: 0.8190\n",
      "Epoch 261/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8309\n",
      "Epoch 261: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3662 - acc: 0.8314 - val_loss: 0.3674 - val_acc: 0.8145\n",
      "Epoch 262/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8326\n",
      "Epoch 262: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3675 - acc: 0.8329 - val_loss: 0.3554 - val_acc: 0.8295\n",
      "Epoch 263/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8335\n",
      "Epoch 263: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3646 - acc: 0.8337 - val_loss: 0.3723 - val_acc: 0.8157\n",
      "Epoch 264/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8293\n",
      "Epoch 264: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3726 - acc: 0.8297 - val_loss: 0.3622 - val_acc: 0.8424\n",
      "Epoch 265/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8313\n",
      "Epoch 265: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3707 - acc: 0.8323 - val_loss: 0.3576 - val_acc: 0.8441\n",
      "Epoch 266/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8253\n",
      "Epoch 266: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3786 - acc: 0.8258 - val_loss: 0.3564 - val_acc: 0.8404\n",
      "Epoch 267/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8324\n",
      "Epoch 267: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3676 - acc: 0.8325 - val_loss: 0.3707 - val_acc: 0.8364\n",
      "Epoch 268/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8279\n",
      "Epoch 268: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3728 - acc: 0.8281 - val_loss: 0.3681 - val_acc: 0.8388\n",
      "Epoch 269/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8312\n",
      "Epoch 269: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3701 - acc: 0.8315 - val_loss: 0.3673 - val_acc: 0.8258\n",
      "Epoch 270/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8376\n",
      "Epoch 270: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3622 - acc: 0.8379 - val_loss: 0.3390 - val_acc: 0.8485\n",
      "Epoch 271/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8297\n",
      "Epoch 271: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3680 - acc: 0.8297 - val_loss: 0.3773 - val_acc: 0.8056\n",
      "Epoch 272/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8293\n",
      "Epoch 272: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3725 - acc: 0.8294 - val_loss: 0.3612 - val_acc: 0.8283\n",
      "Epoch 273/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8340\n",
      "Epoch 273: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3656 - acc: 0.8340 - val_loss: 0.3600 - val_acc: 0.8331\n",
      "Epoch 274/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8317\n",
      "Epoch 274: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3718 - acc: 0.8324 - val_loss: 0.3846 - val_acc: 0.8072\n",
      "Epoch 275/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8307\n",
      "Epoch 275: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3713 - acc: 0.8307 - val_loss: 0.3678 - val_acc: 0.8429\n",
      "Epoch 276/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8294\n",
      "Epoch 276: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3689 - acc: 0.8296 - val_loss: 0.3693 - val_acc: 0.8327\n",
      "Epoch 277/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8387\n",
      "Epoch 277: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3637 - acc: 0.8387 - val_loss: 0.3749 - val_acc: 0.8190\n",
      "Epoch 278/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8358\n",
      "Epoch 278: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3663 - acc: 0.8360 - val_loss: 0.3853 - val_acc: 0.8230\n",
      "Epoch 279/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8293\n",
      "Epoch 279: val_acc did not improve from 0.85338\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3680 - acc: 0.8297 - val_loss: 0.3864 - val_acc: 0.8125\n",
      "Epoch 280/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8342\n",
      "Epoch 280: val_acc improved from 0.85338 to 0.86270, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3661 - acc: 0.8348 - val_loss: 0.3415 - val_acc: 0.8627\n",
      "Epoch 281/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8370\n",
      "Epoch 281: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3633 - acc: 0.8371 - val_loss: 0.3727 - val_acc: 0.8234\n",
      "Epoch 282/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8339\n",
      "Epoch 282: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3750 - acc: 0.8341 - val_loss: 0.3681 - val_acc: 0.8283\n",
      "Epoch 283/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3765 - acc: 0.8340\n",
      "Epoch 283: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3765 - acc: 0.8340 - val_loss: 0.3591 - val_acc: 0.8372\n",
      "Epoch 284/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8327\n",
      "Epoch 284: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3676 - acc: 0.8336 - val_loss: 0.3668 - val_acc: 0.8489\n",
      "Epoch 285/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8349\n",
      "Epoch 285: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3645 - acc: 0.8353 - val_loss: 0.3619 - val_acc: 0.8441\n",
      "Epoch 286/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8273\n",
      "Epoch 286: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3721 - acc: 0.8280 - val_loss: 0.3635 - val_acc: 0.8287\n",
      "Epoch 287/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8277\n",
      "Epoch 287: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3744 - acc: 0.8279 - val_loss: 0.3643 - val_acc: 0.8271\n",
      "Epoch 288/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8358\n",
      "Epoch 288: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3695 - acc: 0.8358 - val_loss: 0.3812 - val_acc: 0.8149\n",
      "Epoch 289/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8301\n",
      "Epoch 289: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3698 - acc: 0.8306 - val_loss: 0.3693 - val_acc: 0.8437\n",
      "Epoch 290/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8347\n",
      "Epoch 290: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3597 - acc: 0.8347 - val_loss: 0.3706 - val_acc: 0.8169\n",
      "Epoch 291/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8313\n",
      "Epoch 291: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3621 - acc: 0.8317 - val_loss: 0.3530 - val_acc: 0.8607\n",
      "Epoch 292/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8355\n",
      "Epoch 292: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3614 - acc: 0.8357 - val_loss: 0.3761 - val_acc: 0.8198\n",
      "Epoch 293/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8318\n",
      "Epoch 293: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3717 - acc: 0.8327 - val_loss: 0.3546 - val_acc: 0.8469\n",
      "Epoch 294/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3577 - acc: 0.8412\n",
      "Epoch 294: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3577 - acc: 0.8412 - val_loss: 0.3699 - val_acc: 0.8165\n",
      "Epoch 295/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3626 - acc: 0.8354\n",
      "Epoch 295: val_acc did not improve from 0.86270\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3626 - acc: 0.8354 - val_loss: 0.3606 - val_acc: 0.8416\n",
      "Epoch 296/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8325\n",
      "Epoch 296: val_acc improved from 0.86270 to 0.86310, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3588 - acc: 0.8332 - val_loss: 0.3541 - val_acc: 0.8631\n",
      "Epoch 297/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8346\n",
      "Epoch 297: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3624 - acc: 0.8351 - val_loss: 0.3713 - val_acc: 0.8218\n",
      "Epoch 298/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8308\n",
      "Epoch 298: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3664 - acc: 0.8314 - val_loss: 0.3451 - val_acc: 0.8530\n",
      "Epoch 299/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8345\n",
      "Epoch 299: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3603 - acc: 0.8350 - val_loss: 0.3554 - val_acc: 0.8562\n",
      "Epoch 300/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8298\n",
      "Epoch 300: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3632 - acc: 0.8304 - val_loss: 0.3659 - val_acc: 0.8481\n",
      "Epoch 301/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8373\n",
      "Epoch 301: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3669 - acc: 0.8379 - val_loss: 0.3541 - val_acc: 0.8303\n",
      "Epoch 302/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8390\n",
      "Epoch 302: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3539 - acc: 0.8390 - val_loss: 0.3637 - val_acc: 0.8303\n",
      "Epoch 303/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8355\n",
      "Epoch 303: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3598 - acc: 0.8360 - val_loss: 0.3703 - val_acc: 0.8343\n",
      "Epoch 304/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8314\n",
      "Epoch 304: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3718 - acc: 0.8314 - val_loss: 0.3729 - val_acc: 0.8088\n",
      "Epoch 305/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8348\n",
      "Epoch 305: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3522 - acc: 0.8349 - val_loss: 0.4345 - val_acc: 0.8323\n",
      "Epoch 306/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8329\n",
      "Epoch 306: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3674 - acc: 0.8333 - val_loss: 0.3552 - val_acc: 0.8372\n",
      "Epoch 307/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8321\n",
      "Epoch 307: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3644 - acc: 0.8322 - val_loss: 0.3746 - val_acc: 0.8194\n",
      "Epoch 308/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8319\n",
      "Epoch 308: val_acc did not improve from 0.86310\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3668 - acc: 0.8319 - val_loss: 0.3668 - val_acc: 0.8214\n",
      "Epoch 309/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8344\n",
      "Epoch 309: val_acc improved from 0.86310 to 0.86351, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3723 - acc: 0.8350 - val_loss: 0.3478 - val_acc: 0.8635\n",
      "Epoch 310/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8320\n",
      "Epoch 310: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3712 - acc: 0.8326 - val_loss: 0.3632 - val_acc: 0.8287\n",
      "Epoch 311/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8361\n",
      "Epoch 311: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3668 - acc: 0.8362 - val_loss: 0.3553 - val_acc: 0.8384\n",
      "Epoch 312/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8388\n",
      "Epoch 312: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3579 - acc: 0.8384 - val_loss: 0.3780 - val_acc: 0.8113\n",
      "Epoch 313/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8414\n",
      "Epoch 313: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3547 - acc: 0.8418 - val_loss: 0.3581 - val_acc: 0.8461\n",
      "Epoch 314/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8390\n",
      "Epoch 314: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3624 - acc: 0.8391 - val_loss: 0.3613 - val_acc: 0.8303\n",
      "Epoch 315/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8349\n",
      "Epoch 315: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3609 - acc: 0.8349 - val_loss: 0.3602 - val_acc: 0.8352\n",
      "Epoch 316/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8380\n",
      "Epoch 316: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3547 - acc: 0.8380 - val_loss: 0.3584 - val_acc: 0.8319\n",
      "Epoch 317/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8384\n",
      "Epoch 317: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3577 - acc: 0.8386 - val_loss: 0.3356 - val_acc: 0.8574\n",
      "Epoch 318/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8327\n",
      "Epoch 318: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3672 - acc: 0.8331 - val_loss: 0.3616 - val_acc: 0.8404\n",
      "Epoch 319/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8409\n",
      "Epoch 319: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3534 - acc: 0.8417 - val_loss: 0.3466 - val_acc: 0.8562\n",
      "Epoch 320/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8354\n",
      "Epoch 320: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3613 - acc: 0.8362 - val_loss: 0.3568 - val_acc: 0.8433\n",
      "Epoch 321/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8376\n",
      "Epoch 321: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3590 - acc: 0.8377 - val_loss: 0.3590 - val_acc: 0.8250\n",
      "Epoch 322/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8384\n",
      "Epoch 322: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3627 - acc: 0.8385 - val_loss: 0.3876 - val_acc: 0.8076\n",
      "Epoch 323/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8332\n",
      "Epoch 323: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3602 - acc: 0.8336 - val_loss: 0.3572 - val_acc: 0.8384\n",
      "Epoch 324/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8368\n",
      "Epoch 324: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3592 - acc: 0.8372 - val_loss: 0.3672 - val_acc: 0.8242\n",
      "Epoch 325/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8333\n",
      "Epoch 325: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3612 - acc: 0.8338 - val_loss: 0.3822 - val_acc: 0.8157\n",
      "Epoch 326/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8319\n",
      "Epoch 326: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3683 - acc: 0.8319 - val_loss: 0.3635 - val_acc: 0.8287\n",
      "Epoch 327/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8379\n",
      "Epoch 327: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3555 - acc: 0.8385 - val_loss: 0.3569 - val_acc: 0.8461\n",
      "Epoch 328/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8330\n",
      "Epoch 328: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3594 - acc: 0.8338 - val_loss: 0.3609 - val_acc: 0.8218\n",
      "Epoch 329/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8298\n",
      "Epoch 329: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3664 - acc: 0.8300 - val_loss: 0.3848 - val_acc: 0.8141\n",
      "Epoch 330/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8384\n",
      "Epoch 330: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3571 - acc: 0.8391 - val_loss: 0.3578 - val_acc: 0.8396\n",
      "Epoch 331/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8316\n",
      "Epoch 331: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3669 - acc: 0.8316 - val_loss: 0.3638 - val_acc: 0.8283\n",
      "Epoch 332/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8368\n",
      "Epoch 332: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3642 - acc: 0.8368 - val_loss: 0.3665 - val_acc: 0.8360\n",
      "Epoch 333/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8371\n",
      "Epoch 333: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3582 - acc: 0.8374 - val_loss: 0.3747 - val_acc: 0.8173\n",
      "Epoch 334/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8335\n",
      "Epoch 334: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3565 - acc: 0.8335 - val_loss: 0.3627 - val_acc: 0.8380\n",
      "Epoch 335/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8315\n",
      "Epoch 335: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3652 - acc: 0.8324 - val_loss: 0.4183 - val_acc: 0.8461\n",
      "Epoch 336/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8331\n",
      "Epoch 336: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3723 - acc: 0.8335 - val_loss: 0.3701 - val_acc: 0.8299\n",
      "Epoch 337/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8365\n",
      "Epoch 337: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3542 - acc: 0.8369 - val_loss: 0.3647 - val_acc: 0.8250\n",
      "Epoch 338/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8392\n",
      "Epoch 338: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3584 - acc: 0.8391 - val_loss: 0.3512 - val_acc: 0.8437\n",
      "Epoch 339/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8378\n",
      "Epoch 339: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3564 - acc: 0.8385 - val_loss: 0.3543 - val_acc: 0.8493\n",
      "Epoch 340/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8353\n",
      "Epoch 340: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3604 - acc: 0.8358 - val_loss: 0.3667 - val_acc: 0.8133\n",
      "Epoch 341/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3521 - acc: 0.8386\n",
      "Epoch 341: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3521 - acc: 0.8386 - val_loss: 0.3804 - val_acc: 0.8194\n",
      "Epoch 342/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8399\n",
      "Epoch 342: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3517 - acc: 0.8401 - val_loss: 0.3572 - val_acc: 0.8384\n",
      "Epoch 343/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8365\n",
      "Epoch 343: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3571 - acc: 0.8371 - val_loss: 0.3618 - val_acc: 0.8429\n",
      "Epoch 344/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8427\n",
      "Epoch 344: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3536 - acc: 0.8435 - val_loss: 0.3798 - val_acc: 0.8408\n",
      "Epoch 345/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8379\n",
      "Epoch 345: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3572 - acc: 0.8387 - val_loss: 0.3580 - val_acc: 0.8433\n",
      "Epoch 346/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8366\n",
      "Epoch 346: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3567 - acc: 0.8370 - val_loss: 0.3577 - val_acc: 0.8392\n",
      "Epoch 347/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8322\n",
      "Epoch 347: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3591 - acc: 0.8326 - val_loss: 0.3719 - val_acc: 0.8250\n",
      "Epoch 348/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8342\n",
      "Epoch 348: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3512 - acc: 0.8353 - val_loss: 0.3585 - val_acc: 0.8522\n",
      "Epoch 349/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8350\n",
      "Epoch 349: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3656 - acc: 0.8352 - val_loss: 0.3617 - val_acc: 0.8424\n",
      "Epoch 350/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8326\n",
      "Epoch 350: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3643 - acc: 0.8327 - val_loss: 0.3641 - val_acc: 0.8295\n",
      "Epoch 351/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8354\n",
      "Epoch 351: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3624 - acc: 0.8357 - val_loss: 0.3840 - val_acc: 0.8141\n",
      "Epoch 352/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8330\n",
      "Epoch 352: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3663 - acc: 0.8341 - val_loss: 0.3622 - val_acc: 0.8234\n",
      "Epoch 353/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8349\n",
      "Epoch 353: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3574 - acc: 0.8355 - val_loss: 0.3608 - val_acc: 0.8335\n",
      "Epoch 354/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3603 - acc: 0.8380\n",
      "Epoch 354: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3603 - acc: 0.8380 - val_loss: 0.3719 - val_acc: 0.8331\n",
      "Epoch 355/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8364\n",
      "Epoch 355: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3582 - acc: 0.8366 - val_loss: 0.3708 - val_acc: 0.8343\n",
      "Epoch 356/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8373\n",
      "Epoch 356: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3551 - acc: 0.8378 - val_loss: 0.3754 - val_acc: 0.8202\n",
      "Epoch 357/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3572 - acc: 0.8386\n",
      "Epoch 357: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3573 - acc: 0.8388 - val_loss: 0.3667 - val_acc: 0.8262\n",
      "Epoch 358/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8401\n",
      "Epoch 358: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3581 - acc: 0.8397 - val_loss: 0.3891 - val_acc: 0.8181\n",
      "Epoch 359/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8360\n",
      "Epoch 359: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3580 - acc: 0.8364 - val_loss: 0.3690 - val_acc: 0.8404\n",
      "Epoch 360/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8382\n",
      "Epoch 360: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3534 - acc: 0.8390 - val_loss: 0.3657 - val_acc: 0.8437\n",
      "Epoch 361/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8342\n",
      "Epoch 361: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3609 - acc: 0.8341 - val_loss: 0.3617 - val_acc: 0.8258\n",
      "Epoch 362/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8356\n",
      "Epoch 362: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3591 - acc: 0.8356 - val_loss: 0.3479 - val_acc: 0.8485\n",
      "Epoch 363/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8412\n",
      "Epoch 363: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3500 - acc: 0.8416 - val_loss: 0.3519 - val_acc: 0.8505\n",
      "Epoch 364/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8381\n",
      "Epoch 364: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3547 - acc: 0.8383 - val_loss: 0.3684 - val_acc: 0.8416\n",
      "Epoch 365/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8333\n",
      "Epoch 365: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3644 - acc: 0.8337 - val_loss: 0.3814 - val_acc: 0.8076\n",
      "Epoch 366/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8295\n",
      "Epoch 366: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3619 - acc: 0.8301 - val_loss: 0.3709 - val_acc: 0.8303\n",
      "Epoch 367/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3593 - acc: 0.8411\n",
      "Epoch 367: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3593 - acc: 0.8411 - val_loss: 0.3605 - val_acc: 0.8246\n",
      "Epoch 368/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8395\n",
      "Epoch 368: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3605 - acc: 0.8400 - val_loss: 0.3555 - val_acc: 0.8457\n",
      "Epoch 369/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8372\n",
      "Epoch 369: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3650 - acc: 0.8376 - val_loss: 0.3725 - val_acc: 0.8287\n",
      "Epoch 370/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8373\n",
      "Epoch 370: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3599 - acc: 0.8371 - val_loss: 0.3585 - val_acc: 0.8287\n",
      "Epoch 371/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8384\n",
      "Epoch 371: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3589 - acc: 0.8389 - val_loss: 0.3644 - val_acc: 0.8206\n",
      "Epoch 372/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8361\n",
      "Epoch 372: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3610 - acc: 0.8361 - val_loss: 0.3705 - val_acc: 0.8343\n",
      "Epoch 373/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8366\n",
      "Epoch 373: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3616 - acc: 0.8365 - val_loss: 0.3817 - val_acc: 0.8279\n",
      "Epoch 374/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3696 - acc: 0.8351\n",
      "Epoch 374: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3696 - acc: 0.8351 - val_loss: 0.3732 - val_acc: 0.8388\n",
      "Epoch 375/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8373\n",
      "Epoch 375: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3597 - acc: 0.8379 - val_loss: 0.3466 - val_acc: 0.8429\n",
      "Epoch 376/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8361\n",
      "Epoch 376: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3593 - acc: 0.8362 - val_loss: 0.3692 - val_acc: 0.8218\n",
      "Epoch 377/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3727 - acc: 0.8281\n",
      "Epoch 377: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3727 - acc: 0.8281 - val_loss: 0.3636 - val_acc: 0.8190\n",
      "Epoch 378/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3574 - acc: 0.8419\n",
      "Epoch 378: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3574 - acc: 0.8419 - val_loss: 0.3566 - val_acc: 0.8343\n",
      "Epoch 379/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3528 - acc: 0.8371\n",
      "Epoch 379: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3528 - acc: 0.8371 - val_loss: 0.3636 - val_acc: 0.8356\n",
      "Epoch 380/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8412\n",
      "Epoch 380: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3542 - acc: 0.8418 - val_loss: 0.3453 - val_acc: 0.8599\n",
      "Epoch 381/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8339\n",
      "Epoch 381: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3558 - acc: 0.8343 - val_loss: 0.3493 - val_acc: 0.8404\n",
      "Epoch 382/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8370\n",
      "Epoch 382: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3570 - acc: 0.8376 - val_loss: 0.3511 - val_acc: 0.8424\n",
      "Epoch 383/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8423\n",
      "Epoch 383: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3493 - acc: 0.8427 - val_loss: 0.3661 - val_acc: 0.8514\n",
      "Epoch 384/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8384\n",
      "Epoch 384: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3606 - acc: 0.8383 - val_loss: 0.3614 - val_acc: 0.8384\n",
      "Epoch 385/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8416\n",
      "Epoch 385: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3520 - acc: 0.8418 - val_loss: 0.3559 - val_acc: 0.8485\n",
      "Epoch 386/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8355\n",
      "Epoch 386: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3537 - acc: 0.8361 - val_loss: 0.3586 - val_acc: 0.8384\n",
      "Epoch 387/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8364\n",
      "Epoch 387: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3675 - acc: 0.8364 - val_loss: 0.3535 - val_acc: 0.8473\n",
      "Epoch 388/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8332\n",
      "Epoch 388: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3614 - acc: 0.8332 - val_loss: 0.3683 - val_acc: 0.8416\n",
      "Epoch 389/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8450\n",
      "Epoch 389: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3478 - acc: 0.8450 - val_loss: 0.3712 - val_acc: 0.8416\n",
      "Epoch 390/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8348\n",
      "Epoch 390: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3641 - acc: 0.8349 - val_loss: 0.3870 - val_acc: 0.8092\n",
      "Epoch 391/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8395\n",
      "Epoch 391: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3506 - acc: 0.8400 - val_loss: 0.3948 - val_acc: 0.8169\n",
      "Epoch 392/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8390\n",
      "Epoch 392: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3525 - acc: 0.8394 - val_loss: 0.3607 - val_acc: 0.8299\n",
      "Epoch 393/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8357\n",
      "Epoch 393: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3584 - acc: 0.8359 - val_loss: 0.3545 - val_acc: 0.8566\n",
      "Epoch 394/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8320\n",
      "Epoch 394: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3643 - acc: 0.8332 - val_loss: 0.3716 - val_acc: 0.8335\n",
      "Epoch 395/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8374\n",
      "Epoch 395: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3640 - acc: 0.8379 - val_loss: 0.3749 - val_acc: 0.8279\n",
      "Epoch 396/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8359\n",
      "Epoch 396: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3628 - acc: 0.8363 - val_loss: 0.3559 - val_acc: 0.8339\n",
      "Epoch 397/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8419\n",
      "Epoch 397: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3482 - acc: 0.8422 - val_loss: 0.3634 - val_acc: 0.8441\n",
      "Epoch 398/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8405\n",
      "Epoch 398: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3506 - acc: 0.8409 - val_loss: 0.3733 - val_acc: 0.8343\n",
      "Epoch 399/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8354\n",
      "Epoch 399: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3599 - acc: 0.8357 - val_loss: 0.3782 - val_acc: 0.8262\n",
      "Epoch 400/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3510 - acc: 0.8388\n",
      "Epoch 400: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3508 - acc: 0.8389 - val_loss: 0.3706 - val_acc: 0.8319\n",
      "Epoch 401/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8373\n",
      "Epoch 401: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3587 - acc: 0.8374 - val_loss: 0.3533 - val_acc: 0.8595\n",
      "Epoch 402/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8345\n",
      "Epoch 402: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3591 - acc: 0.8354 - val_loss: 0.3939 - val_acc: 0.8307\n",
      "Epoch 403/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8386\n",
      "Epoch 403: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3532 - acc: 0.8387 - val_loss: 0.3619 - val_acc: 0.8331\n",
      "Epoch 404/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8412\n",
      "Epoch 404: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3518 - acc: 0.8414 - val_loss: 0.3817 - val_acc: 0.8238\n",
      "Epoch 405/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8409\n",
      "Epoch 405: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3540 - acc: 0.8415 - val_loss: 0.3777 - val_acc: 0.8206\n",
      "Epoch 406/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8421\n",
      "Epoch 406: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3449 - acc: 0.8423 - val_loss: 0.3537 - val_acc: 0.8582\n",
      "Epoch 407/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8441\n",
      "Epoch 407: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3519 - acc: 0.8447 - val_loss: 0.3549 - val_acc: 0.8469\n",
      "Epoch 408/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8401\n",
      "Epoch 408: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3575 - acc: 0.8406 - val_loss: 0.3701 - val_acc: 0.8303\n",
      "Epoch 409/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8436\n",
      "Epoch 409: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3490 - acc: 0.8441 - val_loss: 0.3457 - val_acc: 0.8469\n",
      "Epoch 410/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3477 - acc: 0.8464\n",
      "Epoch 410: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3473 - acc: 0.8466 - val_loss: 0.3545 - val_acc: 0.8412\n",
      "Epoch 411/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8378\n",
      "Epoch 411: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3725 - acc: 0.8376 - val_loss: 0.3787 - val_acc: 0.8157\n",
      "Epoch 412/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.8435\n",
      "Epoch 412: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3465 - acc: 0.8440 - val_loss: 0.3579 - val_acc: 0.8437\n",
      "Epoch 413/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8414\n",
      "Epoch 413: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3522 - acc: 0.8414 - val_loss: 0.3687 - val_acc: 0.8165\n",
      "Epoch 414/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8400\n",
      "Epoch 414: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3622 - acc: 0.8401 - val_loss: 0.3648 - val_acc: 0.8267\n",
      "Epoch 415/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8401\n",
      "Epoch 415: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3479 - acc: 0.8407 - val_loss: 0.3654 - val_acc: 0.8190\n",
      "Epoch 416/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8418\n",
      "Epoch 416: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3496 - acc: 0.8423 - val_loss: 0.3591 - val_acc: 0.8303\n",
      "Epoch 417/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8382\n",
      "Epoch 417: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3544 - acc: 0.8388 - val_loss: 0.3592 - val_acc: 0.8331\n",
      "Epoch 418/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8417\n",
      "Epoch 418: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3448 - acc: 0.8417 - val_loss: 0.3586 - val_acc: 0.8384\n",
      "Epoch 419/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8398\n",
      "Epoch 419: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3570 - acc: 0.8406 - val_loss: 0.3671 - val_acc: 0.8433\n",
      "Epoch 420/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3618 - acc: 0.8348\n",
      "Epoch 420: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3617 - acc: 0.8349 - val_loss: 0.3738 - val_acc: 0.8161\n",
      "Epoch 421/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8390\n",
      "Epoch 421: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3507 - acc: 0.8395 - val_loss: 0.3690 - val_acc: 0.8335\n",
      "Epoch 422/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8440\n",
      "Epoch 422: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3504 - acc: 0.8440 - val_loss: 0.3672 - val_acc: 0.8271\n",
      "Epoch 423/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8414\n",
      "Epoch 423: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3456 - acc: 0.8414 - val_loss: 0.3749 - val_acc: 0.8226\n",
      "Epoch 424/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8430\n",
      "Epoch 424: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3526 - acc: 0.8436 - val_loss: 0.3532 - val_acc: 0.8429\n",
      "Epoch 425/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8401\n",
      "Epoch 425: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3509 - acc: 0.8404 - val_loss: 0.3595 - val_acc: 0.8550\n",
      "Epoch 426/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3561 - acc: 0.8388\n",
      "Epoch 426: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3561 - acc: 0.8388 - val_loss: 0.3738 - val_acc: 0.8364\n",
      "Epoch 427/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8381\n",
      "Epoch 427: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3607 - acc: 0.8381 - val_loss: 0.3618 - val_acc: 0.8392\n",
      "Epoch 428/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3520 - acc: 0.8422\n",
      "Epoch 428: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3520 - acc: 0.8422 - val_loss: 0.3656 - val_acc: 0.8433\n",
      "Epoch 429/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8403\n",
      "Epoch 429: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.3489 - acc: 0.8405 - val_loss: 0.3650 - val_acc: 0.8339\n",
      "Epoch 430/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8435\n",
      "Epoch 430: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3541 - acc: 0.8439 - val_loss: 0.3640 - val_acc: 0.8242\n",
      "Epoch 431/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3585 - acc: 0.8372\n",
      "Epoch 431: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3585 - acc: 0.8372 - val_loss: 0.3681 - val_acc: 0.8323\n",
      "Epoch 432/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8374\n",
      "Epoch 432: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3677 - acc: 0.8377 - val_loss: 0.3584 - val_acc: 0.8230\n",
      "Epoch 433/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8452\n",
      "Epoch 433: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3468 - acc: 0.8463 - val_loss: 0.3520 - val_acc: 0.8445\n",
      "Epoch 434/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8336\n",
      "Epoch 434: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3599 - acc: 0.8336 - val_loss: 0.3562 - val_acc: 0.8380\n",
      "Epoch 435/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8418\n",
      "Epoch 435: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3488 - acc: 0.8422 - val_loss: 0.3422 - val_acc: 0.8542\n",
      "Epoch 436/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8419\n",
      "Epoch 436: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3469 - acc: 0.8421 - val_loss: 0.3632 - val_acc: 0.8396\n",
      "Epoch 437/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8378\n",
      "Epoch 437: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3446 - acc: 0.8379 - val_loss: 0.3702 - val_acc: 0.8352\n",
      "Epoch 438/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8404\n",
      "Epoch 438: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3573 - acc: 0.8411 - val_loss: 0.3580 - val_acc: 0.8339\n",
      "Epoch 439/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8366\n",
      "Epoch 439: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3574 - acc: 0.8367 - val_loss: 0.3486 - val_acc: 0.8400\n",
      "Epoch 440/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8434\n",
      "Epoch 440: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3497 - acc: 0.8435 - val_loss: 0.3671 - val_acc: 0.8327\n",
      "Epoch 441/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8363\n",
      "Epoch 441: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3525 - acc: 0.8366 - val_loss: 0.3536 - val_acc: 0.8376\n",
      "Epoch 442/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8391\n",
      "Epoch 442: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3483 - acc: 0.8393 - val_loss: 0.3561 - val_acc: 0.8437\n",
      "Epoch 443/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.8427\n",
      "Epoch 443: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3433 - acc: 0.8432 - val_loss: 0.3611 - val_acc: 0.8465\n",
      "Epoch 444/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8366\n",
      "Epoch 444: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3608 - acc: 0.8371 - val_loss: 0.3704 - val_acc: 0.8319\n",
      "Epoch 445/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8398\n",
      "Epoch 445: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3474 - acc: 0.8403 - val_loss: 0.3489 - val_acc: 0.8461\n",
      "Epoch 446/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8362\n",
      "Epoch 446: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3551 - acc: 0.8364 - val_loss: 0.3805 - val_acc: 0.8262\n",
      "Epoch 447/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8365\n",
      "Epoch 447: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3518 - acc: 0.8367 - val_loss: 0.3557 - val_acc: 0.8392\n",
      "Epoch 448/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8356\n",
      "Epoch 448: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3545 - acc: 0.8356 - val_loss: 0.3752 - val_acc: 0.8307\n",
      "Epoch 449/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8432\n",
      "Epoch 449: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3614 - acc: 0.8441 - val_loss: 0.3560 - val_acc: 0.8441\n",
      "Epoch 450/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8414\n",
      "Epoch 450: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3457 - acc: 0.8418 - val_loss: 0.3587 - val_acc: 0.8258\n",
      "Epoch 451/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3549 - acc: 0.8380\n",
      "Epoch 451: val_acc did not improve from 0.86351\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3549 - acc: 0.8380 - val_loss: 0.3529 - val_acc: 0.8501\n",
      "Epoch 452/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8387\n",
      "Epoch 452: val_acc improved from 0.86351 to 0.86918, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/3/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3519 - acc: 0.8393 - val_loss: 0.3563 - val_acc: 0.8692\n",
      "Epoch 453/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8398\n",
      "Epoch 453: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3479 - acc: 0.8403 - val_loss: 0.3672 - val_acc: 0.8303\n",
      "Epoch 454/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8412\n",
      "Epoch 454: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3523 - acc: 0.8413 - val_loss: 0.3612 - val_acc: 0.8380\n",
      "Epoch 455/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8406\n",
      "Epoch 455: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3484 - acc: 0.8407 - val_loss: 0.3723 - val_acc: 0.8141\n",
      "Epoch 456/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3589 - acc: 0.8400\n",
      "Epoch 456: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3588 - acc: 0.8400 - val_loss: 0.3676 - val_acc: 0.8311\n",
      "Epoch 457/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8388\n",
      "Epoch 457: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3471 - acc: 0.8392 - val_loss: 0.3545 - val_acc: 0.8275\n",
      "Epoch 458/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8391\n",
      "Epoch 458: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3500 - acc: 0.8391 - val_loss: 0.3607 - val_acc: 0.8424\n",
      "Epoch 459/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8414\n",
      "Epoch 459: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3531 - acc: 0.8416 - val_loss: 0.3586 - val_acc: 0.8271\n",
      "Epoch 460/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.8436\n",
      "Epoch 460: val_acc did not improve from 0.86918\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3429 - acc: 0.8443 - val_loss: 0.3523 - val_acc: 0.8254\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 1.5225 - acc: 0.6309\n",
      "Epoch 1: val_acc improved from -inf to 0.68530, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 5s 6ms/step - loss: 1.5221 - acc: 0.6309 - val_loss: 0.6038 - val_acc: 0.6853\n",
      "Epoch 2/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.6351 - acc: 0.6781\n",
      "Epoch 2: val_acc improved from 0.68530 to 0.69866, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.6349 - acc: 0.6783 - val_loss: 0.5982 - val_acc: 0.6987\n",
      "Epoch 3/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.6994\n",
      "Epoch 3: val_acc improved from 0.69866 to 0.72418, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.6088 - acc: 0.6995 - val_loss: 0.5631 - val_acc: 0.7242\n",
      "Epoch 4/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5934 - acc: 0.7098\n",
      "Epoch 4: val_acc improved from 0.72418 to 0.73066, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5932 - acc: 0.7104 - val_loss: 0.5637 - val_acc: 0.7307\n",
      "Epoch 5/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.7164\n",
      "Epoch 5: val_acc did not improve from 0.73066\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5782 - acc: 0.7167 - val_loss: 0.5611 - val_acc: 0.7246\n",
      "Epoch 6/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.7178\n",
      "Epoch 6: val_acc did not improve from 0.73066\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5799 - acc: 0.7178 - val_loss: 0.5639 - val_acc: 0.7262\n",
      "Epoch 7/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.7252\n",
      "Epoch 7: val_acc improved from 0.73066 to 0.73512, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5733 - acc: 0.7251 - val_loss: 0.5385 - val_acc: 0.7351\n",
      "Epoch 8/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.5646 - acc: 0.7307\n",
      "Epoch 8: val_acc improved from 0.73512 to 0.73633, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5645 - acc: 0.7308 - val_loss: 0.5332 - val_acc: 0.7363\n",
      "Epoch 9/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.7288\n",
      "Epoch 9: val_acc did not improve from 0.73633\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5583 - acc: 0.7290 - val_loss: 0.5460 - val_acc: 0.7278\n",
      "Epoch 10/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7298\n",
      "Epoch 10: val_acc did not improve from 0.73633\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5567 - acc: 0.7302 - val_loss: 0.5507 - val_acc: 0.7347\n",
      "Epoch 11/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5550 - acc: 0.7324\n",
      "Epoch 11: val_acc did not improve from 0.73633\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5550 - acc: 0.7324 - val_loss: 0.5387 - val_acc: 0.7266\n",
      "Epoch 12/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5440 - acc: 0.7343\n",
      "Epoch 12: val_acc did not improve from 0.73633\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5440 - acc: 0.7343 - val_loss: 0.5395 - val_acc: 0.7343\n",
      "Epoch 13/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.7373\n",
      "Epoch 13: val_acc improved from 0.73633 to 0.74079, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5406 - acc: 0.7375 - val_loss: 0.5109 - val_acc: 0.7408\n",
      "Epoch 14/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7394\n",
      "Epoch 14: val_acc improved from 0.74079 to 0.74727, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5306 - acc: 0.7395 - val_loss: 0.5084 - val_acc: 0.7473\n",
      "Epoch 15/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5312 - acc: 0.7419\n",
      "Epoch 15: val_acc improved from 0.74727 to 0.75253, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5303 - acc: 0.7424 - val_loss: 0.5027 - val_acc: 0.7525\n",
      "Epoch 16/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.7490\n",
      "Epoch 16: val_acc improved from 0.75253 to 0.75739, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5241 - acc: 0.7499 - val_loss: 0.4982 - val_acc: 0.7574\n",
      "Epoch 17/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.7469\n",
      "Epoch 17: val_acc did not improve from 0.75739\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5253 - acc: 0.7473 - val_loss: 0.5080 - val_acc: 0.7537\n",
      "Epoch 18/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.7484\n",
      "Epoch 18: val_acc did not improve from 0.75739\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5219 - acc: 0.7483 - val_loss: 0.5077 - val_acc: 0.7452\n",
      "Epoch 19/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7532\n",
      "Epoch 19: val_acc did not improve from 0.75739\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5102 - acc: 0.7534 - val_loss: 0.5154 - val_acc: 0.7331\n",
      "Epoch 20/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.7507\n",
      "Epoch 20: val_acc improved from 0.75739 to 0.76104, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5138 - acc: 0.7510 - val_loss: 0.4866 - val_acc: 0.7610\n",
      "Epoch 21/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.7552\n",
      "Epoch 21: val_acc did not improve from 0.76104\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5109 - acc: 0.7554 - val_loss: 0.5059 - val_acc: 0.7465\n",
      "Epoch 22/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.7566\n",
      "Epoch 22: val_acc did not improve from 0.76104\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5071 - acc: 0.7569 - val_loss: 0.5066 - val_acc: 0.7497\n",
      "Epoch 23/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.7592\n",
      "Epoch 23: val_acc did not improve from 0.76104\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5022 - acc: 0.7596 - val_loss: 0.4874 - val_acc: 0.7554\n",
      "Epoch 24/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.7624\n",
      "Epoch 24: val_acc improved from 0.76104 to 0.76266, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4961 - acc: 0.7624 - val_loss: 0.4833 - val_acc: 0.7627\n",
      "Epoch 25/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.7629\n",
      "Epoch 25: val_acc did not improve from 0.76266\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4953 - acc: 0.7632 - val_loss: 0.4854 - val_acc: 0.7627\n",
      "Epoch 26/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.7602\n",
      "Epoch 26: val_acc did not improve from 0.76266\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4992 - acc: 0.7609 - val_loss: 0.4975 - val_acc: 0.7550\n",
      "Epoch 27/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4978 - acc: 0.7653\n",
      "Epoch 27: val_acc did not improve from 0.76266\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4974 - acc: 0.7654 - val_loss: 0.5045 - val_acc: 0.7412\n",
      "Epoch 28/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.7677\n",
      "Epoch 28: val_acc improved from 0.76266 to 0.76914, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4904 - acc: 0.7683 - val_loss: 0.4585 - val_acc: 0.7691\n",
      "Epoch 29/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4810 - acc: 0.7726\n",
      "Epoch 29: val_acc improved from 0.76914 to 0.77238, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4808 - acc: 0.7729 - val_loss: 0.4660 - val_acc: 0.7724\n",
      "Epoch 30/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4837 - acc: 0.7689\n",
      "Epoch 30: val_acc did not improve from 0.77238\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4831 - acc: 0.7696 - val_loss: 0.4751 - val_acc: 0.7614\n",
      "Epoch 31/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.7638\n",
      "Epoch 31: val_acc did not improve from 0.77238\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4915 - acc: 0.7633 - val_loss: 0.4996 - val_acc: 0.7469\n",
      "Epoch 32/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4866 - acc: 0.7659\n",
      "Epoch 32: val_acc improved from 0.77238 to 0.77481, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4861 - acc: 0.7666 - val_loss: 0.4678 - val_acc: 0.7748\n",
      "Epoch 33/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.7733\n",
      "Epoch 33: val_acc improved from 0.77481 to 0.77805, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4814 - acc: 0.7736 - val_loss: 0.4584 - val_acc: 0.7780\n",
      "Epoch 34/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.7770\n",
      "Epoch 34: val_acc did not improve from 0.77805\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4801 - acc: 0.7774 - val_loss: 0.4618 - val_acc: 0.7687\n",
      "Epoch 35/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4724 - acc: 0.7768\n",
      "Epoch 35: val_acc did not improve from 0.77805\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4719 - acc: 0.7771 - val_loss: 0.4511 - val_acc: 0.7780\n",
      "Epoch 36/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.7775\n",
      "Epoch 36: val_acc did not improve from 0.77805\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4780 - acc: 0.7777 - val_loss: 0.4664 - val_acc: 0.7708\n",
      "Epoch 37/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.7831\n",
      "Epoch 37: val_acc did not improve from 0.77805\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4668 - acc: 0.7834 - val_loss: 0.4576 - val_acc: 0.7740\n",
      "Epoch 38/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4652 - acc: 0.7801\n",
      "Epoch 38: val_acc did not improve from 0.77805\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4645 - acc: 0.7808 - val_loss: 0.4731 - val_acc: 0.7618\n",
      "Epoch 39/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.7808\n",
      "Epoch 39: val_acc improved from 0.77805 to 0.79668, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4610 - acc: 0.7818 - val_loss: 0.4404 - val_acc: 0.7967\n",
      "Epoch 40/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.7842\n",
      "Epoch 40: val_acc did not improve from 0.79668\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4614 - acc: 0.7846 - val_loss: 0.4671 - val_acc: 0.7728\n",
      "Epoch 41/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.7809\n",
      "Epoch 41: val_acc did not improve from 0.79668\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4684 - acc: 0.7810 - val_loss: 0.4624 - val_acc: 0.7695\n",
      "Epoch 42/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.7763\n",
      "Epoch 42: val_acc did not improve from 0.79668\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4693 - acc: 0.7771 - val_loss: 0.4490 - val_acc: 0.7793\n",
      "Epoch 43/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.7835\n",
      "Epoch 43: val_acc did not improve from 0.79668\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4659 - acc: 0.7836 - val_loss: 0.4582 - val_acc: 0.7667\n",
      "Epoch 44/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4588 - acc: 0.7856\n",
      "Epoch 44: val_acc did not improve from 0.79668\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4588 - acc: 0.7856 - val_loss: 0.4656 - val_acc: 0.7699\n",
      "Epoch 45/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.7809\n",
      "Epoch 45: val_acc did not improve from 0.79668\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4595 - acc: 0.7816 - val_loss: 0.4351 - val_acc: 0.7886\n",
      "Epoch 46/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.7867\n",
      "Epoch 46: val_acc did not improve from 0.79668\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4577 - acc: 0.7867 - val_loss: 0.4367 - val_acc: 0.7930\n",
      "Epoch 47/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4651 - acc: 0.7827\n",
      "Epoch 47: val_acc improved from 0.79668 to 0.79789, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4647 - acc: 0.7831 - val_loss: 0.4376 - val_acc: 0.7979\n",
      "Epoch 48/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.7893\n",
      "Epoch 48: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4544 - acc: 0.7896 - val_loss: 0.4525 - val_acc: 0.7809\n",
      "Epoch 49/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4501 - acc: 0.7904\n",
      "Epoch 49: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4493 - acc: 0.7908 - val_loss: 0.4512 - val_acc: 0.7756\n",
      "Epoch 50/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.7827\n",
      "Epoch 50: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4550 - acc: 0.7837 - val_loss: 0.4349 - val_acc: 0.7861\n",
      "Epoch 51/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.7843\n",
      "Epoch 51: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4559 - acc: 0.7846 - val_loss: 0.4605 - val_acc: 0.7845\n",
      "Epoch 52/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4582 - acc: 0.7857\n",
      "Epoch 52: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4576 - acc: 0.7860 - val_loss: 0.4318 - val_acc: 0.7789\n",
      "Epoch 53/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4534 - acc: 0.7901\n",
      "Epoch 53: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4523 - acc: 0.7908 - val_loss: 0.4331 - val_acc: 0.7890\n",
      "Epoch 54/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.7879\n",
      "Epoch 54: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4499 - acc: 0.7885 - val_loss: 0.4476 - val_acc: 0.7687\n",
      "Epoch 55/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.7920\n",
      "Epoch 55: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4401 - acc: 0.7921 - val_loss: 0.4402 - val_acc: 0.7821\n",
      "Epoch 56/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.7903\n",
      "Epoch 56: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4451 - acc: 0.7913 - val_loss: 0.4487 - val_acc: 0.7813\n",
      "Epoch 57/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.7873\n",
      "Epoch 57: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4458 - acc: 0.7875 - val_loss: 0.4322 - val_acc: 0.7861\n",
      "Epoch 58/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4394 - acc: 0.7918\n",
      "Epoch 58: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4393 - acc: 0.7919 - val_loss: 0.4339 - val_acc: 0.7955\n",
      "Epoch 59/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.7919\n",
      "Epoch 59: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4356 - acc: 0.7931 - val_loss: 0.4190 - val_acc: 0.7979\n",
      "Epoch 60/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4436 - acc: 0.7872\n",
      "Epoch 60: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4436 - acc: 0.7872 - val_loss: 0.4381 - val_acc: 0.7866\n",
      "Epoch 61/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.7974\n",
      "Epoch 61: val_acc did not improve from 0.79789\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4336 - acc: 0.7977 - val_loss: 0.4281 - val_acc: 0.7918\n",
      "Epoch 62/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.7947\n",
      "Epoch 62: val_acc improved from 0.79789 to 0.79992, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4353 - acc: 0.7950 - val_loss: 0.4143 - val_acc: 0.7999\n",
      "Epoch 63/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.7940\n",
      "Epoch 63: val_acc did not improve from 0.79992\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4379 - acc: 0.7941 - val_loss: 0.4300 - val_acc: 0.7914\n",
      "Epoch 64/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.7937\n",
      "Epoch 64: val_acc did not improve from 0.79992\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4360 - acc: 0.7941 - val_loss: 0.4368 - val_acc: 0.7793\n",
      "Epoch 65/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.7954\n",
      "Epoch 65: val_acc did not improve from 0.79992\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4322 - acc: 0.7959 - val_loss: 0.4313 - val_acc: 0.7995\n",
      "Epoch 66/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.7903\n",
      "Epoch 66: val_acc improved from 0.79992 to 0.81085, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4426 - acc: 0.7905 - val_loss: 0.4093 - val_acc: 0.8109\n",
      "Epoch 67/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.7969\n",
      "Epoch 67: val_acc did not improve from 0.81085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4364 - acc: 0.7969 - val_loss: 0.4372 - val_acc: 0.7947\n",
      "Epoch 68/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8006\n",
      "Epoch 68: val_acc did not improve from 0.81085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4257 - acc: 0.8012 - val_loss: 0.4344 - val_acc: 0.7841\n",
      "Epoch 69/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.7972\n",
      "Epoch 69: val_acc did not improve from 0.81085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4298 - acc: 0.7974 - val_loss: 0.4259 - val_acc: 0.7987\n",
      "Epoch 70/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.7956\n",
      "Epoch 70: val_acc did not improve from 0.81085\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4314 - acc: 0.7962 - val_loss: 0.4281 - val_acc: 0.7906\n",
      "Epoch 71/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.7975\n",
      "Epoch 71: val_acc improved from 0.81085 to 0.81126, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4315 - acc: 0.7981 - val_loss: 0.4040 - val_acc: 0.8113\n",
      "Epoch 72/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.7958\n",
      "Epoch 72: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4291 - acc: 0.7960 - val_loss: 0.4367 - val_acc: 0.7813\n",
      "Epoch 73/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.7981\n",
      "Epoch 73: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4311 - acc: 0.7981 - val_loss: 0.4196 - val_acc: 0.7959\n",
      "Epoch 74/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.8009\n",
      "Epoch 74: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4307 - acc: 0.8008 - val_loss: 0.4148 - val_acc: 0.8072\n",
      "Epoch 75/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4270 - acc: 0.7999\n",
      "Epoch 75: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4270 - acc: 0.7999 - val_loss: 0.4343 - val_acc: 0.7898\n",
      "Epoch 76/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.7985\n",
      "Epoch 76: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4240 - acc: 0.7985 - val_loss: 0.4270 - val_acc: 0.8048\n",
      "Epoch 77/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4268 - acc: 0.7979\n",
      "Epoch 77: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4268 - acc: 0.7979 - val_loss: 0.4379 - val_acc: 0.7906\n",
      "Epoch 78/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.7948\n",
      "Epoch 78: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4305 - acc: 0.7952 - val_loss: 0.4207 - val_acc: 0.7991\n",
      "Epoch 79/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.7968\n",
      "Epoch 79: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4276 - acc: 0.7968 - val_loss: 0.4323 - val_acc: 0.7894\n",
      "Epoch 80/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.7998\n",
      "Epoch 80: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4263 - acc: 0.8002 - val_loss: 0.4140 - val_acc: 0.7991\n",
      "Epoch 81/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.7950\n",
      "Epoch 81: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4252 - acc: 0.7957 - val_loss: 0.4143 - val_acc: 0.8011\n",
      "Epoch 82/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8035\n",
      "Epoch 82: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4199 - acc: 0.8037 - val_loss: 0.4099 - val_acc: 0.8068\n",
      "Epoch 83/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8038\n",
      "Epoch 83: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4182 - acc: 0.8041 - val_loss: 0.4132 - val_acc: 0.8040\n",
      "Epoch 84/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8042\n",
      "Epoch 84: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4175 - acc: 0.8044 - val_loss: 0.4228 - val_acc: 0.7975\n",
      "Epoch 85/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8018\n",
      "Epoch 85: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4240 - acc: 0.8022 - val_loss: 0.4515 - val_acc: 0.7809\n",
      "Epoch 86/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.7995\n",
      "Epoch 86: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4248 - acc: 0.7999 - val_loss: 0.4155 - val_acc: 0.8088\n",
      "Epoch 87/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.7883\n",
      "Epoch 87: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4369 - acc: 0.7885 - val_loss: 0.4190 - val_acc: 0.7938\n",
      "Epoch 88/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.7881\n",
      "Epoch 88: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4376 - acc: 0.7886 - val_loss: 0.4250 - val_acc: 0.8003\n",
      "Epoch 89/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4308 - acc: 0.7995\n",
      "Epoch 89: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4308 - acc: 0.7995 - val_loss: 0.4183 - val_acc: 0.8028\n",
      "Epoch 90/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.7991\n",
      "Epoch 90: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4171 - acc: 0.7990 - val_loss: 0.4309 - val_acc: 0.7995\n",
      "Epoch 91/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8029\n",
      "Epoch 91: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4201 - acc: 0.8028 - val_loss: 0.4403 - val_acc: 0.7922\n",
      "Epoch 92/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.8006\n",
      "Epoch 92: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4245 - acc: 0.8009 - val_loss: 0.4028 - val_acc: 0.8096\n",
      "Epoch 93/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8051\n",
      "Epoch 93: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4148 - acc: 0.8056 - val_loss: 0.4131 - val_acc: 0.8007\n",
      "Epoch 94/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8052\n",
      "Epoch 94: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4190 - acc: 0.8052 - val_loss: 0.4150 - val_acc: 0.8019\n",
      "Epoch 95/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8069\n",
      "Epoch 95: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4056 - acc: 0.8069 - val_loss: 0.4238 - val_acc: 0.7870\n",
      "Epoch 96/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8033\n",
      "Epoch 96: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4170 - acc: 0.8034 - val_loss: 0.4172 - val_acc: 0.7870\n",
      "Epoch 97/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8073\n",
      "Epoch 97: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4116 - acc: 0.8073 - val_loss: 0.4227 - val_acc: 0.8080\n",
      "Epoch 98/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8070\n",
      "Epoch 98: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4115 - acc: 0.8072 - val_loss: 0.4022 - val_acc: 0.8109\n",
      "Epoch 99/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4154 - acc: 0.8026\n",
      "Epoch 99: val_acc did not improve from 0.81126\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4154 - acc: 0.8026 - val_loss: 0.4286 - val_acc: 0.8109\n",
      "Epoch 100/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4196 - acc: 0.8047\n",
      "Epoch 100: val_acc improved from 0.81126 to 0.81369, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4185 - acc: 0.8054 - val_loss: 0.4127 - val_acc: 0.8137\n",
      "Epoch 101/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8096\n",
      "Epoch 101: val_acc did not improve from 0.81369\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4075 - acc: 0.8096 - val_loss: 0.4067 - val_acc: 0.8109\n",
      "Epoch 102/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4133 - acc: 0.8065\n",
      "Epoch 102: val_acc did not improve from 0.81369\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4121 - acc: 0.8071 - val_loss: 0.4029 - val_acc: 0.8064\n",
      "Epoch 103/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8060\n",
      "Epoch 103: val_acc improved from 0.81369 to 0.81896, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3991 - acc: 0.8060 - val_loss: 0.4083 - val_acc: 0.8190\n",
      "Epoch 104/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8067\n",
      "Epoch 104: val_acc did not improve from 0.81896\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4065 - acc: 0.8076 - val_loss: 0.4237 - val_acc: 0.7975\n",
      "Epoch 105/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8121\n",
      "Epoch 105: val_acc did not improve from 0.81896\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4020 - acc: 0.8124 - val_loss: 0.4285 - val_acc: 0.8023\n",
      "Epoch 106/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8108\n",
      "Epoch 106: val_acc did not improve from 0.81896\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4052 - acc: 0.8111 - val_loss: 0.4245 - val_acc: 0.8064\n",
      "Epoch 107/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4195 - acc: 0.8048\n",
      "Epoch 107: val_acc did not improve from 0.81896\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4181 - acc: 0.8057 - val_loss: 0.4027 - val_acc: 0.8080\n",
      "Epoch 108/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8056\n",
      "Epoch 108: val_acc did not improve from 0.81896\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4110 - acc: 0.8059 - val_loss: 0.4162 - val_acc: 0.7922\n",
      "Epoch 109/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8073\n",
      "Epoch 109: val_acc improved from 0.81896 to 0.82139, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4102 - acc: 0.8077 - val_loss: 0.3874 - val_acc: 0.8214\n",
      "Epoch 110/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8070\n",
      "Epoch 110: val_acc did not improve from 0.82139\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4083 - acc: 0.8076 - val_loss: 0.4039 - val_acc: 0.8048\n",
      "Epoch 111/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8172\n",
      "Epoch 111: val_acc improved from 0.82139 to 0.82746, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3978 - acc: 0.8172 - val_loss: 0.3818 - val_acc: 0.8275\n",
      "Epoch 112/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8035\n",
      "Epoch 112: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4115 - acc: 0.8038 - val_loss: 0.4144 - val_acc: 0.7955\n",
      "Epoch 113/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8142\n",
      "Epoch 113: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4008 - acc: 0.8144 - val_loss: 0.3908 - val_acc: 0.8121\n",
      "Epoch 114/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8105\n",
      "Epoch 114: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4033 - acc: 0.8105 - val_loss: 0.4015 - val_acc: 0.7967\n",
      "Epoch 115/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8099\n",
      "Epoch 115: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4061 - acc: 0.8101 - val_loss: 0.3991 - val_acc: 0.7955\n",
      "Epoch 116/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8107\n",
      "Epoch 116: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4052 - acc: 0.8112 - val_loss: 0.3929 - val_acc: 0.8141\n",
      "Epoch 117/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3962 - acc: 0.8153\n",
      "Epoch 117: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3952 - acc: 0.8158 - val_loss: 0.3892 - val_acc: 0.8137\n",
      "Epoch 118/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4015 - acc: 0.8140\n",
      "Epoch 118: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4015 - acc: 0.8140 - val_loss: 0.3877 - val_acc: 0.8190\n",
      "Epoch 119/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8120\n",
      "Epoch 119: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4003 - acc: 0.8123 - val_loss: 0.3911 - val_acc: 0.8117\n",
      "Epoch 120/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8109\n",
      "Epoch 120: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4050 - acc: 0.8118 - val_loss: 0.4069 - val_acc: 0.8003\n",
      "Epoch 121/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3906 - acc: 0.8182\n",
      "Epoch 121: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3906 - acc: 0.8182 - val_loss: 0.4040 - val_acc: 0.8036\n",
      "Epoch 122/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8106\n",
      "Epoch 122: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4025 - acc: 0.8107 - val_loss: 0.3877 - val_acc: 0.8121\n",
      "Epoch 123/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8168\n",
      "Epoch 123: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3993 - acc: 0.8171 - val_loss: 0.3871 - val_acc: 0.8145\n",
      "Epoch 124/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8138\n",
      "Epoch 124: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3977 - acc: 0.8138 - val_loss: 0.3927 - val_acc: 0.8113\n",
      "Epoch 125/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8108\n",
      "Epoch 125: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4018 - acc: 0.8111 - val_loss: 0.4068 - val_acc: 0.8072\n",
      "Epoch 126/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8133\n",
      "Epoch 126: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4007 - acc: 0.8139 - val_loss: 0.4045 - val_acc: 0.8044\n",
      "Epoch 127/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8149\n",
      "Epoch 127: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4036 - acc: 0.8152 - val_loss: 0.4163 - val_acc: 0.7938\n",
      "Epoch 128/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8106\n",
      "Epoch 128: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4043 - acc: 0.8110 - val_loss: 0.4076 - val_acc: 0.7894\n",
      "Epoch 129/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8144\n",
      "Epoch 129: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3992 - acc: 0.8149 - val_loss: 0.4063 - val_acc: 0.8036\n",
      "Epoch 130/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8179\n",
      "Epoch 130: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3899 - acc: 0.8182 - val_loss: 0.3803 - val_acc: 0.8190\n",
      "Epoch 131/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8122\n",
      "Epoch 131: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4047 - acc: 0.8129 - val_loss: 0.3907 - val_acc: 0.7999\n",
      "Epoch 132/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8174\n",
      "Epoch 132: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3926 - acc: 0.8182 - val_loss: 0.3900 - val_acc: 0.8104\n",
      "Epoch 133/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8148\n",
      "Epoch 133: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3948 - acc: 0.8157 - val_loss: 0.3913 - val_acc: 0.8165\n",
      "Epoch 134/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8190\n",
      "Epoch 134: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3867 - acc: 0.8193 - val_loss: 0.3912 - val_acc: 0.8157\n",
      "Epoch 135/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8102\n",
      "Epoch 135: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4041 - acc: 0.8103 - val_loss: 0.3912 - val_acc: 0.8210\n",
      "Epoch 136/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3913 - acc: 0.8168\n",
      "Epoch 136: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3913 - acc: 0.8168 - val_loss: 0.3856 - val_acc: 0.8177\n",
      "Epoch 137/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8192\n",
      "Epoch 137: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3889 - acc: 0.8198 - val_loss: 0.3805 - val_acc: 0.8185\n",
      "Epoch 138/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8170\n",
      "Epoch 138: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3979 - acc: 0.8176 - val_loss: 0.3882 - val_acc: 0.8173\n",
      "Epoch 139/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8205\n",
      "Epoch 139: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3951 - acc: 0.8207 - val_loss: 0.4010 - val_acc: 0.8133\n",
      "Epoch 140/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8127\n",
      "Epoch 140: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3998 - acc: 0.8131 - val_loss: 0.4021 - val_acc: 0.8226\n",
      "Epoch 141/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3965 - acc: 0.8159\n",
      "Epoch 141: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3965 - acc: 0.8159 - val_loss: 0.4040 - val_acc: 0.8165\n",
      "Epoch 142/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8132\n",
      "Epoch 142: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4065 - acc: 0.8139 - val_loss: 0.3957 - val_acc: 0.8040\n",
      "Epoch 143/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3932 - acc: 0.8135\n",
      "Epoch 143: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3924 - acc: 0.8139 - val_loss: 0.3812 - val_acc: 0.8218\n",
      "Epoch 144/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8233\n",
      "Epoch 144: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3852 - acc: 0.8240 - val_loss: 0.3897 - val_acc: 0.8194\n",
      "Epoch 145/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8156\n",
      "Epoch 145: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3920 - acc: 0.8159 - val_loss: 0.3844 - val_acc: 0.8149\n",
      "Epoch 146/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8188\n",
      "Epoch 146: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3836 - acc: 0.8191 - val_loss: 0.3935 - val_acc: 0.8044\n",
      "Epoch 147/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8120\n",
      "Epoch 147: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3913 - acc: 0.8124 - val_loss: 0.4157 - val_acc: 0.8052\n",
      "Epoch 148/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4028 - acc: 0.8151\n",
      "Epoch 148: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4028 - acc: 0.8151 - val_loss: 0.3917 - val_acc: 0.8137\n",
      "Epoch 149/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8171\n",
      "Epoch 149: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3938 - acc: 0.8178 - val_loss: 0.4073 - val_acc: 0.8052\n",
      "Epoch 150/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8130\n",
      "Epoch 150: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3979 - acc: 0.8133 - val_loss: 0.4257 - val_acc: 0.7910\n",
      "Epoch 151/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8240\n",
      "Epoch 151: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3879 - acc: 0.8241 - val_loss: 0.3798 - val_acc: 0.8161\n",
      "Epoch 152/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8233\n",
      "Epoch 152: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3887 - acc: 0.8233 - val_loss: 0.4229 - val_acc: 0.8056\n",
      "Epoch 153/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8176\n",
      "Epoch 153: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3929 - acc: 0.8187 - val_loss: 0.4063 - val_acc: 0.8068\n",
      "Epoch 154/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3867 - acc: 0.8225\n",
      "Epoch 154: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3867 - acc: 0.8225 - val_loss: 0.4314 - val_acc: 0.8064\n",
      "Epoch 155/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3918 - acc: 0.8212\n",
      "Epoch 155: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3918 - acc: 0.8212 - val_loss: 0.3807 - val_acc: 0.8181\n",
      "Epoch 156/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3828 - acc: 0.8196\n",
      "Epoch 156: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3828 - acc: 0.8196 - val_loss: 0.3929 - val_acc: 0.8052\n",
      "Epoch 157/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8174\n",
      "Epoch 157: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3912 - acc: 0.8174 - val_loss: 0.4139 - val_acc: 0.8190\n",
      "Epoch 158/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8160\n",
      "Epoch 158: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3983 - acc: 0.8162 - val_loss: 0.3962 - val_acc: 0.8088\n",
      "Epoch 159/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8213\n",
      "Epoch 159: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3876 - acc: 0.8216 - val_loss: 0.3793 - val_acc: 0.8137\n",
      "Epoch 160/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8161\n",
      "Epoch 160: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3912 - acc: 0.8162 - val_loss: 0.3971 - val_acc: 0.8015\n",
      "Epoch 161/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8193\n",
      "Epoch 161: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3834 - acc: 0.8202 - val_loss: 0.4239 - val_acc: 0.8044\n",
      "Epoch 162/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8169\n",
      "Epoch 162: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3911 - acc: 0.8170 - val_loss: 0.4250 - val_acc: 0.8048\n",
      "Epoch 163/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8179\n",
      "Epoch 163: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3900 - acc: 0.8181 - val_loss: 0.4252 - val_acc: 0.8068\n",
      "Epoch 164/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8241\n",
      "Epoch 164: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3853 - acc: 0.8245 - val_loss: 0.3781 - val_acc: 0.8141\n",
      "Epoch 165/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8215\n",
      "Epoch 165: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3867 - acc: 0.8219 - val_loss: 0.4038 - val_acc: 0.8080\n",
      "Epoch 166/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8242\n",
      "Epoch 166: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3807 - acc: 0.8247 - val_loss: 0.4153 - val_acc: 0.8104\n",
      "Epoch 167/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8223\n",
      "Epoch 167: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3821 - acc: 0.8225 - val_loss: 0.3864 - val_acc: 0.8117\n",
      "Epoch 168/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8291\n",
      "Epoch 168: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3807 - acc: 0.8295 - val_loss: 0.4306 - val_acc: 0.7975\n",
      "Epoch 169/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8254\n",
      "Epoch 169: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3860 - acc: 0.8256 - val_loss: 0.3803 - val_acc: 0.8088\n",
      "Epoch 170/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8248\n",
      "Epoch 170: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3848 - acc: 0.8250 - val_loss: 0.3876 - val_acc: 0.8181\n",
      "Epoch 171/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3854 - acc: 0.8201\n",
      "Epoch 171: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3854 - acc: 0.8201 - val_loss: 0.3762 - val_acc: 0.8246\n",
      "Epoch 172/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8245\n",
      "Epoch 172: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3856 - acc: 0.8245 - val_loss: 0.3771 - val_acc: 0.8202\n",
      "Epoch 173/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8235\n",
      "Epoch 173: val_acc did not improve from 0.82746\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3827 - acc: 0.8242 - val_loss: 0.3735 - val_acc: 0.8202\n",
      "Epoch 174/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8277\n",
      "Epoch 174: val_acc improved from 0.82746 to 0.83070, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3802 - acc: 0.8281 - val_loss: 0.3802 - val_acc: 0.8307\n",
      "Epoch 175/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8232\n",
      "Epoch 175: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3881 - acc: 0.8235 - val_loss: 0.3717 - val_acc: 0.8279\n",
      "Epoch 176/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8234\n",
      "Epoch 176: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3881 - acc: 0.8243 - val_loss: 0.3749 - val_acc: 0.8141\n",
      "Epoch 177/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8208\n",
      "Epoch 177: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3764 - acc: 0.8214 - val_loss: 0.3752 - val_acc: 0.8242\n",
      "Epoch 178/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3828 - acc: 0.8243\n",
      "Epoch 178: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3828 - acc: 0.8243 - val_loss: 0.3978 - val_acc: 0.8157\n",
      "Epoch 179/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8282\n",
      "Epoch 179: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3805 - acc: 0.8282 - val_loss: 0.3989 - val_acc: 0.8133\n",
      "Epoch 180/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3844 - acc: 0.8241\n",
      "Epoch 180: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3837 - acc: 0.8247 - val_loss: 0.4041 - val_acc: 0.8145\n",
      "Epoch 181/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8221\n",
      "Epoch 181: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3825 - acc: 0.8226 - val_loss: 0.4160 - val_acc: 0.8137\n",
      "Epoch 182/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3773 - acc: 0.8234\n",
      "Epoch 182: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3773 - acc: 0.8234 - val_loss: 0.4302 - val_acc: 0.8254\n",
      "Epoch 183/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3844 - acc: 0.8260\n",
      "Epoch 183: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3844 - acc: 0.8260 - val_loss: 0.3936 - val_acc: 0.8161\n",
      "Epoch 184/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3888 - acc: 0.8219\n",
      "Epoch 184: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3885 - acc: 0.8222 - val_loss: 0.3835 - val_acc: 0.8279\n",
      "Epoch 185/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8200\n",
      "Epoch 185: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3856 - acc: 0.8210 - val_loss: 0.4083 - val_acc: 0.8056\n",
      "Epoch 186/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3778 - acc: 0.8234\n",
      "Epoch 186: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3778 - acc: 0.8234 - val_loss: 0.4223 - val_acc: 0.7999\n",
      "Epoch 187/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8165\n",
      "Epoch 187: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3948 - acc: 0.8170 - val_loss: 0.4113 - val_acc: 0.8084\n",
      "Epoch 188/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8236\n",
      "Epoch 188: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3790 - acc: 0.8237 - val_loss: 0.3836 - val_acc: 0.8149\n",
      "Epoch 189/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8269\n",
      "Epoch 189: val_acc did not improve from 0.83070\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3768 - acc: 0.8277 - val_loss: 0.3903 - val_acc: 0.8218\n",
      "Epoch 190/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8236\n",
      "Epoch 190: val_acc improved from 0.83070 to 0.84083, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3790 - acc: 0.8244 - val_loss: 0.3686 - val_acc: 0.8408\n",
      "Epoch 191/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3759 - acc: 0.8255\n",
      "Epoch 191: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3759 - acc: 0.8255 - val_loss: 0.3888 - val_acc: 0.8169\n",
      "Epoch 192/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.8268\n",
      "Epoch 192: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3734 - acc: 0.8268 - val_loss: 0.4021 - val_acc: 0.8117\n",
      "Epoch 193/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8240\n",
      "Epoch 193: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.3766 - acc: 0.8245 - val_loss: 0.3638 - val_acc: 0.8315\n",
      "Epoch 194/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8194\n",
      "Epoch 194: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3806 - acc: 0.8201 - val_loss: 0.3872 - val_acc: 0.8100\n",
      "Epoch 195/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8221\n",
      "Epoch 195: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3794 - acc: 0.8221 - val_loss: 0.3701 - val_acc: 0.8226\n",
      "Epoch 196/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8255\n",
      "Epoch 196: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3773 - acc: 0.8258 - val_loss: 0.3727 - val_acc: 0.8315\n",
      "Epoch 197/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8251\n",
      "Epoch 197: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3751 - acc: 0.8254 - val_loss: 0.3863 - val_acc: 0.8234\n",
      "Epoch 198/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8257\n",
      "Epoch 198: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3748 - acc: 0.8263 - val_loss: 0.3696 - val_acc: 0.8230\n",
      "Epoch 199/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8258\n",
      "Epoch 199: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3743 - acc: 0.8262 - val_loss: 0.3841 - val_acc: 0.8056\n",
      "Epoch 200/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8305\n",
      "Epoch 200: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3665 - acc: 0.8304 - val_loss: 0.3766 - val_acc: 0.8210\n",
      "Epoch 201/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8231\n",
      "Epoch 201: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3721 - acc: 0.8235 - val_loss: 0.3822 - val_acc: 0.8109\n",
      "Epoch 202/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8266\n",
      "Epoch 202: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3747 - acc: 0.8266 - val_loss: 0.3964 - val_acc: 0.8092\n",
      "Epoch 203/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8229\n",
      "Epoch 203: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3761 - acc: 0.8232 - val_loss: 0.3582 - val_acc: 0.8299\n",
      "Epoch 204/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3784 - acc: 0.8192\n",
      "Epoch 204: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3784 - acc: 0.8192 - val_loss: 0.3841 - val_acc: 0.8084\n",
      "Epoch 205/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8306\n",
      "Epoch 205: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3707 - acc: 0.8305 - val_loss: 0.3661 - val_acc: 0.8275\n",
      "Epoch 206/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3699 - acc: 0.8301\n",
      "Epoch 206: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3693 - acc: 0.8302 - val_loss: 0.3983 - val_acc: 0.8056\n",
      "Epoch 207/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8275\n",
      "Epoch 207: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3783 - acc: 0.8280 - val_loss: 0.3659 - val_acc: 0.8295\n",
      "Epoch 208/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8281\n",
      "Epoch 208: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3754 - acc: 0.8283 - val_loss: 0.3727 - val_acc: 0.8210\n",
      "Epoch 209/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8253\n",
      "Epoch 209: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3720 - acc: 0.8252 - val_loss: 0.3812 - val_acc: 0.8084\n",
      "Epoch 210/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8272\n",
      "Epoch 210: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3777 - acc: 0.8275 - val_loss: 0.3832 - val_acc: 0.8194\n",
      "Epoch 211/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8240\n",
      "Epoch 211: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3804 - acc: 0.8246 - val_loss: 0.3980 - val_acc: 0.8299\n",
      "Epoch 212/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8266\n",
      "Epoch 212: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3756 - acc: 0.8271 - val_loss: 0.3932 - val_acc: 0.8185\n",
      "Epoch 213/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8312\n",
      "Epoch 213: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3704 - acc: 0.8311 - val_loss: 0.3870 - val_acc: 0.8214\n",
      "Epoch 214/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8269\n",
      "Epoch 214: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3792 - acc: 0.8274 - val_loss: 0.3718 - val_acc: 0.8299\n",
      "Epoch 215/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8286\n",
      "Epoch 215: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3774 - acc: 0.8290 - val_loss: 0.3568 - val_acc: 0.8380\n",
      "Epoch 216/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8302\n",
      "Epoch 216: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3672 - acc: 0.8302 - val_loss: 0.3845 - val_acc: 0.8311\n",
      "Epoch 217/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8244\n",
      "Epoch 217: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3805 - acc: 0.8244 - val_loss: 0.3849 - val_acc: 0.8307\n",
      "Epoch 218/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8256\n",
      "Epoch 218: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3777 - acc: 0.8255 - val_loss: 0.3720 - val_acc: 0.8283\n",
      "Epoch 219/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8324\n",
      "Epoch 219: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3645 - acc: 0.8326 - val_loss: 0.4511 - val_acc: 0.8311\n",
      "Epoch 220/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3710 - acc: 0.8273\n",
      "Epoch 220: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3710 - acc: 0.8273 - val_loss: 0.3738 - val_acc: 0.8258\n",
      "Epoch 221/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8278\n",
      "Epoch 221: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3752 - acc: 0.8283 - val_loss: 0.3872 - val_acc: 0.8218\n",
      "Epoch 222/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8348\n",
      "Epoch 222: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3635 - acc: 0.8352 - val_loss: 0.3716 - val_acc: 0.8323\n",
      "Epoch 223/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8235\n",
      "Epoch 223: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3867 - acc: 0.8239 - val_loss: 0.3934 - val_acc: 0.7938\n",
      "Epoch 224/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8261\n",
      "Epoch 224: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3798 - acc: 0.8263 - val_loss: 0.3817 - val_acc: 0.8177\n",
      "Epoch 225/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.8288\n",
      "Epoch 225: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3692 - acc: 0.8295 - val_loss: 0.3659 - val_acc: 0.8254\n",
      "Epoch 226/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8316\n",
      "Epoch 226: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3677 - acc: 0.8319 - val_loss: 0.3937 - val_acc: 0.8291\n",
      "Epoch 227/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8309\n",
      "Epoch 227: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3706 - acc: 0.8315 - val_loss: 0.4106 - val_acc: 0.8214\n",
      "Epoch 228/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8253\n",
      "Epoch 228: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3737 - acc: 0.8258 - val_loss: 0.3863 - val_acc: 0.8262\n",
      "Epoch 229/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8340\n",
      "Epoch 229: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3667 - acc: 0.8343 - val_loss: 0.3690 - val_acc: 0.8214\n",
      "Epoch 230/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.8292\n",
      "Epoch 230: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3718 - acc: 0.8300 - val_loss: 0.3779 - val_acc: 0.8254\n",
      "Epoch 231/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8338\n",
      "Epoch 231: val_acc improved from 0.84083 to 0.84731, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3602 - acc: 0.8347 - val_loss: 0.3623 - val_acc: 0.8473\n",
      "Epoch 232/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8309\n",
      "Epoch 232: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3693 - acc: 0.8310 - val_loss: 0.4068 - val_acc: 0.8165\n",
      "Epoch 233/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3592 - acc: 0.8333\n",
      "Epoch 233: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3592 - acc: 0.8333 - val_loss: 0.3750 - val_acc: 0.8319\n",
      "Epoch 234/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8302\n",
      "Epoch 234: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3671 - acc: 0.8302 - val_loss: 0.3907 - val_acc: 0.8177\n",
      "Epoch 235/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3654 - acc: 0.8337\n",
      "Epoch 235: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3646 - acc: 0.8341 - val_loss: 0.3958 - val_acc: 0.8250\n",
      "Epoch 236/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8287\n",
      "Epoch 236: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3731 - acc: 0.8291 - val_loss: 0.3998 - val_acc: 0.8206\n",
      "Epoch 237/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8316\n",
      "Epoch 237: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3622 - acc: 0.8316 - val_loss: 0.3721 - val_acc: 0.8299\n",
      "Epoch 238/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8330\n",
      "Epoch 238: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3657 - acc: 0.8334 - val_loss: 0.3740 - val_acc: 0.8396\n",
      "Epoch 239/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8327\n",
      "Epoch 239: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3637 - acc: 0.8330 - val_loss: 0.3737 - val_acc: 0.8384\n",
      "Epoch 240/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.8307\n",
      "Epoch 240: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3679 - acc: 0.8312 - val_loss: 0.3800 - val_acc: 0.8234\n",
      "Epoch 241/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8321\n",
      "Epoch 241: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3651 - acc: 0.8324 - val_loss: 0.3769 - val_acc: 0.8185\n",
      "Epoch 242/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8304\n",
      "Epoch 242: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3700 - acc: 0.8308 - val_loss: 0.3983 - val_acc: 0.8194\n",
      "Epoch 243/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3724 - acc: 0.8269\n",
      "Epoch 243: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3724 - acc: 0.8269 - val_loss: 0.4020 - val_acc: 0.8307\n",
      "Epoch 244/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8334\n",
      "Epoch 244: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3652 - acc: 0.8334 - val_loss: 0.3773 - val_acc: 0.8315\n",
      "Epoch 245/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8308\n",
      "Epoch 245: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3709 - acc: 0.8309 - val_loss: 0.3719 - val_acc: 0.8372\n",
      "Epoch 246/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8379\n",
      "Epoch 246: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3606 - acc: 0.8383 - val_loss: 0.4054 - val_acc: 0.8173\n",
      "Epoch 247/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8330\n",
      "Epoch 247: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3662 - acc: 0.8337 - val_loss: 0.3917 - val_acc: 0.8254\n",
      "Epoch 248/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8334\n",
      "Epoch 248: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3632 - acc: 0.8338 - val_loss: 0.3608 - val_acc: 0.8380\n",
      "Epoch 249/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8345\n",
      "Epoch 249: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3670 - acc: 0.8345 - val_loss: 0.4137 - val_acc: 0.8185\n",
      "Epoch 250/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8349\n",
      "Epoch 250: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3581 - acc: 0.8349 - val_loss: 0.3921 - val_acc: 0.8185\n",
      "Epoch 251/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8273\n",
      "Epoch 251: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3766 - acc: 0.8277 - val_loss: 0.3792 - val_acc: 0.8343\n",
      "Epoch 252/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8310\n",
      "Epoch 252: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3607 - acc: 0.8319 - val_loss: 0.3753 - val_acc: 0.8307\n",
      "Epoch 253/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8316\n",
      "Epoch 253: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3639 - acc: 0.8319 - val_loss: 0.3894 - val_acc: 0.8335\n",
      "Epoch 254/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8343\n",
      "Epoch 254: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3626 - acc: 0.8347 - val_loss: 0.3995 - val_acc: 0.8210\n",
      "Epoch 255/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8292\n",
      "Epoch 255: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3711 - acc: 0.8300 - val_loss: 0.3705 - val_acc: 0.8404\n",
      "Epoch 256/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8280\n",
      "Epoch 256: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3671 - acc: 0.8281 - val_loss: 0.3795 - val_acc: 0.8262\n",
      "Epoch 257/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8362\n",
      "Epoch 257: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3641 - acc: 0.8365 - val_loss: 0.3876 - val_acc: 0.8315\n",
      "Epoch 258/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8359\n",
      "Epoch 258: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3582 - acc: 0.8360 - val_loss: 0.3757 - val_acc: 0.8348\n",
      "Epoch 259/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8349\n",
      "Epoch 259: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3600 - acc: 0.8353 - val_loss: 0.4013 - val_acc: 0.8149\n",
      "Epoch 260/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8311\n",
      "Epoch 260: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3629 - acc: 0.8315 - val_loss: 0.3716 - val_acc: 0.8343\n",
      "Epoch 261/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8320\n",
      "Epoch 261: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3657 - acc: 0.8324 - val_loss: 0.3845 - val_acc: 0.8190\n",
      "Epoch 262/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8293\n",
      "Epoch 262: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3686 - acc: 0.8295 - val_loss: 0.3715 - val_acc: 0.8226\n",
      "Epoch 263/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8338\n",
      "Epoch 263: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3600 - acc: 0.8341 - val_loss: 0.3674 - val_acc: 0.8360\n",
      "Epoch 264/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3573 - acc: 0.8359\n",
      "Epoch 264: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3573 - acc: 0.8359 - val_loss: 0.3977 - val_acc: 0.8437\n",
      "Epoch 265/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8340\n",
      "Epoch 265: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3595 - acc: 0.8343 - val_loss: 0.3906 - val_acc: 0.8250\n",
      "Epoch 266/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3604 - acc: 0.8361\n",
      "Epoch 266: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3604 - acc: 0.8361 - val_loss: 0.4015 - val_acc: 0.8307\n",
      "Epoch 267/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8279\n",
      "Epoch 267: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.3712 - acc: 0.8280 - val_loss: 0.3763 - val_acc: 0.8291\n",
      "Epoch 268/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8357\n",
      "Epoch 268: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3538 - acc: 0.8360 - val_loss: 0.3760 - val_acc: 0.8384\n",
      "Epoch 269/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8364\n",
      "Epoch 269: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3653 - acc: 0.8365 - val_loss: 0.3793 - val_acc: 0.8214\n",
      "Epoch 270/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8322\n",
      "Epoch 270: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3665 - acc: 0.8326 - val_loss: 0.3800 - val_acc: 0.8206\n",
      "Epoch 271/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8338\n",
      "Epoch 271: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3587 - acc: 0.8343 - val_loss: 0.3677 - val_acc: 0.8319\n",
      "Epoch 272/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8320\n",
      "Epoch 272: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3613 - acc: 0.8322 - val_loss: 0.3817 - val_acc: 0.8234\n",
      "Epoch 273/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3656 - acc: 0.8296\n",
      "Epoch 273: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3656 - acc: 0.8296 - val_loss: 0.3783 - val_acc: 0.8295\n",
      "Epoch 274/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8314\n",
      "Epoch 274: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3676 - acc: 0.8324 - val_loss: 0.3713 - val_acc: 0.8226\n",
      "Epoch 275/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8353\n",
      "Epoch 275: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3560 - acc: 0.8353 - val_loss: 0.3822 - val_acc: 0.8323\n",
      "Epoch 276/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8279\n",
      "Epoch 276: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3741 - acc: 0.8280 - val_loss: 0.3982 - val_acc: 0.8445\n",
      "Epoch 277/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3634 - acc: 0.8308\n",
      "Epoch 277: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3622 - acc: 0.8315 - val_loss: 0.4353 - val_acc: 0.8416\n",
      "Epoch 278/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8301\n",
      "Epoch 278: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3680 - acc: 0.8305 - val_loss: 0.3786 - val_acc: 0.8343\n",
      "Epoch 279/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8284\n",
      "Epoch 279: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3726 - acc: 0.8292 - val_loss: 0.3908 - val_acc: 0.8234\n",
      "Epoch 280/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8382\n",
      "Epoch 280: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3538 - acc: 0.8384 - val_loss: 0.3763 - val_acc: 0.8218\n",
      "Epoch 281/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8330\n",
      "Epoch 281: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3608 - acc: 0.8330 - val_loss: 0.4137 - val_acc: 0.8169\n",
      "Epoch 282/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8320\n",
      "Epoch 282: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3574 - acc: 0.8327 - val_loss: 0.3869 - val_acc: 0.8279\n",
      "Epoch 283/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8336\n",
      "Epoch 283: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3601 - acc: 0.8341 - val_loss: 0.4182 - val_acc: 0.8250\n",
      "Epoch 284/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8288\n",
      "Epoch 284: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3632 - acc: 0.8297 - val_loss: 0.3949 - val_acc: 0.8339\n",
      "Epoch 285/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8331\n",
      "Epoch 285: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3700 - acc: 0.8335 - val_loss: 0.3921 - val_acc: 0.8319\n",
      "Epoch 286/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8350\n",
      "Epoch 286: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3516 - acc: 0.8352 - val_loss: 0.3768 - val_acc: 0.8267\n",
      "Epoch 287/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8335\n",
      "Epoch 287: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3606 - acc: 0.8336 - val_loss: 0.3948 - val_acc: 0.8149\n",
      "Epoch 288/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8277\n",
      "Epoch 288: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3650 - acc: 0.8281 - val_loss: 0.3696 - val_acc: 0.8202\n",
      "Epoch 289/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8368\n",
      "Epoch 289: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3592 - acc: 0.8371 - val_loss: 0.3754 - val_acc: 0.8291\n",
      "Epoch 290/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8378\n",
      "Epoch 290: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3570 - acc: 0.8381 - val_loss: 0.3793 - val_acc: 0.8190\n",
      "Epoch 291/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.8333\n",
      "Epoch 291: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3597 - acc: 0.8339 - val_loss: 0.3727 - val_acc: 0.8210\n",
      "Epoch 292/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8371\n",
      "Epoch 292: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3585 - acc: 0.8375 - val_loss: 0.3823 - val_acc: 0.8169\n",
      "Epoch 293/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8331\n",
      "Epoch 293: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3650 - acc: 0.8331 - val_loss: 0.3930 - val_acc: 0.8283\n",
      "Epoch 294/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8338\n",
      "Epoch 294: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3673 - acc: 0.8342 - val_loss: 0.3745 - val_acc: 0.8275\n",
      "Epoch 295/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8323\n",
      "Epoch 295: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3572 - acc: 0.8327 - val_loss: 0.3604 - val_acc: 0.8412\n",
      "Epoch 296/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8364\n",
      "Epoch 296: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3579 - acc: 0.8370 - val_loss: 0.3706 - val_acc: 0.8226\n",
      "Epoch 297/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8352\n",
      "Epoch 297: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3601 - acc: 0.8357 - val_loss: 0.4080 - val_acc: 0.8356\n",
      "Epoch 298/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8352\n",
      "Epoch 298: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3594 - acc: 0.8359 - val_loss: 0.3818 - val_acc: 0.8303\n",
      "Epoch 299/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8349\n",
      "Epoch 299: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3589 - acc: 0.8352 - val_loss: 0.3753 - val_acc: 0.8283\n",
      "Epoch 300/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8382\n",
      "Epoch 300: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3555 - acc: 0.8383 - val_loss: 0.3943 - val_acc: 0.8339\n",
      "Epoch 301/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8385\n",
      "Epoch 301: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3593 - acc: 0.8389 - val_loss: 0.3687 - val_acc: 0.8267\n",
      "Epoch 302/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3620 - acc: 0.8355\n",
      "Epoch 302: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3620 - acc: 0.8355 - val_loss: 0.3613 - val_acc: 0.8372\n",
      "Epoch 303/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8347\n",
      "Epoch 303: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3609 - acc: 0.8352 - val_loss: 0.3653 - val_acc: 0.8222\n",
      "Epoch 304/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8340\n",
      "Epoch 304: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3621 - acc: 0.8344 - val_loss: 0.3758 - val_acc: 0.8331\n",
      "Epoch 305/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8301\n",
      "Epoch 305: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3659 - acc: 0.8301 - val_loss: 0.3813 - val_acc: 0.8230\n",
      "Epoch 306/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8358\n",
      "Epoch 306: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3566 - acc: 0.8360 - val_loss: 0.3730 - val_acc: 0.8441\n",
      "Epoch 307/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8294\n",
      "Epoch 307: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3630 - acc: 0.8295 - val_loss: 0.3944 - val_acc: 0.8364\n",
      "Epoch 308/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8373\n",
      "Epoch 308: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3550 - acc: 0.8374 - val_loss: 0.3987 - val_acc: 0.8307\n",
      "Epoch 309/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.8368\n",
      "Epoch 309: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3596 - acc: 0.8370 - val_loss: 0.3676 - val_acc: 0.8335\n",
      "Epoch 310/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8351\n",
      "Epoch 310: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3611 - acc: 0.8355 - val_loss: 0.3802 - val_acc: 0.8356\n",
      "Epoch 311/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8338\n",
      "Epoch 311: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3554 - acc: 0.8338 - val_loss: 0.4141 - val_acc: 0.8258\n",
      "Epoch 312/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3595 - acc: 0.8386\n",
      "Epoch 312: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3595 - acc: 0.8386 - val_loss: 0.3831 - val_acc: 0.8457\n",
      "Epoch 313/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8361\n",
      "Epoch 313: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3573 - acc: 0.8369 - val_loss: 0.3789 - val_acc: 0.8372\n",
      "Epoch 314/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8359\n",
      "Epoch 314: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3532 - acc: 0.8362 - val_loss: 0.3611 - val_acc: 0.8453\n",
      "Epoch 315/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8376\n",
      "Epoch 315: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3521 - acc: 0.8382 - val_loss: 0.3898 - val_acc: 0.8137\n",
      "Epoch 316/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8345\n",
      "Epoch 316: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3620 - acc: 0.8348 - val_loss: 0.3876 - val_acc: 0.8076\n",
      "Epoch 317/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8391\n",
      "Epoch 317: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3612 - acc: 0.8393 - val_loss: 0.3851 - val_acc: 0.8202\n",
      "Epoch 318/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8332\n",
      "Epoch 318: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3521 - acc: 0.8337 - val_loss: 0.3815 - val_acc: 0.8287\n",
      "Epoch 319/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8354\n",
      "Epoch 319: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3505 - acc: 0.8366 - val_loss: 0.4326 - val_acc: 0.8222\n",
      "Epoch 320/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8343\n",
      "Epoch 320: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3608 - acc: 0.8346 - val_loss: 0.3946 - val_acc: 0.8254\n",
      "Epoch 321/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8355\n",
      "Epoch 321: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3561 - acc: 0.8353 - val_loss: 0.4154 - val_acc: 0.8246\n",
      "Epoch 322/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8333\n",
      "Epoch 322: val_acc did not improve from 0.84731\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3561 - acc: 0.8340 - val_loss: 0.3897 - val_acc: 0.8295\n",
      "Epoch 323/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3495 - acc: 0.8364\n",
      "Epoch 323: val_acc improved from 0.84731 to 0.85946, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/4/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3495 - acc: 0.8364 - val_loss: 0.4063 - val_acc: 0.8595\n",
      "Epoch 324/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8371\n",
      "Epoch 324: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3548 - acc: 0.8375 - val_loss: 0.4071 - val_acc: 0.8177\n",
      "Epoch 325/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8311\n",
      "Epoch 325: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3657 - acc: 0.8313 - val_loss: 0.3949 - val_acc: 0.8218\n",
      "Epoch 326/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8306\n",
      "Epoch 326: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3670 - acc: 0.8309 - val_loss: 0.3977 - val_acc: 0.8230\n",
      "Epoch 327/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8396\n",
      "Epoch 327: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3496 - acc: 0.8397 - val_loss: 0.4127 - val_acc: 0.8275\n",
      "Epoch 328/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8398\n",
      "Epoch 328: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3504 - acc: 0.8403 - val_loss: 0.4019 - val_acc: 0.8343\n",
      "Epoch 329/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8354\n",
      "Epoch 329: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3556 - acc: 0.8354 - val_loss: 0.3826 - val_acc: 0.8372\n",
      "Epoch 330/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8352\n",
      "Epoch 330: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3613 - acc: 0.8358 - val_loss: 0.3892 - val_acc: 0.8303\n",
      "Epoch 331/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8466\n",
      "Epoch 331: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3474 - acc: 0.8468 - val_loss: 0.3644 - val_acc: 0.8339\n",
      "Epoch 332/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8340\n",
      "Epoch 332: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3633 - acc: 0.8344 - val_loss: 0.3798 - val_acc: 0.8429\n",
      "Epoch 333/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8305\n",
      "Epoch 333: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3640 - acc: 0.8307 - val_loss: 0.3866 - val_acc: 0.8206\n",
      "Epoch 334/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8343\n",
      "Epoch 334: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3496 - acc: 0.8346 - val_loss: 0.4064 - val_acc: 0.8295\n",
      "Epoch 335/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8386\n",
      "Epoch 335: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3494 - acc: 0.8386 - val_loss: 0.3765 - val_acc: 0.8465\n",
      "Epoch 336/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8374\n",
      "Epoch 336: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3555 - acc: 0.8377 - val_loss: 0.3910 - val_acc: 0.8190\n",
      "Epoch 337/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8373\n",
      "Epoch 337: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3495 - acc: 0.8376 - val_loss: 0.3851 - val_acc: 0.8384\n",
      "Epoch 338/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8315\n",
      "Epoch 338: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3596 - acc: 0.8319 - val_loss: 0.3717 - val_acc: 0.8303\n",
      "Epoch 339/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8330\n",
      "Epoch 339: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3668 - acc: 0.8332 - val_loss: 0.3765 - val_acc: 0.8246\n",
      "Epoch 340/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8392\n",
      "Epoch 340: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3569 - acc: 0.8395 - val_loss: 0.3548 - val_acc: 0.8372\n",
      "Epoch 341/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8356\n",
      "Epoch 341: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3602 - acc: 0.8357 - val_loss: 0.3792 - val_acc: 0.8392\n",
      "Epoch 342/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8357\n",
      "Epoch 342: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3590 - acc: 0.8360 - val_loss: 0.3715 - val_acc: 0.8157\n",
      "Epoch 343/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8389\n",
      "Epoch 343: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3639 - acc: 0.8391 - val_loss: 0.3627 - val_acc: 0.8368\n",
      "Epoch 344/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3541 - acc: 0.8327\n",
      "Epoch 344: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3541 - acc: 0.8327 - val_loss: 0.3768 - val_acc: 0.8400\n",
      "Epoch 345/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8361\n",
      "Epoch 345: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3539 - acc: 0.8365 - val_loss: 0.3748 - val_acc: 0.8206\n",
      "Epoch 346/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8426\n",
      "Epoch 346: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3467 - acc: 0.8428 - val_loss: 0.3883 - val_acc: 0.8157\n",
      "Epoch 347/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8406\n",
      "Epoch 347: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3489 - acc: 0.8407 - val_loss: 0.3823 - val_acc: 0.8153\n",
      "Epoch 348/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8380\n",
      "Epoch 348: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3505 - acc: 0.8380 - val_loss: 0.3748 - val_acc: 0.8343\n",
      "Epoch 349/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8401\n",
      "Epoch 349: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3489 - acc: 0.8404 - val_loss: 0.3681 - val_acc: 0.8190\n",
      "Epoch 350/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8356\n",
      "Epoch 350: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3655 - acc: 0.8358 - val_loss: 0.3759 - val_acc: 0.8206\n",
      "Epoch 351/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3510 - acc: 0.8381\n",
      "Epoch 351: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3510 - acc: 0.8381 - val_loss: 0.3812 - val_acc: 0.8364\n",
      "Epoch 352/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8363\n",
      "Epoch 352: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3551 - acc: 0.8364 - val_loss: 0.3477 - val_acc: 0.8595\n",
      "Epoch 353/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8323\n",
      "Epoch 353: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3602 - acc: 0.8326 - val_loss: 0.3888 - val_acc: 0.8173\n",
      "Epoch 354/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8370\n",
      "Epoch 354: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3502 - acc: 0.8379 - val_loss: 0.3848 - val_acc: 0.8376\n",
      "Epoch 355/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8374\n",
      "Epoch 355: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3545 - acc: 0.8374 - val_loss: 0.4260 - val_acc: 0.8262\n",
      "Epoch 356/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8384\n",
      "Epoch 356: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3499 - acc: 0.8384 - val_loss: 0.3913 - val_acc: 0.8327\n",
      "Epoch 357/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8363\n",
      "Epoch 357: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3532 - acc: 0.8370 - val_loss: 0.4281 - val_acc: 0.8222\n",
      "Epoch 358/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8394\n",
      "Epoch 358: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3524 - acc: 0.8395 - val_loss: 0.4145 - val_acc: 0.8181\n",
      "Epoch 359/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8400\n",
      "Epoch 359: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3437 - acc: 0.8408 - val_loss: 0.4266 - val_acc: 0.8392\n",
      "Epoch 360/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8408\n",
      "Epoch 360: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3463 - acc: 0.8413 - val_loss: 0.4215 - val_acc: 0.8242\n",
      "Epoch 361/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8353\n",
      "Epoch 361: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3559 - acc: 0.8363 - val_loss: 0.3871 - val_acc: 0.8380\n",
      "Epoch 362/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8391\n",
      "Epoch 362: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3513 - acc: 0.8396 - val_loss: 0.4184 - val_acc: 0.8133\n",
      "Epoch 363/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3671 - acc: 0.8395\n",
      "Epoch 363: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3661 - acc: 0.8401 - val_loss: 0.3897 - val_acc: 0.8416\n",
      "Epoch 364/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8404\n",
      "Epoch 364: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3536 - acc: 0.8411 - val_loss: 0.3897 - val_acc: 0.8449\n",
      "Epoch 365/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8364\n",
      "Epoch 365: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3513 - acc: 0.8371 - val_loss: 0.3779 - val_acc: 0.8376\n",
      "Epoch 366/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8392\n",
      "Epoch 366: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3586 - acc: 0.8393 - val_loss: 0.3914 - val_acc: 0.8198\n",
      "Epoch 367/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8373\n",
      "Epoch 367: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3538 - acc: 0.8377 - val_loss: 0.4162 - val_acc: 0.8283\n",
      "Epoch 368/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8327\n",
      "Epoch 368: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3628 - acc: 0.8330 - val_loss: 0.3987 - val_acc: 0.8190\n",
      "Epoch 369/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8364\n",
      "Epoch 369: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3559 - acc: 0.8366 - val_loss: 0.4147 - val_acc: 0.8262\n",
      "Epoch 370/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8397\n",
      "Epoch 370: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3541 - acc: 0.8404 - val_loss: 0.3702 - val_acc: 0.8388\n",
      "Epoch 371/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8348\n",
      "Epoch 371: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3514 - acc: 0.8351 - val_loss: 0.3717 - val_acc: 0.8315\n",
      "Epoch 372/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3488 - acc: 0.8413\n",
      "Epoch 372: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3480 - acc: 0.8418 - val_loss: 0.3767 - val_acc: 0.8469\n",
      "Epoch 373/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3480 - acc: 0.8377\n",
      "Epoch 373: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3480 - acc: 0.8377 - val_loss: 0.3827 - val_acc: 0.8262\n",
      "Epoch 374/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8399\n",
      "Epoch 374: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3539 - acc: 0.8398 - val_loss: 0.3748 - val_acc: 0.8453\n",
      "Epoch 375/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8355\n",
      "Epoch 375: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3542 - acc: 0.8354 - val_loss: 0.3660 - val_acc: 0.8356\n",
      "Epoch 376/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8396\n",
      "Epoch 376: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3489 - acc: 0.8398 - val_loss: 0.3898 - val_acc: 0.8449\n",
      "Epoch 377/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8424\n",
      "Epoch 377: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3560 - acc: 0.8428 - val_loss: 0.3907 - val_acc: 0.8307\n",
      "Epoch 378/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3572 - acc: 0.8372\n",
      "Epoch 378: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3558 - acc: 0.8376 - val_loss: 0.3587 - val_acc: 0.8412\n",
      "Epoch 379/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8357\n",
      "Epoch 379: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3569 - acc: 0.8361 - val_loss: 0.3905 - val_acc: 0.8267\n",
      "Epoch 380/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8377\n",
      "Epoch 380: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3551 - acc: 0.8382 - val_loss: 0.3761 - val_acc: 0.8388\n",
      "Epoch 381/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8418\n",
      "Epoch 381: val_acc did not improve from 0.85946\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3468 - acc: 0.8421 - val_loss: 0.3832 - val_acc: 0.8271\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 96, 1)             0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               49664     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,249\n",
      "Trainable params: 181,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 1.4041 - acc: 0.6316\n",
      "Epoch 1: val_acc improved from -inf to 0.69299, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 1.4041 - acc: 0.6316 - val_loss: 0.6284 - val_acc: 0.6930\n",
      "Epoch 2/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.6299 - acc: 0.6818\n",
      "Epoch 2: val_acc improved from 0.69299 to 0.71284, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.6291 - acc: 0.6821 - val_loss: 0.5855 - val_acc: 0.7128\n",
      "Epoch 3/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.7056\n",
      "Epoch 3: val_acc improved from 0.71284 to 0.72458, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.6021 - acc: 0.7062 - val_loss: 0.5645 - val_acc: 0.7246\n",
      "Epoch 4/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.7125\n",
      "Epoch 4: val_acc improved from 0.72458 to 0.72580, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5865 - acc: 0.7126 - val_loss: 0.5695 - val_acc: 0.7258\n",
      "Epoch 5/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.7197\n",
      "Epoch 5: val_acc improved from 0.72580 to 0.74484, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5782 - acc: 0.7202 - val_loss: 0.5466 - val_acc: 0.7448\n",
      "Epoch 6/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.5758 - acc: 0.7221\n",
      "Epoch 6: val_acc did not improve from 0.74484\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5751 - acc: 0.7226 - val_loss: 0.5410 - val_acc: 0.7331\n",
      "Epoch 7/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.7245\n",
      "Epoch 7: val_acc did not improve from 0.74484\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5685 - acc: 0.7246 - val_loss: 0.5320 - val_acc: 0.7359\n",
      "Epoch 8/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5599 - acc: 0.7293\n",
      "Epoch 8: val_acc did not improve from 0.74484\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5599 - acc: 0.7293 - val_loss: 0.5407 - val_acc: 0.7424\n",
      "Epoch 9/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.7335\n",
      "Epoch 9: val_acc improved from 0.74484 to 0.75051, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5570 - acc: 0.7339 - val_loss: 0.5300 - val_acc: 0.7505\n",
      "Epoch 10/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5474 - acc: 0.7374\n",
      "Epoch 10: val_acc did not improve from 0.75051\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5474 - acc: 0.7374 - val_loss: 0.5331 - val_acc: 0.7420\n",
      "Epoch 11/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7323\n",
      "Epoch 11: val_acc improved from 0.75051 to 0.75942, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5450 - acc: 0.7327 - val_loss: 0.5032 - val_acc: 0.7594\n",
      "Epoch 12/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7419\n",
      "Epoch 12: val_acc did not improve from 0.75942\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5380 - acc: 0.7426 - val_loss: 0.5047 - val_acc: 0.7525\n",
      "Epoch 13/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.7465\n",
      "Epoch 13: val_acc did not improve from 0.75942\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5301 - acc: 0.7468 - val_loss: 0.5076 - val_acc: 0.7554\n",
      "Epoch 14/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.7442\n",
      "Epoch 14: val_acc did not improve from 0.75942\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5311 - acc: 0.7444 - val_loss: 0.5162 - val_acc: 0.7424\n",
      "Epoch 15/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.7472\n",
      "Epoch 15: val_acc improved from 0.75942 to 0.76711, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5284 - acc: 0.7473 - val_loss: 0.4827 - val_acc: 0.7671\n",
      "Epoch 16/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.7456\n",
      "Epoch 16: val_acc improved from 0.76711 to 0.76833, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5285 - acc: 0.7460 - val_loss: 0.4754 - val_acc: 0.7683\n",
      "Epoch 17/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.7506\n",
      "Epoch 17: val_acc improved from 0.76833 to 0.76995, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5203 - acc: 0.7512 - val_loss: 0.4804 - val_acc: 0.7699\n",
      "Epoch 18/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.7533\n",
      "Epoch 18: val_acc did not improve from 0.76995\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5177 - acc: 0.7534 - val_loss: 0.4910 - val_acc: 0.7578\n",
      "Epoch 19/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.7587\n",
      "Epoch 19: val_acc did not improve from 0.76995\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5141 - acc: 0.7589 - val_loss: 0.4934 - val_acc: 0.7533\n",
      "Epoch 20/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.7581\n",
      "Epoch 20: val_acc did not improve from 0.76995\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5125 - acc: 0.7583 - val_loss: 0.4810 - val_acc: 0.7651\n",
      "Epoch 21/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.7625\n",
      "Epoch 21: val_acc improved from 0.76995 to 0.77400, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5077 - acc: 0.7630 - val_loss: 0.4674 - val_acc: 0.7740\n",
      "Epoch 22/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.7609\n",
      "Epoch 22: val_acc improved from 0.77400 to 0.77724, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5060 - acc: 0.7618 - val_loss: 0.4703 - val_acc: 0.7772\n",
      "Epoch 23/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.5063 - acc: 0.7529\n",
      "Epoch 23: val_acc did not improve from 0.77724\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5063 - acc: 0.7529 - val_loss: 0.4914 - val_acc: 0.7558\n",
      "Epoch 24/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.7628\n",
      "Epoch 24: val_acc did not improve from 0.77724\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5007 - acc: 0.7629 - val_loss: 0.4673 - val_acc: 0.7687\n",
      "Epoch 25/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4966 - acc: 0.7663\n",
      "Epoch 25: val_acc did not improve from 0.77724\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4962 - acc: 0.7663 - val_loss: 0.4745 - val_acc: 0.7639\n",
      "Epoch 26/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.7672\n",
      "Epoch 26: val_acc did not improve from 0.77724\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4949 - acc: 0.7675 - val_loss: 0.4874 - val_acc: 0.7631\n",
      "Epoch 27/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.7700\n",
      "Epoch 27: val_acc improved from 0.77724 to 0.78088, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4894 - acc: 0.7705 - val_loss: 0.4522 - val_acc: 0.7809\n",
      "Epoch 28/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.7627\n",
      "Epoch 28: val_acc did not improve from 0.78088\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.5057 - acc: 0.7627 - val_loss: 0.4711 - val_acc: 0.7785\n",
      "Epoch 29/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.7611\n",
      "Epoch 29: val_acc did not improve from 0.78088\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4994 - acc: 0.7615 - val_loss: 0.4896 - val_acc: 0.7643\n",
      "Epoch 30/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4887 - acc: 0.7686\n",
      "Epoch 30: val_acc did not improve from 0.78088\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4883 - acc: 0.7689 - val_loss: 0.4526 - val_acc: 0.7752\n",
      "Epoch 31/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.7712\n",
      "Epoch 31: val_acc did not improve from 0.78088\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4868 - acc: 0.7716 - val_loss: 0.4592 - val_acc: 0.7764\n",
      "Epoch 32/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.7697\n",
      "Epoch 32: val_acc did not improve from 0.78088\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4915 - acc: 0.7700 - val_loss: 0.4584 - val_acc: 0.7708\n",
      "Epoch 33/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.7708\n",
      "Epoch 33: val_acc did not improve from 0.78088\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4865 - acc: 0.7716 - val_loss: 0.4552 - val_acc: 0.7768\n",
      "Epoch 34/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4809 - acc: 0.7772\n",
      "Epoch 34: val_acc improved from 0.78088 to 0.78331, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4802 - acc: 0.7777 - val_loss: 0.4524 - val_acc: 0.7833\n",
      "Epoch 35/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.7746\n",
      "Epoch 35: val_acc did not improve from 0.78331\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4828 - acc: 0.7748 - val_loss: 0.4617 - val_acc: 0.7768\n",
      "Epoch 36/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.7721\n",
      "Epoch 36: val_acc improved from 0.78331 to 0.78493, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4844 - acc: 0.7723 - val_loss: 0.4492 - val_acc: 0.7849\n",
      "Epoch 37/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.7667\n",
      "Epoch 37: val_acc did not improve from 0.78493\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4890 - acc: 0.7669 - val_loss: 0.4594 - val_acc: 0.7699\n",
      "Epoch 38/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.7739\n",
      "Epoch 38: val_acc did not improve from 0.78493\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4770 - acc: 0.7744 - val_loss: 0.4418 - val_acc: 0.7813\n",
      "Epoch 39/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4761 - acc: 0.7781\n",
      "Epoch 39: val_acc did not improve from 0.78493\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4761 - acc: 0.7781 - val_loss: 0.4459 - val_acc: 0.7825\n",
      "Epoch 40/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.7798\n",
      "Epoch 40: val_acc did not improve from 0.78493\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4696 - acc: 0.7801 - val_loss: 0.4450 - val_acc: 0.7809\n",
      "Epoch 41/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4831 - acc: 0.7722\n",
      "Epoch 41: val_acc improved from 0.78493 to 0.78736, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4820 - acc: 0.7730 - val_loss: 0.4417 - val_acc: 0.7874\n",
      "Epoch 42/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4858 - acc: 0.7769\n",
      "Epoch 42: val_acc did not improve from 0.78736\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4856 - acc: 0.7773 - val_loss: 0.4755 - val_acc: 0.7785\n",
      "Epoch 43/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4794 - acc: 0.7740\n",
      "Epoch 43: val_acc improved from 0.78736 to 0.78939, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4784 - acc: 0.7745 - val_loss: 0.4329 - val_acc: 0.7894\n",
      "Epoch 44/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4629 - acc: 0.7863\n",
      "Epoch 44: val_acc did not improve from 0.78939\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4626 - acc: 0.7864 - val_loss: 0.4449 - val_acc: 0.7849\n",
      "Epoch 45/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.7782\n",
      "Epoch 45: val_acc did not improve from 0.78939\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4695 - acc: 0.7785 - val_loss: 0.4372 - val_acc: 0.7829\n",
      "Epoch 46/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.7797\n",
      "Epoch 46: val_acc did not improve from 0.78939\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4689 - acc: 0.7800 - val_loss: 0.4293 - val_acc: 0.7853\n",
      "Epoch 47/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.7832\n",
      "Epoch 47: val_acc improved from 0.78939 to 0.79303, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4604 - acc: 0.7840 - val_loss: 0.4439 - val_acc: 0.7930\n",
      "Epoch 48/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.7780\n",
      "Epoch 48: val_acc improved from 0.79303 to 0.80397, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4622 - acc: 0.7784 - val_loss: 0.4142 - val_acc: 0.8040\n",
      "Epoch 49/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.7836\n",
      "Epoch 49: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4616 - acc: 0.7839 - val_loss: 0.4446 - val_acc: 0.7785\n",
      "Epoch 50/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4664 - acc: 0.7810\n",
      "Epoch 50: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4664 - acc: 0.7810 - val_loss: 0.4310 - val_acc: 0.7861\n",
      "Epoch 51/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.7842\n",
      "Epoch 51: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4625 - acc: 0.7841 - val_loss: 0.4378 - val_acc: 0.7793\n",
      "Epoch 52/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.7846\n",
      "Epoch 52: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4580 - acc: 0.7844 - val_loss: 0.4375 - val_acc: 0.7825\n",
      "Epoch 53/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.7831\n",
      "Epoch 53: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4611 - acc: 0.7833 - val_loss: 0.4249 - val_acc: 0.7938\n",
      "Epoch 54/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.7875\n",
      "Epoch 54: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4507 - acc: 0.7878 - val_loss: 0.4165 - val_acc: 0.8003\n",
      "Epoch 55/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.7866\n",
      "Epoch 55: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4549 - acc: 0.7872 - val_loss: 0.4196 - val_acc: 0.7890\n",
      "Epoch 56/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4581 - acc: 0.7870\n",
      "Epoch 56: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4581 - acc: 0.7870 - val_loss: 0.4169 - val_acc: 0.7918\n",
      "Epoch 57/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4555 - acc: 0.7894\n",
      "Epoch 57: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4555 - acc: 0.7895 - val_loss: 0.4313 - val_acc: 0.7849\n",
      "Epoch 58/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.7884\n",
      "Epoch 58: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4495 - acc: 0.7889 - val_loss: 0.4302 - val_acc: 0.7845\n",
      "Epoch 59/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.7874\n",
      "Epoch 59: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4515 - acc: 0.7879 - val_loss: 0.4394 - val_acc: 0.7756\n",
      "Epoch 60/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4528 - acc: 0.7879\n",
      "Epoch 60: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4525 - acc: 0.7879 - val_loss: 0.4203 - val_acc: 0.7951\n",
      "Epoch 61/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4485 - acc: 0.7874\n",
      "Epoch 61: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4484 - acc: 0.7874 - val_loss: 0.4259 - val_acc: 0.7938\n",
      "Epoch 62/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.7840\n",
      "Epoch 62: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4630 - acc: 0.7843 - val_loss: 0.4347 - val_acc: 0.7829\n",
      "Epoch 63/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.7856\n",
      "Epoch 63: val_acc did not improve from 0.80397\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4537 - acc: 0.7860 - val_loss: 0.4136 - val_acc: 0.7930\n",
      "Epoch 64/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.7841\n",
      "Epoch 64: val_acc improved from 0.80397 to 0.80761, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4549 - acc: 0.7848 - val_loss: 0.4084 - val_acc: 0.8076\n",
      "Epoch 65/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.7909\n",
      "Epoch 65: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4485 - acc: 0.7911 - val_loss: 0.4174 - val_acc: 0.7947\n",
      "Epoch 66/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.7910\n",
      "Epoch 66: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4398 - acc: 0.7916 - val_loss: 0.4269 - val_acc: 0.7805\n",
      "Epoch 67/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.7928\n",
      "Epoch 67: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4360 - acc: 0.7936 - val_loss: 0.4262 - val_acc: 0.7947\n",
      "Epoch 68/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.7944\n",
      "Epoch 68: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4404 - acc: 0.7947 - val_loss: 0.4297 - val_acc: 0.7817\n",
      "Epoch 69/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.7886\n",
      "Epoch 69: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4509 - acc: 0.7888 - val_loss: 0.4217 - val_acc: 0.7922\n",
      "Epoch 70/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4483 - acc: 0.7872\n",
      "Epoch 70: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4476 - acc: 0.7876 - val_loss: 0.4196 - val_acc: 0.7930\n",
      "Epoch 71/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4473 - acc: 0.7904\n",
      "Epoch 71: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4473 - acc: 0.7904 - val_loss: 0.4130 - val_acc: 0.8011\n",
      "Epoch 72/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.7894\n",
      "Epoch 72: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4338 - acc: 0.7896 - val_loss: 0.3986 - val_acc: 0.7995\n",
      "Epoch 73/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.7900\n",
      "Epoch 73: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4370 - acc: 0.7904 - val_loss: 0.4122 - val_acc: 0.7987\n",
      "Epoch 74/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4421 - acc: 0.7917\n",
      "Epoch 74: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4412 - acc: 0.7924 - val_loss: 0.3983 - val_acc: 0.8044\n",
      "Epoch 75/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.7900\n",
      "Epoch 75: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4441 - acc: 0.7907 - val_loss: 0.3989 - val_acc: 0.8052\n",
      "Epoch 76/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4439 - acc: 0.7914\n",
      "Epoch 76: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4437 - acc: 0.7916 - val_loss: 0.4110 - val_acc: 0.7959\n",
      "Epoch 77/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.7911\n",
      "Epoch 77: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4385 - acc: 0.7912 - val_loss: 0.4241 - val_acc: 0.7789\n",
      "Epoch 78/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.7920\n",
      "Epoch 78: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4386 - acc: 0.7922 - val_loss: 0.3933 - val_acc: 0.8040\n",
      "Epoch 79/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4388 - acc: 0.7939\n",
      "Epoch 79: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4388 - acc: 0.7939 - val_loss: 0.4083 - val_acc: 0.7983\n",
      "Epoch 80/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.7982\n",
      "Epoch 80: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4313 - acc: 0.7988 - val_loss: 0.4127 - val_acc: 0.8011\n",
      "Epoch 81/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4379 - acc: 0.7918\n",
      "Epoch 81: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4371 - acc: 0.7923 - val_loss: 0.4222 - val_acc: 0.7886\n",
      "Epoch 82/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.7950\n",
      "Epoch 82: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4358 - acc: 0.7950 - val_loss: 0.3923 - val_acc: 0.8044\n",
      "Epoch 83/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.7969\n",
      "Epoch 83: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4355 - acc: 0.7970 - val_loss: 0.4046 - val_acc: 0.7926\n",
      "Epoch 84/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.7903\n",
      "Epoch 84: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4431 - acc: 0.7907 - val_loss: 0.4206 - val_acc: 0.7882\n",
      "Epoch 85/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.7862\n",
      "Epoch 85: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4475 - acc: 0.7865 - val_loss: 0.4016 - val_acc: 0.8036\n",
      "Epoch 86/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4384 - acc: 0.7887\n",
      "Epoch 86: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4384 - acc: 0.7887 - val_loss: 0.4001 - val_acc: 0.8048\n",
      "Epoch 87/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4349 - acc: 0.7934\n",
      "Epoch 87: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4347 - acc: 0.7935 - val_loss: 0.4252 - val_acc: 0.7922\n",
      "Epoch 88/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.7945\n",
      "Epoch 88: val_acc did not improve from 0.80761\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4366 - acc: 0.7947 - val_loss: 0.4060 - val_acc: 0.7971\n",
      "Epoch 89/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8019\n",
      "Epoch 89: val_acc improved from 0.80761 to 0.81369, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4271 - acc: 0.8025 - val_loss: 0.4000 - val_acc: 0.8137\n",
      "Epoch 90/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4292 - acc: 0.8008\n",
      "Epoch 90: val_acc did not improve from 0.81369\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4292 - acc: 0.8008 - val_loss: 0.4159 - val_acc: 0.7898\n",
      "Epoch 91/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.7931\n",
      "Epoch 91: val_acc improved from 0.81369 to 0.81652, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4325 - acc: 0.7940 - val_loss: 0.4029 - val_acc: 0.8165\n",
      "Epoch 92/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8016\n",
      "Epoch 92: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4331 - acc: 0.8021 - val_loss: 0.4071 - val_acc: 0.8007\n",
      "Epoch 93/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.7924\n",
      "Epoch 93: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4350 - acc: 0.7923 - val_loss: 0.4115 - val_acc: 0.7833\n",
      "Epoch 94/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.7948\n",
      "Epoch 94: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4318 - acc: 0.7951 - val_loss: 0.4080 - val_acc: 0.7967\n",
      "Epoch 95/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4387 - acc: 0.7926\n",
      "Epoch 95: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4379 - acc: 0.7934 - val_loss: 0.4009 - val_acc: 0.8036\n",
      "Epoch 96/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.7933\n",
      "Epoch 96: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4340 - acc: 0.7938 - val_loss: 0.4268 - val_acc: 0.7857\n",
      "Epoch 97/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.7921\n",
      "Epoch 97: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4323 - acc: 0.7924 - val_loss: 0.4136 - val_acc: 0.7983\n",
      "Epoch 98/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4291 - acc: 0.7971\n",
      "Epoch 98: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4289 - acc: 0.7972 - val_loss: 0.4236 - val_acc: 0.7942\n",
      "Epoch 99/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4297 - acc: 0.7941\n",
      "Epoch 99: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4293 - acc: 0.7943 - val_loss: 0.4126 - val_acc: 0.7926\n",
      "Epoch 100/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4290 - acc: 0.8013\n",
      "Epoch 100: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4288 - acc: 0.8015 - val_loss: 0.4003 - val_acc: 0.8048\n",
      "Epoch 101/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4210 - acc: 0.7978\n",
      "Epoch 101: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4204 - acc: 0.7983 - val_loss: 0.4103 - val_acc: 0.7991\n",
      "Epoch 102/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.7983\n",
      "Epoch 102: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4287 - acc: 0.7985 - val_loss: 0.3970 - val_acc: 0.7995\n",
      "Epoch 103/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.7963\n",
      "Epoch 103: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4247 - acc: 0.7966 - val_loss: 0.3850 - val_acc: 0.8137\n",
      "Epoch 104/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4184 - acc: 0.8033\n",
      "Epoch 104: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4184 - acc: 0.8033 - val_loss: 0.3815 - val_acc: 0.8096\n",
      "Epoch 105/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.7980\n",
      "Epoch 105: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4257 - acc: 0.7980 - val_loss: 0.4177 - val_acc: 0.7870\n",
      "Epoch 106/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8037\n",
      "Epoch 106: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4193 - acc: 0.8041 - val_loss: 0.4165 - val_acc: 0.8080\n",
      "Epoch 107/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.7970\n",
      "Epoch 107: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4238 - acc: 0.7977 - val_loss: 0.4050 - val_acc: 0.7991\n",
      "Epoch 108/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.7969\n",
      "Epoch 108: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4321 - acc: 0.7973 - val_loss: 0.3967 - val_acc: 0.8044\n",
      "Epoch 109/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.7945\n",
      "Epoch 109: val_acc did not improve from 0.81652\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4249 - acc: 0.7948 - val_loss: 0.4149 - val_acc: 0.8048\n",
      "Epoch 110/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.7958\n",
      "Epoch 110: val_acc improved from 0.81652 to 0.81855, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4264 - acc: 0.7959 - val_loss: 0.3813 - val_acc: 0.8185\n",
      "Epoch 111/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.8038\n",
      "Epoch 111: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4219 - acc: 0.8041 - val_loss: 0.3957 - val_acc: 0.8048\n",
      "Epoch 112/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.7942\n",
      "Epoch 112: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4310 - acc: 0.7941 - val_loss: 0.3986 - val_acc: 0.8104\n",
      "Epoch 113/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4186 - acc: 0.8029\n",
      "Epoch 113: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4186 - acc: 0.8029 - val_loss: 0.4030 - val_acc: 0.8040\n",
      "Epoch 114/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8080\n",
      "Epoch 114: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4149 - acc: 0.8082 - val_loss: 0.4094 - val_acc: 0.7963\n",
      "Epoch 115/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8036\n",
      "Epoch 115: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4160 - acc: 0.8039 - val_loss: 0.4013 - val_acc: 0.8084\n",
      "Epoch 116/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8030\n",
      "Epoch 116: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4259 - acc: 0.8030 - val_loss: 0.4060 - val_acc: 0.7983\n",
      "Epoch 117/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8012\n",
      "Epoch 117: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4170 - acc: 0.8013 - val_loss: 0.3942 - val_acc: 0.8084\n",
      "Epoch 118/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.7989\n",
      "Epoch 118: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4227 - acc: 0.7994 - val_loss: 0.3976 - val_acc: 0.8084\n",
      "Epoch 119/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8019\n",
      "Epoch 119: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4172 - acc: 0.8022 - val_loss: 0.3947 - val_acc: 0.8064\n",
      "Epoch 120/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4072 - acc: 0.8087\n",
      "Epoch 120: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4072 - acc: 0.8087 - val_loss: 0.3989 - val_acc: 0.8100\n",
      "Epoch 121/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.7987\n",
      "Epoch 121: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4191 - acc: 0.7990 - val_loss: 0.3904 - val_acc: 0.7995\n",
      "Epoch 122/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8002\n",
      "Epoch 122: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4181 - acc: 0.8009 - val_loss: 0.3960 - val_acc: 0.8068\n",
      "Epoch 123/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4230 - acc: 0.8021\n",
      "Epoch 123: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4222 - acc: 0.8027 - val_loss: 0.3869 - val_acc: 0.8149\n",
      "Epoch 124/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8008\n",
      "Epoch 124: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4224 - acc: 0.8018 - val_loss: 0.4031 - val_acc: 0.7987\n",
      "Epoch 125/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4120 - acc: 0.8055\n",
      "Epoch 125: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4120 - acc: 0.8055 - val_loss: 0.4016 - val_acc: 0.8109\n",
      "Epoch 126/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8001\n",
      "Epoch 126: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4194 - acc: 0.8003 - val_loss: 0.4040 - val_acc: 0.7983\n",
      "Epoch 127/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8010\n",
      "Epoch 127: val_acc did not improve from 0.81855\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4203 - acc: 0.8016 - val_loss: 0.4171 - val_acc: 0.7934\n",
      "Epoch 128/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.7949\n",
      "Epoch 128: val_acc improved from 0.81855 to 0.82260, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4274 - acc: 0.7955 - val_loss: 0.3792 - val_acc: 0.8226\n",
      "Epoch 129/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8090\n",
      "Epoch 129: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4099 - acc: 0.8094 - val_loss: 0.4137 - val_acc: 0.7987\n",
      "Epoch 130/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8009\n",
      "Epoch 130: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4178 - acc: 0.8015 - val_loss: 0.3946 - val_acc: 0.8145\n",
      "Epoch 131/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.7956\n",
      "Epoch 131: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4199 - acc: 0.7963 - val_loss: 0.4071 - val_acc: 0.7975\n",
      "Epoch 132/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8036\n",
      "Epoch 132: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4206 - acc: 0.8038 - val_loss: 0.4107 - val_acc: 0.7967\n",
      "Epoch 133/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8025\n",
      "Epoch 133: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4217 - acc: 0.8027 - val_loss: 0.3965 - val_acc: 0.8149\n",
      "Epoch 134/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8068\n",
      "Epoch 134: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4168 - acc: 0.8071 - val_loss: 0.3781 - val_acc: 0.8133\n",
      "Epoch 135/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8056\n",
      "Epoch 135: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4166 - acc: 0.8061 - val_loss: 0.3890 - val_acc: 0.8125\n",
      "Epoch 136/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8086\n",
      "Epoch 136: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4171 - acc: 0.8087 - val_loss: 0.3979 - val_acc: 0.8052\n",
      "Epoch 137/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8045\n",
      "Epoch 137: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4106 - acc: 0.8049 - val_loss: 0.3927 - val_acc: 0.8076\n",
      "Epoch 138/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8045\n",
      "Epoch 138: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4125 - acc: 0.8045 - val_loss: 0.3963 - val_acc: 0.8015\n",
      "Epoch 139/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8000\n",
      "Epoch 139: val_acc did not improve from 0.82260\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4102 - acc: 0.8001 - val_loss: 0.3882 - val_acc: 0.8080\n",
      "Epoch 140/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8043\n",
      "Epoch 140: val_acc improved from 0.82260 to 0.82949, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4137 - acc: 0.8044 - val_loss: 0.3813 - val_acc: 0.8295\n",
      "Epoch 141/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8023\n",
      "Epoch 141: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4184 - acc: 0.8026 - val_loss: 0.3834 - val_acc: 0.8149\n",
      "Epoch 142/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8009\n",
      "Epoch 142: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4207 - acc: 0.8013 - val_loss: 0.3965 - val_acc: 0.8052\n",
      "Epoch 143/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8027\n",
      "Epoch 143: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4142 - acc: 0.8028 - val_loss: 0.3982 - val_acc: 0.8145\n",
      "Epoch 144/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4068 - acc: 0.8039\n",
      "Epoch 144: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4068 - acc: 0.8039 - val_loss: 0.3956 - val_acc: 0.8121\n",
      "Epoch 145/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8078\n",
      "Epoch 145: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4132 - acc: 0.8080 - val_loss: 0.3877 - val_acc: 0.8194\n",
      "Epoch 146/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.8114\n",
      "Epoch 146: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4061 - acc: 0.8117 - val_loss: 0.3916 - val_acc: 0.8234\n",
      "Epoch 147/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8024\n",
      "Epoch 147: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4119 - acc: 0.8033 - val_loss: 0.3942 - val_acc: 0.8242\n",
      "Epoch 148/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8066\n",
      "Epoch 148: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4070 - acc: 0.8065 - val_loss: 0.3917 - val_acc: 0.8173\n",
      "Epoch 149/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4050 - acc: 0.8094\n",
      "Epoch 149: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4050 - acc: 0.8094 - val_loss: 0.3818 - val_acc: 0.8230\n",
      "Epoch 150/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4103 - acc: 0.8055\n",
      "Epoch 150: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4103 - acc: 0.8055 - val_loss: 0.3995 - val_acc: 0.8048\n",
      "Epoch 151/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4098 - acc: 0.8073\n",
      "Epoch 151: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4098 - acc: 0.8073 - val_loss: 0.3771 - val_acc: 0.8287\n",
      "Epoch 152/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8086\n",
      "Epoch 152: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4000 - acc: 0.8089 - val_loss: 0.4031 - val_acc: 0.8113\n",
      "Epoch 153/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8045\n",
      "Epoch 153: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4100 - acc: 0.8048 - val_loss: 0.3914 - val_acc: 0.8096\n",
      "Epoch 154/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8100\n",
      "Epoch 154: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4058 - acc: 0.8101 - val_loss: 0.3912 - val_acc: 0.8068\n",
      "Epoch 155/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8084\n",
      "Epoch 155: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4031 - acc: 0.8085 - val_loss: 0.3997 - val_acc: 0.7983\n",
      "Epoch 156/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3952 - acc: 0.8124\n",
      "Epoch 156: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3952 - acc: 0.8124 - val_loss: 0.3808 - val_acc: 0.8129\n",
      "Epoch 157/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8085\n",
      "Epoch 157: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3991 - acc: 0.8088 - val_loss: 0.4193 - val_acc: 0.8060\n",
      "Epoch 158/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8085\n",
      "Epoch 158: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4016 - acc: 0.8087 - val_loss: 0.3825 - val_acc: 0.8238\n",
      "Epoch 159/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8070\n",
      "Epoch 159: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4084 - acc: 0.8072 - val_loss: 0.3861 - val_acc: 0.8185\n",
      "Epoch 160/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8039\n",
      "Epoch 160: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4147 - acc: 0.8040 - val_loss: 0.3952 - val_acc: 0.8214\n",
      "Epoch 161/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8092\n",
      "Epoch 161: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4103 - acc: 0.8100 - val_loss: 0.3890 - val_acc: 0.8246\n",
      "Epoch 162/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8087\n",
      "Epoch 162: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3998 - acc: 0.8095 - val_loss: 0.3742 - val_acc: 0.8222\n",
      "Epoch 163/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8090\n",
      "Epoch 163: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4061 - acc: 0.8091 - val_loss: 0.3884 - val_acc: 0.8121\n",
      "Epoch 164/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8039\n",
      "Epoch 164: val_acc did not improve from 0.82949\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4093 - acc: 0.8041 - val_loss: 0.4172 - val_acc: 0.8036\n",
      "Epoch 165/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8115\n",
      "Epoch 165: val_acc improved from 0.82949 to 0.82989, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3996 - acc: 0.8119 - val_loss: 0.3848 - val_acc: 0.8299\n",
      "Epoch 166/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8109\n",
      "Epoch 166: val_acc did not improve from 0.82989\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4070 - acc: 0.8110 - val_loss: 0.4125 - val_acc: 0.8052\n",
      "Epoch 167/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8079\n",
      "Epoch 167: val_acc did not improve from 0.82989\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4072 - acc: 0.8080 - val_loss: 0.3763 - val_acc: 0.8287\n",
      "Epoch 168/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8078\n",
      "Epoch 168: val_acc improved from 0.82989 to 0.83556, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4010 - acc: 0.8080 - val_loss: 0.3679 - val_acc: 0.8356\n",
      "Epoch 169/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3960 - acc: 0.8126\n",
      "Epoch 169: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3960 - acc: 0.8126 - val_loss: 0.3791 - val_acc: 0.8262\n",
      "Epoch 170/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8123\n",
      "Epoch 170: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3971 - acc: 0.8128 - val_loss: 0.3855 - val_acc: 0.8117\n",
      "Epoch 171/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8051\n",
      "Epoch 171: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4018 - acc: 0.8052 - val_loss: 0.3861 - val_acc: 0.8319\n",
      "Epoch 172/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8106\n",
      "Epoch 172: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3983 - acc: 0.8108 - val_loss: 0.3747 - val_acc: 0.8181\n",
      "Epoch 173/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8074\n",
      "Epoch 173: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4073 - acc: 0.8078 - val_loss: 0.3825 - val_acc: 0.8157\n",
      "Epoch 174/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8089\n",
      "Epoch 174: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4020 - acc: 0.8091 - val_loss: 0.4006 - val_acc: 0.8048\n",
      "Epoch 175/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8102\n",
      "Epoch 175: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3937 - acc: 0.8111 - val_loss: 0.3847 - val_acc: 0.8145\n",
      "Epoch 176/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8121\n",
      "Epoch 176: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3983 - acc: 0.8124 - val_loss: 0.3784 - val_acc: 0.8072\n",
      "Epoch 177/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8053\n",
      "Epoch 177: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4082 - acc: 0.8059 - val_loss: 0.3793 - val_acc: 0.8307\n",
      "Epoch 178/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8086\n",
      "Epoch 178: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3966 - acc: 0.8086 - val_loss: 0.3803 - val_acc: 0.8076\n",
      "Epoch 179/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8021\n",
      "Epoch 179: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4041 - acc: 0.8019 - val_loss: 0.4266 - val_acc: 0.7898\n",
      "Epoch 180/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8046\n",
      "Epoch 180: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4010 - acc: 0.8046 - val_loss: 0.3894 - val_acc: 0.8343\n",
      "Epoch 181/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8046\n",
      "Epoch 181: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4050 - acc: 0.8051 - val_loss: 0.3874 - val_acc: 0.8072\n",
      "Epoch 182/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8122\n",
      "Epoch 182: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3953 - acc: 0.8125 - val_loss: 0.3908 - val_acc: 0.8088\n",
      "Epoch 183/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8112\n",
      "Epoch 183: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3963 - acc: 0.8119 - val_loss: 0.4205 - val_acc: 0.7951\n",
      "Epoch 184/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8108\n",
      "Epoch 184: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4022 - acc: 0.8112 - val_loss: 0.3758 - val_acc: 0.8214\n",
      "Epoch 185/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8052\n",
      "Epoch 185: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4054 - acc: 0.8054 - val_loss: 0.3884 - val_acc: 0.8194\n",
      "Epoch 186/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8136\n",
      "Epoch 186: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3994 - acc: 0.8137 - val_loss: 0.3923 - val_acc: 0.8185\n",
      "Epoch 187/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8110\n",
      "Epoch 187: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3966 - acc: 0.8110 - val_loss: 0.3964 - val_acc: 0.8129\n",
      "Epoch 188/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8138\n",
      "Epoch 188: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3965 - acc: 0.8143 - val_loss: 0.3762 - val_acc: 0.8177\n",
      "Epoch 189/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8106\n",
      "Epoch 189: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4004 - acc: 0.8107 - val_loss: 0.4107 - val_acc: 0.8040\n",
      "Epoch 190/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8075\n",
      "Epoch 190: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3983 - acc: 0.8080 - val_loss: 0.3824 - val_acc: 0.8129\n",
      "Epoch 191/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8120\n",
      "Epoch 191: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3973 - acc: 0.8126 - val_loss: 0.3918 - val_acc: 0.8113\n",
      "Epoch 192/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8102\n",
      "Epoch 192: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3989 - acc: 0.8103 - val_loss: 0.4093 - val_acc: 0.8015\n",
      "Epoch 193/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8093\n",
      "Epoch 193: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3986 - acc: 0.8101 - val_loss: 0.3820 - val_acc: 0.8113\n",
      "Epoch 194/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8120\n",
      "Epoch 194: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3913 - acc: 0.8126 - val_loss: 0.3813 - val_acc: 0.8141\n",
      "Epoch 195/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4020 - acc: 0.8079\n",
      "Epoch 195: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4012 - acc: 0.8084 - val_loss: 0.3799 - val_acc: 0.8145\n",
      "Epoch 196/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8103\n",
      "Epoch 196: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4013 - acc: 0.8103 - val_loss: 0.3875 - val_acc: 0.8246\n",
      "Epoch 197/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8121\n",
      "Epoch 197: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3995 - acc: 0.8121 - val_loss: 0.4052 - val_acc: 0.8023\n",
      "Epoch 198/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3982 - acc: 0.8122\n",
      "Epoch 198: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3982 - acc: 0.8122 - val_loss: 0.3993 - val_acc: 0.8141\n",
      "Epoch 199/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8150\n",
      "Epoch 199: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3878 - acc: 0.8158 - val_loss: 0.3905 - val_acc: 0.8238\n",
      "Epoch 200/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8077\n",
      "Epoch 200: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3993 - acc: 0.8081 - val_loss: 0.3810 - val_acc: 0.8335\n",
      "Epoch 201/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3941 - acc: 0.8114\n",
      "Epoch 201: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3941 - acc: 0.8114 - val_loss: 0.4173 - val_acc: 0.7955\n",
      "Epoch 202/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8131\n",
      "Epoch 202: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3975 - acc: 0.8132 - val_loss: 0.3990 - val_acc: 0.8084\n",
      "Epoch 203/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8113\n",
      "Epoch 203: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3915 - acc: 0.8113 - val_loss: 0.3910 - val_acc: 0.8023\n",
      "Epoch 204/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8117\n",
      "Epoch 204: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4036 - acc: 0.8116 - val_loss: 0.3815 - val_acc: 0.8230\n",
      "Epoch 205/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8099\n",
      "Epoch 205: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3945 - acc: 0.8104 - val_loss: 0.3890 - val_acc: 0.8141\n",
      "Epoch 206/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8095\n",
      "Epoch 206: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4013 - acc: 0.8100 - val_loss: 0.3750 - val_acc: 0.8145\n",
      "Epoch 207/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8152\n",
      "Epoch 207: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3899 - acc: 0.8153 - val_loss: 0.3851 - val_acc: 0.8137\n",
      "Epoch 208/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8092\n",
      "Epoch 208: val_acc did not improve from 0.83556\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3974 - acc: 0.8097 - val_loss: 0.4030 - val_acc: 0.8028\n",
      "Epoch 209/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8147\n",
      "Epoch 209: val_acc improved from 0.83556 to 0.84083, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3929 - acc: 0.8146 - val_loss: 0.3754 - val_acc: 0.8408\n",
      "Epoch 210/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8136\n",
      "Epoch 210: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3896 - acc: 0.8137 - val_loss: 0.4107 - val_acc: 0.8072\n",
      "Epoch 211/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8131\n",
      "Epoch 211: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3954 - acc: 0.8133 - val_loss: 0.3688 - val_acc: 0.8299\n",
      "Epoch 212/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8093\n",
      "Epoch 212: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4010 - acc: 0.8095 - val_loss: 0.3809 - val_acc: 0.8254\n",
      "Epoch 213/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8137\n",
      "Epoch 213: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3998 - acc: 0.8137 - val_loss: 0.4011 - val_acc: 0.8210\n",
      "Epoch 214/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8094\n",
      "Epoch 214: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3988 - acc: 0.8095 - val_loss: 0.3822 - val_acc: 0.8141\n",
      "Epoch 215/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8129\n",
      "Epoch 215: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3893 - acc: 0.8133 - val_loss: 0.3990 - val_acc: 0.8194\n",
      "Epoch 216/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8124\n",
      "Epoch 216: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3982 - acc: 0.8123 - val_loss: 0.3979 - val_acc: 0.8185\n",
      "Epoch 217/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8107\n",
      "Epoch 217: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3988 - acc: 0.8111 - val_loss: 0.3798 - val_acc: 0.8242\n",
      "Epoch 218/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8161\n",
      "Epoch 218: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3880 - acc: 0.8161 - val_loss: 0.3853 - val_acc: 0.8092\n",
      "Epoch 219/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8112\n",
      "Epoch 219: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3900 - acc: 0.8116 - val_loss: 0.3834 - val_acc: 0.8271\n",
      "Epoch 220/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8096\n",
      "Epoch 220: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3977 - acc: 0.8099 - val_loss: 0.3771 - val_acc: 0.8267\n",
      "Epoch 221/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8168\n",
      "Epoch 221: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3922 - acc: 0.8169 - val_loss: 0.3784 - val_acc: 0.8145\n",
      "Epoch 222/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8182\n",
      "Epoch 222: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3885 - acc: 0.8183 - val_loss: 0.3721 - val_acc: 0.8279\n",
      "Epoch 223/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3920 - acc: 0.8140\n",
      "Epoch 223: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3909 - acc: 0.8143 - val_loss: 0.4067 - val_acc: 0.8125\n",
      "Epoch 224/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8179\n",
      "Epoch 224: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3838 - acc: 0.8182 - val_loss: 0.3920 - val_acc: 0.8048\n",
      "Epoch 225/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8079\n",
      "Epoch 225: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3956 - acc: 0.8081 - val_loss: 0.4000 - val_acc: 0.8145\n",
      "Epoch 226/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3930 - acc: 0.8153\n",
      "Epoch 226: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3930 - acc: 0.8153 - val_loss: 0.3952 - val_acc: 0.8023\n",
      "Epoch 227/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8105\n",
      "Epoch 227: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3986 - acc: 0.8106 - val_loss: 0.3862 - val_acc: 0.8165\n",
      "Epoch 228/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8140\n",
      "Epoch 228: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3949 - acc: 0.8145 - val_loss: 0.3871 - val_acc: 0.8226\n",
      "Epoch 229/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8181\n",
      "Epoch 229: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3884 - acc: 0.8184 - val_loss: 0.4032 - val_acc: 0.8315\n",
      "Epoch 230/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3909 - acc: 0.8137\n",
      "Epoch 230: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3909 - acc: 0.8137 - val_loss: 0.3834 - val_acc: 0.8230\n",
      "Epoch 231/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8164\n",
      "Epoch 231: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3909 - acc: 0.8162 - val_loss: 0.4101 - val_acc: 0.8088\n",
      "Epoch 232/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8131\n",
      "Epoch 232: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3866 - acc: 0.8136 - val_loss: 0.3688 - val_acc: 0.8194\n",
      "Epoch 233/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8172\n",
      "Epoch 233: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3902 - acc: 0.8172 - val_loss: 0.3836 - val_acc: 0.8331\n",
      "Epoch 234/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8136\n",
      "Epoch 234: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3854 - acc: 0.8141 - val_loss: 0.3886 - val_acc: 0.8210\n",
      "Epoch 235/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.4036 - acc: 0.8094\n",
      "Epoch 235: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.4036 - acc: 0.8094 - val_loss: 0.3820 - val_acc: 0.8291\n",
      "Epoch 236/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8162\n",
      "Epoch 236: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3838 - acc: 0.8162 - val_loss: 0.3864 - val_acc: 0.8096\n",
      "Epoch 237/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8161\n",
      "Epoch 237: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3842 - acc: 0.8167 - val_loss: 0.3996 - val_acc: 0.8161\n",
      "Epoch 238/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8133\n",
      "Epoch 238: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3918 - acc: 0.8134 - val_loss: 0.4182 - val_acc: 0.8088\n",
      "Epoch 239/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8183\n",
      "Epoch 239: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3836 - acc: 0.8189 - val_loss: 0.4091 - val_acc: 0.8100\n",
      "Epoch 240/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8167\n",
      "Epoch 240: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3810 - acc: 0.8168 - val_loss: 0.3782 - val_acc: 0.8258\n",
      "Epoch 241/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8108\n",
      "Epoch 241: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3937 - acc: 0.8111 - val_loss: 0.3793 - val_acc: 0.8096\n",
      "Epoch 242/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8132\n",
      "Epoch 242: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3895 - acc: 0.8137 - val_loss: 0.3979 - val_acc: 0.8287\n",
      "Epoch 243/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8175\n",
      "Epoch 243: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3880 - acc: 0.8175 - val_loss: 0.3669 - val_acc: 0.8190\n",
      "Epoch 244/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8101\n",
      "Epoch 244: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3966 - acc: 0.8102 - val_loss: 0.3875 - val_acc: 0.8092\n",
      "Epoch 245/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8118\n",
      "Epoch 245: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3912 - acc: 0.8120 - val_loss: 0.3894 - val_acc: 0.8076\n",
      "Epoch 246/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8145\n",
      "Epoch 246: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3845 - acc: 0.8146 - val_loss: 0.3718 - val_acc: 0.8230\n",
      "Epoch 247/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3887 - acc: 0.8126\n",
      "Epoch 247: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3887 - acc: 0.8126 - val_loss: 0.3978 - val_acc: 0.8190\n",
      "Epoch 248/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8109\n",
      "Epoch 248: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3879 - acc: 0.8114 - val_loss: 0.4125 - val_acc: 0.8153\n",
      "Epoch 249/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3911 - acc: 0.8103\n",
      "Epoch 249: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3911 - acc: 0.8103 - val_loss: 0.4050 - val_acc: 0.8117\n",
      "Epoch 250/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8143\n",
      "Epoch 250: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3827 - acc: 0.8143 - val_loss: 0.3896 - val_acc: 0.8250\n",
      "Epoch 251/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8181\n",
      "Epoch 251: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3802 - acc: 0.8182 - val_loss: 0.3861 - val_acc: 0.8339\n",
      "Epoch 252/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8152\n",
      "Epoch 252: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3943 - acc: 0.8153 - val_loss: 0.3902 - val_acc: 0.8177\n",
      "Epoch 253/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8171\n",
      "Epoch 253: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3785 - acc: 0.8175 - val_loss: 0.3833 - val_acc: 0.8157\n",
      "Epoch 254/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8125\n",
      "Epoch 254: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3919 - acc: 0.8121 - val_loss: 0.4018 - val_acc: 0.8125\n",
      "Epoch 255/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8182\n",
      "Epoch 255: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3890 - acc: 0.8185 - val_loss: 0.3832 - val_acc: 0.8117\n",
      "Epoch 256/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8223\n",
      "Epoch 256: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3859 - acc: 0.8228 - val_loss: 0.3931 - val_acc: 0.8343\n",
      "Epoch 257/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8118\n",
      "Epoch 257: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3912 - acc: 0.8123 - val_loss: 0.3807 - val_acc: 0.8254\n",
      "Epoch 258/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8185\n",
      "Epoch 258: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3834 - acc: 0.8187 - val_loss: 0.3820 - val_acc: 0.8080\n",
      "Epoch 259/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8148\n",
      "Epoch 259: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3932 - acc: 0.8155 - val_loss: 0.3769 - val_acc: 0.8230\n",
      "Epoch 260/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8213\n",
      "Epoch 260: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3828 - acc: 0.8214 - val_loss: 0.3922 - val_acc: 0.8380\n",
      "Epoch 261/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8214\n",
      "Epoch 261: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3768 - acc: 0.8217 - val_loss: 0.3845 - val_acc: 0.8226\n",
      "Epoch 262/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3736 - acc: 0.8177\n",
      "Epoch 262: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3736 - acc: 0.8177 - val_loss: 0.3777 - val_acc: 0.8299\n",
      "Epoch 263/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8200\n",
      "Epoch 263: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3781 - acc: 0.8203 - val_loss: 0.3899 - val_acc: 0.8181\n",
      "Epoch 264/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8167\n",
      "Epoch 264: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3813 - acc: 0.8173 - val_loss: 0.3892 - val_acc: 0.8157\n",
      "Epoch 265/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8134\n",
      "Epoch 265: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3861 - acc: 0.8139 - val_loss: 0.3673 - val_acc: 0.8380\n",
      "Epoch 266/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3867 - acc: 0.8154\n",
      "Epoch 266: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3867 - acc: 0.8154 - val_loss: 0.3995 - val_acc: 0.8149\n",
      "Epoch 267/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8168\n",
      "Epoch 267: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3873 - acc: 0.8171 - val_loss: 0.3997 - val_acc: 0.8104\n",
      "Epoch 268/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8143\n",
      "Epoch 268: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3816 - acc: 0.8144 - val_loss: 0.3955 - val_acc: 0.8214\n",
      "Epoch 269/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8165\n",
      "Epoch 269: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3841 - acc: 0.8171 - val_loss: 0.3768 - val_acc: 0.8242\n",
      "Epoch 270/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8135\n",
      "Epoch 270: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3947 - acc: 0.8137 - val_loss: 0.3915 - val_acc: 0.8088\n",
      "Epoch 271/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8205\n",
      "Epoch 271: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3766 - acc: 0.8210 - val_loss: 0.4037 - val_acc: 0.8287\n",
      "Epoch 272/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8136\n",
      "Epoch 272: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3837 - acc: 0.8137 - val_loss: 0.3808 - val_acc: 0.8190\n",
      "Epoch 273/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8167\n",
      "Epoch 273: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3841 - acc: 0.8176 - val_loss: 0.3861 - val_acc: 0.8364\n",
      "Epoch 274/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3845 - acc: 0.8181\n",
      "Epoch 274: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3845 - acc: 0.8181 - val_loss: 0.3826 - val_acc: 0.8129\n",
      "Epoch 275/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8189\n",
      "Epoch 275: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3766 - acc: 0.8195 - val_loss: 0.3950 - val_acc: 0.8096\n",
      "Epoch 276/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8220\n",
      "Epoch 276: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3749 - acc: 0.8228 - val_loss: 0.3670 - val_acc: 0.8380\n",
      "Epoch 277/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8198\n",
      "Epoch 277: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3875 - acc: 0.8198 - val_loss: 0.3837 - val_acc: 0.8396\n",
      "Epoch 278/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8131\n",
      "Epoch 278: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3809 - acc: 0.8134 - val_loss: 0.3778 - val_acc: 0.8198\n",
      "Epoch 279/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8176\n",
      "Epoch 279: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3817 - acc: 0.8177 - val_loss: 0.3775 - val_acc: 0.8307\n",
      "Epoch 280/2000\n",
      "608/618 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8172\n",
      "Epoch 280: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3831 - acc: 0.8178 - val_loss: 0.3723 - val_acc: 0.8262\n",
      "Epoch 281/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8187\n",
      "Epoch 281: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3775 - acc: 0.8189 - val_loss: 0.3903 - val_acc: 0.8258\n",
      "Epoch 282/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8185\n",
      "Epoch 282: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3944 - acc: 0.8190 - val_loss: 0.3908 - val_acc: 0.8129\n",
      "Epoch 283/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8171\n",
      "Epoch 283: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3818 - acc: 0.8174 - val_loss: 0.3965 - val_acc: 0.8165\n",
      "Epoch 284/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.8126\n",
      "Epoch 284: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3829 - acc: 0.8129 - val_loss: 0.3852 - val_acc: 0.8194\n",
      "Epoch 285/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8184\n",
      "Epoch 285: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3785 - acc: 0.8187 - val_loss: 0.3736 - val_acc: 0.8315\n",
      "Epoch 286/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8241\n",
      "Epoch 286: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3746 - acc: 0.8243 - val_loss: 0.3937 - val_acc: 0.8246\n",
      "Epoch 287/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8195\n",
      "Epoch 287: val_acc did not improve from 0.84083\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3792 - acc: 0.8202 - val_loss: 0.3895 - val_acc: 0.8194\n",
      "Epoch 288/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8213\n",
      "Epoch 288: val_acc improved from 0.84083 to 0.84407, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3795 - acc: 0.8215 - val_loss: 0.3738 - val_acc: 0.8441\n",
      "Epoch 289/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3782 - acc: 0.8189\n",
      "Epoch 289: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3780 - acc: 0.8190 - val_loss: 0.3912 - val_acc: 0.8230\n",
      "Epoch 290/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8225\n",
      "Epoch 290: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.3815 - acc: 0.8229 - val_loss: 0.3632 - val_acc: 0.8429\n",
      "Epoch 291/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3750 - acc: 0.8172\n",
      "Epoch 291: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3746 - acc: 0.8175 - val_loss: 0.3765 - val_acc: 0.8433\n",
      "Epoch 292/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8203\n",
      "Epoch 292: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3762 - acc: 0.8209 - val_loss: 0.3930 - val_acc: 0.8149\n",
      "Epoch 293/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8175\n",
      "Epoch 293: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3794 - acc: 0.8184 - val_loss: 0.3994 - val_acc: 0.8388\n",
      "Epoch 294/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8169\n",
      "Epoch 294: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 7ms/step - loss: 0.3901 - acc: 0.8170 - val_loss: 0.3735 - val_acc: 0.8246\n",
      "Epoch 295/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8192\n",
      "Epoch 295: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3762 - acc: 0.8192 - val_loss: 0.3981 - val_acc: 0.8287\n",
      "Epoch 296/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8276\n",
      "Epoch 296: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3683 - acc: 0.8279 - val_loss: 0.3902 - val_acc: 0.8388\n",
      "Epoch 297/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8165\n",
      "Epoch 297: val_acc did not improve from 0.84407\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3808 - acc: 0.8169 - val_loss: 0.3835 - val_acc: 0.8234\n",
      "Epoch 298/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8205\n",
      "Epoch 298: val_acc improved from 0.84407 to 0.84690, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3782 - acc: 0.8210 - val_loss: 0.3865 - val_acc: 0.8469\n",
      "Epoch 299/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8200\n",
      "Epoch 299: val_acc did not improve from 0.84690\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3769 - acc: 0.8204 - val_loss: 0.3629 - val_acc: 0.8396\n",
      "Epoch 300/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8177\n",
      "Epoch 300: val_acc did not improve from 0.84690\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3874 - acc: 0.8178 - val_loss: 0.3763 - val_acc: 0.8254\n",
      "Epoch 301/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8169\n",
      "Epoch 301: val_acc did not improve from 0.84690\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3880 - acc: 0.8167 - val_loss: 0.3827 - val_acc: 0.8420\n",
      "Epoch 302/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8185\n",
      "Epoch 302: val_acc did not improve from 0.84690\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3824 - acc: 0.8190 - val_loss: 0.3910 - val_acc: 0.8190\n",
      "Epoch 303/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8097\n",
      "Epoch 303: val_acc improved from 0.84690 to 0.84771, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3946 - acc: 0.8098 - val_loss: 0.3703 - val_acc: 0.8477\n",
      "Epoch 304/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8207\n",
      "Epoch 304: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3671 - acc: 0.8208 - val_loss: 0.3884 - val_acc: 0.8275\n",
      "Epoch 305/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8173\n",
      "Epoch 305: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3792 - acc: 0.8180 - val_loss: 0.4183 - val_acc: 0.8331\n",
      "Epoch 306/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8200\n",
      "Epoch 306: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3784 - acc: 0.8203 - val_loss: 0.3847 - val_acc: 0.8343\n",
      "Epoch 307/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8251\n",
      "Epoch 307: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3699 - acc: 0.8251 - val_loss: 0.3846 - val_acc: 0.8206\n",
      "Epoch 308/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8122\n",
      "Epoch 308: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3815 - acc: 0.8121 - val_loss: 0.3765 - val_acc: 0.8303\n",
      "Epoch 309/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8182\n",
      "Epoch 309: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3770 - acc: 0.8182 - val_loss: 0.3909 - val_acc: 0.8238\n",
      "Epoch 310/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8204\n",
      "Epoch 310: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3795 - acc: 0.8206 - val_loss: 0.3992 - val_acc: 0.8234\n",
      "Epoch 311/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8200\n",
      "Epoch 311: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3728 - acc: 0.8203 - val_loss: 0.3994 - val_acc: 0.8307\n",
      "Epoch 312/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8189\n",
      "Epoch 312: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3811 - acc: 0.8185 - val_loss: 0.3998 - val_acc: 0.8153\n",
      "Epoch 313/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8224\n",
      "Epoch 313: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3754 - acc: 0.8224 - val_loss: 0.3875 - val_acc: 0.8279\n",
      "Epoch 314/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8206\n",
      "Epoch 314: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3719 - acc: 0.8204 - val_loss: 0.3716 - val_acc: 0.8307\n",
      "Epoch 315/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8208\n",
      "Epoch 315: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3742 - acc: 0.8214 - val_loss: 0.3721 - val_acc: 0.8445\n",
      "Epoch 316/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8209\n",
      "Epoch 316: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3729 - acc: 0.8217 - val_loss: 0.3869 - val_acc: 0.8311\n",
      "Epoch 317/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8224\n",
      "Epoch 317: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3757 - acc: 0.8224 - val_loss: 0.3716 - val_acc: 0.8218\n",
      "Epoch 318/2000\n",
      "613/618 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8149\n",
      "Epoch 318: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3849 - acc: 0.8157 - val_loss: 0.3770 - val_acc: 0.8372\n",
      "Epoch 319/2000\n",
      "609/618 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8235\n",
      "Epoch 319: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3787 - acc: 0.8241 - val_loss: 0.4068 - val_acc: 0.8210\n",
      "Epoch 320/2000\n",
      "611/618 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8194\n",
      "Epoch 320: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3830 - acc: 0.8197 - val_loss: 0.3892 - val_acc: 0.8218\n",
      "Epoch 321/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.8233\n",
      "Epoch 321: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3734 - acc: 0.8234 - val_loss: 0.3984 - val_acc: 0.8299\n",
      "Epoch 322/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8219\n",
      "Epoch 322: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3690 - acc: 0.8221 - val_loss: 0.3801 - val_acc: 0.8388\n",
      "Epoch 323/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8172\n",
      "Epoch 323: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3774 - acc: 0.8174 - val_loss: 0.3885 - val_acc: 0.8352\n",
      "Epoch 324/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8183\n",
      "Epoch 324: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3806 - acc: 0.8184 - val_loss: 0.3989 - val_acc: 0.8100\n",
      "Epoch 325/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8187\n",
      "Epoch 325: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3761 - acc: 0.8187 - val_loss: 0.3746 - val_acc: 0.8392\n",
      "Epoch 326/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3703 - acc: 0.8260\n",
      "Epoch 326: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3703 - acc: 0.8260 - val_loss: 0.3793 - val_acc: 0.8262\n",
      "Epoch 327/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8227\n",
      "Epoch 327: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3718 - acc: 0.8229 - val_loss: 0.3820 - val_acc: 0.8275\n",
      "Epoch 328/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3710 - acc: 0.8244\n",
      "Epoch 328: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3710 - acc: 0.8244 - val_loss: 0.3850 - val_acc: 0.8246\n",
      "Epoch 329/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.8207\n",
      "Epoch 329: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3723 - acc: 0.8207 - val_loss: 0.3836 - val_acc: 0.8315\n",
      "Epoch 330/2000\n",
      "617/618 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8221\n",
      "Epoch 330: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3728 - acc: 0.8221 - val_loss: 0.3783 - val_acc: 0.8234\n",
      "Epoch 331/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8178\n",
      "Epoch 331: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3796 - acc: 0.8182 - val_loss: 0.3787 - val_acc: 0.8291\n",
      "Epoch 332/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8221\n",
      "Epoch 332: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3650 - acc: 0.8226 - val_loss: 0.3844 - val_acc: 0.8311\n",
      "Epoch 333/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8223\n",
      "Epoch 333: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3706 - acc: 0.8223 - val_loss: 0.3910 - val_acc: 0.8319\n",
      "Epoch 334/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8230\n",
      "Epoch 334: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3727 - acc: 0.8231 - val_loss: 0.3819 - val_acc: 0.8364\n",
      "Epoch 335/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8225\n",
      "Epoch 335: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3717 - acc: 0.8228 - val_loss: 0.3825 - val_acc: 0.8262\n",
      "Epoch 336/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8168\n",
      "Epoch 336: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3749 - acc: 0.8171 - val_loss: 0.3896 - val_acc: 0.8311\n",
      "Epoch 337/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8213\n",
      "Epoch 337: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3770 - acc: 0.8218 - val_loss: 0.3845 - val_acc: 0.8412\n",
      "Epoch 338/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8250\n",
      "Epoch 338: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3705 - acc: 0.8255 - val_loss: 0.3780 - val_acc: 0.8331\n",
      "Epoch 339/2000\n",
      "610/618 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8224\n",
      "Epoch 339: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3653 - acc: 0.8229 - val_loss: 0.3685 - val_acc: 0.8376\n",
      "Epoch 340/2000\n",
      "616/618 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8261\n",
      "Epoch 340: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3659 - acc: 0.8264 - val_loss: 0.3618 - val_acc: 0.8315\n",
      "Epoch 341/2000\n",
      "615/618 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8201\n",
      "Epoch 341: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3694 - acc: 0.8204 - val_loss: 0.3849 - val_acc: 0.8190\n",
      "Epoch 342/2000\n",
      "612/618 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8211\n",
      "Epoch 342: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3678 - acc: 0.8213 - val_loss: 0.3987 - val_acc: 0.8291\n",
      "Epoch 343/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3754 - acc: 0.8157\n",
      "Epoch 343: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3754 - acc: 0.8157 - val_loss: 0.3863 - val_acc: 0.8323\n",
      "Epoch 344/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8192\n",
      "Epoch 344: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3721 - acc: 0.8197 - val_loss: 0.3927 - val_acc: 0.8429\n",
      "Epoch 345/2000\n",
      "618/618 [==============================] - ETA: 0s - loss: 0.3696 - acc: 0.8230\n",
      "Epoch 345: val_acc did not improve from 0.84771\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3696 - acc: 0.8230 - val_loss: 0.3831 - val_acc: 0.8262\n",
      "Epoch 346/2000\n",
      "614/618 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8222\n",
      "Epoch 346: val_acc improved from 0.84771 to 0.84974, saving model to train_logs/logs7/DWT_Relative_ANN_512_256_Adam/5/best_model.h5\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 0.3716 - acc: 0.8227 - val_loss: 0.3501 - val_acc: 0.8497\n",
      "97/97 [==============================] - 1s 4ms/step - loss: 0.3636 - acc: 0.8435\n"
     ]
    }
   ],
   "source": [
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(columns=range(1, 6))\n",
    "    training_time = pd.DataFrame(columns=[f'CPU_Time_{i}' for i in range(1, 6)] + [f'Wall_Time_{i}' for i in range(1, 6)])\n",
    "    \n",
    "\n",
    "    train_temp_dir = train_dir\n",
    "    train = tf.data.Dataset.load(train_temp_dir)\n",
    "    flattened_train = train.unbatch()\n",
    "    \n",
    "    train_data = list(flattened_train.as_numpy_iterator())\n",
    "    train_size = len(train_data)\n",
    "    # print(train_data[0].shape)\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(train_data), 1):\n",
    "        train_fold_data = ([train_data[i][0] for i in train_index], [train_data[i][1] for i in train_index])\n",
    "        val_fold_data = ([train_data[i][0] for i in val_index], [train_data[i][1] for i in val_index])\n",
    "        \n",
    "        train_fold = tf.data.Dataset.from_tensor_slices(train_fold_data).batch(16)\n",
    "        val_fold = tf.data.Dataset.from_tensor_slices(val_fold_data).batch(16)\n",
    "        \n",
    "        log_path = os.path.join(log_dir, str(fold))\n",
    "        \n",
    "        model = create_model()\n",
    "        model.summary()\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "        \n",
    "        cpu_start = time.process_time()\n",
    "        wt_start = time.time()\n",
    "        \n",
    "        history = model.fit(train_fold, epochs=epochs, validation_data=val_fold, callbacks=myCallbacks(log_path))\n",
    "        \n",
    "        wt_end = time.time()\n",
    "        cpu_end = time.process_time()\n",
    "        wall_time = wt_end - wt_start\n",
    "        cpu_time = cpu_end - cpu_start\n",
    "        \n",
    "        training_time.loc[f'CPU_Time_{fold}'] = cpu_time\n",
    "        training_time.loc[f'Wall_Time_{fold}'] = wall_time\n",
    "        \n",
    "        recap.loc[fold] = history.history['acc'][-1]\n",
    "    \n",
    "    # Evaluate on the test dataset after cross-validation\n",
    "    test_temp_dir = test_dir\n",
    "    test_ds = tf.data.Dataset.load(test_temp_dir)\n",
    "    results = model.evaluate(test_ds, callbacks=myCallbacks(log_path))\n",
    "    \n",
    "    recap[f'test'] = results[1]\n",
    "    \n",
    "    log_recap_dir = os.path.join(log_dir, 'Recap')\n",
    "    if not os.path.exists(log_recap_dir):\n",
    "        os.makedirs(log_recap_dir)\n",
    "    \n",
    "    recap.to_csv(os.path.join(log_recap_dir, 'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_recap_dir, 'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "test_dir = \"datasets/tf_batch/dwt_relative/segment_1 seconds/test\"\n",
    "test_ds = tf.data.Dataset.load(test_dir)\n",
    "model_dir = [f\"train_logs/logs7/DWT_Relative_ANN_512_256_Adam/{i}/best_model.h5\" for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_test = test_ds.unbatch()\n",
    "test_data = list(flattened_test.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_value = np.array([test_data[i][0] for i in range(len(test_data))])\n",
    "test_data_label = np.array([test_data[i][1] for i in range(len(test_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3087, 96)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_value.reshape(test_data_value.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 1s 4ms/step - loss: 0.3720 - acc: 0.8529\n",
      "0.3719835877418518 0.8529316782951355\n",
      "97/97 [==============================] - 0s 3ms/step\n",
      "97/97 [==============================] - 1s 3ms/step - loss: 0.4179 - acc: 0.8578\n",
      "0.41791585087776184 0.8577907085418701\n",
      "97/97 [==============================] - 0s 3ms/step\n",
      "97/97 [==============================] - 1s 4ms/step - loss: 0.4168 - acc: 0.8552\n",
      "0.4167852997779846 0.8551992177963257\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4353 - acc: 0.8529\n",
      "0.4353410005569458 0.8529316782951355\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 1s 3ms/step - loss: 0.3636 - acc: 0.8435\n",
      "0.36358773708343506 0.8435373902320862\n",
      "97/97 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, model_path in enumerate(model_dir):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(loss, acc)\n",
    "    pred = model.predict(test_data_value.reshape(test_data_value.shape[0], -1 ))\n",
    "    results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5555558 , 0.5712844 , 0.59311384, ..., 0.5987314 , 0.59132904,\n",
       "       0.5922044 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].reshape(results[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_data_label, pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'autism'])\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Result: DWT ANN for model in fold {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: DWT ANN for model in fold 1\n",
      "Precision: 0.83589329021827\n",
      "Recall: 0.9773156899810964\n",
      "Accuracy: 0.8529316488500162\n",
      "F1-score: 0.9010893246187364\n",
      "\n",
      "\n",
      "Result: DWT ANN for model in fold 2\n",
      "Precision: 0.8398865018240779\n",
      "Recall: 0.9792060491493384\n",
      "Accuracy: 0.8577907353417558\n",
      "F1-score: 0.9042112153611174\n",
      "\n",
      "\n",
      "Result: DWT ANN for model in fold 3\n",
      "Precision: 0.8396418396418397\n",
      "Recall: 0.974952741020794\n",
      "Accuracy: 0.8551992225461613\n",
      "F1-score: 0.9022523507544283\n",
      "\n",
      "\n",
      "Result: DWT ANN for model in fold 4\n",
      "Precision: 0.8394607843137255\n",
      "Recall: 0.97117202268431\n",
      "Accuracy: 0.8529316488500162\n",
      "F1-score: 0.9005258545135846\n",
      "\n",
      "\n",
      "Result: DWT ANN for model in fold 5\n",
      "Precision: 0.8241365621278285\n",
      "Recall: 0.9810964083175804\n",
      "Accuracy: 0.8435374149659864\n",
      "F1-score: 0.8957928802588997\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    test_data_label = test_data_label.astype(bool)  \n",
    "\n",
    "    print(f\"Result: DWT ANN for model in fold {i}\")\n",
    "\n",
    "    TP = np.sum((test_data_label == True) & (pred == True))\n",
    "    FP = np.sum((test_data_label == False) & (pred == True))\n",
    "    TN = np.sum((test_data_label == False) & (pred == False))\n",
    "    FN = np.sum((test_data_label == True) & (pred == False))  \n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# for i, res in enumerate(results, 1):\n",
    "#     pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "\n",
    "#     print(f\"Result: FFT ANN for model in fold {i}\")\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = accuracy_score(test_data_label, pred)\n",
    "#     print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#     # Calculate precision\n",
    "#     precision = precision_score(test_data_label, pred)\n",
    "#     print(\"Precision:\", precision)\n",
    "\n",
    "#     # Calculate recall (sensitivity)\n",
    "#     recall = recall_score(test_data_label, pred)\n",
    "#     print(\"Recall (Sensitivity):\", recall)\n",
    "\n",
    "#     # Calculate F1-score\n",
    "#     f1 = f1_score(test_data_label, pred)\n",
    "#     print(\"F1-Score:\", f1)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for log_dir in log_dirs:\n",
    "#     recap = pd.DataFrame(index=lags, columns=folds)\n",
    "#     training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "#     for fold in range(1,3):\n",
    "#         for lag in lags:\n",
    "#             if fold == 2:\n",
    "#                 train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "#             train_temp_dir = train_dir + '_' + str(lag)\n",
    "#             test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "#             train = tf.data.Dataset.load(train_temp_dir)\n",
    "#             test_ds = tf.data.Dataset.load(test_temp_dir)\n",
    "\n",
    "#             train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "#             train_ds = train.take(train_size)\n",
    "#             val_ds = train.skip(train_size)\n",
    "\n",
    "#             log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "#             model = create_model()\n",
    "#             model.summary()\n",
    "\n",
    "#             model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "#             cpu_start = time.process_time()\n",
    "#             wt_start = time.time()\n",
    "\n",
    "#             history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             wt_end = time.time()\n",
    "#             cpu_end = time.process_time()\n",
    "#             wall_time = wt_end - wt_start\n",
    "#             cpu_time = cpu_end - cpu_start\n",
    "#             training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "#             training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "#             results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "#             recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "#             recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "#     log_dir = os.path.join(log_dir,'Recap')\n",
    "#     if not os.path.exists(log_dir):\n",
    "#         os.makedirs(log_dir)\n",
    "#     recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "#     training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for log_dir in log_dirs:\n",
    "#     recap = pd.DataFrame(index=lags, columns=folds)\n",
    "#     training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "#     for fold in range(1,3):\n",
    "#         for lag in lags:\n",
    "#             if fold == 2:\n",
    "#                 train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "#             train_temp_dir = train_dir + '_' + str(lag)\n",
    "#             test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "#             train = get_batch(train_temp_dir)\n",
    "#             test_ds = get_batch(test_temp_dir)\n",
    "\n",
    "#             train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "#             train_ds = train.take(train_size)\n",
    "#             val_ds = train.skip(train_size)\n",
    "\n",
    "#             log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "#             model = create_model()\n",
    "#             model.summary()\n",
    "\n",
    "#             model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "#             cpu_start = time.process_time()\n",
    "#             wt_start = time.time()\n",
    "\n",
    "#             history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             wt_end = time.time()\n",
    "#             cpu_end = time.process_time()\n",
    "#             wall_time = wt_end - wt_start\n",
    "#             cpu_time = cpu_end - cpu_start\n",
    "#             training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "#             training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "#             results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "#             recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "#             recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "#             recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "#     log_dir = os.path.join(log_dir,'Recap')\n",
    "#     if not os.path.exists(log_dir):\n",
    "#         os.makedirs(log_dir)\n",
    "#     recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "#     training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tensorboard --logdir c:/Users/farra/Documents/Pribadi/EEG/EEG-Autism-Classification --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farra\\Documents\\Pribadi\\EEG\\EEG-Autism-Classification\n"
     ]
    }
   ],
   "source": [
    "! cd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
