{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, excluded_name=[]):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):        \n",
    "        folder = os.path.join(directory, foldername)\n",
    "        # print(folder)\n",
    "        # print(os.listdir(folder))\n",
    "        for name in os.listdir(folder):\n",
    "            if name in excluded_name:\n",
    "                # print(name)\n",
    "                continue\n",
    "            filename = os.path.join(folder, name)\n",
    "            # print(filename)\n",
    "            for files in os.listdir(filename):\n",
    "                rel_path = os.path.join(filename, files)\n",
    "                # print(rel_path)\n",
    "                temp_label = folder\n",
    "                if \"autism\" in temp_label:\n",
    "                    label = 'autism'\n",
    "                else:\n",
    "                    label = 'normal'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64).reshape(-1,1)\n",
    "                                \n",
    "                temp_data.loc[0, \"data\"] = rwb\n",
    "                temp_data['label'] = label\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"autism\": 1, \"normal\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_value(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "    missing_indices = np.where(np.isnan(series_list).any(axis=1))[0]\n",
    "\n",
    "    clean_data = data.drop(index=data.index[missing_indices])\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "# rwb = np.load(\"datasets/features/rwb/segment_1 seconds/autism_256/bader/Bader_segment_100.csv_bispectrum.npy\")\n",
    "# rwb.astype(np.float64).reshape(-1,1)\n",
    "# temp_data.loc[0, \"data\"] = rwb\n",
    "# temp_data['label'] = \"autism\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data, des_path):\n",
    "    if not os.path.exists(des_path):\n",
    "        os.makedirs(des_path)\n",
    "    data.save(des_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, train_split: float):\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n",
    "        data['data'],\n",
    "        data[['label', 'label_map']],\n",
    "        train_size=train_split,\n",
    "        stratify=data['label_map']\n",
    "    )\n",
    "\n",
    "    train_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "    test_df = pd.DataFrame(columns=['data', 'label', 'label_map'])\n",
    "\n",
    "    train_df[\"data\"] = train_x\n",
    "    train_df[['label', 'label_map']] = train_y\n",
    "\n",
    "    test_df[\"data\"] = test_x\n",
    "    test_df[['label', 'label_map']] = test_y\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    # loading extracted feature & label\n",
    "    # x = get_dataset(path, lag, excluded_name)\n",
    "\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = data[\"label_map\"].values\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list))\n",
    "\n",
    "    # train_size = int(train_split * len(labels_list))  \n",
    "    # test_size = len(labels_list) - train_size  \n",
    "\n",
    "    # train_dataset = dataset.take(train_size)\n",
    "    # test_dataset = dataset.skip(train_size)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_transform(data):    \n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values  \n",
    "\n",
    "    relative_energy = np.zeros([series_list.shape[0], series_list.shape[1]])\n",
    "    for i, series in enumerate(series_list):\n",
    "        total_energy = np.sum(series)\n",
    "        relative_energy[i] = [(s / total_energy) * 100 for s in series]\n",
    "    relative_energy_list = [arr for arr in relative_energy]\n",
    "    data[\"data\"] = relative_energy_list\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(data):\n",
    "    series_list = np.vstack(data[\"data\"].values)\n",
    "    labels_list = data[\"label_map\"].values    \n",
    "\n",
    "    epsilon = 1e-9  # To avoid log(0) issues\n",
    "    log_transformed_series = np.log(series_list + epsilon)\n",
    "\n",
    "    log_transformed_list = [arr for arr in log_transformed_series]\n",
    "    data[\"data\"] = log_transformed_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/features/fft_relative/segment_1 seconds\"\n",
    "\n",
    "train_dir = \"datasets/tf_batch/fft_relative/segment_1 seconds/train\"\n",
    "test_dir = \"datasets/tf_batch/fft_relative/segment_1 seconds/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15434, 3)\n",
      "(15399, 3)\n",
      "(15399, 3)\n",
      "train: (12319, 3)\n",
      "test: (3080, 3)\n"
     ]
    }
   ],
   "source": [
    "excluded = [\"zyad\"]\n",
    "train_split = 0.8\n",
    "# LAG = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "\n",
    "# for lag in LAG:\n",
    "data = get_dataset(data_dir, excluded)\n",
    "print(data.shape)\n",
    "data = remove_missing_value(data)\n",
    "print(data.shape)\n",
    "# data = relative_transform(data)\n",
    "# data = remove_missing_value(data)\n",
    "print(data.shape)\n",
    "train_data, test_data = get_train_test(data, train_split)\n",
    "print(\"train:\", train_data.shape)\n",
    "print(\"test:\", test_data.shape)\n",
    "\n",
    "train_batch = get_batch(train_data)\n",
    "test_batch = get_batch(test_data)\n",
    "tf.data.Dataset.save(train_batch, f\"{train_dir}\")\n",
    "tf.data.Dataset.save(test_batch, f\"{test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative_transform(data)\n",
    "# rel_list = [arr for arr in rel]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load(\"datasets/features/fft_relative/segment_1 seconds/normal_/amer/Amer_segment_1.csv_fft_rel.npy\")\n",
    "\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "print(signal.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(80,)))\n",
    "    model.add(layers.Reshape((80, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags = [256]\n",
    "folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000\n",
    "log_dirs = ['train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102841/2764964901.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(list(train_data.as_numpy_iterator())).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(385, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.load(train_dir)\n",
    "np.array(list(train_data.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 80, 1)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               41472     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,057\n",
      "Trainable params: 173,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.6168 - acc: 0.7201\n",
      "Epoch 1: val_acc improved from -inf to 0.78774, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.6146 - acc: 0.7211 - val_loss: 0.4365 - val_acc: 0.7877\n",
      "Epoch 2/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7699\n",
      "Epoch 2: val_acc improved from 0.78774 to 0.80317, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.5021 - acc: 0.7703 - val_loss: 0.4243 - val_acc: 0.8032\n",
      "Epoch 3/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.7867\n",
      "Epoch 3: val_acc improved from 0.80317 to 0.80682, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.4755 - acc: 0.7865 - val_loss: 0.4068 - val_acc: 0.8068\n",
      "Epoch 4/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.7944\n",
      "Epoch 4: val_acc improved from 0.80682 to 0.80763, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.4524 - acc: 0.7945 - val_loss: 0.4033 - val_acc: 0.8076\n",
      "Epoch 5/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8005\n",
      "Epoch 5: val_acc improved from 0.80763 to 0.81534, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.4413 - acc: 0.8007 - val_loss: 0.4037 - val_acc: 0.8153\n",
      "Epoch 6/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.4309 - acc: 0.8095\n",
      "Epoch 6: val_acc did not improve from 0.81534\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4314 - acc: 0.8093 - val_loss: 0.3967 - val_acc: 0.8105\n",
      "Epoch 7/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8135\n",
      "Epoch 7: val_acc improved from 0.81534 to 0.82468, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4175 - acc: 0.8136 - val_loss: 0.3903 - val_acc: 0.8247\n",
      "Epoch 8/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.4162 - acc: 0.8227\n",
      "Epoch 8: val_acc improved from 0.82468 to 0.83442, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4162 - acc: 0.8227 - val_loss: 0.3777 - val_acc: 0.8344\n",
      "Epoch 9/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8257\n",
      "Epoch 9: val_acc did not improve from 0.83442\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.4109 - acc: 0.8258 - val_loss: 0.3852 - val_acc: 0.8247\n",
      "Epoch 10/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.4092 - acc: 0.8250\n",
      "Epoch 10: val_acc did not improve from 0.83442\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.4092 - acc: 0.8250 - val_loss: 0.4016 - val_acc: 0.8198\n",
      "Epoch 11/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8288\n",
      "Epoch 11: val_acc did not improve from 0.83442\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3995 - acc: 0.8292 - val_loss: 0.3913 - val_acc: 0.8320\n",
      "Epoch 12/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8346\n",
      "Epoch 12: val_acc improved from 0.83442 to 0.84943, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3969 - acc: 0.8343 - val_loss: 0.3739 - val_acc: 0.8494\n",
      "Epoch 13/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8345\n",
      "Epoch 13: val_acc did not improve from 0.84943\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3938 - acc: 0.8345 - val_loss: 0.3822 - val_acc: 0.8393\n",
      "Epoch 14/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8410\n",
      "Epoch 14: val_acc did not improve from 0.84943\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3910 - acc: 0.8414 - val_loss: 0.3952 - val_acc: 0.8401\n",
      "Epoch 15/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8435\n",
      "Epoch 15: val_acc did not improve from 0.84943\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3845 - acc: 0.8437 - val_loss: 0.3956 - val_acc: 0.8425\n",
      "Epoch 16/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8450\n",
      "Epoch 16: val_acc did not improve from 0.84943\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3793 - acc: 0.8452 - val_loss: 0.3749 - val_acc: 0.8462\n",
      "Epoch 17/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8486\n",
      "Epoch 17: val_acc improved from 0.84943 to 0.85106, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3802 - acc: 0.8490 - val_loss: 0.3819 - val_acc: 0.8511\n",
      "Epoch 18/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8492\n",
      "Epoch 18: val_acc did not improve from 0.85106\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3780 - acc: 0.8493 - val_loss: 0.3835 - val_acc: 0.8474\n",
      "Epoch 19/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3747 - acc: 0.8468\n",
      "Epoch 19: val_acc did not improve from 0.85106\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3747 - acc: 0.8468 - val_loss: 0.3714 - val_acc: 0.8482\n",
      "Epoch 20/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8531\n",
      "Epoch 20: val_acc improved from 0.85106 to 0.85877, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3654 - acc: 0.8533 - val_loss: 0.3690 - val_acc: 0.8588\n",
      "Epoch 21/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8517\n",
      "Epoch 21: val_acc did not improve from 0.85877\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3778 - acc: 0.8516 - val_loss: 0.3709 - val_acc: 0.8555\n",
      "Epoch 22/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8550\n",
      "Epoch 22: val_acc did not improve from 0.85877\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3613 - acc: 0.8547 - val_loss: 0.3760 - val_acc: 0.8555\n",
      "Epoch 23/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8548\n",
      "Epoch 23: val_acc improved from 0.85877 to 0.86242, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3649 - acc: 0.8549 - val_loss: 0.3908 - val_acc: 0.8624\n",
      "Epoch 24/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8576\n",
      "Epoch 24: val_acc did not improve from 0.86242\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3545 - acc: 0.8572 - val_loss: 0.3753 - val_acc: 0.8539\n",
      "Epoch 25/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8628\n",
      "Epoch 25: val_acc did not improve from 0.86242\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3621 - acc: 0.8627 - val_loss: 0.3806 - val_acc: 0.8555\n",
      "Epoch 26/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8578\n",
      "Epoch 26: val_acc did not improve from 0.86242\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3587 - acc: 0.8578 - val_loss: 0.3772 - val_acc: 0.8580\n",
      "Epoch 27/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8612\n",
      "Epoch 27: val_acc did not improve from 0.86242\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3559 - acc: 0.8613 - val_loss: 0.3830 - val_acc: 0.8523\n",
      "Epoch 28/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3500 - acc: 0.8633\n",
      "Epoch 28: val_acc improved from 0.86242 to 0.86972, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3500 - acc: 0.8633 - val_loss: 0.3626 - val_acc: 0.8697\n",
      "Epoch 29/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8658\n",
      "Epoch 29: val_acc did not improve from 0.86972\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3507 - acc: 0.8660 - val_loss: 0.3649 - val_acc: 0.8661\n",
      "Epoch 30/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3517 - acc: 0.8652\n",
      "Epoch 30: val_acc did not improve from 0.86972\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3517 - acc: 0.8652 - val_loss: 0.3712 - val_acc: 0.8628\n",
      "Epoch 31/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8677\n",
      "Epoch 31: val_acc did not improve from 0.86972\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3392 - acc: 0.8676 - val_loss: 0.3731 - val_acc: 0.8677\n",
      "Epoch 32/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8657\n",
      "Epoch 32: val_acc did not improve from 0.86972\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3513 - acc: 0.8654 - val_loss: 0.3767 - val_acc: 0.8632\n",
      "Epoch 33/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8724\n",
      "Epoch 33: val_acc improved from 0.86972 to 0.87338, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3430 - acc: 0.8722 - val_loss: 0.3526 - val_acc: 0.8734\n",
      "Epoch 34/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8694\n",
      "Epoch 34: val_acc did not improve from 0.87338\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3398 - acc: 0.8696 - val_loss: 0.3629 - val_acc: 0.8713\n",
      "Epoch 35/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8753\n",
      "Epoch 35: val_acc improved from 0.87338 to 0.87459, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3423 - acc: 0.8752 - val_loss: 0.3647 - val_acc: 0.8746\n",
      "Epoch 36/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8713\n",
      "Epoch 36: val_acc did not improve from 0.87459\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3504 - acc: 0.8714 - val_loss: 0.3564 - val_acc: 0.8734\n",
      "Epoch 37/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8708\n",
      "Epoch 37: val_acc improved from 0.87459 to 0.87500, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3416 - acc: 0.8708 - val_loss: 0.3918 - val_acc: 0.8750\n",
      "Epoch 38/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3317 - acc: 0.8723\n",
      "Epoch 38: val_acc improved from 0.87500 to 0.87662, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3317 - acc: 0.8723 - val_loss: 0.3936 - val_acc: 0.8766\n",
      "Epoch 39/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8742\n",
      "Epoch 39: val_acc did not improve from 0.87662\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3347 - acc: 0.8742 - val_loss: 0.3714 - val_acc: 0.8730\n",
      "Epoch 40/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8801\n",
      "Epoch 40: val_acc did not improve from 0.87662\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3351 - acc: 0.8796 - val_loss: 0.3799 - val_acc: 0.8713\n",
      "Epoch 41/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.8785\n",
      "Epoch 41: val_acc did not improve from 0.87662\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3321 - acc: 0.8788 - val_loss: 0.3722 - val_acc: 0.8738\n",
      "Epoch 42/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8804\n",
      "Epoch 42: val_acc improved from 0.87662 to 0.87865, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3228 - acc: 0.8805 - val_loss: 0.3811 - val_acc: 0.8787\n",
      "Epoch 43/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8789\n",
      "Epoch 43: val_acc did not improve from 0.87865\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3388 - acc: 0.8789 - val_loss: 0.3794 - val_acc: 0.8685\n",
      "Epoch 44/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8787\n",
      "Epoch 44: val_acc did not improve from 0.87865\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3314 - acc: 0.8786 - val_loss: 0.3635 - val_acc: 0.8787\n",
      "Epoch 45/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8782\n",
      "Epoch 45: val_acc did not improve from 0.87865\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3303 - acc: 0.8782 - val_loss: 0.3831 - val_acc: 0.8778\n",
      "Epoch 46/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8782\n",
      "Epoch 46: val_acc improved from 0.87865 to 0.87946, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3277 - acc: 0.8779 - val_loss: 0.3617 - val_acc: 0.8795\n",
      "Epoch 47/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8808\n",
      "Epoch 47: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3399 - acc: 0.8813 - val_loss: 0.3878 - val_acc: 0.8778\n",
      "Epoch 48/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8810\n",
      "Epoch 48: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3245 - acc: 0.8809 - val_loss: 0.3783 - val_acc: 0.8718\n",
      "Epoch 49/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8819\n",
      "Epoch 49: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3285 - acc: 0.8819 - val_loss: 0.3776 - val_acc: 0.8750\n",
      "Epoch 50/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8824\n",
      "Epoch 50: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3292 - acc: 0.8824 - val_loss: 0.3526 - val_acc: 0.8787\n",
      "Epoch 51/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8797\n",
      "Epoch 51: val_acc improved from 0.87946 to 0.88149, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3239 - acc: 0.8800 - val_loss: 0.3825 - val_acc: 0.8815\n",
      "Epoch 52/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8825\n",
      "Epoch 52: val_acc did not improve from 0.88149\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3319 - acc: 0.8828 - val_loss: 0.3889 - val_acc: 0.8758\n",
      "Epoch 53/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8820\n",
      "Epoch 53: val_acc did not improve from 0.88149\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3272 - acc: 0.8820 - val_loss: 0.3692 - val_acc: 0.8782\n",
      "Epoch 54/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3210 - acc: 0.8823\n",
      "Epoch 54: val_acc improved from 0.88149 to 0.88352, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3203 - acc: 0.8825 - val_loss: 0.3689 - val_acc: 0.8835\n",
      "Epoch 55/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8853\n",
      "Epoch 55: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3317 - acc: 0.8854 - val_loss: 0.3730 - val_acc: 0.8791\n",
      "Epoch 56/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3155 - acc: 0.8812\n",
      "Epoch 56: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3155 - acc: 0.8812 - val_loss: 0.3739 - val_acc: 0.8819\n",
      "Epoch 57/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.8868\n",
      "Epoch 57: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3200 - acc: 0.8870 - val_loss: 0.3911 - val_acc: 0.8807\n",
      "Epoch 58/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8843\n",
      "Epoch 58: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3278 - acc: 0.8843 - val_loss: 0.3669 - val_acc: 0.8770\n",
      "Epoch 59/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.8889\n",
      "Epoch 59: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3132 - acc: 0.8887 - val_loss: 0.3701 - val_acc: 0.8823\n",
      "Epoch 60/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3170 - acc: 0.8868\n",
      "Epoch 60: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3167 - acc: 0.8870 - val_loss: 0.3606 - val_acc: 0.8803\n",
      "Epoch 61/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.8860\n",
      "Epoch 61: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3109 - acc: 0.8865 - val_loss: 0.3697 - val_acc: 0.8823\n",
      "Epoch 62/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8883\n",
      "Epoch 62: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3124 - acc: 0.8885 - val_loss: 0.3820 - val_acc: 0.8807\n",
      "Epoch 63/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8903\n",
      "Epoch 63: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3212 - acc: 0.8904 - val_loss: 0.3871 - val_acc: 0.8795\n",
      "Epoch 64/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.8899\n",
      "Epoch 64: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3185 - acc: 0.8900 - val_loss: 0.3974 - val_acc: 0.8835\n",
      "Epoch 65/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.8868\n",
      "Epoch 65: val_acc did not improve from 0.88352\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3111 - acc: 0.8865 - val_loss: 0.3648 - val_acc: 0.8799\n",
      "Epoch 66/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8884\n",
      "Epoch 66: val_acc improved from 0.88352 to 0.88799, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3144 - acc: 0.8889 - val_loss: 0.3651 - val_acc: 0.8880\n",
      "Epoch 67/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.8911\n",
      "Epoch 67: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3071 - acc: 0.8909 - val_loss: 0.3672 - val_acc: 0.8823\n",
      "Epoch 68/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8916\n",
      "Epoch 68: val_acc improved from 0.88799 to 0.88880, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3123 - acc: 0.8913 - val_loss: 0.3538 - val_acc: 0.8888\n",
      "Epoch 69/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8905\n",
      "Epoch 69: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3133 - acc: 0.8902 - val_loss: 0.3810 - val_acc: 0.8673\n",
      "Epoch 70/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3198 - acc: 0.8914\n",
      "Epoch 70: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3198 - acc: 0.8914 - val_loss: 0.3417 - val_acc: 0.8843\n",
      "Epoch 71/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8946\n",
      "Epoch 71: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3165 - acc: 0.8944 - val_loss: 0.3613 - val_acc: 0.8880\n",
      "Epoch 72/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.8935\n",
      "Epoch 72: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3080 - acc: 0.8933 - val_loss: 0.3659 - val_acc: 0.8851\n",
      "Epoch 73/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8928\n",
      "Epoch 73: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3059 - acc: 0.8928 - val_loss: 0.3529 - val_acc: 0.8880\n",
      "Epoch 74/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.8934\n",
      "Epoch 74: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3031 - acc: 0.8934 - val_loss: 0.3542 - val_acc: 0.8851\n",
      "Epoch 75/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3151 - acc: 0.8920\n",
      "Epoch 75: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3151 - acc: 0.8920 - val_loss: 0.3728 - val_acc: 0.8843\n",
      "Epoch 76/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.8928\n",
      "Epoch 76: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3104 - acc: 0.8929 - val_loss: 0.3622 - val_acc: 0.8847\n",
      "Epoch 77/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.8970\n",
      "Epoch 77: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3108 - acc: 0.8971 - val_loss: 0.3663 - val_acc: 0.8791\n",
      "Epoch 78/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.8949\n",
      "Epoch 78: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3123 - acc: 0.8948 - val_loss: 0.3682 - val_acc: 0.8868\n",
      "Epoch 79/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.8963\n",
      "Epoch 79: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3007 - acc: 0.8964 - val_loss: 0.3740 - val_acc: 0.8884\n",
      "Epoch 80/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.8950\n",
      "Epoch 80: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3106 - acc: 0.8953 - val_loss: 0.3960 - val_acc: 0.8823\n",
      "Epoch 81/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.8950\n",
      "Epoch 81: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3034 - acc: 0.8950 - val_loss: 0.3504 - val_acc: 0.8888\n",
      "Epoch 82/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8983\n",
      "Epoch 82: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3159 - acc: 0.8985 - val_loss: 0.3537 - val_acc: 0.8884\n",
      "Epoch 83/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.8944\n",
      "Epoch 83: val_acc did not improve from 0.88880\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3006 - acc: 0.8942 - val_loss: 0.3659 - val_acc: 0.8888\n",
      "Epoch 84/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.8986\n",
      "Epoch 84: val_acc improved from 0.88880 to 0.89164, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2989 - acc: 0.8986 - val_loss: 0.3783 - val_acc: 0.8916\n",
      "Epoch 85/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8963\n",
      "Epoch 85: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3042 - acc: 0.8964 - val_loss: 0.3728 - val_acc: 0.8884\n",
      "Epoch 86/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2961 - acc: 0.8996\n",
      "Epoch 86: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2961 - acc: 0.8996 - val_loss: 0.3763 - val_acc: 0.8916\n",
      "Epoch 87/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.8970\n",
      "Epoch 87: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2945 - acc: 0.8970 - val_loss: 0.4266 - val_acc: 0.8815\n",
      "Epoch 88/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8984\n",
      "Epoch 88: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3112 - acc: 0.8980 - val_loss: 0.4358 - val_acc: 0.8851\n",
      "Epoch 89/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.8956\n",
      "Epoch 89: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3047 - acc: 0.8959 - val_loss: 0.3697 - val_acc: 0.8892\n",
      "Epoch 90/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9006\n",
      "Epoch 90: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3060 - acc: 0.9003 - val_loss: 0.3570 - val_acc: 0.8880\n",
      "Epoch 91/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.8967\n",
      "Epoch 91: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3053 - acc: 0.8967 - val_loss: 0.3819 - val_acc: 0.8856\n",
      "Epoch 92/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.8982\n",
      "Epoch 92: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2963 - acc: 0.8984 - val_loss: 0.3750 - val_acc: 0.8904\n",
      "Epoch 93/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9009\n",
      "Epoch 93: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3065 - acc: 0.9007 - val_loss: 0.3560 - val_acc: 0.8892\n",
      "Epoch 94/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9005\n",
      "Epoch 94: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2822 - acc: 0.9006 - val_loss: 0.3762 - val_acc: 0.8916\n",
      "Epoch 95/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9000\n",
      "Epoch 95: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2924 - acc: 0.9004 - val_loss: 0.3581 - val_acc: 0.8904\n",
      "Epoch 96/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.8982\n",
      "Epoch 96: val_acc did not improve from 0.89164\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2998 - acc: 0.8984 - val_loss: 0.3815 - val_acc: 0.8860\n",
      "Epoch 97/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.9008\n",
      "Epoch 97: val_acc improved from 0.89164 to 0.89286, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3210 - acc: 0.9011 - val_loss: 0.3565 - val_acc: 0.8929\n",
      "Epoch 98/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.8977\n",
      "Epoch 98: val_acc improved from 0.89286 to 0.89610, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3052 - acc: 0.8981 - val_loss: 0.3556 - val_acc: 0.8961\n",
      "Epoch 99/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.9025\n",
      "Epoch 99: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3009 - acc: 0.9024 - val_loss: 0.3962 - val_acc: 0.8912\n",
      "Epoch 100/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.8982\n",
      "Epoch 100: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2953 - acc: 0.8983 - val_loss: 0.3722 - val_acc: 0.8831\n",
      "Epoch 101/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9041\n",
      "Epoch 101: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2978 - acc: 0.9041 - val_loss: 0.3503 - val_acc: 0.8856\n",
      "Epoch 102/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9010\n",
      "Epoch 102: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2930 - acc: 0.9012 - val_loss: 0.3679 - val_acc: 0.8843\n",
      "Epoch 103/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9024\n",
      "Epoch 103: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3022 - acc: 0.9023 - val_loss: 0.3593 - val_acc: 0.8925\n",
      "Epoch 104/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9025\n",
      "Epoch 104: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2908 - acc: 0.9027 - val_loss: 0.3516 - val_acc: 0.8945\n",
      "Epoch 105/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9037\n",
      "Epoch 105: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2933 - acc: 0.9037 - val_loss: 0.3537 - val_acc: 0.8912\n",
      "Epoch 106/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9038\n",
      "Epoch 106: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3060 - acc: 0.9041 - val_loss: 0.3691 - val_acc: 0.8941\n",
      "Epoch 107/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9004\n",
      "Epoch 107: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2940 - acc: 0.9005 - val_loss: 0.3759 - val_acc: 0.8892\n",
      "Epoch 108/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9052\n",
      "Epoch 108: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2953 - acc: 0.9054 - val_loss: 0.3970 - val_acc: 0.8892\n",
      "Epoch 109/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9025\n",
      "Epoch 109: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2922 - acc: 0.9029 - val_loss: 0.4183 - val_acc: 0.8908\n",
      "Epoch 110/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9051\n",
      "Epoch 110: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2930 - acc: 0.9052 - val_loss: 0.3922 - val_acc: 0.8880\n",
      "Epoch 111/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.8989\n",
      "Epoch 111: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2945 - acc: 0.8990 - val_loss: 0.3627 - val_acc: 0.8912\n",
      "Epoch 112/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9049\n",
      "Epoch 112: val_acc improved from 0.89610 to 0.89773, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3067 - acc: 0.9050 - val_loss: 0.3496 - val_acc: 0.8977\n",
      "Epoch 113/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9038\n",
      "Epoch 113: val_acc did not improve from 0.89773\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3018 - acc: 0.9038 - val_loss: 0.3499 - val_acc: 0.8929\n",
      "Epoch 114/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9047\n",
      "Epoch 114: val_acc did not improve from 0.89773\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2928 - acc: 0.9049 - val_loss: 0.3531 - val_acc: 0.8957\n",
      "Epoch 115/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9047\n",
      "Epoch 115: val_acc did not improve from 0.89773\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2867 - acc: 0.9048 - val_loss: 0.4046 - val_acc: 0.8941\n",
      "Epoch 116/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9036\n",
      "Epoch 116: val_acc did not improve from 0.89773\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3018 - acc: 0.9038 - val_loss: 0.3554 - val_acc: 0.8953\n",
      "Epoch 117/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2812 - acc: 0.9094\n",
      "Epoch 117: val_acc did not improve from 0.89773\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2812 - acc: 0.9094 - val_loss: 0.3864 - val_acc: 0.8965\n",
      "Epoch 118/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2899 - acc: 0.9088\n",
      "Epoch 118: val_acc did not improve from 0.89773\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2899 - acc: 0.9088 - val_loss: 0.3522 - val_acc: 0.8916\n",
      "Epoch 119/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9088\n",
      "Epoch 119: val_acc improved from 0.89773 to 0.90057, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2858 - acc: 0.9088 - val_loss: 0.3639 - val_acc: 0.9006\n",
      "Epoch 120/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9034\n",
      "Epoch 120: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3008 - acc: 0.9034 - val_loss: 0.3487 - val_acc: 0.8973\n",
      "Epoch 121/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9065\n",
      "Epoch 121: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2882 - acc: 0.9066 - val_loss: 0.3752 - val_acc: 0.8973\n",
      "Epoch 122/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9097\n",
      "Epoch 122: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3019 - acc: 0.9100 - val_loss: 0.3541 - val_acc: 0.8937\n",
      "Epoch 123/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9031\n",
      "Epoch 123: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2907 - acc: 0.9033 - val_loss: 0.3378 - val_acc: 0.8949\n",
      "Epoch 124/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9053\n",
      "Epoch 124: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2992 - acc: 0.9054 - val_loss: 0.3790 - val_acc: 0.8965\n",
      "Epoch 125/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9035\n",
      "Epoch 125: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2940 - acc: 0.9039 - val_loss: 0.4307 - val_acc: 0.8912\n",
      "Epoch 126/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9064\n",
      "Epoch 126: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2905 - acc: 0.9065 - val_loss: 0.4136 - val_acc: 0.8945\n",
      "Epoch 127/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9034\n",
      "Epoch 127: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2915 - acc: 0.9032 - val_loss: 0.3459 - val_acc: 0.8957\n",
      "Epoch 128/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9087\n",
      "Epoch 128: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2863 - acc: 0.9087 - val_loss: 0.3808 - val_acc: 0.8929\n",
      "Epoch 129/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9064\n",
      "Epoch 129: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2962 - acc: 0.9063 - val_loss: 0.4049 - val_acc: 0.8920\n",
      "Epoch 130/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9036\n",
      "Epoch 130: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3160 - acc: 0.9034 - val_loss: 0.3751 - val_acc: 0.8904\n",
      "Epoch 131/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2849 - acc: 0.9081\n",
      "Epoch 131: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2849 - acc: 0.9081 - val_loss: 0.3660 - val_acc: 0.8957\n",
      "Epoch 132/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9112\n",
      "Epoch 132: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2851 - acc: 0.9114 - val_loss: 0.3741 - val_acc: 0.8949\n",
      "Epoch 133/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9057\n",
      "Epoch 133: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2856 - acc: 0.9058 - val_loss: 0.4061 - val_acc: 0.8900\n",
      "Epoch 134/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2991 - acc: 0.9093\n",
      "Epoch 134: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2991 - acc: 0.9093 - val_loss: 0.4310 - val_acc: 0.8925\n",
      "Epoch 135/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9097\n",
      "Epoch 135: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3021 - acc: 0.9100 - val_loss: 0.3688 - val_acc: 0.8929\n",
      "Epoch 136/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3006 - acc: 0.9108\n",
      "Epoch 136: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3006 - acc: 0.9108 - val_loss: 0.3813 - val_acc: 0.8973\n",
      "Epoch 137/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9076\n",
      "Epoch 137: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3050 - acc: 0.9074 - val_loss: 0.3684 - val_acc: 0.8860\n",
      "Epoch 138/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9098\n",
      "Epoch 138: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2881 - acc: 0.9099 - val_loss: 0.3777 - val_acc: 0.8945\n",
      "Epoch 139/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9095\n",
      "Epoch 139: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3037 - acc: 0.9092 - val_loss: 0.4038 - val_acc: 0.8957\n",
      "Epoch 140/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2834 - acc: 0.9077\n",
      "Epoch 140: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2834 - acc: 0.9077 - val_loss: 0.3780 - val_acc: 0.8953\n",
      "Epoch 141/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9114\n",
      "Epoch 141: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2951 - acc: 0.9117 - val_loss: 0.3957 - val_acc: 0.8969\n",
      "Epoch 142/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9061\n",
      "Epoch 142: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2887 - acc: 0.9060 - val_loss: 0.3830 - val_acc: 0.8929\n",
      "Epoch 143/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9105\n",
      "Epoch 143: val_acc improved from 0.90057 to 0.90260, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2792 - acc: 0.9106 - val_loss: 0.3709 - val_acc: 0.9026\n",
      "Epoch 144/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9118\n",
      "Epoch 144: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2794 - acc: 0.9119 - val_loss: 0.3633 - val_acc: 0.8985\n",
      "Epoch 145/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9101\n",
      "Epoch 145: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2933 - acc: 0.9101 - val_loss: 0.4011 - val_acc: 0.8904\n",
      "Epoch 146/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9088\n",
      "Epoch 146: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2789 - acc: 0.9090 - val_loss: 0.3362 - val_acc: 0.8969\n",
      "Epoch 147/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2868 - acc: 0.9094\n",
      "Epoch 147: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2868 - acc: 0.9094 - val_loss: 0.3379 - val_acc: 0.8969\n",
      "Epoch 148/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9119\n",
      "Epoch 148: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2691 - acc: 0.9120 - val_loss: 0.3859 - val_acc: 0.8953\n",
      "Epoch 149/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9099\n",
      "Epoch 149: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2878 - acc: 0.9098 - val_loss: 0.3403 - val_acc: 0.8989\n",
      "Epoch 150/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9088\n",
      "Epoch 150: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2987 - acc: 0.9090 - val_loss: 0.3673 - val_acc: 0.9006\n",
      "Epoch 151/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9124\n",
      "Epoch 151: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2932 - acc: 0.9124 - val_loss: 0.3298 - val_acc: 0.8933\n",
      "Epoch 152/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9129\n",
      "Epoch 152: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2824 - acc: 0.9131 - val_loss: 0.3976 - val_acc: 0.8953\n",
      "Epoch 153/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9101\n",
      "Epoch 153: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2914 - acc: 0.9107 - val_loss: 0.4078 - val_acc: 0.8961\n",
      "Epoch 154/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9142\n",
      "Epoch 154: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2808 - acc: 0.9144 - val_loss: 0.3701 - val_acc: 0.8957\n",
      "Epoch 155/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9091\n",
      "Epoch 155: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2920 - acc: 0.9093 - val_loss: 0.4290 - val_acc: 0.8884\n",
      "Epoch 156/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9079\n",
      "Epoch 156: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2958 - acc: 0.9081 - val_loss: 0.3840 - val_acc: 0.9014\n",
      "Epoch 157/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9159\n",
      "Epoch 157: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2833 - acc: 0.9159 - val_loss: 0.3804 - val_acc: 0.9018\n",
      "Epoch 158/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9115\n",
      "Epoch 158: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2930 - acc: 0.9113 - val_loss: 0.4174 - val_acc: 0.8880\n",
      "Epoch 159/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.9120\n",
      "Epoch 159: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2729 - acc: 0.9120 - val_loss: 0.3582 - val_acc: 0.8969\n",
      "Epoch 160/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9122\n",
      "Epoch 160: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3060 - acc: 0.9122 - val_loss: 0.3264 - val_acc: 0.8977\n",
      "Epoch 161/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9073\n",
      "Epoch 161: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2844 - acc: 0.9077 - val_loss: 0.3631 - val_acc: 0.9014\n",
      "Epoch 162/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9097\n",
      "Epoch 162: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2848 - acc: 0.9097 - val_loss: 0.3347 - val_acc: 0.8994\n",
      "Epoch 163/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9126\n",
      "Epoch 163: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2817 - acc: 0.9129 - val_loss: 0.4296 - val_acc: 0.8973\n",
      "Epoch 164/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9164\n",
      "Epoch 164: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3019 - acc: 0.9163 - val_loss: 0.3593 - val_acc: 0.9026\n",
      "Epoch 165/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2948 - acc: 0.9155\n",
      "Epoch 165: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2948 - acc: 0.9155 - val_loss: 0.3940 - val_acc: 0.8973\n",
      "Epoch 166/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9123\n",
      "Epoch 166: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2706 - acc: 0.9124 - val_loss: 0.3494 - val_acc: 0.8916\n",
      "Epoch 167/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9133\n",
      "Epoch 167: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2858 - acc: 0.9132 - val_loss: 0.3468 - val_acc: 0.8994\n",
      "Epoch 168/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9128\n",
      "Epoch 168: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2831 - acc: 0.9128 - val_loss: 0.3639 - val_acc: 0.8953\n",
      "Epoch 169/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9127\n",
      "Epoch 169: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2897 - acc: 0.9126 - val_loss: 0.3627 - val_acc: 0.9018\n",
      "Epoch 170/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2908 - acc: 0.9156\n",
      "Epoch 170: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2908 - acc: 0.9156 - val_loss: 0.3518 - val_acc: 0.9010\n",
      "Epoch 171/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2825 - acc: 0.9152\n",
      "Epoch 171: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2825 - acc: 0.9152 - val_loss: 0.3578 - val_acc: 0.8981\n",
      "Epoch 172/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9171\n",
      "Epoch 172: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2837 - acc: 0.9175 - val_loss: 0.3713 - val_acc: 0.8998\n",
      "Epoch 173/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9127\n",
      "Epoch 173: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2854 - acc: 0.9130 - val_loss: 0.3865 - val_acc: 0.8937\n",
      "Epoch 174/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9160\n",
      "Epoch 174: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2789 - acc: 0.9159 - val_loss: 0.3982 - val_acc: 0.8994\n",
      "Epoch 175/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9148\n",
      "Epoch 175: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2839 - acc: 0.9150 - val_loss: 0.3543 - val_acc: 0.8981\n",
      "Epoch 176/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9176\n",
      "Epoch 176: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2877 - acc: 0.9174 - val_loss: 0.3733 - val_acc: 0.8989\n",
      "Epoch 177/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9153\n",
      "Epoch 177: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2881 - acc: 0.9153 - val_loss: 0.3625 - val_acc: 0.8994\n",
      "Epoch 178/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2842 - acc: 0.9114\n",
      "Epoch 178: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2842 - acc: 0.9114 - val_loss: 0.3881 - val_acc: 0.8985\n",
      "Epoch 179/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9134\n",
      "Epoch 179: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3013 - acc: 0.9133 - val_loss: 0.3884 - val_acc: 0.9010\n",
      "Epoch 180/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9120\n",
      "Epoch 180: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2967 - acc: 0.9123 - val_loss: 0.3809 - val_acc: 0.9006\n",
      "Epoch 181/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9131\n",
      "Epoch 181: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2949 - acc: 0.9130 - val_loss: 0.3487 - val_acc: 0.8989\n",
      "Epoch 182/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9156\n",
      "Epoch 182: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2898 - acc: 0.9157 - val_loss: 0.3681 - val_acc: 0.8985\n",
      "Epoch 183/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9181\n",
      "Epoch 183: val_acc improved from 0.90260 to 0.90584, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2818 - acc: 0.9183 - val_loss: 0.3671 - val_acc: 0.9058\n",
      "Epoch 184/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9155\n",
      "Epoch 184: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3017 - acc: 0.9157 - val_loss: 0.3375 - val_acc: 0.9018\n",
      "Epoch 185/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2786 - acc: 0.9147\n",
      "Epoch 185: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2786 - acc: 0.9147 - val_loss: 0.3888 - val_acc: 0.8989\n",
      "Epoch 186/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9121\n",
      "Epoch 186: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2872 - acc: 0.9124 - val_loss: 0.3772 - val_acc: 0.8900\n",
      "Epoch 187/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9153\n",
      "Epoch 187: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2828 - acc: 0.9151 - val_loss: 0.3605 - val_acc: 0.9022\n",
      "Epoch 188/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9147\n",
      "Epoch 188: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2695 - acc: 0.9146 - val_loss: 0.3870 - val_acc: 0.8985\n",
      "Epoch 189/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2845 - acc: 0.9152\n",
      "Epoch 189: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2845 - acc: 0.9152 - val_loss: 0.3381 - val_acc: 0.8994\n",
      "Epoch 190/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3016 - acc: 0.9144\n",
      "Epoch 190: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3016 - acc: 0.9144 - val_loss: 0.3513 - val_acc: 0.9006\n",
      "Epoch 191/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9159\n",
      "Epoch 191: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2763 - acc: 0.9158 - val_loss: 0.3852 - val_acc: 0.9006\n",
      "Epoch 192/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9200\n",
      "Epoch 192: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2870 - acc: 0.9194 - val_loss: 0.3754 - val_acc: 0.9018\n",
      "Epoch 193/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2979 - acc: 0.9132\n",
      "Epoch 193: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2979 - acc: 0.9132 - val_loss: 0.4339 - val_acc: 0.9018\n",
      "Epoch 194/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9155\n",
      "Epoch 194: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3064 - acc: 0.9154 - val_loss: 0.3541 - val_acc: 0.9006\n",
      "Epoch 195/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9154\n",
      "Epoch 195: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2730 - acc: 0.9158 - val_loss: 0.3693 - val_acc: 0.8969\n",
      "Epoch 196/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2761 - acc: 0.9157\n",
      "Epoch 196: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2761 - acc: 0.9157 - val_loss: 0.3775 - val_acc: 0.9046\n",
      "Epoch 197/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9144\n",
      "Epoch 197: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2946 - acc: 0.9145 - val_loss: 0.3477 - val_acc: 0.9050\n",
      "Epoch 198/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9147\n",
      "Epoch 198: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2879 - acc: 0.9148 - val_loss: 0.3473 - val_acc: 0.9038\n",
      "Epoch 199/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2857 - acc: 0.9176\n",
      "Epoch 199: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2857 - acc: 0.9176 - val_loss: 0.3647 - val_acc: 0.9022\n",
      "Epoch 200/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9158\n",
      "Epoch 200: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2990 - acc: 0.9162 - val_loss: 0.3999 - val_acc: 0.9038\n",
      "Epoch 201/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9147\n",
      "Epoch 201: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2876 - acc: 0.9146 - val_loss: 0.3448 - val_acc: 0.9018\n",
      "Epoch 202/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9171\n",
      "Epoch 202: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2890 - acc: 0.9169 - val_loss: 0.3435 - val_acc: 0.9010\n",
      "Epoch 203/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9166\n",
      "Epoch 203: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2910 - acc: 0.9167 - val_loss: 0.3846 - val_acc: 0.8981\n",
      "Epoch 204/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9197\n",
      "Epoch 204: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2719 - acc: 0.9197 - val_loss: 0.3671 - val_acc: 0.9014\n",
      "Epoch 205/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9154\n",
      "Epoch 205: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2901 - acc: 0.9156 - val_loss: 0.3689 - val_acc: 0.9006\n",
      "Epoch 206/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9164\n",
      "Epoch 206: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3012 - acc: 0.9161 - val_loss: 0.3387 - val_acc: 0.8973\n",
      "Epoch 207/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9177\n",
      "Epoch 207: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2782 - acc: 0.9179 - val_loss: 0.3629 - val_acc: 0.9018\n",
      "Epoch 208/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.9183\n",
      "Epoch 208: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2900 - acc: 0.9183 - val_loss: 0.3631 - val_acc: 0.9014\n",
      "Epoch 209/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2756 - acc: 0.9182\n",
      "Epoch 209: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2756 - acc: 0.9182 - val_loss: 0.3593 - val_acc: 0.9034\n",
      "Epoch 210/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9142\n",
      "Epoch 210: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2836 - acc: 0.9142 - val_loss: 0.3709 - val_acc: 0.8994\n",
      "Epoch 211/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9165\n",
      "Epoch 211: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2894 - acc: 0.9164 - val_loss: 0.3758 - val_acc: 0.9054\n",
      "Epoch 212/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9157\n",
      "Epoch 212: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2969 - acc: 0.9160 - val_loss: 0.3699 - val_acc: 0.8998\n",
      "Epoch 213/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9165\n",
      "Epoch 213: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2816 - acc: 0.9166 - val_loss: 0.3845 - val_acc: 0.8977\n",
      "Epoch 214/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9179\n",
      "Epoch 214: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2772 - acc: 0.9178 - val_loss: 0.3904 - val_acc: 0.8977\n",
      "Epoch 215/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9178\n",
      "Epoch 215: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2875 - acc: 0.9175 - val_loss: 0.3557 - val_acc: 0.8969\n",
      "Epoch 216/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9200\n",
      "Epoch 216: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3092 - acc: 0.9201 - val_loss: 0.3845 - val_acc: 0.8994\n",
      "Epoch 217/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9192\n",
      "Epoch 217: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.3049 - acc: 0.9189 - val_loss: 0.3864 - val_acc: 0.9018\n",
      "Epoch 218/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9180\n",
      "Epoch 218: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2956 - acc: 0.9182 - val_loss: 0.3736 - val_acc: 0.8973\n",
      "Epoch 219/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9190\n",
      "Epoch 219: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2760 - acc: 0.9190 - val_loss: 0.3738 - val_acc: 0.9002\n",
      "Epoch 220/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9170\n",
      "Epoch 220: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2855 - acc: 0.9173 - val_loss: 0.3791 - val_acc: 0.9006\n",
      "Epoch 221/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9186\n",
      "Epoch 221: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2845 - acc: 0.9189 - val_loss: 0.3412 - val_acc: 0.9022\n",
      "Epoch 222/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2767 - acc: 0.9215\n",
      "Epoch 222: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2767 - acc: 0.9215 - val_loss: 0.3326 - val_acc: 0.8945\n",
      "Epoch 223/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9188\n",
      "Epoch 223: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2690 - acc: 0.9182 - val_loss: 0.3458 - val_acc: 0.8969\n",
      "Epoch 224/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9167\n",
      "Epoch 224: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2816 - acc: 0.9167 - val_loss: 0.3911 - val_acc: 0.9030\n",
      "Epoch 225/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9179\n",
      "Epoch 225: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2888 - acc: 0.9179 - val_loss: 0.3272 - val_acc: 0.9018\n",
      "Epoch 226/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9156\n",
      "Epoch 226: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2875 - acc: 0.9156 - val_loss: 0.3886 - val_acc: 0.8981\n",
      "Epoch 227/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9199\n",
      "Epoch 227: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2900 - acc: 0.9200 - val_loss: 0.3738 - val_acc: 0.9042\n",
      "Epoch 228/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9188\n",
      "Epoch 228: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2835 - acc: 0.9189 - val_loss: 0.3752 - val_acc: 0.8977\n",
      "Epoch 229/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9192\n",
      "Epoch 229: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2864 - acc: 0.9191 - val_loss: 0.3959 - val_acc: 0.8989\n",
      "Epoch 230/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.9177\n",
      "Epoch 230: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2948 - acc: 0.9175 - val_loss: 0.3534 - val_acc: 0.9006\n",
      "Epoch 231/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2939 - acc: 0.9195\n",
      "Epoch 231: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2939 - acc: 0.9195 - val_loss: 0.3495 - val_acc: 0.9034\n",
      "Epoch 232/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9196\n",
      "Epoch 232: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2879 - acc: 0.9196 - val_loss: 0.4061 - val_acc: 0.8908\n",
      "Epoch 233/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9186\n",
      "Epoch 233: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2936 - acc: 0.9184 - val_loss: 0.3515 - val_acc: 0.8977\n",
      "Epoch 234/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2649 - acc: 0.9198\n",
      "Epoch 234: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2649 - acc: 0.9198 - val_loss: 0.3586 - val_acc: 0.9006\n",
      "Epoch 235/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9166\n",
      "Epoch 235: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2822 - acc: 0.9170 - val_loss: 0.3674 - val_acc: 0.9022\n",
      "Epoch 236/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2880 - acc: 0.9169\n",
      "Epoch 236: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2880 - acc: 0.9169 - val_loss: 0.4057 - val_acc: 0.9030\n",
      "Epoch 237/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9195\n",
      "Epoch 237: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2842 - acc: 0.9195 - val_loss: 0.3519 - val_acc: 0.9030\n",
      "Epoch 238/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2963 - acc: 0.9152\n",
      "Epoch 238: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2963 - acc: 0.9152 - val_loss: 0.3451 - val_acc: 0.8994\n",
      "Epoch 239/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9227\n",
      "Epoch 239: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2791 - acc: 0.9231 - val_loss: 0.3471 - val_acc: 0.9002\n",
      "Epoch 240/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9203\n",
      "Epoch 240: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2646 - acc: 0.9204 - val_loss: 0.4270 - val_acc: 0.8941\n",
      "Epoch 241/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9186\n",
      "Epoch 241: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2942 - acc: 0.9184 - val_loss: 0.3548 - val_acc: 0.8994\n",
      "Epoch 242/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9192\n",
      "Epoch 242: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2794 - acc: 0.9196 - val_loss: 0.3970 - val_acc: 0.9026\n",
      "Epoch 243/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9213\n",
      "Epoch 243: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2747 - acc: 0.9213 - val_loss: 0.3699 - val_acc: 0.8981\n",
      "Epoch 244/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9207\n",
      "Epoch 244: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3004 - acc: 0.9208 - val_loss: 0.3384 - val_acc: 0.9026\n",
      "Epoch 245/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9228\n",
      "Epoch 245: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2767 - acc: 0.9228 - val_loss: 0.3479 - val_acc: 0.9018\n",
      "Epoch 246/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9180\n",
      "Epoch 246: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2849 - acc: 0.9180 - val_loss: 0.3683 - val_acc: 0.9014\n",
      "Epoch 247/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9164\n",
      "Epoch 247: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2755 - acc: 0.9165 - val_loss: 0.3968 - val_acc: 0.8985\n",
      "Epoch 248/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9209\n",
      "Epoch 248: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2785 - acc: 0.9213 - val_loss: 0.4051 - val_acc: 0.9022\n",
      "Epoch 249/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9206\n",
      "Epoch 249: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2698 - acc: 0.9210 - val_loss: 0.4115 - val_acc: 0.8989\n",
      "Epoch 250/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9213\n",
      "Epoch 250: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2650 - acc: 0.9213 - val_loss: 0.3783 - val_acc: 0.9002\n",
      "Epoch 251/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9235\n",
      "Epoch 251: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2883 - acc: 0.9237 - val_loss: 0.3829 - val_acc: 0.9038\n",
      "Epoch 252/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9195\n",
      "Epoch 252: val_acc did not improve from 0.90584\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2817 - acc: 0.9193 - val_loss: 0.3643 - val_acc: 0.8965\n",
      "Epoch 253/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9162\n",
      "Epoch 253: val_acc improved from 0.90584 to 0.90909, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2878 - acc: 0.9161 - val_loss: 0.3657 - val_acc: 0.9091\n",
      "Epoch 254/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9240\n",
      "Epoch 254: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2772 - acc: 0.9241 - val_loss: 0.3915 - val_acc: 0.8998\n",
      "Epoch 255/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9213\n",
      "Epoch 255: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3050 - acc: 0.9214 - val_loss: 0.3805 - val_acc: 0.9054\n",
      "Epoch 256/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9216\n",
      "Epoch 256: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2942 - acc: 0.9217 - val_loss: 0.3664 - val_acc: 0.9030\n",
      "Epoch 257/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9204\n",
      "Epoch 257: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2918 - acc: 0.9203 - val_loss: 0.3966 - val_acc: 0.9022\n",
      "Epoch 258/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9176\n",
      "Epoch 258: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3069 - acc: 0.9179 - val_loss: 0.3620 - val_acc: 0.9046\n",
      "Epoch 259/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9225\n",
      "Epoch 259: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2794 - acc: 0.9223 - val_loss: 0.3646 - val_acc: 0.9079\n",
      "Epoch 260/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9190\n",
      "Epoch 260: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3098 - acc: 0.9190 - val_loss: 0.3924 - val_acc: 0.9010\n",
      "Epoch 261/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2742 - acc: 0.9252\n",
      "Epoch 261: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2742 - acc: 0.9252 - val_loss: 0.3549 - val_acc: 0.9030\n",
      "Epoch 262/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9221\n",
      "Epoch 262: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2893 - acc: 0.9222 - val_loss: 0.3877 - val_acc: 0.9079\n",
      "Epoch 263/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9228\n",
      "Epoch 263: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2890 - acc: 0.9227 - val_loss: 0.3891 - val_acc: 0.9054\n",
      "Epoch 264/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9196\n",
      "Epoch 264: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2728 - acc: 0.9195 - val_loss: 0.3609 - val_acc: 0.8989\n",
      "Epoch 265/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9203\n",
      "Epoch 265: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2844 - acc: 0.9205 - val_loss: 0.4230 - val_acc: 0.8981\n",
      "Epoch 266/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2791 - acc: 0.9248\n",
      "Epoch 266: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2791 - acc: 0.9248 - val_loss: 0.3454 - val_acc: 0.9042\n",
      "Epoch 267/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2871 - acc: 0.9205\n",
      "Epoch 267: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2871 - acc: 0.9205 - val_loss: 0.3477 - val_acc: 0.9010\n",
      "Epoch 268/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9188\n",
      "Epoch 268: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2790 - acc: 0.9189 - val_loss: 0.3544 - val_acc: 0.9054\n",
      "Epoch 269/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9213\n",
      "Epoch 269: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2618 - acc: 0.9213 - val_loss: 0.3825 - val_acc: 0.9018\n",
      "Epoch 270/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9176\n",
      "Epoch 270: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 5ms/step - loss: 0.2860 - acc: 0.9177 - val_loss: 0.3504 - val_acc: 0.9079\n",
      "Epoch 271/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9197\n",
      "Epoch 271: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2784 - acc: 0.9195 - val_loss: 0.3681 - val_acc: 0.9050\n",
      "Epoch 272/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9253\n",
      "Epoch 272: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2816 - acc: 0.9252 - val_loss: 0.3919 - val_acc: 0.9046\n",
      "Epoch 273/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9234\n",
      "Epoch 273: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3027 - acc: 0.9236 - val_loss: 0.3921 - val_acc: 0.9002\n",
      "Epoch 274/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9219\n",
      "Epoch 274: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2984 - acc: 0.9219 - val_loss: 0.3803 - val_acc: 0.9046\n",
      "Epoch 275/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9211\n",
      "Epoch 275: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2872 - acc: 0.9211 - val_loss: 0.3573 - val_acc: 0.9050\n",
      "Epoch 276/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9244\n",
      "Epoch 276: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2725 - acc: 0.9244 - val_loss: 0.3487 - val_acc: 0.9079\n",
      "Epoch 277/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9242\n",
      "Epoch 277: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2766 - acc: 0.9241 - val_loss: 0.3788 - val_acc: 0.9010\n",
      "Epoch 278/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9228\n",
      "Epoch 278: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2910 - acc: 0.9230 - val_loss: 0.4098 - val_acc: 0.9050\n",
      "Epoch 279/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9209\n",
      "Epoch 279: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2775 - acc: 0.9211 - val_loss: 0.3503 - val_acc: 0.9034\n",
      "Epoch 280/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9234\n",
      "Epoch 280: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2991 - acc: 0.9232 - val_loss: 0.4021 - val_acc: 0.8961\n",
      "Epoch 281/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9216\n",
      "Epoch 281: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2699 - acc: 0.9219 - val_loss: 0.3900 - val_acc: 0.9071\n",
      "Epoch 282/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9206\n",
      "Epoch 282: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2866 - acc: 0.9204 - val_loss: 0.3637 - val_acc: 0.9042\n",
      "Epoch 283/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9233\n",
      "Epoch 283: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2713 - acc: 0.9230 - val_loss: 0.3678 - val_acc: 0.9014\n",
      "Epoch 284/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9218\n",
      "Epoch 284: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2853 - acc: 0.9220 - val_loss: 0.3518 - val_acc: 0.9050\n",
      "Epoch 285/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9211\n",
      "Epoch 285: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2876 - acc: 0.9213 - val_loss: 0.3537 - val_acc: 0.9058\n",
      "Epoch 286/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9254\n",
      "Epoch 286: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2718 - acc: 0.9254 - val_loss: 0.3577 - val_acc: 0.8989\n",
      "Epoch 287/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9230\n",
      "Epoch 287: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2558 - acc: 0.9232 - val_loss: 0.3815 - val_acc: 0.9026\n",
      "Epoch 288/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9222\n",
      "Epoch 288: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2818 - acc: 0.9225 - val_loss: 0.3951 - val_acc: 0.9038\n",
      "Epoch 289/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9233\n",
      "Epoch 289: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2984 - acc: 0.9236 - val_loss: 0.3925 - val_acc: 0.9079\n",
      "Epoch 290/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.9182\n",
      "Epoch 290: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3126 - acc: 0.9183 - val_loss: 0.3955 - val_acc: 0.9067\n",
      "Epoch 291/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9244\n",
      "Epoch 291: val_acc improved from 0.90909 to 0.90950, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2796 - acc: 0.9243 - val_loss: 0.3783 - val_acc: 0.9095\n",
      "Epoch 292/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9255\n",
      "Epoch 292: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2982 - acc: 0.9256 - val_loss: 0.3587 - val_acc: 0.9091\n",
      "Epoch 293/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9198\n",
      "Epoch 293: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2927 - acc: 0.9194 - val_loss: 0.3618 - val_acc: 0.9018\n",
      "Epoch 294/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2775 - acc: 0.9223\n",
      "Epoch 294: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2775 - acc: 0.9223 - val_loss: 0.3614 - val_acc: 0.8989\n",
      "Epoch 295/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9178\n",
      "Epoch 295: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3002 - acc: 0.9178 - val_loss: 0.4104 - val_acc: 0.9014\n",
      "Epoch 296/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9232\n",
      "Epoch 296: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2733 - acc: 0.9228 - val_loss: 0.3431 - val_acc: 0.8998\n",
      "Epoch 297/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9230\n",
      "Epoch 297: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2678 - acc: 0.9229 - val_loss: 0.3721 - val_acc: 0.9050\n",
      "Epoch 298/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9222\n",
      "Epoch 298: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2843 - acc: 0.9223 - val_loss: 0.3613 - val_acc: 0.9010\n",
      "Epoch 299/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9225\n",
      "Epoch 299: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2693 - acc: 0.9226 - val_loss: 0.4027 - val_acc: 0.9014\n",
      "Epoch 300/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9235\n",
      "Epoch 300: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2869 - acc: 0.9235 - val_loss: 0.3873 - val_acc: 0.9014\n",
      "Epoch 301/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9239\n",
      "Epoch 301: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2884 - acc: 0.9239 - val_loss: 0.3489 - val_acc: 0.9038\n",
      "Epoch 302/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.9217\n",
      "Epoch 302: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3079 - acc: 0.9217 - val_loss: 0.3838 - val_acc: 0.9046\n",
      "Epoch 303/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.9231\n",
      "Epoch 303: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3216 - acc: 0.9227 - val_loss: 0.3746 - val_acc: 0.8949\n",
      "Epoch 304/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9263\n",
      "Epoch 304: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2664 - acc: 0.9264 - val_loss: 0.4161 - val_acc: 0.8998\n",
      "Epoch 305/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9195\n",
      "Epoch 305: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2848 - acc: 0.9196 - val_loss: 0.3789 - val_acc: 0.9079\n",
      "Epoch 306/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9246\n",
      "Epoch 306: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2696 - acc: 0.9244 - val_loss: 0.3595 - val_acc: 0.8981\n",
      "Epoch 307/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2786 - acc: 0.9239\n",
      "Epoch 307: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2775 - acc: 0.9242 - val_loss: 0.3961 - val_acc: 0.9062\n",
      "Epoch 308/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9251\n",
      "Epoch 308: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2908 - acc: 0.9252 - val_loss: 0.3609 - val_acc: 0.8953\n",
      "Epoch 309/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9229\n",
      "Epoch 309: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3128 - acc: 0.9231 - val_loss: 0.3581 - val_acc: 0.9010\n",
      "Epoch 310/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9242\n",
      "Epoch 310: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3028 - acc: 0.9239 - val_loss: 0.3694 - val_acc: 0.8908\n",
      "Epoch 311/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2731 - acc: 0.9245\n",
      "Epoch 311: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2731 - acc: 0.9245 - val_loss: 0.3830 - val_acc: 0.9058\n",
      "Epoch 312/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9263\n",
      "Epoch 312: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2877 - acc: 0.9267 - val_loss: 0.3881 - val_acc: 0.9091\n",
      "Epoch 313/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9220\n",
      "Epoch 313: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2863 - acc: 0.9217 - val_loss: 0.3331 - val_acc: 0.9058\n",
      "Epoch 314/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9250\n",
      "Epoch 314: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2815 - acc: 0.9251 - val_loss: 0.3939 - val_acc: 0.9006\n",
      "Epoch 315/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3012 - acc: 0.9216\n",
      "Epoch 315: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3012 - acc: 0.9216 - val_loss: 0.3730 - val_acc: 0.9014\n",
      "Epoch 316/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9238\n",
      "Epoch 316: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2744 - acc: 0.9233 - val_loss: 0.3548 - val_acc: 0.9075\n",
      "Epoch 317/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9258\n",
      "Epoch 317: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2735 - acc: 0.9257 - val_loss: 0.4130 - val_acc: 0.9091\n",
      "Epoch 318/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9238\n",
      "Epoch 318: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3007 - acc: 0.9239 - val_loss: 0.3939 - val_acc: 0.9075\n",
      "Epoch 319/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9300\n",
      "Epoch 319: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2737 - acc: 0.9302 - val_loss: 0.3913 - val_acc: 0.8994\n",
      "Epoch 320/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9266\n",
      "Epoch 320: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2851 - acc: 0.9266 - val_loss: 0.3928 - val_acc: 0.9038\n",
      "Epoch 321/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9246\n",
      "Epoch 321: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2821 - acc: 0.9245 - val_loss: 0.3577 - val_acc: 0.9050\n",
      "Epoch 322/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9273\n",
      "Epoch 322: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2723 - acc: 0.9269 - val_loss: 0.3747 - val_acc: 0.9079\n",
      "Epoch 323/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9230\n",
      "Epoch 323: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2848 - acc: 0.9228 - val_loss: 0.3914 - val_acc: 0.9046\n",
      "Epoch 324/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9257\n",
      "Epoch 324: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3003 - acc: 0.9258 - val_loss: 0.3910 - val_acc: 0.9095\n",
      "Epoch 325/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9247\n",
      "Epoch 325: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2838 - acc: 0.9244 - val_loss: 0.3995 - val_acc: 0.9046\n",
      "Epoch 326/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9247\n",
      "Epoch 326: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3016 - acc: 0.9248 - val_loss: 0.4035 - val_acc: 0.9054\n",
      "Epoch 327/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9200\n",
      "Epoch 327: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2839 - acc: 0.9201 - val_loss: 0.3819 - val_acc: 0.9038\n",
      "Epoch 328/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9217\n",
      "Epoch 328: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3138 - acc: 0.9217 - val_loss: 0.3850 - val_acc: 0.9010\n",
      "Epoch 329/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9246\n",
      "Epoch 329: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2784 - acc: 0.9243 - val_loss: 0.3907 - val_acc: 0.8981\n",
      "Epoch 330/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2611 - acc: 0.9245\n",
      "Epoch 330: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2611 - acc: 0.9245 - val_loss: 0.3838 - val_acc: 0.9034\n",
      "Epoch 331/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9254\n",
      "Epoch 331: val_acc did not improve from 0.90950\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2802 - acc: 0.9253 - val_loss: 0.4237 - val_acc: 0.9071\n",
      "Epoch 332/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9267\n",
      "Epoch 332: val_acc improved from 0.90950 to 0.91071, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2815 - acc: 0.9266 - val_loss: 0.3451 - val_acc: 0.9107\n",
      "Epoch 333/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9260\n",
      "Epoch 333: val_acc did not improve from 0.91071\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2759 - acc: 0.9257 - val_loss: 0.3434 - val_acc: 0.9071\n",
      "Epoch 334/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9198\n",
      "Epoch 334: val_acc did not improve from 0.91071\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2943 - acc: 0.9195 - val_loss: 0.3381 - val_acc: 0.9079\n",
      "Epoch 335/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9255\n",
      "Epoch 335: val_acc did not improve from 0.91071\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2982 - acc: 0.9254 - val_loss: 0.3855 - val_acc: 0.9058\n",
      "Epoch 336/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9261\n",
      "Epoch 336: val_acc did not improve from 0.91071\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2823 - acc: 0.9265 - val_loss: 0.3904 - val_acc: 0.9062\n",
      "Epoch 337/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9236\n",
      "Epoch 337: val_acc improved from 0.91071 to 0.91193, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/1/best_model.h5\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3042 - acc: 0.9234 - val_loss: 0.3777 - val_acc: 0.9119\n",
      "Epoch 338/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9225\n",
      "Epoch 338: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3034 - acc: 0.9223 - val_loss: 0.3987 - val_acc: 0.9022\n",
      "Epoch 339/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9249\n",
      "Epoch 339: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2763 - acc: 0.9250 - val_loss: 0.3977 - val_acc: 0.9083\n",
      "Epoch 340/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9229\n",
      "Epoch 340: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2689 - acc: 0.9230 - val_loss: 0.4084 - val_acc: 0.9079\n",
      "Epoch 341/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.9245\n",
      "Epoch 341: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3106 - acc: 0.9245 - val_loss: 0.3503 - val_acc: 0.9030\n",
      "Epoch 342/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9215\n",
      "Epoch 342: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2856 - acc: 0.9215 - val_loss: 0.3427 - val_acc: 0.9079\n",
      "Epoch 343/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9262\n",
      "Epoch 343: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2770 - acc: 0.9260 - val_loss: 0.3453 - val_acc: 0.9107\n",
      "Epoch 344/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9240\n",
      "Epoch 344: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2774 - acc: 0.9241 - val_loss: 0.4412 - val_acc: 0.9010\n",
      "Epoch 345/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9215\n",
      "Epoch 345: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2657 - acc: 0.9218 - val_loss: 0.4064 - val_acc: 0.9071\n",
      "Epoch 346/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9267\n",
      "Epoch 346: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2536 - acc: 0.9268 - val_loss: 0.4053 - val_acc: 0.9079\n",
      "Epoch 347/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9226\n",
      "Epoch 347: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2863 - acc: 0.9226 - val_loss: 0.4505 - val_acc: 0.9034\n",
      "Epoch 348/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9256\n",
      "Epoch 348: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2747 - acc: 0.9254 - val_loss: 0.4347 - val_acc: 0.9018\n",
      "Epoch 349/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9223\n",
      "Epoch 349: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3010 - acc: 0.9223 - val_loss: 0.3470 - val_acc: 0.9042\n",
      "Epoch 350/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9255\n",
      "Epoch 350: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2776 - acc: 0.9257 - val_loss: 0.3691 - val_acc: 0.9050\n",
      "Epoch 351/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9240\n",
      "Epoch 351: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2811 - acc: 0.9240 - val_loss: 0.3936 - val_acc: 0.9038\n",
      "Epoch 352/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9250\n",
      "Epoch 352: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2882 - acc: 0.9252 - val_loss: 0.4031 - val_acc: 0.9018\n",
      "Epoch 353/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9257\n",
      "Epoch 353: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2609 - acc: 0.9253 - val_loss: 0.3875 - val_acc: 0.8989\n",
      "Epoch 354/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2730 - acc: 0.9259\n",
      "Epoch 354: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2730 - acc: 0.9259 - val_loss: 0.3727 - val_acc: 0.9062\n",
      "Epoch 355/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.9217\n",
      "Epoch 355: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2621 - acc: 0.9219 - val_loss: 0.4108 - val_acc: 0.8998\n",
      "Epoch 356/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2910 - acc: 0.9241\n",
      "Epoch 356: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2910 - acc: 0.9241 - val_loss: 0.4646 - val_acc: 0.9030\n",
      "Epoch 357/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.9253\n",
      "Epoch 357: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3165 - acc: 0.9253 - val_loss: 0.4091 - val_acc: 0.9071\n",
      "Epoch 358/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9266\n",
      "Epoch 358: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2659 - acc: 0.9268 - val_loss: 0.4086 - val_acc: 0.9062\n",
      "Epoch 359/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9260\n",
      "Epoch 359: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2631 - acc: 0.9261 - val_loss: 0.3947 - val_acc: 0.9062\n",
      "Epoch 360/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9252\n",
      "Epoch 360: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3118 - acc: 0.9252 - val_loss: 0.3340 - val_acc: 0.9103\n",
      "Epoch 361/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9250\n",
      "Epoch 361: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2689 - acc: 0.9254 - val_loss: 0.4292 - val_acc: 0.8981\n",
      "Epoch 362/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3170 - acc: 0.9228\n",
      "Epoch 362: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3149 - acc: 0.9230 - val_loss: 0.3718 - val_acc: 0.9071\n",
      "Epoch 363/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9298\n",
      "Epoch 363: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2811 - acc: 0.9298 - val_loss: 0.3489 - val_acc: 0.9046\n",
      "Epoch 364/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2918 - acc: 0.9261\n",
      "Epoch 364: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2918 - acc: 0.9261 - val_loss: 0.4258 - val_acc: 0.8973\n",
      "Epoch 365/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9270\n",
      "Epoch 365: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3012 - acc: 0.9267 - val_loss: 0.3594 - val_acc: 0.8989\n",
      "Epoch 366/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9288\n",
      "Epoch 366: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2720 - acc: 0.9288 - val_loss: 0.4001 - val_acc: 0.9022\n",
      "Epoch 367/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9255\n",
      "Epoch 367: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2806 - acc: 0.9257 - val_loss: 0.3766 - val_acc: 0.9062\n",
      "Epoch 368/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9257\n",
      "Epoch 368: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2805 - acc: 0.9254 - val_loss: 0.3575 - val_acc: 0.9026\n",
      "Epoch 369/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9247\n",
      "Epoch 369: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2690 - acc: 0.9247 - val_loss: 0.4327 - val_acc: 0.9095\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 80, 1)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               41472     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,057\n",
      "Trainable params: 173,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.7284\n",
      "Epoch 1: val_acc improved from -inf to 0.77597, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.5954 - acc: 0.7291 - val_loss: 0.4748 - val_acc: 0.7760\n",
      "Epoch 2/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.7711\n",
      "Epoch 2: val_acc improved from 0.77597 to 0.79627, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.5011 - acc: 0.7716 - val_loss: 0.4532 - val_acc: 0.7963\n",
      "Epoch 3/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.7875\n",
      "Epoch 3: val_acc did not improve from 0.79627\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.4758 - acc: 0.7882 - val_loss: 0.4353 - val_acc: 0.7906\n",
      "Epoch 4/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.7932\n",
      "Epoch 4: val_acc improved from 0.79627 to 0.81656, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4496 - acc: 0.7938 - val_loss: 0.4116 - val_acc: 0.8166\n",
      "Epoch 5/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.4326 - acc: 0.8020\n",
      "Epoch 5: val_acc did not improve from 0.81656\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4326 - acc: 0.8020 - val_loss: 0.4219 - val_acc: 0.8113\n",
      "Epoch 6/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8129\n",
      "Epoch 6: val_acc did not improve from 0.81656\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.4260 - acc: 0.8132 - val_loss: 0.4183 - val_acc: 0.8036\n",
      "Epoch 7/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8163\n",
      "Epoch 7: val_acc improved from 0.81656 to 0.82468, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4186 - acc: 0.8162 - val_loss: 0.3967 - val_acc: 0.8247\n",
      "Epoch 8/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8188\n",
      "Epoch 8: val_acc improved from 0.82468 to 0.82589, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4138 - acc: 0.8188 - val_loss: 0.3931 - val_acc: 0.8259\n",
      "Epoch 9/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8290\n",
      "Epoch 9: val_acc improved from 0.82589 to 0.82995, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4040 - acc: 0.8289 - val_loss: 0.3913 - val_acc: 0.8300\n",
      "Epoch 10/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8275\n",
      "Epoch 10: val_acc improved from 0.82995 to 0.83279, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4087 - acc: 0.8279 - val_loss: 0.3849 - val_acc: 0.8328\n",
      "Epoch 11/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3944 - acc: 0.8321\n",
      "Epoch 11: val_acc improved from 0.83279 to 0.84578, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3933 - acc: 0.8320 - val_loss: 0.3739 - val_acc: 0.8458\n",
      "Epoch 12/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8361\n",
      "Epoch 12: val_acc improved from 0.84578 to 0.84943, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3957 - acc: 0.8361 - val_loss: 0.3623 - val_acc: 0.8494\n",
      "Epoch 13/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8392\n",
      "Epoch 13: val_acc did not improve from 0.84943\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3875 - acc: 0.8393 - val_loss: 0.3675 - val_acc: 0.8413\n",
      "Epoch 14/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8412\n",
      "Epoch 14: val_acc improved from 0.84943 to 0.84984, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3734 - acc: 0.8419 - val_loss: 0.3638 - val_acc: 0.8498\n",
      "Epoch 15/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8423\n",
      "Epoch 15: val_acc did not improve from 0.84984\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3835 - acc: 0.8423 - val_loss: 0.3775 - val_acc: 0.8397\n",
      "Epoch 16/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8436\n",
      "Epoch 16: val_acc did not improve from 0.84984\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3763 - acc: 0.8438 - val_loss: 0.3595 - val_acc: 0.8458\n",
      "Epoch 17/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8535\n",
      "Epoch 17: val_acc improved from 0.84984 to 0.85349, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3720 - acc: 0.8538 - val_loss: 0.3583 - val_acc: 0.8535\n",
      "Epoch 18/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8497\n",
      "Epoch 18: val_acc improved from 0.85349 to 0.85471, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3630 - acc: 0.8502 - val_loss: 0.3561 - val_acc: 0.8547\n",
      "Epoch 19/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8491\n",
      "Epoch 19: val_acc improved from 0.85471 to 0.85552, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3662 - acc: 0.8496 - val_loss: 0.3607 - val_acc: 0.8555\n",
      "Epoch 20/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8481\n",
      "Epoch 20: val_acc did not improve from 0.85552\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3628 - acc: 0.8482 - val_loss: 0.3535 - val_acc: 0.8527\n",
      "Epoch 21/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8558\n",
      "Epoch 21: val_acc did not improve from 0.85552\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3651 - acc: 0.8562 - val_loss: 0.3558 - val_acc: 0.8498\n",
      "Epoch 22/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8571\n",
      "Epoch 22: val_acc did not improve from 0.85552\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3598 - acc: 0.8571 - val_loss: 0.3643 - val_acc: 0.8498\n",
      "Epoch 23/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8576\n",
      "Epoch 23: val_acc did not improve from 0.85552\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3584 - acc: 0.8576 - val_loss: 0.3670 - val_acc: 0.8450\n",
      "Epoch 24/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8588\n",
      "Epoch 24: val_acc improved from 0.85552 to 0.85958, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3570 - acc: 0.8589 - val_loss: 0.3447 - val_acc: 0.8596\n",
      "Epoch 25/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8607\n",
      "Epoch 25: val_acc improved from 0.85958 to 0.86201, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3551 - acc: 0.8614 - val_loss: 0.3512 - val_acc: 0.8620\n",
      "Epoch 26/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.8615\n",
      "Epoch 26: val_acc did not improve from 0.86201\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3425 - acc: 0.8615 - val_loss: 0.3536 - val_acc: 0.8494\n",
      "Epoch 27/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8651\n",
      "Epoch 27: val_acc did not improve from 0.86201\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3513 - acc: 0.8651 - val_loss: 0.3523 - val_acc: 0.8571\n",
      "Epoch 28/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8624\n",
      "Epoch 28: val_acc did not improve from 0.86201\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3491 - acc: 0.8623 - val_loss: 0.3437 - val_acc: 0.8523\n",
      "Epoch 29/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8656\n",
      "Epoch 29: val_acc improved from 0.86201 to 0.86404, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3479 - acc: 0.8657 - val_loss: 0.3489 - val_acc: 0.8640\n",
      "Epoch 30/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8678\n",
      "Epoch 30: val_acc did not improve from 0.86404\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3385 - acc: 0.8676 - val_loss: 0.3440 - val_acc: 0.8571\n",
      "Epoch 31/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8634\n",
      "Epoch 31: val_acc did not improve from 0.86404\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3407 - acc: 0.8634 - val_loss: 0.3498 - val_acc: 0.8539\n",
      "Epoch 32/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8686\n",
      "Epoch 32: val_acc did not improve from 0.86404\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3418 - acc: 0.8689 - val_loss: 0.3609 - val_acc: 0.8588\n",
      "Epoch 33/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8664\n",
      "Epoch 33: val_acc did not improve from 0.86404\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3376 - acc: 0.8667 - val_loss: 0.3868 - val_acc: 0.8519\n",
      "Epoch 34/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8708\n",
      "Epoch 34: val_acc did not improve from 0.86404\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3483 - acc: 0.8709 - val_loss: 0.3461 - val_acc: 0.8620\n",
      "Epoch 35/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8708\n",
      "Epoch 35: val_acc did not improve from 0.86404\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3354 - acc: 0.8710 - val_loss: 0.3407 - val_acc: 0.8620\n",
      "Epoch 36/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8701\n",
      "Epoch 36: val_acc did not improve from 0.86404\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.3430 - acc: 0.8704 - val_loss: 0.3544 - val_acc: 0.8575\n",
      "Epoch 37/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8681\n",
      "Epoch 37: val_acc improved from 0.86404 to 0.86932, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3470 - acc: 0.8681 - val_loss: 0.3315 - val_acc: 0.8693\n",
      "Epoch 38/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8713\n",
      "Epoch 38: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3307 - acc: 0.8714 - val_loss: 0.3451 - val_acc: 0.8620\n",
      "Epoch 39/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8762\n",
      "Epoch 39: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3341 - acc: 0.8765 - val_loss: 0.3676 - val_acc: 0.8539\n",
      "Epoch 40/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3331 - acc: 0.8779\n",
      "Epoch 40: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3331 - acc: 0.8779 - val_loss: 0.3668 - val_acc: 0.8584\n",
      "Epoch 41/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8753\n",
      "Epoch 41: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3265 - acc: 0.8753 - val_loss: 0.3475 - val_acc: 0.8632\n",
      "Epoch 42/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8773\n",
      "Epoch 42: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3348 - acc: 0.8773 - val_loss: 0.3527 - val_acc: 0.8640\n",
      "Epoch 43/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8742\n",
      "Epoch 43: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3307 - acc: 0.8743 - val_loss: 0.3532 - val_acc: 0.8628\n",
      "Epoch 44/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8787\n",
      "Epoch 44: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3323 - acc: 0.8789 - val_loss: 0.3619 - val_acc: 0.8689\n",
      "Epoch 45/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8775\n",
      "Epoch 45: val_acc improved from 0.86932 to 0.87054, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3351 - acc: 0.8779 - val_loss: 0.3546 - val_acc: 0.8705\n",
      "Epoch 46/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8806\n",
      "Epoch 46: val_acc did not improve from 0.87054\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3217 - acc: 0.8806 - val_loss: 0.3508 - val_acc: 0.8673\n",
      "Epoch 47/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.8819\n",
      "Epoch 47: val_acc improved from 0.87054 to 0.87459, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3326 - acc: 0.8817 - val_loss: 0.3425 - val_acc: 0.8746\n",
      "Epoch 48/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8804\n",
      "Epoch 48: val_acc did not improve from 0.87459\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3294 - acc: 0.8811 - val_loss: 0.3494 - val_acc: 0.8689\n",
      "Epoch 49/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8825\n",
      "Epoch 49: val_acc did not improve from 0.87459\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3168 - acc: 0.8826 - val_loss: 0.3493 - val_acc: 0.8701\n",
      "Epoch 50/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8786\n",
      "Epoch 50: val_acc did not improve from 0.87459\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3295 - acc: 0.8785 - val_loss: 0.3368 - val_acc: 0.8742\n",
      "Epoch 51/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3252 - acc: 0.8816\n",
      "Epoch 51: val_acc improved from 0.87459 to 0.87500, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3252 - acc: 0.8816 - val_loss: 0.3323 - val_acc: 0.8750\n",
      "Epoch 52/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8792\n",
      "Epoch 52: val_acc did not improve from 0.87500\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3201 - acc: 0.8795 - val_loss: 0.3331 - val_acc: 0.8649\n",
      "Epoch 53/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8824\n",
      "Epoch 53: val_acc did not improve from 0.87500\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3242 - acc: 0.8825 - val_loss: 0.3376 - val_acc: 0.8689\n",
      "Epoch 54/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8855\n",
      "Epoch 54: val_acc improved from 0.87500 to 0.87946, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3224 - acc: 0.8857 - val_loss: 0.3541 - val_acc: 0.8795\n",
      "Epoch 55/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.8844\n",
      "Epoch 55: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3192 - acc: 0.8845 - val_loss: 0.3364 - val_acc: 0.8774\n",
      "Epoch 56/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8817\n",
      "Epoch 56: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3187 - acc: 0.8818 - val_loss: 0.3366 - val_acc: 0.8787\n",
      "Epoch 57/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8874\n",
      "Epoch 57: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3236 - acc: 0.8877 - val_loss: 0.3736 - val_acc: 0.8661\n",
      "Epoch 58/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8852\n",
      "Epoch 58: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3227 - acc: 0.8853 - val_loss: 0.3570 - val_acc: 0.8750\n",
      "Epoch 59/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.8853\n",
      "Epoch 59: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3068 - acc: 0.8855 - val_loss: 0.3497 - val_acc: 0.8770\n",
      "Epoch 60/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3136 - acc: 0.8897\n",
      "Epoch 60: val_acc improved from 0.87946 to 0.88271, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3136 - acc: 0.8897 - val_loss: 0.3381 - val_acc: 0.8827\n",
      "Epoch 61/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8856\n",
      "Epoch 61: val_acc did not improve from 0.88271\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3127 - acc: 0.8854 - val_loss: 0.3241 - val_acc: 0.8778\n",
      "Epoch 62/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.8900\n",
      "Epoch 62: val_acc did not improve from 0.88271\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3101 - acc: 0.8902 - val_loss: 0.3891 - val_acc: 0.8600\n",
      "Epoch 63/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.8907\n",
      "Epoch 63: val_acc improved from 0.88271 to 0.88799, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3078 - acc: 0.8909 - val_loss: 0.3671 - val_acc: 0.8880\n",
      "Epoch 64/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.8902\n",
      "Epoch 64: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3120 - acc: 0.8909 - val_loss: 0.3524 - val_acc: 0.8762\n",
      "Epoch 65/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.8924\n",
      "Epoch 65: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2989 - acc: 0.8923 - val_loss: 0.3446 - val_acc: 0.8778\n",
      "Epoch 66/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.8889\n",
      "Epoch 66: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3074 - acc: 0.8894 - val_loss: 0.3230 - val_acc: 0.8831\n",
      "Epoch 67/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8895\n",
      "Epoch 67: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3117 - acc: 0.8897 - val_loss: 0.3432 - val_acc: 0.8831\n",
      "Epoch 68/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8913\n",
      "Epoch 68: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3092 - acc: 0.8917 - val_loss: 0.3579 - val_acc: 0.8799\n",
      "Epoch 69/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.8914\n",
      "Epoch 69: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3046 - acc: 0.8915 - val_loss: 0.3869 - val_acc: 0.8730\n",
      "Epoch 70/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8910\n",
      "Epoch 70: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3129 - acc: 0.8911 - val_loss: 0.3606 - val_acc: 0.8774\n",
      "Epoch 71/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.8932\n",
      "Epoch 71: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3079 - acc: 0.8928 - val_loss: 0.3339 - val_acc: 0.8835\n",
      "Epoch 72/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.8942\n",
      "Epoch 72: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2964 - acc: 0.8941 - val_loss: 0.3584 - val_acc: 0.8681\n",
      "Epoch 73/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8909\n",
      "Epoch 73: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3179 - acc: 0.8910 - val_loss: 0.3349 - val_acc: 0.8795\n",
      "Epoch 74/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.8939\n",
      "Epoch 74: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3072 - acc: 0.8942 - val_loss: 0.3477 - val_acc: 0.8851\n",
      "Epoch 75/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.8925\n",
      "Epoch 75: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3143 - acc: 0.8923 - val_loss: 0.3290 - val_acc: 0.8876\n",
      "Epoch 76/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3060 - acc: 0.8949\n",
      "Epoch 76: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3060 - acc: 0.8949 - val_loss: 0.3624 - val_acc: 0.8791\n",
      "Epoch 77/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.8962\n",
      "Epoch 77: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3029 - acc: 0.8962 - val_loss: 0.3415 - val_acc: 0.8823\n",
      "Epoch 78/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8954\n",
      "Epoch 78: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3040 - acc: 0.8956 - val_loss: 0.3728 - val_acc: 0.8778\n",
      "Epoch 79/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2991 - acc: 0.8965\n",
      "Epoch 79: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 3s 6ms/step - loss: 0.2988 - acc: 0.8966 - val_loss: 0.3396 - val_acc: 0.8835\n",
      "Epoch 80/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.8942\n",
      "Epoch 80: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3040 - acc: 0.8940 - val_loss: 0.3491 - val_acc: 0.8689\n",
      "Epoch 81/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.8960\n",
      "Epoch 81: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2980 - acc: 0.8963 - val_loss: 0.3615 - val_acc: 0.8811\n",
      "Epoch 82/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8924\n",
      "Epoch 82: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3133 - acc: 0.8925 - val_loss: 0.3777 - val_acc: 0.8831\n",
      "Epoch 83/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.8950\n",
      "Epoch 83: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3054 - acc: 0.8952 - val_loss: 0.3684 - val_acc: 0.8799\n",
      "Epoch 84/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.8973\n",
      "Epoch 84: val_acc did not improve from 0.88799\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2992 - acc: 0.8974 - val_loss: 0.3577 - val_acc: 0.8839\n",
      "Epoch 85/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.8991\n",
      "Epoch 85: val_acc improved from 0.88799 to 0.89407, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2918 - acc: 0.8992 - val_loss: 0.3421 - val_acc: 0.8941\n",
      "Epoch 86/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3038 - acc: 0.8956\n",
      "Epoch 86: val_acc did not improve from 0.89407\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3038 - acc: 0.8956 - val_loss: 0.3621 - val_acc: 0.8811\n",
      "Epoch 87/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8998\n",
      "Epoch 87: val_acc did not improve from 0.89407\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2911 - acc: 0.8998 - val_loss: 0.3575 - val_acc: 0.8782\n",
      "Epoch 88/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.8990\n",
      "Epoch 88: val_acc did not improve from 0.89407\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2971 - acc: 0.8991 - val_loss: 0.3522 - val_acc: 0.8831\n",
      "Epoch 89/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.8985\n",
      "Epoch 89: val_acc did not improve from 0.89407\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2969 - acc: 0.8986 - val_loss: 0.3711 - val_acc: 0.8860\n",
      "Epoch 90/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.8952\n",
      "Epoch 90: val_acc did not improve from 0.89407\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3042 - acc: 0.8956 - val_loss: 0.3605 - val_acc: 0.8791\n",
      "Epoch 91/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.8993\n",
      "Epoch 91: val_acc did not improve from 0.89407\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3056 - acc: 0.8994 - val_loss: 0.3655 - val_acc: 0.8937\n",
      "Epoch 92/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.8976\n",
      "Epoch 92: val_acc improved from 0.89407 to 0.89651, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3018 - acc: 0.8977 - val_loss: 0.3269 - val_acc: 0.8965\n",
      "Epoch 93/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.8970\n",
      "Epoch 93: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3072 - acc: 0.8975 - val_loss: 0.3409 - val_acc: 0.8953\n",
      "Epoch 94/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9009\n",
      "Epoch 94: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2928 - acc: 0.9011 - val_loss: 0.3541 - val_acc: 0.8900\n",
      "Epoch 95/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.8974\n",
      "Epoch 95: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3033 - acc: 0.8974 - val_loss: 0.3510 - val_acc: 0.8811\n",
      "Epoch 96/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.8994\n",
      "Epoch 96: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2977 - acc: 0.8997 - val_loss: 0.3380 - val_acc: 0.8860\n",
      "Epoch 97/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9028\n",
      "Epoch 97: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3015 - acc: 0.9030 - val_loss: 0.3381 - val_acc: 0.8831\n",
      "Epoch 98/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.8988\n",
      "Epoch 98: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2896 - acc: 0.8991 - val_loss: 0.3595 - val_acc: 0.8807\n",
      "Epoch 99/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.8991\n",
      "Epoch 99: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3017 - acc: 0.8993 - val_loss: 0.3749 - val_acc: 0.8734\n",
      "Epoch 100/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.8955\n",
      "Epoch 100: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3095 - acc: 0.8957 - val_loss: 0.3322 - val_acc: 0.8876\n",
      "Epoch 101/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9005\n",
      "Epoch 101: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2955 - acc: 0.9007 - val_loss: 0.3519 - val_acc: 0.8835\n",
      "Epoch 102/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.8966\n",
      "Epoch 102: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2908 - acc: 0.8967 - val_loss: 0.3636 - val_acc: 0.8713\n",
      "Epoch 103/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9040\n",
      "Epoch 103: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2873 - acc: 0.9041 - val_loss: 0.3341 - val_acc: 0.8937\n",
      "Epoch 104/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9022\n",
      "Epoch 104: val_acc did not improve from 0.89651\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2870 - acc: 0.9026 - val_loss: 0.3603 - val_acc: 0.8856\n",
      "Epoch 105/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9018\n",
      "Epoch 105: val_acc improved from 0.89651 to 0.89692, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2862 - acc: 0.9022 - val_loss: 0.3333 - val_acc: 0.8969\n",
      "Epoch 106/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9002\n",
      "Epoch 106: val_acc did not improve from 0.89692\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3031 - acc: 0.9005 - val_loss: 0.3735 - val_acc: 0.8839\n",
      "Epoch 107/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9042\n",
      "Epoch 107: val_acc did not improve from 0.89692\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2989 - acc: 0.9039 - val_loss: 0.3461 - val_acc: 0.8819\n",
      "Epoch 108/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2899 - acc: 0.9033\n",
      "Epoch 108: val_acc did not improve from 0.89692\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2899 - acc: 0.9033 - val_loss: 0.3412 - val_acc: 0.8843\n",
      "Epoch 109/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9042\n",
      "Epoch 109: val_acc did not improve from 0.89692\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2846 - acc: 0.9042 - val_loss: 0.3620 - val_acc: 0.8864\n",
      "Epoch 110/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9066\n",
      "Epoch 110: val_acc did not improve from 0.89692\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2944 - acc: 0.9063 - val_loss: 0.4037 - val_acc: 0.8803\n",
      "Epoch 111/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2996 - acc: 0.8995\n",
      "Epoch 111: val_acc improved from 0.89692 to 0.89732, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2996 - acc: 0.8995 - val_loss: 0.3439 - val_acc: 0.8973\n",
      "Epoch 112/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9055\n",
      "Epoch 112: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2849 - acc: 0.9055 - val_loss: 0.3594 - val_acc: 0.8896\n",
      "Epoch 113/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9009\n",
      "Epoch 113: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2900 - acc: 0.9014 - val_loss: 0.3932 - val_acc: 0.8856\n",
      "Epoch 114/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9038\n",
      "Epoch 114: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2976 - acc: 0.9041 - val_loss: 0.3860 - val_acc: 0.8864\n",
      "Epoch 115/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2860 - acc: 0.9088\n",
      "Epoch 115: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2860 - acc: 0.9088 - val_loss: 0.3579 - val_acc: 0.8868\n",
      "Epoch 116/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9036\n",
      "Epoch 116: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3037 - acc: 0.9037 - val_loss: 0.3512 - val_acc: 0.8937\n",
      "Epoch 117/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9055\n",
      "Epoch 117: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2928 - acc: 0.9055 - val_loss: 0.3246 - val_acc: 0.8953\n",
      "Epoch 118/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9049\n",
      "Epoch 118: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2936 - acc: 0.9049 - val_loss: 0.4188 - val_acc: 0.8884\n",
      "Epoch 119/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9067\n",
      "Epoch 119: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2841 - acc: 0.9068 - val_loss: 0.3499 - val_acc: 0.8929\n",
      "Epoch 120/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9048\n",
      "Epoch 120: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2804 - acc: 0.9049 - val_loss: 0.3420 - val_acc: 0.8969\n",
      "Epoch 121/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9056\n",
      "Epoch 121: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2854 - acc: 0.9059 - val_loss: 0.3975 - val_acc: 0.8892\n",
      "Epoch 122/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9073\n",
      "Epoch 122: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2792 - acc: 0.9075 - val_loss: 0.3655 - val_acc: 0.8856\n",
      "Epoch 123/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9068\n",
      "Epoch 123: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2815 - acc: 0.9068 - val_loss: 0.3763 - val_acc: 0.8892\n",
      "Epoch 124/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9094\n",
      "Epoch 124: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2844 - acc: 0.9098 - val_loss: 0.3482 - val_acc: 0.8949\n",
      "Epoch 125/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9039\n",
      "Epoch 125: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2842 - acc: 0.9044 - val_loss: 0.3651 - val_acc: 0.8888\n",
      "Epoch 126/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9053\n",
      "Epoch 126: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2962 - acc: 0.9057 - val_loss: 0.3199 - val_acc: 0.8925\n",
      "Epoch 127/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2798 - acc: 0.9058\n",
      "Epoch 127: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2798 - acc: 0.9058 - val_loss: 0.3362 - val_acc: 0.8945\n",
      "Epoch 128/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9042\n",
      "Epoch 128: val_acc improved from 0.89732 to 0.90057, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2886 - acc: 0.9040 - val_loss: 0.3693 - val_acc: 0.9006\n",
      "Epoch 129/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9097\n",
      "Epoch 129: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2867 - acc: 0.9101 - val_loss: 0.3491 - val_acc: 0.8981\n",
      "Epoch 130/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9086\n",
      "Epoch 130: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2805 - acc: 0.9088 - val_loss: 0.3550 - val_acc: 0.8892\n",
      "Epoch 131/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2942 - acc: 0.9094\n",
      "Epoch 131: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2942 - acc: 0.9094 - val_loss: 0.3685 - val_acc: 0.8839\n",
      "Epoch 132/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9099\n",
      "Epoch 132: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2936 - acc: 0.9102 - val_loss: 0.3577 - val_acc: 0.8843\n",
      "Epoch 133/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.9057\n",
      "Epoch 133: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3083 - acc: 0.9059 - val_loss: 0.3586 - val_acc: 0.8949\n",
      "Epoch 134/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9102\n",
      "Epoch 134: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3103 - acc: 0.9097 - val_loss: 0.3333 - val_acc: 0.8941\n",
      "Epoch 135/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9062\n",
      "Epoch 135: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2884 - acc: 0.9066 - val_loss: 0.3581 - val_acc: 0.9002\n",
      "Epoch 136/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9096\n",
      "Epoch 136: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2818 - acc: 0.9097 - val_loss: 0.3795 - val_acc: 0.8949\n",
      "Epoch 137/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9073\n",
      "Epoch 137: val_acc did not improve from 0.90057\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2903 - acc: 0.9075 - val_loss: 0.3383 - val_acc: 0.8945\n",
      "Epoch 138/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9100\n",
      "Epoch 138: val_acc improved from 0.90057 to 0.90219, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2891 - acc: 0.9101 - val_loss: 0.3363 - val_acc: 0.9022\n",
      "Epoch 139/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9070\n",
      "Epoch 139: val_acc did not improve from 0.90219\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3012 - acc: 0.9073 - val_loss: 0.3737 - val_acc: 0.8847\n",
      "Epoch 140/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9076\n",
      "Epoch 140: val_acc did not improve from 0.90219\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2840 - acc: 0.9078 - val_loss: 0.3279 - val_acc: 0.9018\n",
      "Epoch 141/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3026 - acc: 0.9062\n",
      "Epoch 141: val_acc did not improve from 0.90219\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3026 - acc: 0.9062 - val_loss: 0.3558 - val_acc: 0.8941\n",
      "Epoch 142/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3141 - acc: 0.9110\n",
      "Epoch 142: val_acc did not improve from 0.90219\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3141 - acc: 0.9110 - val_loss: 0.4390 - val_acc: 0.8981\n",
      "Epoch 143/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9089\n",
      "Epoch 143: val_acc did not improve from 0.90219\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2895 - acc: 0.9091 - val_loss: 0.3798 - val_acc: 0.8965\n",
      "Epoch 144/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9094\n",
      "Epoch 144: val_acc did not improve from 0.90219\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2927 - acc: 0.9095 - val_loss: 0.3907 - val_acc: 0.8945\n",
      "Epoch 145/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2849 - acc: 0.9086\n",
      "Epoch 145: val_acc improved from 0.90219 to 0.90666, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2849 - acc: 0.9086 - val_loss: 0.3362 - val_acc: 0.9067\n",
      "Epoch 146/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2750 - acc: 0.9085\n",
      "Epoch 146: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2750 - acc: 0.9085 - val_loss: 0.3528 - val_acc: 0.8989\n",
      "Epoch 147/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9097\n",
      "Epoch 147: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3130 - acc: 0.9104 - val_loss: 0.3652 - val_acc: 0.8920\n",
      "Epoch 148/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9095\n",
      "Epoch 148: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2811 - acc: 0.9096 - val_loss: 0.4981 - val_acc: 0.8819\n",
      "Epoch 149/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9118\n",
      "Epoch 149: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3011 - acc: 0.9119 - val_loss: 0.3998 - val_acc: 0.8827\n",
      "Epoch 150/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9121\n",
      "Epoch 150: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2750 - acc: 0.9123 - val_loss: 0.3444 - val_acc: 0.8998\n",
      "Epoch 151/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.9132\n",
      "Epoch 151: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2713 - acc: 0.9133 - val_loss: 0.3733 - val_acc: 0.8994\n",
      "Epoch 152/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9097\n",
      "Epoch 152: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2909 - acc: 0.9098 - val_loss: 0.3766 - val_acc: 0.8973\n",
      "Epoch 153/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9062\n",
      "Epoch 153: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2839 - acc: 0.9063 - val_loss: 0.3397 - val_acc: 0.8981\n",
      "Epoch 154/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9103\n",
      "Epoch 154: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3105 - acc: 0.9103 - val_loss: 0.3722 - val_acc: 0.8994\n",
      "Epoch 155/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9137\n",
      "Epoch 155: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2859 - acc: 0.9137 - val_loss: 0.3841 - val_acc: 0.8900\n",
      "Epoch 156/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9084\n",
      "Epoch 156: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2958 - acc: 0.9089 - val_loss: 0.4019 - val_acc: 0.8920\n",
      "Epoch 157/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9072\n",
      "Epoch 157: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2930 - acc: 0.9074 - val_loss: 0.3603 - val_acc: 0.9002\n",
      "Epoch 158/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9121\n",
      "Epoch 158: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2973 - acc: 0.9125 - val_loss: 0.3784 - val_acc: 0.8937\n",
      "Epoch 159/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9106\n",
      "Epoch 159: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2852 - acc: 0.9108 - val_loss: 0.4082 - val_acc: 0.8888\n",
      "Epoch 160/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9114\n",
      "Epoch 160: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2812 - acc: 0.9114 - val_loss: 0.3455 - val_acc: 0.8961\n",
      "Epoch 161/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2930 - acc: 0.9111\n",
      "Epoch 161: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2930 - acc: 0.9111 - val_loss: 0.3424 - val_acc: 0.8965\n",
      "Epoch 162/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9163\n",
      "Epoch 162: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2798 - acc: 0.9166 - val_loss: 0.3648 - val_acc: 0.9026\n",
      "Epoch 163/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9138\n",
      "Epoch 163: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2796 - acc: 0.9143 - val_loss: 0.3368 - val_acc: 0.9058\n",
      "Epoch 164/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9154\n",
      "Epoch 164: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2825 - acc: 0.9158 - val_loss: 0.3855 - val_acc: 0.8880\n",
      "Epoch 165/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9118\n",
      "Epoch 165: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2854 - acc: 0.9119 - val_loss: 0.3447 - val_acc: 0.8998\n",
      "Epoch 166/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9110\n",
      "Epoch 166: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2766 - acc: 0.9111 - val_loss: 0.3653 - val_acc: 0.8949\n",
      "Epoch 167/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9147\n",
      "Epoch 167: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2790 - acc: 0.9151 - val_loss: 0.3339 - val_acc: 0.9054\n",
      "Epoch 168/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9100\n",
      "Epoch 168: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2840 - acc: 0.9106 - val_loss: 0.4019 - val_acc: 0.8961\n",
      "Epoch 169/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9148\n",
      "Epoch 169: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2907 - acc: 0.9153 - val_loss: 0.3736 - val_acc: 0.8994\n",
      "Epoch 170/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9103\n",
      "Epoch 170: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2885 - acc: 0.9105 - val_loss: 0.4080 - val_acc: 0.8920\n",
      "Epoch 171/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9116\n",
      "Epoch 171: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2952 - acc: 0.9118 - val_loss: 0.3581 - val_acc: 0.8965\n",
      "Epoch 172/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9102\n",
      "Epoch 172: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2877 - acc: 0.9101 - val_loss: 0.3612 - val_acc: 0.8856\n",
      "Epoch 173/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9079\n",
      "Epoch 173: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3079 - acc: 0.9080 - val_loss: 0.3429 - val_acc: 0.8920\n",
      "Epoch 174/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9148\n",
      "Epoch 174: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3006 - acc: 0.9149 - val_loss: 0.3929 - val_acc: 0.8985\n",
      "Epoch 175/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9114\n",
      "Epoch 175: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3023 - acc: 0.9115 - val_loss: 0.3692 - val_acc: 0.8900\n",
      "Epoch 176/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9125\n",
      "Epoch 176: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2822 - acc: 0.9127 - val_loss: 0.3242 - val_acc: 0.8953\n",
      "Epoch 177/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9124\n",
      "Epoch 177: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2768 - acc: 0.9124 - val_loss: 0.3740 - val_acc: 0.8973\n",
      "Epoch 178/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9122\n",
      "Epoch 178: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2895 - acc: 0.9123 - val_loss: 0.3874 - val_acc: 0.8961\n",
      "Epoch 179/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9161\n",
      "Epoch 179: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2760 - acc: 0.9163 - val_loss: 0.3798 - val_acc: 0.8953\n",
      "Epoch 180/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9125\n",
      "Epoch 180: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2918 - acc: 0.9130 - val_loss: 0.3344 - val_acc: 0.9030\n",
      "Epoch 181/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2767 - acc: 0.9145\n",
      "Epoch 181: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2767 - acc: 0.9145 - val_loss: 0.3762 - val_acc: 0.9026\n",
      "Epoch 182/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2679 - acc: 0.9164\n",
      "Epoch 182: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2679 - acc: 0.9164 - val_loss: 0.3841 - val_acc: 0.8957\n",
      "Epoch 183/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9139\n",
      "Epoch 183: val_acc improved from 0.90666 to 0.90869, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2910 - acc: 0.9139 - val_loss: 0.3575 - val_acc: 0.9087\n",
      "Epoch 184/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9098\n",
      "Epoch 184: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2870 - acc: 0.9098 - val_loss: 0.4008 - val_acc: 0.9010\n",
      "Epoch 185/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9146\n",
      "Epoch 185: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2787 - acc: 0.9147 - val_loss: 0.3908 - val_acc: 0.9006\n",
      "Epoch 186/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9154\n",
      "Epoch 186: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2905 - acc: 0.9155 - val_loss: 0.3607 - val_acc: 0.8868\n",
      "Epoch 187/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9149\n",
      "Epoch 187: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2892 - acc: 0.9147 - val_loss: 0.3603 - val_acc: 0.8998\n",
      "Epoch 188/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.9152\n",
      "Epoch 188: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2749 - acc: 0.9152 - val_loss: 0.3673 - val_acc: 0.8977\n",
      "Epoch 189/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9182\n",
      "Epoch 189: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2700 - acc: 0.9183 - val_loss: 0.4430 - val_acc: 0.8900\n",
      "Epoch 190/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9148\n",
      "Epoch 190: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2851 - acc: 0.9149 - val_loss: 0.3572 - val_acc: 0.9062\n",
      "Epoch 191/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9163\n",
      "Epoch 191: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2869 - acc: 0.9162 - val_loss: 0.3564 - val_acc: 0.8929\n",
      "Epoch 192/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9157\n",
      "Epoch 192: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2852 - acc: 0.9157 - val_loss: 0.3799 - val_acc: 0.9034\n",
      "Epoch 193/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9120\n",
      "Epoch 193: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3023 - acc: 0.9125 - val_loss: 0.3412 - val_acc: 0.9006\n",
      "Epoch 194/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9153\n",
      "Epoch 194: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2697 - acc: 0.9156 - val_loss: 0.4041 - val_acc: 0.8888\n",
      "Epoch 195/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9180\n",
      "Epoch 195: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2779 - acc: 0.9181 - val_loss: 0.4306 - val_acc: 0.8977\n",
      "Epoch 196/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9149\n",
      "Epoch 196: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2876 - acc: 0.9152 - val_loss: 0.3507 - val_acc: 0.9014\n",
      "Epoch 197/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9145\n",
      "Epoch 197: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2832 - acc: 0.9147 - val_loss: 0.3728 - val_acc: 0.9046\n",
      "Epoch 198/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9126\n",
      "Epoch 198: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3049 - acc: 0.9128 - val_loss: 0.3480 - val_acc: 0.9018\n",
      "Epoch 199/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9133\n",
      "Epoch 199: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2940 - acc: 0.9134 - val_loss: 0.4096 - val_acc: 0.8953\n",
      "Epoch 200/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9147\n",
      "Epoch 200: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2843 - acc: 0.9151 - val_loss: 0.4174 - val_acc: 0.8969\n",
      "Epoch 201/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.9135\n",
      "Epoch 201: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2971 - acc: 0.9136 - val_loss: 0.3693 - val_acc: 0.8981\n",
      "Epoch 202/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9144\n",
      "Epoch 202: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2786 - acc: 0.9146 - val_loss: 0.3477 - val_acc: 0.9046\n",
      "Epoch 203/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9144\n",
      "Epoch 203: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2915 - acc: 0.9147 - val_loss: 0.3236 - val_acc: 0.8989\n",
      "Epoch 204/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9143\n",
      "Epoch 204: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2684 - acc: 0.9144 - val_loss: 0.3527 - val_acc: 0.8957\n",
      "Epoch 205/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9171\n",
      "Epoch 205: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2947 - acc: 0.9171 - val_loss: 0.3828 - val_acc: 0.8888\n",
      "Epoch 206/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9202\n",
      "Epoch 206: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2826 - acc: 0.9203 - val_loss: 0.3973 - val_acc: 0.9026\n",
      "Epoch 207/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9138\n",
      "Epoch 207: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2972 - acc: 0.9142 - val_loss: 0.3737 - val_acc: 0.9083\n",
      "Epoch 208/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9188\n",
      "Epoch 208: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2855 - acc: 0.9190 - val_loss: 0.4169 - val_acc: 0.8925\n",
      "Epoch 209/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9173\n",
      "Epoch 209: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2942 - acc: 0.9173 - val_loss: 0.3988 - val_acc: 0.8937\n",
      "Epoch 210/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9154\n",
      "Epoch 210: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2932 - acc: 0.9155 - val_loss: 0.3951 - val_acc: 0.8969\n",
      "Epoch 211/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2874 - acc: 0.9168\n",
      "Epoch 211: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2874 - acc: 0.9168 - val_loss: 0.3831 - val_acc: 0.8945\n",
      "Epoch 212/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9154\n",
      "Epoch 212: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2735 - acc: 0.9155 - val_loss: 0.3874 - val_acc: 0.8884\n",
      "Epoch 213/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9182\n",
      "Epoch 213: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2794 - acc: 0.9186 - val_loss: 0.3585 - val_acc: 0.9022\n",
      "Epoch 214/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9168\n",
      "Epoch 214: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2964 - acc: 0.9172 - val_loss: 0.3394 - val_acc: 0.9026\n",
      "Epoch 215/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9175\n",
      "Epoch 215: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2831 - acc: 0.9174 - val_loss: 0.3964 - val_acc: 0.9038\n",
      "Epoch 216/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9153\n",
      "Epoch 216: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3127 - acc: 0.9155 - val_loss: 0.3766 - val_acc: 0.9046\n",
      "Epoch 217/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2712 - acc: 0.9156\n",
      "Epoch 217: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2712 - acc: 0.9156 - val_loss: 0.4015 - val_acc: 0.8957\n",
      "Epoch 218/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9165\n",
      "Epoch 218: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2802 - acc: 0.9168 - val_loss: 0.4206 - val_acc: 0.8965\n",
      "Epoch 219/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9175\n",
      "Epoch 219: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2921 - acc: 0.9176 - val_loss: 0.3976 - val_acc: 0.8965\n",
      "Epoch 220/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9175\n",
      "Epoch 220: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2816 - acc: 0.9175 - val_loss: 0.3708 - val_acc: 0.9054\n",
      "Epoch 221/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9208\n",
      "Epoch 221: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2878 - acc: 0.9210 - val_loss: 0.3949 - val_acc: 0.9006\n",
      "Epoch 222/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9196\n",
      "Epoch 222: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3028 - acc: 0.9197 - val_loss: 0.3585 - val_acc: 0.8969\n",
      "Epoch 223/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9185\n",
      "Epoch 223: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2670 - acc: 0.9188 - val_loss: 0.4032 - val_acc: 0.9083\n",
      "Epoch 224/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9135\n",
      "Epoch 224: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2791 - acc: 0.9136 - val_loss: 0.4283 - val_acc: 0.8945\n",
      "Epoch 225/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9170\n",
      "Epoch 225: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3104 - acc: 0.9168 - val_loss: 0.4050 - val_acc: 0.8929\n",
      "Epoch 226/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9162\n",
      "Epoch 226: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2836 - acc: 0.9164 - val_loss: 0.3632 - val_acc: 0.8953\n",
      "Epoch 227/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2763 - acc: 0.9182\n",
      "Epoch 227: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2763 - acc: 0.9182 - val_loss: 0.3794 - val_acc: 0.8985\n",
      "Epoch 228/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2669 - acc: 0.9173\n",
      "Epoch 228: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2669 - acc: 0.9173 - val_loss: 0.3809 - val_acc: 0.9042\n",
      "Epoch 229/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9199\n",
      "Epoch 229: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2908 - acc: 0.9200 - val_loss: 0.3856 - val_acc: 0.8961\n",
      "Epoch 230/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9186\n",
      "Epoch 230: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3036 - acc: 0.9186 - val_loss: 0.3740 - val_acc: 0.9087\n",
      "Epoch 231/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2767 - acc: 0.9180\n",
      "Epoch 231: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2767 - acc: 0.9180 - val_loss: 0.4139 - val_acc: 0.8977\n",
      "Epoch 232/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9196\n",
      "Epoch 232: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2922 - acc: 0.9202 - val_loss: 0.3729 - val_acc: 0.9067\n",
      "Epoch 233/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9196\n",
      "Epoch 233: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2805 - acc: 0.9198 - val_loss: 0.3940 - val_acc: 0.9026\n",
      "Epoch 234/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9213\n",
      "Epoch 234: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2685 - acc: 0.9215 - val_loss: 0.3803 - val_acc: 0.9006\n",
      "Epoch 235/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9181\n",
      "Epoch 235: val_acc improved from 0.90869 to 0.91193, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2805 - acc: 0.9178 - val_loss: 0.3823 - val_acc: 0.9119\n",
      "Epoch 236/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9198\n",
      "Epoch 236: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2937 - acc: 0.9197 - val_loss: 0.3437 - val_acc: 0.9071\n",
      "Epoch 237/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2853 - acc: 0.9170\n",
      "Epoch 237: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2853 - acc: 0.9170 - val_loss: 0.3719 - val_acc: 0.8981\n",
      "Epoch 238/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9200\n",
      "Epoch 238: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2837 - acc: 0.9202 - val_loss: 0.3658 - val_acc: 0.8981\n",
      "Epoch 239/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2769 - acc: 0.9208\n",
      "Epoch 239: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2769 - acc: 0.9208 - val_loss: 0.4061 - val_acc: 0.8945\n",
      "Epoch 240/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9168\n",
      "Epoch 240: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2914 - acc: 0.9170 - val_loss: 0.3790 - val_acc: 0.8969\n",
      "Epoch 241/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9192\n",
      "Epoch 241: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2851 - acc: 0.9197 - val_loss: 0.4070 - val_acc: 0.9042\n",
      "Epoch 242/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9211\n",
      "Epoch 242: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2926 - acc: 0.9213 - val_loss: 0.4126 - val_acc: 0.9002\n",
      "Epoch 243/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2991 - acc: 0.9191\n",
      "Epoch 243: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2982 - acc: 0.9188 - val_loss: 0.3824 - val_acc: 0.8989\n",
      "Epoch 244/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9201\n",
      "Epoch 244: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2863 - acc: 0.9202 - val_loss: 0.3994 - val_acc: 0.9018\n",
      "Epoch 245/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9199\n",
      "Epoch 245: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2778 - acc: 0.9198 - val_loss: 0.3678 - val_acc: 0.9054\n",
      "Epoch 246/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9205\n",
      "Epoch 246: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2687 - acc: 0.9206 - val_loss: 0.3968 - val_acc: 0.9042\n",
      "Epoch 247/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9175\n",
      "Epoch 247: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2863 - acc: 0.9178 - val_loss: 0.3404 - val_acc: 0.8973\n",
      "Epoch 248/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9198\n",
      "Epoch 248: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2883 - acc: 0.9202 - val_loss: 0.4293 - val_acc: 0.8977\n",
      "Epoch 249/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9181\n",
      "Epoch 249: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2861 - acc: 0.9188 - val_loss: 0.4043 - val_acc: 0.8973\n",
      "Epoch 250/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9178\n",
      "Epoch 250: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2959 - acc: 0.9175 - val_loss: 0.3934 - val_acc: 0.8916\n",
      "Epoch 251/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9207\n",
      "Epoch 251: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2887 - acc: 0.9210 - val_loss: 0.4074 - val_acc: 0.8969\n",
      "Epoch 252/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.9190\n",
      "Epoch 252: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3072 - acc: 0.9193 - val_loss: 0.3374 - val_acc: 0.9111\n",
      "Epoch 253/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9189\n",
      "Epoch 253: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3018 - acc: 0.9191 - val_loss: 0.4038 - val_acc: 0.9107\n",
      "Epoch 254/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3039 - acc: 0.9222\n",
      "Epoch 254: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3038 - acc: 0.9224 - val_loss: 0.3969 - val_acc: 0.8912\n",
      "Epoch 255/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9195\n",
      "Epoch 255: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2745 - acc: 0.9196 - val_loss: 0.3846 - val_acc: 0.9075\n",
      "Epoch 256/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2786 - acc: 0.9186\n",
      "Epoch 256: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2806 - acc: 0.9187 - val_loss: 0.4286 - val_acc: 0.8957\n",
      "Epoch 257/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9224\n",
      "Epoch 257: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2764 - acc: 0.9227 - val_loss: 0.4237 - val_acc: 0.9034\n",
      "Epoch 258/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9206\n",
      "Epoch 258: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2928 - acc: 0.9208 - val_loss: 0.4262 - val_acc: 0.8961\n",
      "Epoch 259/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2885 - acc: 0.9218\n",
      "Epoch 259: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2885 - acc: 0.9218 - val_loss: 0.3830 - val_acc: 0.8985\n",
      "Epoch 260/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9198\n",
      "Epoch 260: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2843 - acc: 0.9197 - val_loss: 0.4383 - val_acc: 0.8957\n",
      "Epoch 261/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9211\n",
      "Epoch 261: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3122 - acc: 0.9215 - val_loss: 0.4267 - val_acc: 0.8965\n",
      "Epoch 262/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9217\n",
      "Epoch 262: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2794 - acc: 0.9220 - val_loss: 0.3973 - val_acc: 0.9010\n",
      "Epoch 263/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9225\n",
      "Epoch 263: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2751 - acc: 0.9226 - val_loss: 0.4587 - val_acc: 0.8925\n",
      "Epoch 264/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9148\n",
      "Epoch 264: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3047 - acc: 0.9150 - val_loss: 0.3674 - val_acc: 0.9026\n",
      "Epoch 265/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3003 - acc: 0.9186\n",
      "Epoch 265: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2997 - acc: 0.9187 - val_loss: 0.3841 - val_acc: 0.9022\n",
      "Epoch 266/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9170\n",
      "Epoch 266: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2939 - acc: 0.9174 - val_loss: 0.3988 - val_acc: 0.8945\n",
      "Epoch 267/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9261\n",
      "Epoch 267: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2792 - acc: 0.9263 - val_loss: 0.4498 - val_acc: 0.9006\n",
      "Epoch 268/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9202\n",
      "Epoch 268: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3088 - acc: 0.9204 - val_loss: 0.3379 - val_acc: 0.9062\n",
      "Epoch 269/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2827 - acc: 0.9233\n",
      "Epoch 269: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2827 - acc: 0.9233 - val_loss: 0.3965 - val_acc: 0.9018\n",
      "Epoch 270/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2727 - acc: 0.9231\n",
      "Epoch 270: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2727 - acc: 0.9231 - val_loss: 0.3825 - val_acc: 0.9079\n",
      "Epoch 271/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.9203\n",
      "Epoch 271: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3180 - acc: 0.9208 - val_loss: 0.3706 - val_acc: 0.9006\n",
      "Epoch 272/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9204\n",
      "Epoch 272: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2735 - acc: 0.9205 - val_loss: 0.3847 - val_acc: 0.9034\n",
      "Epoch 273/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9223\n",
      "Epoch 273: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2787 - acc: 0.9223 - val_loss: 0.4346 - val_acc: 0.9079\n",
      "Epoch 274/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9184\n",
      "Epoch 274: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3098 - acc: 0.9185 - val_loss: 0.3812 - val_acc: 0.9050\n",
      "Epoch 275/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9243\n",
      "Epoch 275: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2986 - acc: 0.9243 - val_loss: 0.3534 - val_acc: 0.9050\n",
      "Epoch 276/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3168 - acc: 0.9211\n",
      "Epoch 276: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3168 - acc: 0.9211 - val_loss: 0.3394 - val_acc: 0.9075\n",
      "Epoch 277/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2783 - acc: 0.9191\n",
      "Epoch 277: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2783 - acc: 0.9191 - val_loss: 0.4211 - val_acc: 0.8985\n",
      "Epoch 278/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9232\n",
      "Epoch 278: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2711 - acc: 0.9234 - val_loss: 0.4118 - val_acc: 0.9095\n",
      "Epoch 279/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9232\n",
      "Epoch 279: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2967 - acc: 0.9233 - val_loss: 0.3829 - val_acc: 0.9091\n",
      "Epoch 280/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9238\n",
      "Epoch 280: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2910 - acc: 0.9235 - val_loss: 0.3608 - val_acc: 0.9062\n",
      "Epoch 281/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2826 - acc: 0.9204\n",
      "Epoch 281: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2826 - acc: 0.9204 - val_loss: 0.3709 - val_acc: 0.9014\n",
      "Epoch 282/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9209\n",
      "Epoch 282: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2702 - acc: 0.9211 - val_loss: 0.4095 - val_acc: 0.8998\n",
      "Epoch 283/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9214\n",
      "Epoch 283: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3003 - acc: 0.9215 - val_loss: 0.4104 - val_acc: 0.9014\n",
      "Epoch 284/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9211\n",
      "Epoch 284: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2880 - acc: 0.9214 - val_loss: 0.3473 - val_acc: 0.9079\n",
      "Epoch 285/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9218\n",
      "Epoch 285: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3055 - acc: 0.9221 - val_loss: 0.4135 - val_acc: 0.9006\n",
      "Epoch 286/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9200\n",
      "Epoch 286: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3044 - acc: 0.9200 - val_loss: 0.4483 - val_acc: 0.8977\n",
      "Epoch 287/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.9220\n",
      "Epoch 287: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3098 - acc: 0.9219 - val_loss: 0.3540 - val_acc: 0.9014\n",
      "Epoch 288/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9220\n",
      "Epoch 288: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2859 - acc: 0.9221 - val_loss: 0.4010 - val_acc: 0.8998\n",
      "Epoch 289/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9197\n",
      "Epoch 289: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3031 - acc: 0.9198 - val_loss: 0.3787 - val_acc: 0.9006\n",
      "Epoch 290/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9179\n",
      "Epoch 290: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3028 - acc: 0.9182 - val_loss: 0.3985 - val_acc: 0.9083\n",
      "Epoch 291/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9224\n",
      "Epoch 291: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3081 - acc: 0.9221 - val_loss: 0.4182 - val_acc: 0.8868\n",
      "Epoch 292/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2948 - acc: 0.9249\n",
      "Epoch 292: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2948 - acc: 0.9249 - val_loss: 0.4534 - val_acc: 0.9030\n",
      "Epoch 293/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9245\n",
      "Epoch 293: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2616 - acc: 0.9243 - val_loss: 0.4148 - val_acc: 0.9010\n",
      "Epoch 294/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9223\n",
      "Epoch 294: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2840 - acc: 0.9225 - val_loss: 0.4700 - val_acc: 0.8965\n",
      "Epoch 295/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2680 - acc: 0.9210\n",
      "Epoch 295: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2680 - acc: 0.9210 - val_loss: 0.4825 - val_acc: 0.8908\n",
      "Epoch 296/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2973 - acc: 0.9208\n",
      "Epoch 296: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2973 - acc: 0.9208 - val_loss: 0.3821 - val_acc: 0.9091\n",
      "Epoch 297/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9235\n",
      "Epoch 297: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3055 - acc: 0.9234 - val_loss: 0.3531 - val_acc: 0.9087\n",
      "Epoch 298/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9222\n",
      "Epoch 298: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3006 - acc: 0.9227 - val_loss: 0.3972 - val_acc: 0.8961\n",
      "Epoch 299/2000\n",
      "606/616 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9186\n",
      "Epoch 299: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2871 - acc: 0.9188 - val_loss: 0.3641 - val_acc: 0.9050\n",
      "Epoch 300/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2906 - acc: 0.9234\n",
      "Epoch 300: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2906 - acc: 0.9234 - val_loss: 0.4150 - val_acc: 0.9002\n",
      "Epoch 301/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9249\n",
      "Epoch 301: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2855 - acc: 0.9250 - val_loss: 0.3548 - val_acc: 0.9058\n",
      "Epoch 302/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9224\n",
      "Epoch 302: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2708 - acc: 0.9225 - val_loss: 0.4241 - val_acc: 0.9095\n",
      "Epoch 303/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9253\n",
      "Epoch 303: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2893 - acc: 0.9255 - val_loss: 0.3719 - val_acc: 0.9107\n",
      "Epoch 304/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9258\n",
      "Epoch 304: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2680 - acc: 0.9261 - val_loss: 0.4120 - val_acc: 0.9014\n",
      "Epoch 305/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9237\n",
      "Epoch 305: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2717 - acc: 0.9239 - val_loss: 0.3712 - val_acc: 0.9119\n",
      "Epoch 306/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9265\n",
      "Epoch 306: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2825 - acc: 0.9267 - val_loss: 0.3892 - val_acc: 0.9107\n",
      "Epoch 307/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9234\n",
      "Epoch 307: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2801 - acc: 0.9234 - val_loss: 0.3692 - val_acc: 0.9042\n",
      "Epoch 308/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2886 - acc: 0.9237\n",
      "Epoch 308: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2886 - acc: 0.9237 - val_loss: 0.4215 - val_acc: 0.9075\n",
      "Epoch 309/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9256\n",
      "Epoch 309: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2932 - acc: 0.9258 - val_loss: 0.3898 - val_acc: 0.9062\n",
      "Epoch 310/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9253\n",
      "Epoch 310: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2841 - acc: 0.9256 - val_loss: 0.4480 - val_acc: 0.9107\n",
      "Epoch 311/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9229\n",
      "Epoch 311: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2883 - acc: 0.9229 - val_loss: 0.4149 - val_acc: 0.9050\n",
      "Epoch 312/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9214\n",
      "Epoch 312: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3001 - acc: 0.9217 - val_loss: 0.3785 - val_acc: 0.9115\n",
      "Epoch 313/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9220\n",
      "Epoch 313: val_acc improved from 0.91193 to 0.91274, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2866 - acc: 0.9223 - val_loss: 0.3565 - val_acc: 0.9127\n",
      "Epoch 314/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9282\n",
      "Epoch 314: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2638 - acc: 0.9282 - val_loss: 0.3765 - val_acc: 0.8985\n",
      "Epoch 315/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9223\n",
      "Epoch 315: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3009 - acc: 0.9224 - val_loss: 0.3842 - val_acc: 0.9095\n",
      "Epoch 316/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9240\n",
      "Epoch 316: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3121 - acc: 0.9244 - val_loss: 0.3531 - val_acc: 0.9067\n",
      "Epoch 317/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9243\n",
      "Epoch 317: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2683 - acc: 0.9242 - val_loss: 0.4719 - val_acc: 0.8941\n",
      "Epoch 318/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9231\n",
      "Epoch 318: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2768 - acc: 0.9233 - val_loss: 0.3957 - val_acc: 0.9038\n",
      "Epoch 319/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2642 - acc: 0.9237\n",
      "Epoch 319: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2642 - acc: 0.9237 - val_loss: 0.4468 - val_acc: 0.9095\n",
      "Epoch 320/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9207\n",
      "Epoch 320: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2923 - acc: 0.9210 - val_loss: 0.4315 - val_acc: 0.8973\n",
      "Epoch 321/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9247\n",
      "Epoch 321: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3064 - acc: 0.9247 - val_loss: 0.4416 - val_acc: 0.8872\n",
      "Epoch 322/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3034 - acc: 0.9249\n",
      "Epoch 322: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3034 - acc: 0.9249 - val_loss: 0.4182 - val_acc: 0.8981\n",
      "Epoch 323/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9236\n",
      "Epoch 323: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3058 - acc: 0.9236 - val_loss: 0.4497 - val_acc: 0.8961\n",
      "Epoch 324/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2787 - acc: 0.9244\n",
      "Epoch 324: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2787 - acc: 0.9244 - val_loss: 0.3702 - val_acc: 0.9038\n",
      "Epoch 325/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9232\n",
      "Epoch 325: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2549 - acc: 0.9236 - val_loss: 0.4007 - val_acc: 0.9026\n",
      "Epoch 326/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9224\n",
      "Epoch 326: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3041 - acc: 0.9226 - val_loss: 0.4332 - val_acc: 0.9018\n",
      "Epoch 327/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9236\n",
      "Epoch 327: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2995 - acc: 0.9237 - val_loss: 0.4194 - val_acc: 0.8998\n",
      "Epoch 328/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9235\n",
      "Epoch 328: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3046 - acc: 0.9237 - val_loss: 0.4451 - val_acc: 0.9038\n",
      "Epoch 329/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9253\n",
      "Epoch 329: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2938 - acc: 0.9253 - val_loss: 0.4043 - val_acc: 0.8985\n",
      "Epoch 330/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9225\n",
      "Epoch 330: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2907 - acc: 0.9224 - val_loss: 0.3870 - val_acc: 0.8989\n",
      "Epoch 331/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9241\n",
      "Epoch 331: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2862 - acc: 0.9242 - val_loss: 0.4033 - val_acc: 0.9091\n",
      "Epoch 332/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9242\n",
      "Epoch 332: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2692 - acc: 0.9243 - val_loss: 0.4914 - val_acc: 0.9006\n",
      "Epoch 333/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9225\n",
      "Epoch 333: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2830 - acc: 0.9226 - val_loss: 0.3730 - val_acc: 0.9050\n",
      "Epoch 334/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9250\n",
      "Epoch 334: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2661 - acc: 0.9250 - val_loss: 0.4471 - val_acc: 0.8925\n",
      "Epoch 335/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9233\n",
      "Epoch 335: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3049 - acc: 0.9234 - val_loss: 0.3816 - val_acc: 0.9050\n",
      "Epoch 336/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9261\n",
      "Epoch 336: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2848 - acc: 0.9261 - val_loss: 0.4644 - val_acc: 0.9054\n",
      "Epoch 337/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9251\n",
      "Epoch 337: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2836 - acc: 0.9252 - val_loss: 0.3862 - val_acc: 0.9083\n",
      "Epoch 338/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9243\n",
      "Epoch 338: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2990 - acc: 0.9243 - val_loss: 0.3842 - val_acc: 0.9095\n",
      "Epoch 339/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3004 - acc: 0.9223\n",
      "Epoch 339: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3004 - acc: 0.9223 - val_loss: 0.4470 - val_acc: 0.8864\n",
      "Epoch 340/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.9241\n",
      "Epoch 340: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3242 - acc: 0.9244 - val_loss: 0.3424 - val_acc: 0.9062\n",
      "Epoch 341/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9276\n",
      "Epoch 341: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2760 - acc: 0.9278 - val_loss: 0.3704 - val_acc: 0.9095\n",
      "Epoch 342/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9235\n",
      "Epoch 342: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3077 - acc: 0.9237 - val_loss: 0.4433 - val_acc: 0.9099\n",
      "Epoch 343/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.9242\n",
      "Epoch 343: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3111 - acc: 0.9239 - val_loss: 0.4151 - val_acc: 0.9050\n",
      "Epoch 344/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9240\n",
      "Epoch 344: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2855 - acc: 0.9243 - val_loss: 0.4061 - val_acc: 0.9046\n",
      "Epoch 345/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9228\n",
      "Epoch 345: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2917 - acc: 0.9229 - val_loss: 0.4027 - val_acc: 0.9103\n",
      "Epoch 346/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9254\n",
      "Epoch 346: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2903 - acc: 0.9253 - val_loss: 0.4352 - val_acc: 0.8929\n",
      "Epoch 347/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9258\n",
      "Epoch 347: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2720 - acc: 0.9260 - val_loss: 0.4643 - val_acc: 0.8961\n",
      "Epoch 348/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9272\n",
      "Epoch 348: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2958 - acc: 0.9274 - val_loss: 0.4477 - val_acc: 0.9010\n",
      "Epoch 349/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.9291\n",
      "Epoch 349: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3211 - acc: 0.9292 - val_loss: 0.4090 - val_acc: 0.9079\n",
      "Epoch 350/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2944 - acc: 0.9246\n",
      "Epoch 350: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2944 - acc: 0.9246 - val_loss: 0.3811 - val_acc: 0.8969\n",
      "Epoch 351/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9256\n",
      "Epoch 351: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2925 - acc: 0.9257 - val_loss: 0.4027 - val_acc: 0.9010\n",
      "Epoch 352/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9278\n",
      "Epoch 352: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2993 - acc: 0.9280 - val_loss: 0.3709 - val_acc: 0.9103\n",
      "Epoch 353/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9262\n",
      "Epoch 353: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2859 - acc: 0.9263 - val_loss: 0.3768 - val_acc: 0.9062\n",
      "Epoch 354/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3193 - acc: 0.9243\n",
      "Epoch 354: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3193 - acc: 0.9243 - val_loss: 0.3530 - val_acc: 0.9091\n",
      "Epoch 355/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9259\n",
      "Epoch 355: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2875 - acc: 0.9260 - val_loss: 0.4363 - val_acc: 0.9046\n",
      "Epoch 356/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.9280\n",
      "Epoch 356: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3220 - acc: 0.9282 - val_loss: 0.3950 - val_acc: 0.9107\n",
      "Epoch 357/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9294\n",
      "Epoch 357: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3045 - acc: 0.9293 - val_loss: 0.3982 - val_acc: 0.9083\n",
      "Epoch 358/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9258\n",
      "Epoch 358: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3051 - acc: 0.9259 - val_loss: 0.3900 - val_acc: 0.9002\n",
      "Epoch 359/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9244\n",
      "Epoch 359: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3005 - acc: 0.9243 - val_loss: 0.3692 - val_acc: 0.9115\n",
      "Epoch 360/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9261\n",
      "Epoch 360: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3071 - acc: 0.9261 - val_loss: 0.4364 - val_acc: 0.9046\n",
      "Epoch 361/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3658 - acc: 0.9250\n",
      "Epoch 361: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3658 - acc: 0.9250 - val_loss: 0.4113 - val_acc: 0.9091\n",
      "Epoch 362/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9267\n",
      "Epoch 362: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2839 - acc: 0.9268 - val_loss: 0.4066 - val_acc: 0.9018\n",
      "Epoch 363/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.9230\n",
      "Epoch 363: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3157 - acc: 0.9232 - val_loss: 0.4408 - val_acc: 0.9103\n",
      "Epoch 364/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.9282\n",
      "Epoch 364: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3516 - acc: 0.9283 - val_loss: 0.4338 - val_acc: 0.9062\n",
      "Epoch 365/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9277\n",
      "Epoch 365: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3018 - acc: 0.9279 - val_loss: 0.4670 - val_acc: 0.8949\n",
      "Epoch 366/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3317 - acc: 0.9252\n",
      "Epoch 366: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3317 - acc: 0.9252 - val_loss: 0.4596 - val_acc: 0.8937\n",
      "Epoch 367/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9228\n",
      "Epoch 367: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3023 - acc: 0.9228 - val_loss: 0.4061 - val_acc: 0.9058\n",
      "Epoch 368/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.9279\n",
      "Epoch 368: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3363 - acc: 0.9280 - val_loss: 0.4715 - val_acc: 0.9050\n",
      "Epoch 369/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9252\n",
      "Epoch 369: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2942 - acc: 0.9254 - val_loss: 0.3729 - val_acc: 0.9018\n",
      "Epoch 370/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.9247\n",
      "Epoch 370: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3058 - acc: 0.9249 - val_loss: 0.4561 - val_acc: 0.9071\n",
      "Epoch 371/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.9261\n",
      "Epoch 371: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2900 - acc: 0.9261 - val_loss: 0.4285 - val_acc: 0.9107\n",
      "Epoch 372/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9251\n",
      "Epoch 372: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3111 - acc: 0.9252 - val_loss: 0.3889 - val_acc: 0.9058\n",
      "Epoch 373/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9264\n",
      "Epoch 373: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3006 - acc: 0.9262 - val_loss: 0.5146 - val_acc: 0.8925\n",
      "Epoch 374/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9195\n",
      "Epoch 374: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2704 - acc: 0.9197 - val_loss: 0.3948 - val_acc: 0.9107\n",
      "Epoch 375/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9253\n",
      "Epoch 375: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3070 - acc: 0.9254 - val_loss: 0.3770 - val_acc: 0.9099\n",
      "Epoch 376/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9258\n",
      "Epoch 376: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3010 - acc: 0.9257 - val_loss: 0.3773 - val_acc: 0.8945\n",
      "Epoch 377/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2928 - acc: 0.9253\n",
      "Epoch 377: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2928 - acc: 0.9253 - val_loss: 0.4041 - val_acc: 0.9046\n",
      "Epoch 378/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9238\n",
      "Epoch 378: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2903 - acc: 0.9242 - val_loss: 0.4103 - val_acc: 0.9026\n",
      "Epoch 379/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9256\n",
      "Epoch 379: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2789 - acc: 0.9257 - val_loss: 0.3899 - val_acc: 0.9038\n",
      "Epoch 380/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2743 - acc: 0.9252\n",
      "Epoch 380: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2743 - acc: 0.9252 - val_loss: 0.3701 - val_acc: 0.9111\n",
      "Epoch 381/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9247\n",
      "Epoch 381: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2871 - acc: 0.9247 - val_loss: 0.3818 - val_acc: 0.9067\n",
      "Epoch 382/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9276\n",
      "Epoch 382: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2993 - acc: 0.9278 - val_loss: 0.3586 - val_acc: 0.9107\n",
      "Epoch 383/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.9269\n",
      "Epoch 383: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2744 - acc: 0.9269 - val_loss: 0.4344 - val_acc: 0.9030\n",
      "Epoch 384/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9280\n",
      "Epoch 384: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2608 - acc: 0.9280 - val_loss: 0.4824 - val_acc: 0.9095\n",
      "Epoch 385/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9239\n",
      "Epoch 385: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2888 - acc: 0.9240 - val_loss: 0.3931 - val_acc: 0.9095\n",
      "Epoch 386/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.9249\n",
      "Epoch 386: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3555 - acc: 0.9251 - val_loss: 0.4785 - val_acc: 0.9107\n",
      "Epoch 387/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.9260\n",
      "Epoch 387: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3509 - acc: 0.9259 - val_loss: 0.4338 - val_acc: 0.9071\n",
      "Epoch 388/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9232\n",
      "Epoch 388: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2776 - acc: 0.9234 - val_loss: 0.4169 - val_acc: 0.9067\n",
      "Epoch 389/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9268\n",
      "Epoch 389: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3159 - acc: 0.9268 - val_loss: 0.3920 - val_acc: 0.9111\n",
      "Epoch 390/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9290\n",
      "Epoch 390: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3117 - acc: 0.9290 - val_loss: 0.4574 - val_acc: 0.9050\n",
      "Epoch 391/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.9263\n",
      "Epoch 391: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3180 - acc: 0.9264 - val_loss: 0.4693 - val_acc: 0.9050\n",
      "Epoch 392/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9241\n",
      "Epoch 392: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3059 - acc: 0.9239 - val_loss: 0.4666 - val_acc: 0.9071\n",
      "Epoch 393/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.9243\n",
      "Epoch 393: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3304 - acc: 0.9241 - val_loss: 0.4180 - val_acc: 0.8957\n",
      "Epoch 394/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9266\n",
      "Epoch 394: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3013 - acc: 0.9268 - val_loss: 0.4222 - val_acc: 0.9038\n",
      "Epoch 395/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.9258\n",
      "Epoch 395: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3159 - acc: 0.9256 - val_loss: 0.4239 - val_acc: 0.8985\n",
      "Epoch 396/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9287\n",
      "Epoch 396: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2606 - acc: 0.9286 - val_loss: 0.4393 - val_acc: 0.8941\n",
      "Epoch 397/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9248\n",
      "Epoch 397: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2824 - acc: 0.9250 - val_loss: 0.4038 - val_acc: 0.9010\n",
      "Epoch 398/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9303\n",
      "Epoch 398: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3066 - acc: 0.9302 - val_loss: 0.4801 - val_acc: 0.9042\n",
      "Epoch 399/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.9283\n",
      "Epoch 399: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3161 - acc: 0.9283 - val_loss: 0.3920 - val_acc: 0.9062\n",
      "Epoch 400/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9261\n",
      "Epoch 400: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3137 - acc: 0.9262 - val_loss: 0.4172 - val_acc: 0.9034\n",
      "Epoch 401/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9254\n",
      "Epoch 401: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3028 - acc: 0.9254 - val_loss: 0.4254 - val_acc: 0.9022\n",
      "Epoch 402/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9296\n",
      "Epoch 402: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2884 - acc: 0.9297 - val_loss: 0.4444 - val_acc: 0.9018\n",
      "Epoch 403/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9280\n",
      "Epoch 403: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3061 - acc: 0.9273 - val_loss: 0.3951 - val_acc: 0.9054\n",
      "Epoch 404/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9249\n",
      "Epoch 404: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2982 - acc: 0.9250 - val_loss: 0.4120 - val_acc: 0.9022\n",
      "Epoch 405/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.9292\n",
      "Epoch 405: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2728 - acc: 0.9293 - val_loss: 0.4467 - val_acc: 0.9075\n",
      "Epoch 406/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9282\n",
      "Epoch 406: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3082 - acc: 0.9285 - val_loss: 0.4309 - val_acc: 0.9123\n",
      "Epoch 407/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9272\n",
      "Epoch 407: val_acc did not improve from 0.91274\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2987 - acc: 0.9270 - val_loss: 0.4296 - val_acc: 0.8900\n",
      "Epoch 408/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9291\n",
      "Epoch 408: val_acc improved from 0.91274 to 0.91315, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2587 - acc: 0.9295 - val_loss: 0.3904 - val_acc: 0.9131\n",
      "Epoch 409/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9279\n",
      "Epoch 409: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3163 - acc: 0.9282 - val_loss: 0.4141 - val_acc: 0.9099\n",
      "Epoch 410/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.9242\n",
      "Epoch 410: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2965 - acc: 0.9243 - val_loss: 0.4159 - val_acc: 0.9127\n",
      "Epoch 411/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.9272\n",
      "Epoch 411: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3384 - acc: 0.9272 - val_loss: 0.4200 - val_acc: 0.9058\n",
      "Epoch 412/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9247\n",
      "Epoch 412: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2941 - acc: 0.9249 - val_loss: 0.5030 - val_acc: 0.9107\n",
      "Epoch 413/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9269\n",
      "Epoch 413: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3031 - acc: 0.9269 - val_loss: 0.4164 - val_acc: 0.9127\n",
      "Epoch 414/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9266\n",
      "Epoch 414: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3205 - acc: 0.9268 - val_loss: 0.3942 - val_acc: 0.9067\n",
      "Epoch 415/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9274\n",
      "Epoch 415: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2954 - acc: 0.9277 - val_loss: 0.4113 - val_acc: 0.9022\n",
      "Epoch 416/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9294\n",
      "Epoch 416: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2910 - acc: 0.9296 - val_loss: 0.3706 - val_acc: 0.9091\n",
      "Epoch 417/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9271\n",
      "Epoch 417: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3126 - acc: 0.9274 - val_loss: 0.4356 - val_acc: 0.9087\n",
      "Epoch 418/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9270\n",
      "Epoch 418: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3191 - acc: 0.9268 - val_loss: 0.3885 - val_acc: 0.9091\n",
      "Epoch 419/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.9267\n",
      "Epoch 419: val_acc improved from 0.91315 to 0.91599, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3232 - acc: 0.9269 - val_loss: 0.4542 - val_acc: 0.9160\n",
      "Epoch 420/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.9240\n",
      "Epoch 420: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3375 - acc: 0.9241 - val_loss: 0.3917 - val_acc: 0.9006\n",
      "Epoch 421/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9249\n",
      "Epoch 421: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3030 - acc: 0.9249 - val_loss: 0.4101 - val_acc: 0.9083\n",
      "Epoch 422/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.9278\n",
      "Epoch 422: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3350 - acc: 0.9281 - val_loss: 0.4090 - val_acc: 0.9071\n",
      "Epoch 423/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9250\n",
      "Epoch 423: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2801 - acc: 0.9253 - val_loss: 0.4065 - val_acc: 0.9087\n",
      "Epoch 424/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9317\n",
      "Epoch 424: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2953 - acc: 0.9318 - val_loss: 0.4412 - val_acc: 0.9014\n",
      "Epoch 425/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9284\n",
      "Epoch 425: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3012 - acc: 0.9286 - val_loss: 0.4389 - val_acc: 0.8998\n",
      "Epoch 426/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.9282\n",
      "Epoch 426: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3296 - acc: 0.9288 - val_loss: 0.3751 - val_acc: 0.9026\n",
      "Epoch 427/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9275\n",
      "Epoch 427: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2969 - acc: 0.9272 - val_loss: 0.4079 - val_acc: 0.9010\n",
      "Epoch 428/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9240\n",
      "Epoch 428: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2956 - acc: 0.9243 - val_loss: 0.3758 - val_acc: 0.9099\n",
      "Epoch 429/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.9280\n",
      "Epoch 429: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3307 - acc: 0.9281 - val_loss: 0.3838 - val_acc: 0.9002\n",
      "Epoch 430/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.9296\n",
      "Epoch 430: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3302 - acc: 0.9297 - val_loss: 0.4270 - val_acc: 0.8994\n",
      "Epoch 431/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9276\n",
      "Epoch 431: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3144 - acc: 0.9279 - val_loss: 0.4412 - val_acc: 0.9054\n",
      "Epoch 432/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9255\n",
      "Epoch 432: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3026 - acc: 0.9257 - val_loss: 0.4009 - val_acc: 0.9103\n",
      "Epoch 433/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.9259\n",
      "Epoch 433: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3433 - acc: 0.9263 - val_loss: 0.3990 - val_acc: 0.9103\n",
      "Epoch 434/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9293\n",
      "Epoch 434: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2558 - acc: 0.9294 - val_loss: 0.4586 - val_acc: 0.9136\n",
      "Epoch 435/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9313\n",
      "Epoch 435: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2781 - acc: 0.9315 - val_loss: 0.4320 - val_acc: 0.9054\n",
      "Epoch 436/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9242\n",
      "Epoch 436: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2981 - acc: 0.9243 - val_loss: 0.4110 - val_acc: 0.9030\n",
      "Epoch 437/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9273\n",
      "Epoch 437: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3018 - acc: 0.9274 - val_loss: 0.3994 - val_acc: 0.9046\n",
      "Epoch 438/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3043 - acc: 0.9283\n",
      "Epoch 438: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3043 - acc: 0.9283 - val_loss: 0.4314 - val_acc: 0.9046\n",
      "Epoch 439/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.9281\n",
      "Epoch 439: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3205 - acc: 0.9281 - val_loss: 0.4239 - val_acc: 0.9026\n",
      "Epoch 440/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9275\n",
      "Epoch 440: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3161 - acc: 0.9278 - val_loss: 0.4349 - val_acc: 0.9054\n",
      "Epoch 441/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9275\n",
      "Epoch 441: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2742 - acc: 0.9278 - val_loss: 0.4139 - val_acc: 0.9038\n",
      "Epoch 442/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9274\n",
      "Epoch 442: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3173 - acc: 0.9274 - val_loss: 0.4662 - val_acc: 0.9010\n",
      "Epoch 443/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3566 - acc: 0.9289\n",
      "Epoch 443: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3566 - acc: 0.9289 - val_loss: 0.4733 - val_acc: 0.9119\n",
      "Epoch 444/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.9295\n",
      "Epoch 444: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3299 - acc: 0.9296 - val_loss: 0.4129 - val_acc: 0.9002\n",
      "Epoch 445/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9289\n",
      "Epoch 445: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2887 - acc: 0.9289 - val_loss: 0.4122 - val_acc: 0.9042\n",
      "Epoch 446/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9284\n",
      "Epoch 446: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3067 - acc: 0.9285 - val_loss: 0.4952 - val_acc: 0.9006\n",
      "Epoch 447/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.9279\n",
      "Epoch 447: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3399 - acc: 0.9281 - val_loss: 0.4363 - val_acc: 0.9018\n",
      "Epoch 448/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.9295\n",
      "Epoch 448: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3451 - acc: 0.9295 - val_loss: 0.4316 - val_acc: 0.9046\n",
      "Epoch 449/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9264\n",
      "Epoch 449: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3051 - acc: 0.9266 - val_loss: 0.4206 - val_acc: 0.9095\n",
      "Epoch 450/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9286\n",
      "Epoch 450: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3122 - acc: 0.9287 - val_loss: 0.4340 - val_acc: 0.9026\n",
      "Epoch 451/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.9282\n",
      "Epoch 451: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3268 - acc: 0.9284 - val_loss: 0.4457 - val_acc: 0.9107\n",
      "Epoch 452/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.9273\n",
      "Epoch 452: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3532 - acc: 0.9271 - val_loss: 0.3683 - val_acc: 0.9067\n",
      "Epoch 453/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9260\n",
      "Epoch 453: val_acc improved from 0.91599 to 0.91721, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/2/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3151 - acc: 0.9261 - val_loss: 0.3685 - val_acc: 0.9172\n",
      "Epoch 454/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.9305\n",
      "Epoch 454: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3178 - acc: 0.9305 - val_loss: 0.4277 - val_acc: 0.9050\n",
      "Epoch 455/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.9311\n",
      "Epoch 455: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3302 - acc: 0.9312 - val_loss: 0.4266 - val_acc: 0.9058\n",
      "Epoch 456/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.9253\n",
      "Epoch 456: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3694 - acc: 0.9253 - val_loss: 0.4000 - val_acc: 0.8937\n",
      "Epoch 457/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9290\n",
      "Epoch 457: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2966 - acc: 0.9295 - val_loss: 0.4103 - val_acc: 0.9136\n",
      "Epoch 458/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2746 - acc: 0.9288\n",
      "Epoch 458: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2746 - acc: 0.9288 - val_loss: 0.3833 - val_acc: 0.8994\n",
      "Epoch 459/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9275\n",
      "Epoch 459: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2972 - acc: 0.9279 - val_loss: 0.4216 - val_acc: 0.9103\n",
      "Epoch 460/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9312\n",
      "Epoch 460: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2796 - acc: 0.9314 - val_loss: 0.4160 - val_acc: 0.9046\n",
      "Epoch 461/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9282\n",
      "Epoch 461: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2838 - acc: 0.9279 - val_loss: 0.4091 - val_acc: 0.9002\n",
      "Epoch 462/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9288\n",
      "Epoch 462: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3158 - acc: 0.9290 - val_loss: 0.4150 - val_acc: 0.8969\n",
      "Epoch 463/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9289\n",
      "Epoch 463: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2642 - acc: 0.9291 - val_loss: 0.4289 - val_acc: 0.9083\n",
      "Epoch 464/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9308\n",
      "Epoch 464: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2957 - acc: 0.9309 - val_loss: 0.4405 - val_acc: 0.9115\n",
      "Epoch 465/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9320\n",
      "Epoch 465: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2915 - acc: 0.9321 - val_loss: 0.4497 - val_acc: 0.9058\n",
      "Epoch 466/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3321 - acc: 0.9270\n",
      "Epoch 466: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3321 - acc: 0.9270 - val_loss: 0.4301 - val_acc: 0.9127\n",
      "Epoch 467/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9275\n",
      "Epoch 467: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2881 - acc: 0.9277 - val_loss: 0.3949 - val_acc: 0.9131\n",
      "Epoch 468/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9305\n",
      "Epoch 468: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2917 - acc: 0.9305 - val_loss: 0.3960 - val_acc: 0.9164\n",
      "Epoch 469/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9286\n",
      "Epoch 469: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2954 - acc: 0.9286 - val_loss: 0.4238 - val_acc: 0.9099\n",
      "Epoch 470/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.9318\n",
      "Epoch 470: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3192 - acc: 0.9317 - val_loss: 0.4067 - val_acc: 0.9107\n",
      "Epoch 471/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9294\n",
      "Epoch 471: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2969 - acc: 0.9294 - val_loss: 0.4150 - val_acc: 0.9099\n",
      "Epoch 472/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9298\n",
      "Epoch 472: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2993 - acc: 0.9300 - val_loss: 0.4682 - val_acc: 0.9050\n",
      "Epoch 473/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.9314\n",
      "Epoch 473: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3207 - acc: 0.9315 - val_loss: 0.4333 - val_acc: 0.9111\n",
      "Epoch 474/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9284\n",
      "Epoch 474: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2896 - acc: 0.9286 - val_loss: 0.4108 - val_acc: 0.9111\n",
      "Epoch 475/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9311\n",
      "Epoch 475: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2909 - acc: 0.9312 - val_loss: 0.4345 - val_acc: 0.9091\n",
      "Epoch 476/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.9303\n",
      "Epoch 476: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3398 - acc: 0.9305 - val_loss: 0.4203 - val_acc: 0.9014\n",
      "Epoch 477/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9278\n",
      "Epoch 477: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2785 - acc: 0.9281 - val_loss: 0.4540 - val_acc: 0.9075\n",
      "Epoch 478/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3139 - acc: 0.9314\n",
      "Epoch 478: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3139 - acc: 0.9314 - val_loss: 0.4630 - val_acc: 0.9014\n",
      "Epoch 479/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.9309\n",
      "Epoch 479: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3442 - acc: 0.9309 - val_loss: 0.4343 - val_acc: 0.9038\n",
      "Epoch 480/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.9297\n",
      "Epoch 480: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3183 - acc: 0.9294 - val_loss: 0.3765 - val_acc: 0.9030\n",
      "Epoch 481/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9287\n",
      "Epoch 481: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2919 - acc: 0.9288 - val_loss: 0.4508 - val_acc: 0.9030\n",
      "Epoch 482/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9292\n",
      "Epoch 482: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2855 - acc: 0.9293 - val_loss: 0.4453 - val_acc: 0.9087\n",
      "Epoch 483/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9291\n",
      "Epoch 483: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2900 - acc: 0.9292 - val_loss: 0.4294 - val_acc: 0.9058\n",
      "Epoch 484/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9286\n",
      "Epoch 484: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2998 - acc: 0.9288 - val_loss: 0.4495 - val_acc: 0.9107\n",
      "Epoch 485/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9284\n",
      "Epoch 485: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3041 - acc: 0.9284 - val_loss: 0.4284 - val_acc: 0.9103\n",
      "Epoch 486/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9267\n",
      "Epoch 486: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2837 - acc: 0.9268 - val_loss: 0.4521 - val_acc: 0.9079\n",
      "Epoch 487/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9268\n",
      "Epoch 487: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3264 - acc: 0.9268 - val_loss: 0.4104 - val_acc: 0.9136\n",
      "Epoch 488/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3235 - acc: 0.9265\n",
      "Epoch 488: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3235 - acc: 0.9265 - val_loss: 0.4238 - val_acc: 0.8953\n",
      "Epoch 489/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.9314\n",
      "Epoch 489: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3854 - acc: 0.9312 - val_loss: 0.3980 - val_acc: 0.9083\n",
      "Epoch 490/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3174 - acc: 0.9270\n",
      "Epoch 490: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3173 - acc: 0.9270 - val_loss: 0.4167 - val_acc: 0.8973\n",
      "Epoch 491/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.9267\n",
      "Epoch 491: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3246 - acc: 0.9268 - val_loss: 0.4442 - val_acc: 0.9014\n",
      "Epoch 492/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.9281\n",
      "Epoch 492: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3370 - acc: 0.9284 - val_loss: 0.4652 - val_acc: 0.9050\n",
      "Epoch 493/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.9322\n",
      "Epoch 493: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3060 - acc: 0.9323 - val_loss: 0.4464 - val_acc: 0.9111\n",
      "Epoch 494/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.9305\n",
      "Epoch 494: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3314 - acc: 0.9306 - val_loss: 0.4515 - val_acc: 0.9095\n",
      "Epoch 495/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.9291\n",
      "Epoch 495: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3454 - acc: 0.9292 - val_loss: 0.4331 - val_acc: 0.9010\n",
      "Epoch 496/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9276\n",
      "Epoch 496: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2778 - acc: 0.9278 - val_loss: 0.4636 - val_acc: 0.9071\n",
      "Epoch 497/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.9303\n",
      "Epoch 497: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3310 - acc: 0.9306 - val_loss: 0.4268 - val_acc: 0.9107\n",
      "Epoch 498/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9255\n",
      "Epoch 498: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2940 - acc: 0.9256 - val_loss: 0.4083 - val_acc: 0.9022\n",
      "Epoch 499/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.9307\n",
      "Epoch 499: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3438 - acc: 0.9309 - val_loss: 0.4412 - val_acc: 0.9058\n",
      "Epoch 500/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9291\n",
      "Epoch 500: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2808 - acc: 0.9294 - val_loss: 0.4075 - val_acc: 0.9071\n",
      "Epoch 501/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9272\n",
      "Epoch 501: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2780 - acc: 0.9275 - val_loss: 0.4063 - val_acc: 0.9099\n",
      "Epoch 502/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9305\n",
      "Epoch 502: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3047 - acc: 0.9308 - val_loss: 0.4259 - val_acc: 0.8953\n",
      "Epoch 503/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9284\n",
      "Epoch 503: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2649 - acc: 0.9283 - val_loss: 0.4650 - val_acc: 0.9107\n",
      "Epoch 504/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9269\n",
      "Epoch 504: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2930 - acc: 0.9270 - val_loss: 0.4166 - val_acc: 0.8985\n",
      "Epoch 505/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2951 - acc: 0.9301\n",
      "Epoch 505: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2951 - acc: 0.9301 - val_loss: 0.4587 - val_acc: 0.9042\n",
      "Epoch 506/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9273\n",
      "Epoch 506: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2968 - acc: 0.9274 - val_loss: 0.4830 - val_acc: 0.8985\n",
      "Epoch 507/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9324\n",
      "Epoch 507: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2580 - acc: 0.9323 - val_loss: 0.4859 - val_acc: 0.9050\n",
      "Epoch 508/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.9288\n",
      "Epoch 508: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3407 - acc: 0.9287 - val_loss: 0.3982 - val_acc: 0.9034\n",
      "Epoch 509/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9273\n",
      "Epoch 509: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3045 - acc: 0.9274 - val_loss: 0.4338 - val_acc: 0.9054\n",
      "Epoch 510/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3095 - acc: 0.9294\n",
      "Epoch 510: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3095 - acc: 0.9294 - val_loss: 0.4126 - val_acc: 0.9111\n",
      "Epoch 511/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9274\n",
      "Epoch 511: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3140 - acc: 0.9275 - val_loss: 0.4370 - val_acc: 0.9062\n",
      "Epoch 512/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9260\n",
      "Epoch 512: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2807 - acc: 0.9262 - val_loss: 0.4557 - val_acc: 0.9079\n",
      "Epoch 513/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9278\n",
      "Epoch 513: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2954 - acc: 0.9279 - val_loss: 0.4775 - val_acc: 0.9140\n",
      "Epoch 514/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.9295\n",
      "Epoch 514: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3278 - acc: 0.9296 - val_loss: 0.4187 - val_acc: 0.9071\n",
      "Epoch 515/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.9309\n",
      "Epoch 515: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3411 - acc: 0.9311 - val_loss: 0.4518 - val_acc: 0.9075\n",
      "Epoch 516/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9291\n",
      "Epoch 516: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3082 - acc: 0.9292 - val_loss: 0.4512 - val_acc: 0.9014\n",
      "Epoch 517/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2804 - acc: 0.9289\n",
      "Epoch 517: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2804 - acc: 0.9289 - val_loss: 0.4462 - val_acc: 0.9050\n",
      "Epoch 518/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.9279\n",
      "Epoch 518: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3558 - acc: 0.9283 - val_loss: 0.4809 - val_acc: 0.9079\n",
      "Epoch 519/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.9299\n",
      "Epoch 519: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3546 - acc: 0.9299 - val_loss: 0.4425 - val_acc: 0.9087\n",
      "Epoch 520/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.9271\n",
      "Epoch 520: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3528 - acc: 0.9272 - val_loss: 0.4802 - val_acc: 0.9050\n",
      "Epoch 521/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3407 - acc: 0.9269\n",
      "Epoch 521: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3407 - acc: 0.9269 - val_loss: 0.4286 - val_acc: 0.9050\n",
      "Epoch 522/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.9319\n",
      "Epoch 522: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3150 - acc: 0.9319 - val_loss: 0.4245 - val_acc: 0.9099\n",
      "Epoch 523/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9301\n",
      "Epoch 523: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2983 - acc: 0.9299 - val_loss: 0.4373 - val_acc: 0.9042\n",
      "Epoch 524/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.9296\n",
      "Epoch 524: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3503 - acc: 0.9295 - val_loss: 0.4874 - val_acc: 0.9148\n",
      "Epoch 525/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9273\n",
      "Epoch 525: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2918 - acc: 0.9274 - val_loss: 0.4731 - val_acc: 0.9091\n",
      "Epoch 526/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.9301\n",
      "Epoch 526: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3305 - acc: 0.9302 - val_loss: 0.4866 - val_acc: 0.9026\n",
      "Epoch 527/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9323\n",
      "Epoch 527: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2977 - acc: 0.9322 - val_loss: 0.4721 - val_acc: 0.9095\n",
      "Epoch 528/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.9277\n",
      "Epoch 528: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3351 - acc: 0.9278 - val_loss: 0.3851 - val_acc: 0.9087\n",
      "Epoch 529/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2854 - acc: 0.9306\n",
      "Epoch 529: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2854 - acc: 0.9306 - val_loss: 0.4682 - val_acc: 0.9050\n",
      "Epoch 530/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.9291\n",
      "Epoch 530: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3625 - acc: 0.9292 - val_loss: 0.4992 - val_acc: 0.9115\n",
      "Epoch 531/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.9314\n",
      "Epoch 531: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3353 - acc: 0.9314 - val_loss: 0.4137 - val_acc: 0.9075\n",
      "Epoch 532/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.9283\n",
      "Epoch 532: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3336 - acc: 0.9284 - val_loss: 0.4247 - val_acc: 0.8998\n",
      "Epoch 533/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.9281\n",
      "Epoch 533: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3251 - acc: 0.9282 - val_loss: 0.4390 - val_acc: 0.9111\n",
      "Epoch 534/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9307\n",
      "Epoch 534: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3065 - acc: 0.9308 - val_loss: 0.4475 - val_acc: 0.9042\n",
      "Epoch 535/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9283\n",
      "Epoch 535: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3072 - acc: 0.9285 - val_loss: 0.3746 - val_acc: 0.8973\n",
      "Epoch 536/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9293\n",
      "Epoch 536: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2791 - acc: 0.9290 - val_loss: 0.4606 - val_acc: 0.9062\n",
      "Epoch 537/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9288\n",
      "Epoch 537: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2958 - acc: 0.9291 - val_loss: 0.4356 - val_acc: 0.9014\n",
      "Epoch 538/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9298\n",
      "Epoch 538: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2533 - acc: 0.9299 - val_loss: 0.4367 - val_acc: 0.9050\n",
      "Epoch 539/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9317\n",
      "Epoch 539: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3198 - acc: 0.9319 - val_loss: 0.4595 - val_acc: 0.9002\n",
      "Epoch 540/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9274\n",
      "Epoch 540: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2863 - acc: 0.9275 - val_loss: 0.4489 - val_acc: 0.9067\n",
      "Epoch 541/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9300\n",
      "Epoch 541: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3099 - acc: 0.9300 - val_loss: 0.4578 - val_acc: 0.9103\n",
      "Epoch 542/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.9290\n",
      "Epoch 542: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3275 - acc: 0.9290 - val_loss: 0.4617 - val_acc: 0.8994\n",
      "Epoch 543/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.9263\n",
      "Epoch 543: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3262 - acc: 0.9266 - val_loss: 0.4363 - val_acc: 0.9022\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 80, 1)             0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               41472     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,057\n",
      "Trainable params: 173,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.6180 - acc: 0.7231\n",
      "Epoch 1: val_acc improved from -inf to 0.80073, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.6180 - acc: 0.7231 - val_loss: 0.4602 - val_acc: 0.8007\n",
      "Epoch 2/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.7683\n",
      "Epoch 2: val_acc improved from 0.80073 to 0.81412, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.5102 - acc: 0.7688 - val_loss: 0.4336 - val_acc: 0.8141\n",
      "Epoch 3/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.7833\n",
      "Epoch 3: val_acc improved from 0.81412 to 0.81818, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4797 - acc: 0.7837 - val_loss: 0.4107 - val_acc: 0.8182\n",
      "Epoch 4/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.7926\n",
      "Epoch 4: val_acc improved from 0.81818 to 0.82792, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4602 - acc: 0.7937 - val_loss: 0.3877 - val_acc: 0.8279\n",
      "Epoch 5/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.4442 - acc: 0.8013\n",
      "Epoch 5: val_acc improved from 0.82792 to 0.83482, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4442 - acc: 0.8013 - val_loss: 0.3871 - val_acc: 0.8348\n",
      "Epoch 6/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.8049\n",
      "Epoch 6: val_acc improved from 0.83482 to 0.83604, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4319 - acc: 0.8051 - val_loss: 0.3713 - val_acc: 0.8360\n",
      "Epoch 7/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8127\n",
      "Epoch 7: val_acc improved from 0.83604 to 0.84578, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4311 - acc: 0.8129 - val_loss: 0.3699 - val_acc: 0.8458\n",
      "Epoch 8/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8195\n",
      "Epoch 8: val_acc improved from 0.84578 to 0.84659, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4223 - acc: 0.8196 - val_loss: 0.3648 - val_acc: 0.8466\n",
      "Epoch 9/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.4153 - acc: 0.8186\n",
      "Epoch 9: val_acc did not improve from 0.84659\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4153 - acc: 0.8186 - val_loss: 0.3575 - val_acc: 0.8458\n",
      "Epoch 10/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8248\n",
      "Epoch 10: val_acc did not improve from 0.84659\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.4110 - acc: 0.8248 - val_loss: 0.3555 - val_acc: 0.8450\n",
      "Epoch 11/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8265\n",
      "Epoch 11: val_acc improved from 0.84659 to 0.85674, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3986 - acc: 0.8270 - val_loss: 0.3506 - val_acc: 0.8567\n",
      "Epoch 12/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8353\n",
      "Epoch 12: val_acc did not improve from 0.85674\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3993 - acc: 0.8355 - val_loss: 0.3446 - val_acc: 0.8559\n",
      "Epoch 13/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8330\n",
      "Epoch 13: val_acc improved from 0.85674 to 0.86282, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3997 - acc: 0.8334 - val_loss: 0.3397 - val_acc: 0.8628\n",
      "Epoch 14/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8364\n",
      "Epoch 14: val_acc improved from 0.86282 to 0.86445, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3982 - acc: 0.8366 - val_loss: 0.3453 - val_acc: 0.8644\n",
      "Epoch 15/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8372\n",
      "Epoch 15: val_acc did not improve from 0.86445\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3890 - acc: 0.8372 - val_loss: 0.3431 - val_acc: 0.8523\n",
      "Epoch 16/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8423\n",
      "Epoch 16: val_acc improved from 0.86445 to 0.86648, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3791 - acc: 0.8425 - val_loss: 0.3403 - val_acc: 0.8665\n",
      "Epoch 17/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8405\n",
      "Epoch 17: val_acc improved from 0.86648 to 0.86932, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3878 - acc: 0.8411 - val_loss: 0.3373 - val_acc: 0.8693\n",
      "Epoch 18/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8476\n",
      "Epoch 18: val_acc did not improve from 0.86932\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3796 - acc: 0.8487 - val_loss: 0.3550 - val_acc: 0.8628\n",
      "Epoch 19/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3880 - acc: 0.8497\n",
      "Epoch 19: val_acc improved from 0.86932 to 0.87175, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 6s 9ms/step - loss: 0.3880 - acc: 0.8497 - val_loss: 0.3293 - val_acc: 0.8718\n",
      "Epoch 20/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8477\n",
      "Epoch 20: val_acc improved from 0.87175 to 0.87216, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3769 - acc: 0.8481 - val_loss: 0.3278 - val_acc: 0.8722\n",
      "Epoch 21/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8507\n",
      "Epoch 21: val_acc did not improve from 0.87216\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3660 - acc: 0.8515 - val_loss: 0.3348 - val_acc: 0.8709\n",
      "Epoch 22/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8506\n",
      "Epoch 22: val_acc improved from 0.87216 to 0.87459, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3784 - acc: 0.8509 - val_loss: 0.3322 - val_acc: 0.8746\n",
      "Epoch 23/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8529\n",
      "Epoch 23: val_acc did not improve from 0.87459\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3640 - acc: 0.8532 - val_loss: 0.3260 - val_acc: 0.8734\n",
      "Epoch 24/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8519\n",
      "Epoch 24: val_acc did not improve from 0.87459\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3644 - acc: 0.8524 - val_loss: 0.3184 - val_acc: 0.8713\n",
      "Epoch 25/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8568\n",
      "Epoch 25: val_acc improved from 0.87459 to 0.87825, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 6s 9ms/step - loss: 0.3555 - acc: 0.8573 - val_loss: 0.3139 - val_acc: 0.8782\n",
      "Epoch 26/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8609\n",
      "Epoch 26: val_acc improved from 0.87825 to 0.87946, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3620 - acc: 0.8609 - val_loss: 0.3120 - val_acc: 0.8795\n",
      "Epoch 27/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8584\n",
      "Epoch 27: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3591 - acc: 0.8585 - val_loss: 0.3262 - val_acc: 0.8787\n",
      "Epoch 28/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8613\n",
      "Epoch 28: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3565 - acc: 0.8615 - val_loss: 0.3196 - val_acc: 0.8795\n",
      "Epoch 29/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8582\n",
      "Epoch 29: val_acc did not improve from 0.87946\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3625 - acc: 0.8588 - val_loss: 0.3259 - val_acc: 0.8762\n",
      "Epoch 30/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8619\n",
      "Epoch 30: val_acc improved from 0.87946 to 0.88312, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3557 - acc: 0.8621 - val_loss: 0.3144 - val_acc: 0.8831\n",
      "Epoch 31/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8619\n",
      "Epoch 31: val_acc did not improve from 0.88312\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3522 - acc: 0.8617 - val_loss: 0.3093 - val_acc: 0.8766\n",
      "Epoch 32/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3488 - acc: 0.8637\n",
      "Epoch 32: val_acc improved from 0.88312 to 0.88677, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3475 - acc: 0.8640 - val_loss: 0.3078 - val_acc: 0.8868\n",
      "Epoch 33/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8629\n",
      "Epoch 33: val_acc did not improve from 0.88677\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3441 - acc: 0.8631 - val_loss: 0.3076 - val_acc: 0.8815\n",
      "Epoch 34/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8662\n",
      "Epoch 34: val_acc did not improve from 0.88677\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3508 - acc: 0.8666 - val_loss: 0.3281 - val_acc: 0.8851\n",
      "Epoch 35/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8661\n",
      "Epoch 35: val_acc did not improve from 0.88677\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3421 - acc: 0.8662 - val_loss: 0.3100 - val_acc: 0.8847\n",
      "Epoch 36/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8654\n",
      "Epoch 36: val_acc did not improve from 0.88677\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3587 - acc: 0.8662 - val_loss: 0.3124 - val_acc: 0.8860\n",
      "Epoch 37/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8656\n",
      "Epoch 37: val_acc did not improve from 0.88677\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3412 - acc: 0.8659 - val_loss: 0.3105 - val_acc: 0.8835\n",
      "Epoch 38/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8641\n",
      "Epoch 38: val_acc improved from 0.88677 to 0.88839, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3527 - acc: 0.8643 - val_loss: 0.3025 - val_acc: 0.8884\n",
      "Epoch 39/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3332 - acc: 0.8716\n",
      "Epoch 39: val_acc did not improve from 0.88839\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3332 - acc: 0.8716 - val_loss: 0.3094 - val_acc: 0.8872\n",
      "Epoch 40/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8715\n",
      "Epoch 40: val_acc did not improve from 0.88839\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3528 - acc: 0.8717 - val_loss: 0.3071 - val_acc: 0.8880\n",
      "Epoch 41/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3512 - acc: 0.8725\n",
      "Epoch 41: val_acc did not improve from 0.88839\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3512 - acc: 0.8725 - val_loss: 0.3056 - val_acc: 0.8823\n",
      "Epoch 42/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8748\n",
      "Epoch 42: val_acc did not improve from 0.88839\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3342 - acc: 0.8749 - val_loss: 0.2976 - val_acc: 0.8856\n",
      "Epoch 43/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3290 - acc: 0.8742\n",
      "Epoch 43: val_acc improved from 0.88839 to 0.88961, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3290 - acc: 0.8742 - val_loss: 0.2985 - val_acc: 0.8896\n",
      "Epoch 44/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8734\n",
      "Epoch 44: val_acc did not improve from 0.88961\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3427 - acc: 0.8735 - val_loss: 0.3004 - val_acc: 0.8876\n",
      "Epoch 45/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8735\n",
      "Epoch 45: val_acc improved from 0.88961 to 0.89042, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3297 - acc: 0.8737 - val_loss: 0.3175 - val_acc: 0.8904\n",
      "Epoch 46/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3370 - acc: 0.8743\n",
      "Epoch 46: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3370 - acc: 0.8743 - val_loss: 0.3081 - val_acc: 0.8803\n",
      "Epoch 47/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8755\n",
      "Epoch 47: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3237 - acc: 0.8756 - val_loss: 0.3017 - val_acc: 0.8904\n",
      "Epoch 48/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8724\n",
      "Epoch 48: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3281 - acc: 0.8732 - val_loss: 0.2977 - val_acc: 0.8876\n",
      "Epoch 49/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8757\n",
      "Epoch 49: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3217 - acc: 0.8761 - val_loss: 0.3026 - val_acc: 0.8872\n",
      "Epoch 50/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8788\n",
      "Epoch 50: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3368 - acc: 0.8789 - val_loss: 0.3016 - val_acc: 0.8847\n",
      "Epoch 51/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8785\n",
      "Epoch 51: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3300 - acc: 0.8785 - val_loss: 0.3005 - val_acc: 0.8904\n",
      "Epoch 52/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8772\n",
      "Epoch 52: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3268 - acc: 0.8772 - val_loss: 0.3022 - val_acc: 0.8803\n",
      "Epoch 53/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.8776\n",
      "Epoch 53: val_acc did not improve from 0.89042\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3323 - acc: 0.8776 - val_loss: 0.2959 - val_acc: 0.8896\n",
      "Epoch 54/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8840\n",
      "Epoch 54: val_acc improved from 0.89042 to 0.89367, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3225 - acc: 0.8844 - val_loss: 0.3006 - val_acc: 0.8937\n",
      "Epoch 55/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8779\n",
      "Epoch 55: val_acc did not improve from 0.89367\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3396 - acc: 0.8781 - val_loss: 0.2842 - val_acc: 0.8920\n",
      "Epoch 56/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8797\n",
      "Epoch 56: val_acc improved from 0.89367 to 0.89610, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3278 - acc: 0.8798 - val_loss: 0.2943 - val_acc: 0.8961\n",
      "Epoch 57/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8843\n",
      "Epoch 57: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3163 - acc: 0.8846 - val_loss: 0.2985 - val_acc: 0.8908\n",
      "Epoch 58/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8816\n",
      "Epoch 58: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3208 - acc: 0.8814 - val_loss: 0.2879 - val_acc: 0.8941\n",
      "Epoch 59/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8835\n",
      "Epoch 59: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3249 - acc: 0.8835 - val_loss: 0.2968 - val_acc: 0.8941\n",
      "Epoch 60/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8860\n",
      "Epoch 60: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3257 - acc: 0.8863 - val_loss: 0.3057 - val_acc: 0.8916\n",
      "Epoch 61/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.8835\n",
      "Epoch 61: val_acc improved from 0.89610 to 0.89894, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3169 - acc: 0.8837 - val_loss: 0.3024 - val_acc: 0.8989\n",
      "Epoch 62/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8872\n",
      "Epoch 62: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3244 - acc: 0.8872 - val_loss: 0.2857 - val_acc: 0.8969\n",
      "Epoch 63/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8825\n",
      "Epoch 63: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3308 - acc: 0.8826 - val_loss: 0.2896 - val_acc: 0.8896\n",
      "Epoch 64/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8821\n",
      "Epoch 64: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3386 - acc: 0.8822 - val_loss: 0.2952 - val_acc: 0.8945\n",
      "Epoch 65/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8849\n",
      "Epoch 65: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3239 - acc: 0.8848 - val_loss: 0.2929 - val_acc: 0.8929\n",
      "Epoch 66/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8835\n",
      "Epoch 66: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3222 - acc: 0.8836 - val_loss: 0.2994 - val_acc: 0.8977\n",
      "Epoch 67/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3229 - acc: 0.8850\n",
      "Epoch 67: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3229 - acc: 0.8850 - val_loss: 0.3164 - val_acc: 0.8925\n",
      "Epoch 68/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8852\n",
      "Epoch 68: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3232 - acc: 0.8857 - val_loss: 0.3093 - val_acc: 0.8969\n",
      "Epoch 69/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8876\n",
      "Epoch 69: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3160 - acc: 0.8876 - val_loss: 0.2928 - val_acc: 0.8977\n",
      "Epoch 70/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8876\n",
      "Epoch 70: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3244 - acc: 0.8880 - val_loss: 0.2941 - val_acc: 0.8989\n",
      "Epoch 71/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8899\n",
      "Epoch 71: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3217 - acc: 0.8902 - val_loss: 0.2993 - val_acc: 0.8933\n",
      "Epoch 72/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.8883\n",
      "Epoch 72: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3190 - acc: 0.8884 - val_loss: 0.2988 - val_acc: 0.8941\n",
      "Epoch 73/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.8906\n",
      "Epoch 73: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3070 - acc: 0.8912 - val_loss: 0.2974 - val_acc: 0.8973\n",
      "Epoch 74/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.8891\n",
      "Epoch 74: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3032 - acc: 0.8890 - val_loss: 0.2945 - val_acc: 0.8941\n",
      "Epoch 75/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.8885\n",
      "Epoch 75: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3118 - acc: 0.8889 - val_loss: 0.3120 - val_acc: 0.8981\n",
      "Epoch 76/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8897\n",
      "Epoch 76: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3242 - acc: 0.8896 - val_loss: 0.2989 - val_acc: 0.8989\n",
      "Epoch 77/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8920\n",
      "Epoch 77: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3166 - acc: 0.8918 - val_loss: 0.2942 - val_acc: 0.8981\n",
      "Epoch 78/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3117 - acc: 0.8926\n",
      "Epoch 78: val_acc did not improve from 0.89894\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3117 - acc: 0.8926 - val_loss: 0.2958 - val_acc: 0.8981\n",
      "Epoch 79/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.8901\n",
      "Epoch 79: val_acc improved from 0.89894 to 0.89935, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3121 - acc: 0.8907 - val_loss: 0.2828 - val_acc: 0.8994\n",
      "Epoch 80/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.8896\n",
      "Epoch 80: val_acc did not improve from 0.89935\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3156 - acc: 0.8896 - val_loss: 0.2853 - val_acc: 0.8953\n",
      "Epoch 81/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.8924\n",
      "Epoch 81: val_acc improved from 0.89935 to 0.90138, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3034 - acc: 0.8928 - val_loss: 0.3076 - val_acc: 0.9014\n",
      "Epoch 82/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.8958\n",
      "Epoch 82: val_acc did not improve from 0.90138\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3059 - acc: 0.8958 - val_loss: 0.3000 - val_acc: 0.8998\n",
      "Epoch 83/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.8954\n",
      "Epoch 83: val_acc did not improve from 0.90138\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3063 - acc: 0.8955 - val_loss: 0.3020 - val_acc: 0.8929\n",
      "Epoch 84/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8944\n",
      "Epoch 84: val_acc improved from 0.90138 to 0.90260, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3243 - acc: 0.8945 - val_loss: 0.2748 - val_acc: 0.9026\n",
      "Epoch 85/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.8937\n",
      "Epoch 85: val_acc did not improve from 0.90260\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3080 - acc: 0.8938 - val_loss: 0.2742 - val_acc: 0.9010\n",
      "Epoch 86/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.8927\n",
      "Epoch 86: val_acc improved from 0.90260 to 0.90544, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3082 - acc: 0.8926 - val_loss: 0.2786 - val_acc: 0.9054\n",
      "Epoch 87/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.8947\n",
      "Epoch 87: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2983 - acc: 0.8949 - val_loss: 0.2940 - val_acc: 0.9026\n",
      "Epoch 88/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.8950\n",
      "Epoch 88: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3095 - acc: 0.8953 - val_loss: 0.2956 - val_acc: 0.8981\n",
      "Epoch 89/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.8954\n",
      "Epoch 89: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3026 - acc: 0.8958 - val_loss: 0.2967 - val_acc: 0.8989\n",
      "Epoch 90/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.8956\n",
      "Epoch 90: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3063 - acc: 0.8957 - val_loss: 0.2946 - val_acc: 0.8985\n",
      "Epoch 91/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.8983\n",
      "Epoch 91: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2895 - acc: 0.8983 - val_loss: 0.2975 - val_acc: 0.9054\n",
      "Epoch 92/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.8969\n",
      "Epoch 92: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2991 - acc: 0.8972 - val_loss: 0.2757 - val_acc: 0.9046\n",
      "Epoch 93/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9001\n",
      "Epoch 93: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3005 - acc: 0.9004 - val_loss: 0.2965 - val_acc: 0.8994\n",
      "Epoch 94/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2975 - acc: 0.8971\n",
      "Epoch 94: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2975 - acc: 0.8971 - val_loss: 0.2762 - val_acc: 0.9030\n",
      "Epoch 95/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2989 - acc: 0.8984\n",
      "Epoch 95: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2989 - acc: 0.8984 - val_loss: 0.3047 - val_acc: 0.8977\n",
      "Epoch 96/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8956\n",
      "Epoch 96: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3149 - acc: 0.8962 - val_loss: 0.2847 - val_acc: 0.9034\n",
      "Epoch 97/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.8936\n",
      "Epoch 97: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3109 - acc: 0.8941 - val_loss: 0.2844 - val_acc: 0.9026\n",
      "Epoch 98/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.8996\n",
      "Epoch 98: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2995 - acc: 0.8996 - val_loss: 0.2909 - val_acc: 0.9038\n",
      "Epoch 99/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.8951\n",
      "Epoch 99: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3094 - acc: 0.8953 - val_loss: 0.2735 - val_acc: 0.9034\n",
      "Epoch 100/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8996\n",
      "Epoch 100: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3230 - acc: 0.8995 - val_loss: 0.2876 - val_acc: 0.8969\n",
      "Epoch 101/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.8980\n",
      "Epoch 101: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2948 - acc: 0.8981 - val_loss: 0.2874 - val_acc: 0.9018\n",
      "Epoch 102/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9014\n",
      "Epoch 102: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2906 - acc: 0.9015 - val_loss: 0.2892 - val_acc: 0.9014\n",
      "Epoch 103/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2955 - acc: 0.8969\n",
      "Epoch 103: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2945 - acc: 0.8972 - val_loss: 0.2856 - val_acc: 0.9034\n",
      "Epoch 104/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9015\n",
      "Epoch 104: val_acc did not improve from 0.90544\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3131 - acc: 0.9019 - val_loss: 0.3065 - val_acc: 0.9046\n",
      "Epoch 105/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9012\n",
      "Epoch 105: val_acc improved from 0.90544 to 0.90666, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3016 - acc: 0.9011 - val_loss: 0.2804 - val_acc: 0.9067\n",
      "Epoch 106/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.8990\n",
      "Epoch 106: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3057 - acc: 0.8992 - val_loss: 0.2880 - val_acc: 0.9050\n",
      "Epoch 107/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.8973\n",
      "Epoch 107: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3049 - acc: 0.8974 - val_loss: 0.2932 - val_acc: 0.9030\n",
      "Epoch 108/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.8985\n",
      "Epoch 108: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3005 - acc: 0.8992 - val_loss: 0.2867 - val_acc: 0.9058\n",
      "Epoch 109/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3091 - acc: 0.8986\n",
      "Epoch 109: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3091 - acc: 0.8986 - val_loss: 0.2978 - val_acc: 0.9014\n",
      "Epoch 110/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9001\n",
      "Epoch 110: val_acc improved from 0.90666 to 0.90747, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3043 - acc: 0.9002 - val_loss: 0.2900 - val_acc: 0.9075\n",
      "Epoch 111/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9007\n",
      "Epoch 111: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3125 - acc: 0.9012 - val_loss: 0.2918 - val_acc: 0.9038\n",
      "Epoch 112/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9008\n",
      "Epoch 112: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3003 - acc: 0.9010 - val_loss: 0.2932 - val_acc: 0.9050\n",
      "Epoch 113/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9035\n",
      "Epoch 113: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2836 - acc: 0.9036 - val_loss: 0.3018 - val_acc: 0.9034\n",
      "Epoch 114/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.8990\n",
      "Epoch 114: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2888 - acc: 0.8992 - val_loss: 0.2867 - val_acc: 0.9062\n",
      "Epoch 115/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9045\n",
      "Epoch 115: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2870 - acc: 0.9048 - val_loss: 0.3184 - val_acc: 0.9058\n",
      "Epoch 116/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9019\n",
      "Epoch 116: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3052 - acc: 0.9021 - val_loss: 0.3131 - val_acc: 0.9046\n",
      "Epoch 117/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9037\n",
      "Epoch 117: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2961 - acc: 0.9040 - val_loss: 0.2765 - val_acc: 0.9054\n",
      "Epoch 118/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9040\n",
      "Epoch 118: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2905 - acc: 0.9040 - val_loss: 0.2959 - val_acc: 0.9058\n",
      "Epoch 119/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9023\n",
      "Epoch 119: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3016 - acc: 0.9023 - val_loss: 0.2919 - val_acc: 0.9054\n",
      "Epoch 120/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9024\n",
      "Epoch 120: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2787 - acc: 0.9027 - val_loss: 0.3099 - val_acc: 0.9050\n",
      "Epoch 121/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3058 - acc: 0.9005\n",
      "Epoch 121: val_acc improved from 0.90747 to 0.90869, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3058 - acc: 0.9005 - val_loss: 0.3024 - val_acc: 0.9087\n",
      "Epoch 122/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9021\n",
      "Epoch 122: val_acc did not improve from 0.90869\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3047 - acc: 0.9024 - val_loss: 0.2886 - val_acc: 0.9083\n",
      "Epoch 123/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9044\n",
      "Epoch 123: val_acc improved from 0.90869 to 0.90909, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2940 - acc: 0.9046 - val_loss: 0.2765 - val_acc: 0.9091\n",
      "Epoch 124/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9049\n",
      "Epoch 124: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2793 - acc: 0.9048 - val_loss: 0.3155 - val_acc: 0.9026\n",
      "Epoch 125/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9037\n",
      "Epoch 125: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3019 - acc: 0.9039 - val_loss: 0.2983 - val_acc: 0.9038\n",
      "Epoch 126/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9069\n",
      "Epoch 126: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2878 - acc: 0.9070 - val_loss: 0.3020 - val_acc: 0.9050\n",
      "Epoch 127/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.9000\n",
      "Epoch 127: val_acc did not improve from 0.90909\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3080 - acc: 0.8998 - val_loss: 0.2858 - val_acc: 0.9067\n",
      "Epoch 128/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9051\n",
      "Epoch 128: val_acc improved from 0.90909 to 0.90990, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3013 - acc: 0.9058 - val_loss: 0.2839 - val_acc: 0.9099\n",
      "Epoch 129/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2850 - acc: 0.9037\n",
      "Epoch 129: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2850 - acc: 0.9037 - val_loss: 0.3012 - val_acc: 0.9038\n",
      "Epoch 130/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9034\n",
      "Epoch 130: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3007 - acc: 0.9036 - val_loss: 0.2840 - val_acc: 0.9075\n",
      "Epoch 131/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.9015\n",
      "Epoch 131: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3111 - acc: 0.9015 - val_loss: 0.2831 - val_acc: 0.9079\n",
      "Epoch 132/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9052\n",
      "Epoch 132: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2922 - acc: 0.9053 - val_loss: 0.2863 - val_acc: 0.9083\n",
      "Epoch 133/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2899 - acc: 0.9062\n",
      "Epoch 133: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2899 - acc: 0.9062 - val_loss: 0.2944 - val_acc: 0.9054\n",
      "Epoch 134/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9059\n",
      "Epoch 134: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2892 - acc: 0.9064 - val_loss: 0.2725 - val_acc: 0.9071\n",
      "Epoch 135/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9066\n",
      "Epoch 135: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2968 - acc: 0.9065 - val_loss: 0.2810 - val_acc: 0.9067\n",
      "Epoch 136/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9068\n",
      "Epoch 136: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2926 - acc: 0.9067 - val_loss: 0.2820 - val_acc: 0.9091\n",
      "Epoch 137/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.9043\n",
      "Epoch 137: val_acc improved from 0.90990 to 0.91315, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2936 - acc: 0.9044 - val_loss: 0.2764 - val_acc: 0.9131\n",
      "Epoch 138/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9065\n",
      "Epoch 138: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3023 - acc: 0.9063 - val_loss: 0.2697 - val_acc: 0.9095\n",
      "Epoch 139/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2832 - acc: 0.9026\n",
      "Epoch 139: val_acc did not improve from 0.91315\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2832 - acc: 0.9026 - val_loss: 0.3182 - val_acc: 0.9071\n",
      "Epoch 140/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9056\n",
      "Epoch 140: val_acc improved from 0.91315 to 0.91396, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2793 - acc: 0.9060 - val_loss: 0.3019 - val_acc: 0.9140\n",
      "Epoch 141/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9046\n",
      "Epoch 141: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2944 - acc: 0.9048 - val_loss: 0.3037 - val_acc: 0.9115\n",
      "Epoch 142/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9121\n",
      "Epoch 142: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2877 - acc: 0.9123 - val_loss: 0.2974 - val_acc: 0.9087\n",
      "Epoch 143/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9080\n",
      "Epoch 143: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2981 - acc: 0.9083 - val_loss: 0.2983 - val_acc: 0.9058\n",
      "Epoch 144/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9059\n",
      "Epoch 144: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2922 - acc: 0.9061 - val_loss: 0.2900 - val_acc: 0.9058\n",
      "Epoch 145/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2970 - acc: 0.9070\n",
      "Epoch 145: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2970 - acc: 0.9070 - val_loss: 0.2934 - val_acc: 0.9091\n",
      "Epoch 146/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9053\n",
      "Epoch 146: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2952 - acc: 0.9055 - val_loss: 0.2862 - val_acc: 0.9136\n",
      "Epoch 147/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9112\n",
      "Epoch 147: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2751 - acc: 0.9112 - val_loss: 0.2812 - val_acc: 0.9127\n",
      "Epoch 148/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9083\n",
      "Epoch 148: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2898 - acc: 0.9087 - val_loss: 0.2816 - val_acc: 0.9079\n",
      "Epoch 149/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9101\n",
      "Epoch 149: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2977 - acc: 0.9101 - val_loss: 0.2988 - val_acc: 0.9067\n",
      "Epoch 150/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3073 - acc: 0.9046\n",
      "Epoch 150: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3073 - acc: 0.9046 - val_loss: 0.3012 - val_acc: 0.9062\n",
      "Epoch 151/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9074\n",
      "Epoch 151: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2832 - acc: 0.9077 - val_loss: 0.3009 - val_acc: 0.9131\n",
      "Epoch 152/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9120\n",
      "Epoch 152: val_acc did not improve from 0.91396\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2852 - acc: 0.9120 - val_loss: 0.2810 - val_acc: 0.9115\n",
      "Epoch 153/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9111\n",
      "Epoch 153: val_acc improved from 0.91396 to 0.91477, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2836 - acc: 0.9113 - val_loss: 0.2931 - val_acc: 0.9148\n",
      "Epoch 154/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9094\n",
      "Epoch 154: val_acc did not improve from 0.91477\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2831 - acc: 0.9096 - val_loss: 0.2953 - val_acc: 0.9050\n",
      "Epoch 155/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2863 - acc: 0.9111\n",
      "Epoch 155: val_acc did not improve from 0.91477\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2863 - acc: 0.9111 - val_loss: 0.2936 - val_acc: 0.9022\n",
      "Epoch 156/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9089\n",
      "Epoch 156: val_acc did not improve from 0.91477\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2831 - acc: 0.9094 - val_loss: 0.2829 - val_acc: 0.9123\n",
      "Epoch 157/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9093\n",
      "Epoch 157: val_acc did not improve from 0.91477\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2827 - acc: 0.9094 - val_loss: 0.3171 - val_acc: 0.9071\n",
      "Epoch 158/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9082\n",
      "Epoch 158: val_acc improved from 0.91477 to 0.91518, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2987 - acc: 0.9086 - val_loss: 0.2743 - val_acc: 0.9152\n",
      "Epoch 159/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9072\n",
      "Epoch 159: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2970 - acc: 0.9076 - val_loss: 0.2809 - val_acc: 0.9107\n",
      "Epoch 160/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9105\n",
      "Epoch 160: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2832 - acc: 0.9106 - val_loss: 0.3263 - val_acc: 0.9131\n",
      "Epoch 161/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9093\n",
      "Epoch 161: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2839 - acc: 0.9093 - val_loss: 0.2921 - val_acc: 0.9152\n",
      "Epoch 162/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9123\n",
      "Epoch 162: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2767 - acc: 0.9120 - val_loss: 0.3255 - val_acc: 0.9136\n",
      "Epoch 163/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9067\n",
      "Epoch 163: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3004 - acc: 0.9068 - val_loss: 0.3062 - val_acc: 0.9050\n",
      "Epoch 164/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9150\n",
      "Epoch 164: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2750 - acc: 0.9149 - val_loss: 0.2993 - val_acc: 0.9095\n",
      "Epoch 165/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2847 - acc: 0.9103\n",
      "Epoch 165: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2847 - acc: 0.9103 - val_loss: 0.2721 - val_acc: 0.9099\n",
      "Epoch 166/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2752 - acc: 0.9096\n",
      "Epoch 166: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2752 - acc: 0.9096 - val_loss: 0.2965 - val_acc: 0.9071\n",
      "Epoch 167/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9120\n",
      "Epoch 167: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3063 - acc: 0.9127 - val_loss: 0.3167 - val_acc: 0.9140\n",
      "Epoch 168/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9125\n",
      "Epoch 168: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2944 - acc: 0.9127 - val_loss: 0.2836 - val_acc: 0.9054\n",
      "Epoch 169/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9116\n",
      "Epoch 169: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2832 - acc: 0.9117 - val_loss: 0.2969 - val_acc: 0.9103\n",
      "Epoch 170/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9137\n",
      "Epoch 170: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2824 - acc: 0.9139 - val_loss: 0.3229 - val_acc: 0.9099\n",
      "Epoch 171/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9075\n",
      "Epoch 171: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3043 - acc: 0.9076 - val_loss: 0.2769 - val_acc: 0.9087\n",
      "Epoch 172/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9092\n",
      "Epoch 172: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2990 - acc: 0.9091 - val_loss: 0.2661 - val_acc: 0.9136\n",
      "Epoch 173/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2950 - acc: 0.9115\n",
      "Epoch 173: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2950 - acc: 0.9115 - val_loss: 0.2958 - val_acc: 0.9107\n",
      "Epoch 174/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9094\n",
      "Epoch 174: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3024 - acc: 0.9097 - val_loss: 0.2990 - val_acc: 0.9136\n",
      "Epoch 175/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9069\n",
      "Epoch 175: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3115 - acc: 0.9070 - val_loss: 0.3021 - val_acc: 0.9140\n",
      "Epoch 176/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9118\n",
      "Epoch 176: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2875 - acc: 0.9117 - val_loss: 0.3076 - val_acc: 0.9131\n",
      "Epoch 177/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9123\n",
      "Epoch 177: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2764 - acc: 0.9123 - val_loss: 0.3215 - val_acc: 0.9083\n",
      "Epoch 178/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9122\n",
      "Epoch 178: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2835 - acc: 0.9125 - val_loss: 0.3611 - val_acc: 0.9062\n",
      "Epoch 179/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9131\n",
      "Epoch 179: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2757 - acc: 0.9136 - val_loss: 0.2790 - val_acc: 0.9103\n",
      "Epoch 180/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9114\n",
      "Epoch 180: val_acc did not improve from 0.91518\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2842 - acc: 0.9113 - val_loss: 0.3033 - val_acc: 0.9119\n",
      "Epoch 181/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9104\n",
      "Epoch 181: val_acc improved from 0.91518 to 0.91599, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3004 - acc: 0.9106 - val_loss: 0.2909 - val_acc: 0.9160\n",
      "Epoch 182/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.9076\n",
      "Epoch 182: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2972 - acc: 0.9075 - val_loss: 0.2953 - val_acc: 0.9079\n",
      "Epoch 183/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9105\n",
      "Epoch 183: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2984 - acc: 0.9106 - val_loss: 0.3184 - val_acc: 0.9131\n",
      "Epoch 184/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9121\n",
      "Epoch 184: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2850 - acc: 0.9124 - val_loss: 0.2859 - val_acc: 0.9152\n",
      "Epoch 185/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9135\n",
      "Epoch 185: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2758 - acc: 0.9135 - val_loss: 0.2944 - val_acc: 0.9136\n",
      "Epoch 186/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9094\n",
      "Epoch 186: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3133 - acc: 0.9094 - val_loss: 0.2900 - val_acc: 0.9103\n",
      "Epoch 187/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9090\n",
      "Epoch 187: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2991 - acc: 0.9090 - val_loss: 0.2967 - val_acc: 0.9103\n",
      "Epoch 188/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9104\n",
      "Epoch 188: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2792 - acc: 0.9104 - val_loss: 0.2733 - val_acc: 0.9075\n",
      "Epoch 189/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9104\n",
      "Epoch 189: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2971 - acc: 0.9106 - val_loss: 0.2664 - val_acc: 0.9091\n",
      "Epoch 190/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9135\n",
      "Epoch 190: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2824 - acc: 0.9136 - val_loss: 0.2732 - val_acc: 0.9160\n",
      "Epoch 191/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2791 - acc: 0.9165\n",
      "Epoch 191: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2791 - acc: 0.9165 - val_loss: 0.2843 - val_acc: 0.9152\n",
      "Epoch 192/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2901 - acc: 0.9137\n",
      "Epoch 192: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2901 - acc: 0.9137 - val_loss: 0.2966 - val_acc: 0.9107\n",
      "Epoch 193/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9135\n",
      "Epoch 193: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2838 - acc: 0.9137 - val_loss: 0.2908 - val_acc: 0.9119\n",
      "Epoch 194/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9130\n",
      "Epoch 194: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2936 - acc: 0.9132 - val_loss: 0.3055 - val_acc: 0.9152\n",
      "Epoch 195/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9154\n",
      "Epoch 195: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2865 - acc: 0.9155 - val_loss: 0.3184 - val_acc: 0.9042\n",
      "Epoch 196/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9170\n",
      "Epoch 196: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2773 - acc: 0.9171 - val_loss: 0.3131 - val_acc: 0.9123\n",
      "Epoch 197/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9120\n",
      "Epoch 197: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3048 - acc: 0.9120 - val_loss: 0.2881 - val_acc: 0.9087\n",
      "Epoch 198/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9147\n",
      "Epoch 198: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2864 - acc: 0.9148 - val_loss: 0.2703 - val_acc: 0.9156\n",
      "Epoch 199/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9125\n",
      "Epoch 199: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3002 - acc: 0.9126 - val_loss: 0.3116 - val_acc: 0.9095\n",
      "Epoch 200/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9096\n",
      "Epoch 200: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2813 - acc: 0.9100 - val_loss: 0.3199 - val_acc: 0.9123\n",
      "Epoch 201/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9140\n",
      "Epoch 201: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2956 - acc: 0.9143 - val_loss: 0.2764 - val_acc: 0.9156\n",
      "Epoch 202/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.9150\n",
      "Epoch 202: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2800 - acc: 0.9150 - val_loss: 0.3173 - val_acc: 0.9103\n",
      "Epoch 203/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9185\n",
      "Epoch 203: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2879 - acc: 0.9185 - val_loss: 0.3167 - val_acc: 0.9152\n",
      "Epoch 204/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9143\n",
      "Epoch 204: val_acc did not improve from 0.91599\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2788 - acc: 0.9143 - val_loss: 0.3099 - val_acc: 0.9160\n",
      "Epoch 205/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9181\n",
      "Epoch 205: val_acc improved from 0.91599 to 0.91640, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2765 - acc: 0.9184 - val_loss: 0.3282 - val_acc: 0.9164\n",
      "Epoch 206/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9158\n",
      "Epoch 206: val_acc did not improve from 0.91640\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2685 - acc: 0.9163 - val_loss: 0.2862 - val_acc: 0.9140\n",
      "Epoch 207/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9127\n",
      "Epoch 207: val_acc did not improve from 0.91640\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2831 - acc: 0.9128 - val_loss: 0.2997 - val_acc: 0.9140\n",
      "Epoch 208/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9133\n",
      "Epoch 208: val_acc did not improve from 0.91640\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2889 - acc: 0.9133 - val_loss: 0.3153 - val_acc: 0.9152\n",
      "Epoch 209/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9160\n",
      "Epoch 209: val_acc did not improve from 0.91640\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2877 - acc: 0.9160 - val_loss: 0.3038 - val_acc: 0.9164\n",
      "Epoch 210/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9166\n",
      "Epoch 210: val_acc did not improve from 0.91640\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2963 - acc: 0.9164 - val_loss: 0.3069 - val_acc: 0.9103\n",
      "Epoch 211/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9125\n",
      "Epoch 211: val_acc improved from 0.91640 to 0.91721, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3067 - acc: 0.9125 - val_loss: 0.2862 - val_acc: 0.9172\n",
      "Epoch 212/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9157\n",
      "Epoch 212: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2900 - acc: 0.9158 - val_loss: 0.3129 - val_acc: 0.9136\n",
      "Epoch 213/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9211\n",
      "Epoch 213: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2722 - acc: 0.9213 - val_loss: 0.3181 - val_acc: 0.9107\n",
      "Epoch 214/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3003 - acc: 0.9186\n",
      "Epoch 214: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3003 - acc: 0.9186 - val_loss: 0.2817 - val_acc: 0.9119\n",
      "Epoch 215/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.9143\n",
      "Epoch 215: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3148 - acc: 0.9144 - val_loss: 0.2845 - val_acc: 0.9103\n",
      "Epoch 216/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9157\n",
      "Epoch 216: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2816 - acc: 0.9158 - val_loss: 0.3119 - val_acc: 0.9172\n",
      "Epoch 217/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9125\n",
      "Epoch 217: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2940 - acc: 0.9124 - val_loss: 0.3051 - val_acc: 0.9164\n",
      "Epoch 218/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2890 - acc: 0.9169\n",
      "Epoch 218: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2890 - acc: 0.9169 - val_loss: 0.2812 - val_acc: 0.9172\n",
      "Epoch 219/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9130\n",
      "Epoch 219: val_acc did not improve from 0.91721\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2874 - acc: 0.9128 - val_loss: 0.2974 - val_acc: 0.9095\n",
      "Epoch 220/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9138\n",
      "Epoch 220: val_acc improved from 0.91721 to 0.91802, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2692 - acc: 0.9139 - val_loss: 0.2821 - val_acc: 0.9180\n",
      "Epoch 221/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9148\n",
      "Epoch 221: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2817 - acc: 0.9149 - val_loss: 0.3044 - val_acc: 0.9127\n",
      "Epoch 222/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9191\n",
      "Epoch 222: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2911 - acc: 0.9188 - val_loss: 0.2782 - val_acc: 0.9168\n",
      "Epoch 223/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9192\n",
      "Epoch 223: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2813 - acc: 0.9194 - val_loss: 0.2959 - val_acc: 0.9168\n",
      "Epoch 224/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9176\n",
      "Epoch 224: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2955 - acc: 0.9176 - val_loss: 0.3096 - val_acc: 0.9099\n",
      "Epoch 225/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9182\n",
      "Epoch 225: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2818 - acc: 0.9184 - val_loss: 0.2836 - val_acc: 0.9127\n",
      "Epoch 226/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.9103\n",
      "Epoch 226: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3177 - acc: 0.9105 - val_loss: 0.2799 - val_acc: 0.9087\n",
      "Epoch 227/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9174\n",
      "Epoch 227: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2713 - acc: 0.9177 - val_loss: 0.2723 - val_acc: 0.9148\n",
      "Epoch 228/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9204\n",
      "Epoch 228: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2825 - acc: 0.9206 - val_loss: 0.2995 - val_acc: 0.9123\n",
      "Epoch 229/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9194\n",
      "Epoch 229: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2869 - acc: 0.9195 - val_loss: 0.3096 - val_acc: 0.9144\n",
      "Epoch 230/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9173\n",
      "Epoch 230: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2641 - acc: 0.9175 - val_loss: 0.3523 - val_acc: 0.9067\n",
      "Epoch 231/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9167\n",
      "Epoch 231: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2814 - acc: 0.9169 - val_loss: 0.3018 - val_acc: 0.9148\n",
      "Epoch 232/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9169\n",
      "Epoch 232: val_acc did not improve from 0.91802\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3010 - acc: 0.9170 - val_loss: 0.3201 - val_acc: 0.9079\n",
      "Epoch 233/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9184\n",
      "Epoch 233: val_acc improved from 0.91802 to 0.91924, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2828 - acc: 0.9186 - val_loss: 0.3035 - val_acc: 0.9192\n",
      "Epoch 234/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3089 - acc: 0.9160\n",
      "Epoch 234: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3089 - acc: 0.9160 - val_loss: 0.3091 - val_acc: 0.9180\n",
      "Epoch 235/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3199 - acc: 0.9179\n",
      "Epoch 235: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3199 - acc: 0.9179 - val_loss: 0.3133 - val_acc: 0.9103\n",
      "Epoch 236/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.9186\n",
      "Epoch 236: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3179 - acc: 0.9187 - val_loss: 0.3147 - val_acc: 0.9079\n",
      "Epoch 237/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9167\n",
      "Epoch 237: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2870 - acc: 0.9165 - val_loss: 0.2927 - val_acc: 0.9160\n",
      "Epoch 238/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9174\n",
      "Epoch 238: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2867 - acc: 0.9177 - val_loss: 0.3169 - val_acc: 0.9115\n",
      "Epoch 239/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9162\n",
      "Epoch 239: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3036 - acc: 0.9164 - val_loss: 0.2888 - val_acc: 0.9131\n",
      "Epoch 240/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9175\n",
      "Epoch 240: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2984 - acc: 0.9176 - val_loss: 0.2936 - val_acc: 0.9168\n",
      "Epoch 241/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9183\n",
      "Epoch 241: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3008 - acc: 0.9184 - val_loss: 0.3093 - val_acc: 0.9111\n",
      "Epoch 242/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9168\n",
      "Epoch 242: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2836 - acc: 0.9168 - val_loss: 0.3098 - val_acc: 0.9127\n",
      "Epoch 243/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9182\n",
      "Epoch 243: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.2933 - acc: 0.9182 - val_loss: 0.3319 - val_acc: 0.9136\n",
      "Epoch 244/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9157\n",
      "Epoch 244: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2913 - acc: 0.9159 - val_loss: 0.3239 - val_acc: 0.9107\n",
      "Epoch 245/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9198\n",
      "Epoch 245: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3046 - acc: 0.9201 - val_loss: 0.3037 - val_acc: 0.9156\n",
      "Epoch 246/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9207\n",
      "Epoch 246: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2765 - acc: 0.9208 - val_loss: 0.3384 - val_acc: 0.9030\n",
      "Epoch 247/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9167\n",
      "Epoch 247: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2845 - acc: 0.9168 - val_loss: 0.3030 - val_acc: 0.9107\n",
      "Epoch 248/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3210 - acc: 0.9187\n",
      "Epoch 248: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3210 - acc: 0.9187 - val_loss: 0.2945 - val_acc: 0.9067\n",
      "Epoch 249/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2857 - acc: 0.9176\n",
      "Epoch 249: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2857 - acc: 0.9176 - val_loss: 0.3175 - val_acc: 0.9083\n",
      "Epoch 250/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9185\n",
      "Epoch 250: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2883 - acc: 0.9184 - val_loss: 0.2883 - val_acc: 0.9144\n",
      "Epoch 251/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9178\n",
      "Epoch 251: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3008 - acc: 0.9179 - val_loss: 0.2791 - val_acc: 0.9127\n",
      "Epoch 252/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9199\n",
      "Epoch 252: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2815 - acc: 0.9201 - val_loss: 0.3032 - val_acc: 0.9156\n",
      "Epoch 253/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9185\n",
      "Epoch 253: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2952 - acc: 0.9183 - val_loss: 0.2684 - val_acc: 0.9099\n",
      "Epoch 254/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9215\n",
      "Epoch 254: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2836 - acc: 0.9217 - val_loss: 0.2860 - val_acc: 0.9111\n",
      "Epoch 255/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2897 - acc: 0.9200\n",
      "Epoch 255: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2897 - acc: 0.9200 - val_loss: 0.3043 - val_acc: 0.9111\n",
      "Epoch 256/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9171\n",
      "Epoch 256: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3053 - acc: 0.9171 - val_loss: 0.3026 - val_acc: 0.9083\n",
      "Epoch 257/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9185\n",
      "Epoch 257: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3002 - acc: 0.9188 - val_loss: 0.3213 - val_acc: 0.9136\n",
      "Epoch 258/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9178\n",
      "Epoch 258: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3095 - acc: 0.9180 - val_loss: 0.3457 - val_acc: 0.9136\n",
      "Epoch 259/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9191\n",
      "Epoch 259: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2632 - acc: 0.9194 - val_loss: 0.3354 - val_acc: 0.9140\n",
      "Epoch 260/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.9208\n",
      "Epoch 260: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3206 - acc: 0.9215 - val_loss: 0.3074 - val_acc: 0.9160\n",
      "Epoch 261/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9191\n",
      "Epoch 261: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2904 - acc: 0.9190 - val_loss: 0.3426 - val_acc: 0.9144\n",
      "Epoch 262/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9188\n",
      "Epoch 262: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3051 - acc: 0.9192 - val_loss: 0.3072 - val_acc: 0.9038\n",
      "Epoch 263/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2923 - acc: 0.9202\n",
      "Epoch 263: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2923 - acc: 0.9202 - val_loss: 0.3050 - val_acc: 0.9131\n",
      "Epoch 264/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9207\n",
      "Epoch 264: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3161 - acc: 0.9208 - val_loss: 0.3134 - val_acc: 0.9091\n",
      "Epoch 265/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9175\n",
      "Epoch 265: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2595 - acc: 0.9178 - val_loss: 0.3583 - val_acc: 0.9123\n",
      "Epoch 266/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9169\n",
      "Epoch 266: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3019 - acc: 0.9175 - val_loss: 0.3118 - val_acc: 0.9140\n",
      "Epoch 267/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9201\n",
      "Epoch 267: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2775 - acc: 0.9201 - val_loss: 0.3773 - val_acc: 0.9050\n",
      "Epoch 268/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9211\n",
      "Epoch 268: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3061 - acc: 0.9212 - val_loss: 0.2916 - val_acc: 0.9140\n",
      "Epoch 269/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9201\n",
      "Epoch 269: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2892 - acc: 0.9201 - val_loss: 0.3348 - val_acc: 0.9136\n",
      "Epoch 270/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.9206\n",
      "Epoch 270: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3481 - acc: 0.9204 - val_loss: 0.3193 - val_acc: 0.9091\n",
      "Epoch 271/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9203\n",
      "Epoch 271: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2922 - acc: 0.9206 - val_loss: 0.2903 - val_acc: 0.9164\n",
      "Epoch 272/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9196\n",
      "Epoch 272: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2855 - acc: 0.9195 - val_loss: 0.3193 - val_acc: 0.9127\n",
      "Epoch 273/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9209\n",
      "Epoch 273: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2648 - acc: 0.9210 - val_loss: 0.3067 - val_acc: 0.9168\n",
      "Epoch 274/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3059 - acc: 0.9194\n",
      "Epoch 274: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3059 - acc: 0.9194 - val_loss: 0.3145 - val_acc: 0.9127\n",
      "Epoch 275/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9202\n",
      "Epoch 275: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2886 - acc: 0.9205 - val_loss: 0.3086 - val_acc: 0.9168\n",
      "Epoch 276/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.9216\n",
      "Epoch 276: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3279 - acc: 0.9218 - val_loss: 0.3064 - val_acc: 0.9144\n",
      "Epoch 277/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9205\n",
      "Epoch 277: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3138 - acc: 0.9205 - val_loss: 0.3239 - val_acc: 0.9107\n",
      "Epoch 278/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9202\n",
      "Epoch 278: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2908 - acc: 0.9204 - val_loss: 0.3143 - val_acc: 0.9188\n",
      "Epoch 279/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3206 - acc: 0.9182\n",
      "Epoch 279: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3206 - acc: 0.9182 - val_loss: 0.2772 - val_acc: 0.9095\n",
      "Epoch 280/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9214\n",
      "Epoch 280: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2958 - acc: 0.9214 - val_loss: 0.3115 - val_acc: 0.9111\n",
      "Epoch 281/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9226\n",
      "Epoch 281: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2891 - acc: 0.9228 - val_loss: 0.2982 - val_acc: 0.9188\n",
      "Epoch 282/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9224\n",
      "Epoch 282: val_acc did not improve from 0.91924\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3101 - acc: 0.9225 - val_loss: 0.3042 - val_acc: 0.9192\n",
      "Epoch 283/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.9173\n",
      "Epoch 283: val_acc improved from 0.91924 to 0.92248, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3247 - acc: 0.9173 - val_loss: 0.2927 - val_acc: 0.9225\n",
      "Epoch 284/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9215\n",
      "Epoch 284: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2856 - acc: 0.9219 - val_loss: 0.3344 - val_acc: 0.9127\n",
      "Epoch 285/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3055 - acc: 0.9209\n",
      "Epoch 285: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3055 - acc: 0.9209 - val_loss: 0.3259 - val_acc: 0.9140\n",
      "Epoch 286/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3021 - acc: 0.9241\n",
      "Epoch 286: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3021 - acc: 0.9241 - val_loss: 0.3173 - val_acc: 0.9168\n",
      "Epoch 287/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9198\n",
      "Epoch 287: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2982 - acc: 0.9198 - val_loss: 0.3477 - val_acc: 0.9103\n",
      "Epoch 288/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3143 - acc: 0.9250\n",
      "Epoch 288: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3143 - acc: 0.9250 - val_loss: 0.3412 - val_acc: 0.9103\n",
      "Epoch 289/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9185\n",
      "Epoch 289: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3021 - acc: 0.9185 - val_loss: 0.2987 - val_acc: 0.9164\n",
      "Epoch 290/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9231\n",
      "Epoch 290: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2798 - acc: 0.9233 - val_loss: 0.2739 - val_acc: 0.9160\n",
      "Epoch 291/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9216\n",
      "Epoch 291: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3053 - acc: 0.9217 - val_loss: 0.3149 - val_acc: 0.9184\n",
      "Epoch 292/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.9218\n",
      "Epoch 292: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3447 - acc: 0.9221 - val_loss: 0.3035 - val_acc: 0.9131\n",
      "Epoch 293/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.9239\n",
      "Epoch 293: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3339 - acc: 0.9238 - val_loss: 0.2933 - val_acc: 0.9131\n",
      "Epoch 294/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.9177\n",
      "Epoch 294: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3289 - acc: 0.9180 - val_loss: 0.3290 - val_acc: 0.9111\n",
      "Epoch 295/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9220\n",
      "Epoch 295: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2970 - acc: 0.9220 - val_loss: 0.3020 - val_acc: 0.9062\n",
      "Epoch 296/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9200\n",
      "Epoch 296: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2988 - acc: 0.9199 - val_loss: 0.3171 - val_acc: 0.9022\n",
      "Epoch 297/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9253\n",
      "Epoch 297: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2928 - acc: 0.9252 - val_loss: 0.3257 - val_acc: 0.9148\n",
      "Epoch 298/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.9206\n",
      "Epoch 298: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3239 - acc: 0.9210 - val_loss: 0.3262 - val_acc: 0.9111\n",
      "Epoch 299/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.9199\n",
      "Epoch 299: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3171 - acc: 0.9202 - val_loss: 0.2995 - val_acc: 0.9107\n",
      "Epoch 300/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9228\n",
      "Epoch 300: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3049 - acc: 0.9231 - val_loss: 0.3227 - val_acc: 0.9067\n",
      "Epoch 301/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9223\n",
      "Epoch 301: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2803 - acc: 0.9228 - val_loss: 0.3367 - val_acc: 0.9123\n",
      "Epoch 302/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.9218\n",
      "Epoch 302: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3246 - acc: 0.9218 - val_loss: 0.3428 - val_acc: 0.9095\n",
      "Epoch 303/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9222\n",
      "Epoch 303: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3160 - acc: 0.9226 - val_loss: 0.3283 - val_acc: 0.9062\n",
      "Epoch 304/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9206\n",
      "Epoch 304: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2915 - acc: 0.9208 - val_loss: 0.3105 - val_acc: 0.9111\n",
      "Epoch 305/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9194\n",
      "Epoch 305: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3171 - acc: 0.9192 - val_loss: 0.3172 - val_acc: 0.9099\n",
      "Epoch 306/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.9230\n",
      "Epoch 306: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3482 - acc: 0.9230 - val_loss: 0.3212 - val_acc: 0.9079\n",
      "Epoch 307/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9214\n",
      "Epoch 307: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3079 - acc: 0.9213 - val_loss: 0.3419 - val_acc: 0.9119\n",
      "Epoch 308/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9237\n",
      "Epoch 308: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3107 - acc: 0.9238 - val_loss: 0.3222 - val_acc: 0.9079\n",
      "Epoch 309/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2872 - acc: 0.9268\n",
      "Epoch 309: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2872 - acc: 0.9268 - val_loss: 0.2863 - val_acc: 0.9176\n",
      "Epoch 310/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9196\n",
      "Epoch 310: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2846 - acc: 0.9195 - val_loss: 0.3277 - val_acc: 0.9164\n",
      "Epoch 311/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9196\n",
      "Epoch 311: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2901 - acc: 0.9197 - val_loss: 0.3019 - val_acc: 0.9160\n",
      "Epoch 312/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9191\n",
      "Epoch 312: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2878 - acc: 0.9192 - val_loss: 0.3686 - val_acc: 0.9038\n",
      "Epoch 313/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9236\n",
      "Epoch 313: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2888 - acc: 0.9233 - val_loss: 0.3573 - val_acc: 0.9144\n",
      "Epoch 314/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9266\n",
      "Epoch 314: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2922 - acc: 0.9266 - val_loss: 0.3706 - val_acc: 0.9103\n",
      "Epoch 315/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9237\n",
      "Epoch 315: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2873 - acc: 0.9239 - val_loss: 0.3026 - val_acc: 0.9196\n",
      "Epoch 316/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9230\n",
      "Epoch 316: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2823 - acc: 0.9228 - val_loss: 0.3041 - val_acc: 0.9140\n",
      "Epoch 317/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9203\n",
      "Epoch 317: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3090 - acc: 0.9203 - val_loss: 0.2956 - val_acc: 0.9095\n",
      "Epoch 318/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9254\n",
      "Epoch 318: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3142 - acc: 0.9252 - val_loss: 0.2921 - val_acc: 0.9184\n",
      "Epoch 319/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9212\n",
      "Epoch 319: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2755 - acc: 0.9212 - val_loss: 0.3373 - val_acc: 0.9111\n",
      "Epoch 320/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9278\n",
      "Epoch 320: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3038 - acc: 0.9280 - val_loss: 0.3051 - val_acc: 0.9148\n",
      "Epoch 321/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.9198\n",
      "Epoch 321: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3238 - acc: 0.9197 - val_loss: 0.2973 - val_acc: 0.9156\n",
      "Epoch 322/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2879 - acc: 0.9243\n",
      "Epoch 322: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2879 - acc: 0.9243 - val_loss: 0.3003 - val_acc: 0.9184\n",
      "Epoch 323/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9224\n",
      "Epoch 323: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2876 - acc: 0.9225 - val_loss: 0.3089 - val_acc: 0.9115\n",
      "Epoch 324/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9205\n",
      "Epoch 324: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2855 - acc: 0.9208 - val_loss: 0.3545 - val_acc: 0.9103\n",
      "Epoch 325/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2960 - acc: 0.9242\n",
      "Epoch 325: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2960 - acc: 0.9242 - val_loss: 0.3248 - val_acc: 0.9103\n",
      "Epoch 326/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9210\n",
      "Epoch 326: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2867 - acc: 0.9209 - val_loss: 0.3061 - val_acc: 0.9131\n",
      "Epoch 327/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9222\n",
      "Epoch 327: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3051 - acc: 0.9225 - val_loss: 0.3198 - val_acc: 0.9196\n",
      "Epoch 328/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3687 - acc: 0.9197\n",
      "Epoch 328: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3687 - acc: 0.9197 - val_loss: 0.3313 - val_acc: 0.9123\n",
      "Epoch 329/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9232\n",
      "Epoch 329: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2845 - acc: 0.9233 - val_loss: 0.3247 - val_acc: 0.9062\n",
      "Epoch 330/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9180\n",
      "Epoch 330: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3054 - acc: 0.9183 - val_loss: 0.3251 - val_acc: 0.9140\n",
      "Epoch 331/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9197\n",
      "Epoch 331: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3092 - acc: 0.9200 - val_loss: 0.3440 - val_acc: 0.9205\n",
      "Epoch 332/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9224\n",
      "Epoch 332: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2804 - acc: 0.9224 - val_loss: 0.3143 - val_acc: 0.9209\n",
      "Epoch 333/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.9201\n",
      "Epoch 333: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3180 - acc: 0.9205 - val_loss: 0.3311 - val_acc: 0.9184\n",
      "Epoch 334/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9215\n",
      "Epoch 334: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2887 - acc: 0.9217 - val_loss: 0.3196 - val_acc: 0.9188\n",
      "Epoch 335/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3059 - acc: 0.9231\n",
      "Epoch 335: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3059 - acc: 0.9231 - val_loss: 0.3166 - val_acc: 0.9087\n",
      "Epoch 336/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.9247\n",
      "Epoch 336: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3254 - acc: 0.9249 - val_loss: 0.3302 - val_acc: 0.9184\n",
      "Epoch 337/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9185\n",
      "Epoch 337: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3059 - acc: 0.9184 - val_loss: 0.3362 - val_acc: 0.9111\n",
      "Epoch 338/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9235\n",
      "Epoch 338: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2910 - acc: 0.9236 - val_loss: 0.3296 - val_acc: 0.9144\n",
      "Epoch 339/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.9219\n",
      "Epoch 339: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3405 - acc: 0.9221 - val_loss: 0.2887 - val_acc: 0.9196\n",
      "Epoch 340/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9221\n",
      "Epoch 340: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2860 - acc: 0.9221 - val_loss: 0.3355 - val_acc: 0.9144\n",
      "Epoch 341/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.9200\n",
      "Epoch 341: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3434 - acc: 0.9200 - val_loss: 0.3264 - val_acc: 0.9172\n",
      "Epoch 342/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.9238\n",
      "Epoch 342: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3140 - acc: 0.9238 - val_loss: 0.3011 - val_acc: 0.9180\n",
      "Epoch 343/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9197\n",
      "Epoch 343: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2769 - acc: 0.9198 - val_loss: 0.3030 - val_acc: 0.9119\n",
      "Epoch 344/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9271\n",
      "Epoch 344: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2843 - acc: 0.9272 - val_loss: 0.2964 - val_acc: 0.9217\n",
      "Epoch 345/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9218\n",
      "Epoch 345: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2948 - acc: 0.9219 - val_loss: 0.2974 - val_acc: 0.9192\n",
      "Epoch 346/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.9243\n",
      "Epoch 346: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 6ms/step - loss: 0.3422 - acc: 0.9243 - val_loss: 0.2881 - val_acc: 0.9156\n",
      "Epoch 347/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9239\n",
      "Epoch 347: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3089 - acc: 0.9241 - val_loss: 0.2875 - val_acc: 0.9160\n",
      "Epoch 348/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.9233\n",
      "Epoch 348: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3283 - acc: 0.9233 - val_loss: 0.3122 - val_acc: 0.9160\n",
      "Epoch 349/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9244\n",
      "Epoch 349: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3087 - acc: 0.9245 - val_loss: 0.3440 - val_acc: 0.9087\n",
      "Epoch 350/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.9212\n",
      "Epoch 350: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3695 - acc: 0.9216 - val_loss: 0.3219 - val_acc: 0.9172\n",
      "Epoch 351/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9241\n",
      "Epoch 351: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2856 - acc: 0.9245 - val_loss: 0.3411 - val_acc: 0.9148\n",
      "Epoch 352/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.9220\n",
      "Epoch 352: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3104 - acc: 0.9223 - val_loss: 0.2970 - val_acc: 0.9209\n",
      "Epoch 353/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9234\n",
      "Epoch 353: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2602 - acc: 0.9235 - val_loss: 0.3012 - val_acc: 0.9136\n",
      "Epoch 354/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.9241\n",
      "Epoch 354: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3060 - acc: 0.9242 - val_loss: 0.3681 - val_acc: 0.9176\n",
      "Epoch 355/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.9244\n",
      "Epoch 355: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3639 - acc: 0.9246 - val_loss: 0.2905 - val_acc: 0.9172\n",
      "Epoch 356/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3155 - acc: 0.9221\n",
      "Epoch 356: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3155 - acc: 0.9221 - val_loss: 0.3504 - val_acc: 0.9148\n",
      "Epoch 357/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.9191\n",
      "Epoch 357: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3185 - acc: 0.9192 - val_loss: 0.3128 - val_acc: 0.9209\n",
      "Epoch 358/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.9245\n",
      "Epoch 358: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3247 - acc: 0.9245 - val_loss: 0.3294 - val_acc: 0.9192\n",
      "Epoch 359/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.9220\n",
      "Epoch 359: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3411 - acc: 0.9221 - val_loss: 0.3254 - val_acc: 0.9131\n",
      "Epoch 360/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.9221\n",
      "Epoch 360: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3181 - acc: 0.9222 - val_loss: 0.3556 - val_acc: 0.9148\n",
      "Epoch 361/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.9224\n",
      "Epoch 361: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3227 - acc: 0.9226 - val_loss: 0.3386 - val_acc: 0.9131\n",
      "Epoch 362/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9228\n",
      "Epoch 362: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3148 - acc: 0.9229 - val_loss: 0.3577 - val_acc: 0.9152\n",
      "Epoch 363/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9227\n",
      "Epoch 363: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2793 - acc: 0.9227 - val_loss: 0.3437 - val_acc: 0.9192\n",
      "Epoch 364/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.9256\n",
      "Epoch 364: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3424 - acc: 0.9256 - val_loss: 0.3680 - val_acc: 0.9123\n",
      "Epoch 365/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.9286\n",
      "Epoch 365: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3743 - acc: 0.9287 - val_loss: 0.2891 - val_acc: 0.9213\n",
      "Epoch 366/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9251\n",
      "Epoch 366: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3223 - acc: 0.9251 - val_loss: 0.3007 - val_acc: 0.9160\n",
      "Epoch 367/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9214\n",
      "Epoch 367: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3196 - acc: 0.9216 - val_loss: 0.3245 - val_acc: 0.9180\n",
      "Epoch 368/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9261\n",
      "Epoch 368: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3114 - acc: 0.9261 - val_loss: 0.3205 - val_acc: 0.9144\n",
      "Epoch 369/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.9240\n",
      "Epoch 369: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3296 - acc: 0.9238 - val_loss: 0.3204 - val_acc: 0.9156\n",
      "Epoch 370/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9228\n",
      "Epoch 370: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2971 - acc: 0.9228 - val_loss: 0.3118 - val_acc: 0.9131\n",
      "Epoch 371/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.9216\n",
      "Epoch 371: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3447 - acc: 0.9218 - val_loss: 0.3286 - val_acc: 0.9148\n",
      "Epoch 372/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.9219\n",
      "Epoch 372: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3307 - acc: 0.9219 - val_loss: 0.2989 - val_acc: 0.9115\n",
      "Epoch 373/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.9226\n",
      "Epoch 373: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3489 - acc: 0.9230 - val_loss: 0.3247 - val_acc: 0.9136\n",
      "Epoch 374/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.9223\n",
      "Epoch 374: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3524 - acc: 0.9223 - val_loss: 0.3652 - val_acc: 0.9176\n",
      "Epoch 375/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.9267\n",
      "Epoch 375: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3297 - acc: 0.9266 - val_loss: 0.3265 - val_acc: 0.9136\n",
      "Epoch 376/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9231\n",
      "Epoch 376: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3097 - acc: 0.9235 - val_loss: 0.3511 - val_acc: 0.9221\n",
      "Epoch 377/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.9205\n",
      "Epoch 377: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3144 - acc: 0.9204 - val_loss: 0.3115 - val_acc: 0.9107\n",
      "Epoch 378/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9237\n",
      "Epoch 378: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2977 - acc: 0.9238 - val_loss: 0.3078 - val_acc: 0.9071\n",
      "Epoch 379/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9231\n",
      "Epoch 379: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2918 - acc: 0.9232 - val_loss: 0.3066 - val_acc: 0.9087\n",
      "Epoch 380/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9254\n",
      "Epoch 380: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3057 - acc: 0.9257 - val_loss: 0.3060 - val_acc: 0.9087\n",
      "Epoch 381/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.9252\n",
      "Epoch 381: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3306 - acc: 0.9253 - val_loss: 0.3801 - val_acc: 0.9156\n",
      "Epoch 382/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9239\n",
      "Epoch 382: val_acc did not improve from 0.92248\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3249 - acc: 0.9241 - val_loss: 0.3060 - val_acc: 0.9095\n",
      "Epoch 383/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9260\n",
      "Epoch 383: val_acc improved from 0.92248 to 0.92330, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2864 - acc: 0.9260 - val_loss: 0.3027 - val_acc: 0.9233\n",
      "Epoch 384/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.9214\n",
      "Epoch 384: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3382 - acc: 0.9216 - val_loss: 0.3211 - val_acc: 0.9103\n",
      "Epoch 385/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.9269\n",
      "Epoch 385: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3406 - acc: 0.9268 - val_loss: 0.3103 - val_acc: 0.9148\n",
      "Epoch 386/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.9230\n",
      "Epoch 386: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3121 - acc: 0.9229 - val_loss: 0.2827 - val_acc: 0.9156\n",
      "Epoch 387/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9238\n",
      "Epoch 387: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3062 - acc: 0.9240 - val_loss: 0.3079 - val_acc: 0.9213\n",
      "Epoch 388/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9231\n",
      "Epoch 388: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3029 - acc: 0.9232 - val_loss: 0.2904 - val_acc: 0.9160\n",
      "Epoch 389/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9234\n",
      "Epoch 389: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3047 - acc: 0.9236 - val_loss: 0.3210 - val_acc: 0.9229\n",
      "Epoch 390/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2984 - acc: 0.9254\n",
      "Epoch 390: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2984 - acc: 0.9254 - val_loss: 0.3233 - val_acc: 0.9152\n",
      "Epoch 391/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3560 - acc: 0.9268\n",
      "Epoch 391: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3560 - acc: 0.9268 - val_loss: 0.3375 - val_acc: 0.9176\n",
      "Epoch 392/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9288\n",
      "Epoch 392: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3212 - acc: 0.9288 - val_loss: 0.3432 - val_acc: 0.9127\n",
      "Epoch 393/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9268\n",
      "Epoch 393: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3126 - acc: 0.9270 - val_loss: 0.3601 - val_acc: 0.9152\n",
      "Epoch 394/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.9223\n",
      "Epoch 394: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3464 - acc: 0.9226 - val_loss: 0.3340 - val_acc: 0.9103\n",
      "Epoch 395/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3178 - acc: 0.9264\n",
      "Epoch 395: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3178 - acc: 0.9264 - val_loss: 0.3257 - val_acc: 0.9200\n",
      "Epoch 396/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.9290\n",
      "Epoch 396: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3110 - acc: 0.9295 - val_loss: 0.3285 - val_acc: 0.9184\n",
      "Epoch 397/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9253\n",
      "Epoch 397: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2956 - acc: 0.9255 - val_loss: 0.3252 - val_acc: 0.9196\n",
      "Epoch 398/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3210 - acc: 0.9245\n",
      "Epoch 398: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3195 - acc: 0.9247 - val_loss: 0.3116 - val_acc: 0.9221\n",
      "Epoch 399/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9309\n",
      "Epoch 399: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3057 - acc: 0.9311 - val_loss: 0.3133 - val_acc: 0.9225\n",
      "Epoch 400/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9249\n",
      "Epoch 400: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3178 - acc: 0.9250 - val_loss: 0.3468 - val_acc: 0.9160\n",
      "Epoch 401/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.9213\n",
      "Epoch 401: val_acc did not improve from 0.92330\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3370 - acc: 0.9214 - val_loss: 0.3369 - val_acc: 0.9213\n",
      "Epoch 402/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.9250\n",
      "Epoch 402: val_acc improved from 0.92330 to 0.92370, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/3/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3181 - acc: 0.9252 - val_loss: 0.3238 - val_acc: 0.9237\n",
      "Epoch 403/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.9213\n",
      "Epoch 403: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3760 - acc: 0.9215 - val_loss: 0.3305 - val_acc: 0.9205\n",
      "Epoch 404/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3025 - acc: 0.9270\n",
      "Epoch 404: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3025 - acc: 0.9270 - val_loss: 0.3262 - val_acc: 0.9131\n",
      "Epoch 405/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.9237\n",
      "Epoch 405: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3604 - acc: 0.9238 - val_loss: 0.3333 - val_acc: 0.9200\n",
      "Epoch 406/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9252\n",
      "Epoch 406: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2901 - acc: 0.9252 - val_loss: 0.3328 - val_acc: 0.9160\n",
      "Epoch 407/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9270\n",
      "Epoch 407: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3080 - acc: 0.9271 - val_loss: 0.3609 - val_acc: 0.9087\n",
      "Epoch 408/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9249\n",
      "Epoch 408: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2990 - acc: 0.9247 - val_loss: 0.3574 - val_acc: 0.9131\n",
      "Epoch 409/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.9260\n",
      "Epoch 409: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3199 - acc: 0.9262 - val_loss: 0.3372 - val_acc: 0.9144\n",
      "Epoch 410/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3434 - acc: 0.9238\n",
      "Epoch 410: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3434 - acc: 0.9238 - val_loss: 0.3618 - val_acc: 0.9136\n",
      "Epoch 411/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.9256\n",
      "Epoch 411: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3541 - acc: 0.9257 - val_loss: 0.3347 - val_acc: 0.9062\n",
      "Epoch 412/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9256\n",
      "Epoch 412: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3076 - acc: 0.9255 - val_loss: 0.3305 - val_acc: 0.9115\n",
      "Epoch 413/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.9240\n",
      "Epoch 413: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3171 - acc: 0.9242 - val_loss: 0.3025 - val_acc: 0.9196\n",
      "Epoch 414/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9286\n",
      "Epoch 414: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2910 - acc: 0.9286 - val_loss: 0.3158 - val_acc: 0.9140\n",
      "Epoch 415/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9288\n",
      "Epoch 415: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2778 - acc: 0.9290 - val_loss: 0.3397 - val_acc: 0.9196\n",
      "Epoch 416/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.9252\n",
      "Epoch 416: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3365 - acc: 0.9251 - val_loss: 0.3614 - val_acc: 0.9196\n",
      "Epoch 417/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3899 - acc: 0.9251\n",
      "Epoch 417: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3899 - acc: 0.9251 - val_loss: 0.3306 - val_acc: 0.9180\n",
      "Epoch 418/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.9242\n",
      "Epoch 418: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3794 - acc: 0.9242 - val_loss: 0.3424 - val_acc: 0.9172\n",
      "Epoch 419/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.9276\n",
      "Epoch 419: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3410 - acc: 0.9278 - val_loss: 0.2908 - val_acc: 0.9119\n",
      "Epoch 420/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9283\n",
      "Epoch 420: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2870 - acc: 0.9286 - val_loss: 0.3265 - val_acc: 0.9156\n",
      "Epoch 421/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.9250\n",
      "Epoch 421: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3428 - acc: 0.9250 - val_loss: 0.3418 - val_acc: 0.9168\n",
      "Epoch 422/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3273 - acc: 0.9232\n",
      "Epoch 422: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3273 - acc: 0.9232 - val_loss: 0.3368 - val_acc: 0.9136\n",
      "Epoch 423/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9261\n",
      "Epoch 423: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2808 - acc: 0.9260 - val_loss: 0.3342 - val_acc: 0.9111\n",
      "Epoch 424/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9260\n",
      "Epoch 424: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2943 - acc: 0.9261 - val_loss: 0.3143 - val_acc: 0.9144\n",
      "Epoch 425/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9252\n",
      "Epoch 425: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2934 - acc: 0.9254 - val_loss: 0.3140 - val_acc: 0.9103\n",
      "Epoch 426/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.9279\n",
      "Epoch 426: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3166 - acc: 0.9280 - val_loss: 0.3130 - val_acc: 0.9156\n",
      "Epoch 427/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9274\n",
      "Epoch 427: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3024 - acc: 0.9274 - val_loss: 0.3065 - val_acc: 0.9172\n",
      "Epoch 428/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9282\n",
      "Epoch 428: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2938 - acc: 0.9285 - val_loss: 0.3455 - val_acc: 0.9176\n",
      "Epoch 429/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.9292\n",
      "Epoch 429: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3254 - acc: 0.9291 - val_loss: 0.3767 - val_acc: 0.9099\n",
      "Epoch 430/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9282\n",
      "Epoch 430: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3095 - acc: 0.9284 - val_loss: 0.3421 - val_acc: 0.9123\n",
      "Epoch 431/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2907 - acc: 0.9287\n",
      "Epoch 431: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2907 - acc: 0.9287 - val_loss: 0.3173 - val_acc: 0.9131\n",
      "Epoch 432/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.9236\n",
      "Epoch 432: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3269 - acc: 0.9236 - val_loss: 0.3409 - val_acc: 0.9091\n",
      "Epoch 433/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9251\n",
      "Epoch 433: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2757 - acc: 0.9252 - val_loss: 0.3135 - val_acc: 0.9176\n",
      "Epoch 434/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.9234\n",
      "Epoch 434: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3263 - acc: 0.9236 - val_loss: 0.3185 - val_acc: 0.9131\n",
      "Epoch 435/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.9232\n",
      "Epoch 435: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3188 - acc: 0.9231 - val_loss: 0.3283 - val_acc: 0.9148\n",
      "Epoch 436/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.9246\n",
      "Epoch 436: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3341 - acc: 0.9245 - val_loss: 0.3273 - val_acc: 0.9176\n",
      "Epoch 437/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9238\n",
      "Epoch 437: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3031 - acc: 0.9239 - val_loss: 0.3403 - val_acc: 0.9172\n",
      "Epoch 438/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.9233\n",
      "Epoch 438: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3262 - acc: 0.9236 - val_loss: 0.3522 - val_acc: 0.9164\n",
      "Epoch 439/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3413 - acc: 0.9264\n",
      "Epoch 439: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3413 - acc: 0.9264 - val_loss: 0.3366 - val_acc: 0.9107\n",
      "Epoch 440/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.9275\n",
      "Epoch 440: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3206 - acc: 0.9277 - val_loss: 0.3206 - val_acc: 0.9213\n",
      "Epoch 441/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.9264\n",
      "Epoch 441: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3316 - acc: 0.9266 - val_loss: 0.3545 - val_acc: 0.9119\n",
      "Epoch 442/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.9258\n",
      "Epoch 442: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3382 - acc: 0.9261 - val_loss: 0.3324 - val_acc: 0.9160\n",
      "Epoch 443/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9275\n",
      "Epoch 443: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3053 - acc: 0.9275 - val_loss: 0.3631 - val_acc: 0.9168\n",
      "Epoch 444/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9296\n",
      "Epoch 444: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2920 - acc: 0.9295 - val_loss: 0.3655 - val_acc: 0.9140\n",
      "Epoch 445/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9266\n",
      "Epoch 445: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2898 - acc: 0.9269 - val_loss: 0.3192 - val_acc: 0.9184\n",
      "Epoch 446/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9289\n",
      "Epoch 446: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2823 - acc: 0.9291 - val_loss: 0.3780 - val_acc: 0.9184\n",
      "Epoch 447/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.9284\n",
      "Epoch 447: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3482 - acc: 0.9287 - val_loss: 0.3251 - val_acc: 0.9209\n",
      "Epoch 448/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.9239\n",
      "Epoch 448: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4015 - acc: 0.9241 - val_loss: 0.3401 - val_acc: 0.9180\n",
      "Epoch 449/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.9277\n",
      "Epoch 449: val_acc did not improve from 0.92370\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3262 - acc: 0.9280 - val_loss: 0.3441 - val_acc: 0.9205\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 80, 1)             0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               41472     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,057\n",
      "Trainable params: 173,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.6048 - acc: 0.7234\n",
      "Epoch 1: val_acc improved from -inf to 0.79099, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 6s 7ms/step - loss: 0.6038 - acc: 0.7238 - val_loss: 0.4523 - val_acc: 0.7910\n",
      "Epoch 2/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7664\n",
      "Epoch 2: val_acc improved from 0.79099 to 0.80154, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.5177 - acc: 0.7665 - val_loss: 0.4240 - val_acc: 0.8015\n",
      "Epoch 3/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.4713 - acc: 0.7904\n",
      "Epoch 3: val_acc improved from 0.80154 to 0.81494, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4713 - acc: 0.7904 - val_loss: 0.4057 - val_acc: 0.8149\n",
      "Epoch 4/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.7997\n",
      "Epoch 4: val_acc improved from 0.81494 to 0.82427, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4531 - acc: 0.7994 - val_loss: 0.3928 - val_acc: 0.8243\n",
      "Epoch 5/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8017\n",
      "Epoch 5: val_acc improved from 0.82427 to 0.82711, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4437 - acc: 0.8014 - val_loss: 0.3840 - val_acc: 0.8271\n",
      "Epoch 6/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8146\n",
      "Epoch 6: val_acc improved from 0.82711 to 0.82873, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4264 - acc: 0.8145 - val_loss: 0.3757 - val_acc: 0.8287\n",
      "Epoch 7/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.4220 - acc: 0.8175\n",
      "Epoch 7: val_acc improved from 0.82873 to 0.83766, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4220 - acc: 0.8175 - val_loss: 0.3773 - val_acc: 0.8377\n",
      "Epoch 8/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8225\n",
      "Epoch 8: val_acc did not improve from 0.83766\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4141 - acc: 0.8220 - val_loss: 0.3736 - val_acc: 0.8308\n",
      "Epoch 9/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8240\n",
      "Epoch 9: val_acc improved from 0.83766 to 0.84497, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4115 - acc: 0.8242 - val_loss: 0.3659 - val_acc: 0.8450\n",
      "Epoch 10/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8293\n",
      "Epoch 10: val_acc improved from 0.84497 to 0.84537, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4058 - acc: 0.8292 - val_loss: 0.3619 - val_acc: 0.8454\n",
      "Epoch 11/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8321\n",
      "Epoch 11: val_acc improved from 0.84537 to 0.85065, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4012 - acc: 0.8326 - val_loss: 0.3616 - val_acc: 0.8506\n",
      "Epoch 12/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3944 - acc: 0.8377\n",
      "Epoch 12: val_acc did not improve from 0.85065\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3935 - acc: 0.8381 - val_loss: 0.3593 - val_acc: 0.8494\n",
      "Epoch 13/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3888 - acc: 0.8394\n",
      "Epoch 13: val_acc improved from 0.85065 to 0.85146, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3888 - acc: 0.8394 - val_loss: 0.3529 - val_acc: 0.8515\n",
      "Epoch 14/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8442\n",
      "Epoch 14: val_acc did not improve from 0.85146\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3822 - acc: 0.8438 - val_loss: 0.3611 - val_acc: 0.8494\n",
      "Epoch 15/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8465\n",
      "Epoch 15: val_acc improved from 0.85146 to 0.85268, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3900 - acc: 0.8464 - val_loss: 0.3579 - val_acc: 0.8527\n",
      "Epoch 16/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8456\n",
      "Epoch 16: val_acc did not improve from 0.85268\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3831 - acc: 0.8455 - val_loss: 0.3548 - val_acc: 0.8519\n",
      "Epoch 17/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8461\n",
      "Epoch 17: val_acc improved from 0.85268 to 0.85308, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3798 - acc: 0.8461 - val_loss: 0.3554 - val_acc: 0.8531\n",
      "Epoch 18/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3788 - acc: 0.8471\n",
      "Epoch 18: val_acc improved from 0.85308 to 0.85674, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3788 - acc: 0.8471 - val_loss: 0.3475 - val_acc: 0.8567\n",
      "Epoch 19/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8489\n",
      "Epoch 19: val_acc did not improve from 0.85674\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3752 - acc: 0.8491 - val_loss: 0.3674 - val_acc: 0.8559\n",
      "Epoch 20/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8533\n",
      "Epoch 20: val_acc improved from 0.85674 to 0.86201, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3671 - acc: 0.8530 - val_loss: 0.3460 - val_acc: 0.8620\n",
      "Epoch 21/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8521\n",
      "Epoch 21: val_acc improved from 0.86201 to 0.86323, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3656 - acc: 0.8522 - val_loss: 0.3570 - val_acc: 0.8632\n",
      "Epoch 22/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8554\n",
      "Epoch 22: val_acc did not improve from 0.86323\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3686 - acc: 0.8554 - val_loss: 0.3486 - val_acc: 0.8612\n",
      "Epoch 23/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8540\n",
      "Epoch 23: val_acc improved from 0.86323 to 0.86688, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3664 - acc: 0.8538 - val_loss: 0.3405 - val_acc: 0.8669\n",
      "Epoch 24/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8573\n",
      "Epoch 24: val_acc did not improve from 0.86688\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3567 - acc: 0.8572 - val_loss: 0.3334 - val_acc: 0.8653\n",
      "Epoch 25/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8606\n",
      "Epoch 25: val_acc did not improve from 0.86688\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3574 - acc: 0.8606 - val_loss: 0.3561 - val_acc: 0.8657\n",
      "Epoch 26/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8642\n",
      "Epoch 26: val_acc did not improve from 0.86688\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3563 - acc: 0.8638 - val_loss: 0.3403 - val_acc: 0.8661\n",
      "Epoch 27/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8611\n",
      "Epoch 27: val_acc improved from 0.86688 to 0.86769, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3623 - acc: 0.8611 - val_loss: 0.3424 - val_acc: 0.8677\n",
      "Epoch 28/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8652\n",
      "Epoch 28: val_acc improved from 0.86769 to 0.87094, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3611 - acc: 0.8646 - val_loss: 0.3422 - val_acc: 0.8709\n",
      "Epoch 29/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8627\n",
      "Epoch 29: val_acc improved from 0.87094 to 0.87459, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3504 - acc: 0.8628 - val_loss: 0.3444 - val_acc: 0.8746\n",
      "Epoch 30/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8692\n",
      "Epoch 30: val_acc did not improve from 0.87459\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3499 - acc: 0.8692 - val_loss: 0.3462 - val_acc: 0.8709\n",
      "Epoch 31/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8633\n",
      "Epoch 31: val_acc improved from 0.87459 to 0.88190, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3517 - acc: 0.8634 - val_loss: 0.3341 - val_acc: 0.8819\n",
      "Epoch 32/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8694\n",
      "Epoch 32: val_acc did not improve from 0.88190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3470 - acc: 0.8692 - val_loss: 0.3435 - val_acc: 0.8766\n",
      "Epoch 33/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.8690\n",
      "Epoch 33: val_acc did not improve from 0.88190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3471 - acc: 0.8689 - val_loss: 0.3496 - val_acc: 0.8722\n",
      "Epoch 34/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8674\n",
      "Epoch 34: val_acc did not improve from 0.88190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3542 - acc: 0.8674 - val_loss: 0.3505 - val_acc: 0.8722\n",
      "Epoch 35/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3551 - acc: 0.8701\n",
      "Epoch 35: val_acc did not improve from 0.88190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3551 - acc: 0.8701 - val_loss: 0.3419 - val_acc: 0.8791\n",
      "Epoch 36/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8673\n",
      "Epoch 36: val_acc did not improve from 0.88190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3509 - acc: 0.8675 - val_loss: 0.3511 - val_acc: 0.8807\n",
      "Epoch 37/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3347 - acc: 0.8753\n",
      "Epoch 37: val_acc did not improve from 0.88190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3347 - acc: 0.8753 - val_loss: 0.3491 - val_acc: 0.8787\n",
      "Epoch 38/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8737\n",
      "Epoch 38: val_acc did not improve from 0.88190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3369 - acc: 0.8737 - val_loss: 0.3510 - val_acc: 0.8795\n",
      "Epoch 39/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8726\n",
      "Epoch 39: val_acc improved from 0.88190 to 0.88515, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3398 - acc: 0.8728 - val_loss: 0.3339 - val_acc: 0.8851\n",
      "Epoch 40/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8766\n",
      "Epoch 40: val_acc did not improve from 0.88515\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3412 - acc: 0.8764 - val_loss: 0.3316 - val_acc: 0.8835\n",
      "Epoch 41/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.8773\n",
      "Epoch 41: val_acc did not improve from 0.88515\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3300 - acc: 0.8772 - val_loss: 0.3314 - val_acc: 0.8819\n",
      "Epoch 42/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8737\n",
      "Epoch 42: val_acc improved from 0.88515 to 0.88920, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3403 - acc: 0.8739 - val_loss: 0.3429 - val_acc: 0.8892\n",
      "Epoch 43/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8736\n",
      "Epoch 43: val_acc did not improve from 0.88920\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3394 - acc: 0.8739 - val_loss: 0.3306 - val_acc: 0.8839\n",
      "Epoch 44/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.8769\n",
      "Epoch 44: val_acc did not improve from 0.88920\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3331 - acc: 0.8770 - val_loss: 0.3436 - val_acc: 0.8872\n",
      "Epoch 45/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.8777\n",
      "Epoch 45: val_acc did not improve from 0.88920\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3317 - acc: 0.8776 - val_loss: 0.3494 - val_acc: 0.8827\n",
      "Epoch 46/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8766\n",
      "Epoch 46: val_acc did not improve from 0.88920\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3334 - acc: 0.8770 - val_loss: 0.3441 - val_acc: 0.8851\n",
      "Epoch 47/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8817\n",
      "Epoch 47: val_acc did not improve from 0.88920\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3408 - acc: 0.8821 - val_loss: 0.3421 - val_acc: 0.8868\n",
      "Epoch 48/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.8776\n",
      "Epoch 48: val_acc did not improve from 0.88920\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3270 - acc: 0.8778 - val_loss: 0.3609 - val_acc: 0.8831\n",
      "Epoch 49/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8836\n",
      "Epoch 49: val_acc improved from 0.88920 to 0.89002, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3210 - acc: 0.8839 - val_loss: 0.3511 - val_acc: 0.8900\n",
      "Epoch 50/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8797\n",
      "Epoch 50: val_acc did not improve from 0.89002\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3312 - acc: 0.8797 - val_loss: 0.3413 - val_acc: 0.8856\n",
      "Epoch 51/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8827\n",
      "Epoch 51: val_acc improved from 0.89002 to 0.89205, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3314 - acc: 0.8827 - val_loss: 0.3396 - val_acc: 0.8920\n",
      "Epoch 52/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8808\n",
      "Epoch 52: val_acc did not improve from 0.89205\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3317 - acc: 0.8807 - val_loss: 0.3450 - val_acc: 0.8908\n",
      "Epoch 53/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8852\n",
      "Epoch 53: val_acc did not improve from 0.89205\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3168 - acc: 0.8851 - val_loss: 0.3569 - val_acc: 0.8876\n",
      "Epoch 54/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8855\n",
      "Epoch 54: val_acc did not improve from 0.89205\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3252 - acc: 0.8855 - val_loss: 0.3497 - val_acc: 0.8884\n",
      "Epoch 55/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8841\n",
      "Epoch 55: val_acc did not improve from 0.89205\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3345 - acc: 0.8844 - val_loss: 0.3564 - val_acc: 0.8912\n",
      "Epoch 56/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8830\n",
      "Epoch 56: val_acc did not improve from 0.89205\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3333 - acc: 0.8828 - val_loss: 0.3533 - val_acc: 0.8864\n",
      "Epoch 57/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8869\n",
      "Epoch 57: val_acc improved from 0.89205 to 0.89489, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3268 - acc: 0.8869 - val_loss: 0.3457 - val_acc: 0.8949\n",
      "Epoch 58/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8824\n",
      "Epoch 58: val_acc did not improve from 0.89489\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3321 - acc: 0.8826 - val_loss: 0.3532 - val_acc: 0.8904\n",
      "Epoch 59/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8886\n",
      "Epoch 59: val_acc did not improve from 0.89489\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3210 - acc: 0.8887 - val_loss: 0.3637 - val_acc: 0.8920\n",
      "Epoch 60/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8883\n",
      "Epoch 60: val_acc improved from 0.89489 to 0.89610, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3193 - acc: 0.8883 - val_loss: 0.3397 - val_acc: 0.8961\n",
      "Epoch 61/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8852\n",
      "Epoch 61: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3194 - acc: 0.8860 - val_loss: 0.3557 - val_acc: 0.8929\n",
      "Epoch 62/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8874\n",
      "Epoch 62: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3226 - acc: 0.8875 - val_loss: 0.3341 - val_acc: 0.8888\n",
      "Epoch 63/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.8870\n",
      "Epoch 63: val_acc did not improve from 0.89610\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3158 - acc: 0.8871 - val_loss: 0.3490 - val_acc: 0.8933\n",
      "Epoch 64/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8844\n",
      "Epoch 64: val_acc improved from 0.89610 to 0.89732, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3341 - acc: 0.8845 - val_loss: 0.3515 - val_acc: 0.8973\n",
      "Epoch 65/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8885\n",
      "Epoch 65: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3241 - acc: 0.8886 - val_loss: 0.3501 - val_acc: 0.8945\n",
      "Epoch 66/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3152 - acc: 0.8885\n",
      "Epoch 66: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3152 - acc: 0.8885 - val_loss: 0.3488 - val_acc: 0.8908\n",
      "Epoch 67/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8864\n",
      "Epoch 67: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3283 - acc: 0.8861 - val_loss: 0.3454 - val_acc: 0.8937\n",
      "Epoch 68/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8898\n",
      "Epoch 68: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3133 - acc: 0.8905 - val_loss: 0.3713 - val_acc: 0.8949\n",
      "Epoch 69/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3127 - acc: 0.8876\n",
      "Epoch 69: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3127 - acc: 0.8876 - val_loss: 0.3534 - val_acc: 0.8949\n",
      "Epoch 70/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.8894\n",
      "Epoch 70: val_acc did not improve from 0.89732\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3114 - acc: 0.8895 - val_loss: 0.3381 - val_acc: 0.8929\n",
      "Epoch 71/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8890\n",
      "Epoch 71: val_acc improved from 0.89732 to 0.90260, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3246 - acc: 0.8891 - val_loss: 0.3432 - val_acc: 0.9026\n",
      "Epoch 72/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.8945\n",
      "Epoch 72: val_acc improved from 0.90260 to 0.90300, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3071 - acc: 0.8947 - val_loss: 0.3709 - val_acc: 0.9030\n",
      "Epoch 73/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8888\n",
      "Epoch 73: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3269 - acc: 0.8890 - val_loss: 0.3511 - val_acc: 0.8977\n",
      "Epoch 74/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8920\n",
      "Epoch 74: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3218 - acc: 0.8923 - val_loss: 0.3612 - val_acc: 0.8973\n",
      "Epoch 75/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.8900\n",
      "Epoch 75: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3066 - acc: 0.8900 - val_loss: 0.3462 - val_acc: 0.8929\n",
      "Epoch 76/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8932\n",
      "Epoch 76: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3196 - acc: 0.8935 - val_loss: 0.3382 - val_acc: 0.8969\n",
      "Epoch 77/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8906\n",
      "Epoch 77: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3098 - acc: 0.8909 - val_loss: 0.3493 - val_acc: 0.8965\n",
      "Epoch 78/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.8932\n",
      "Epoch 78: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3164 - acc: 0.8932 - val_loss: 0.3825 - val_acc: 0.8961\n",
      "Epoch 79/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.8969\n",
      "Epoch 79: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2953 - acc: 0.8966 - val_loss: 0.3381 - val_acc: 0.8981\n",
      "Epoch 80/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.8910\n",
      "Epoch 80: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2987 - acc: 0.8914 - val_loss: 0.3704 - val_acc: 0.9006\n",
      "Epoch 81/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.8960\n",
      "Epoch 81: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3047 - acc: 0.8961 - val_loss: 0.3523 - val_acc: 0.9010\n",
      "Epoch 82/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.8935\n",
      "Epoch 82: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3073 - acc: 0.8938 - val_loss: 0.3594 - val_acc: 0.8998\n",
      "Epoch 83/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3058 - acc: 0.8950\n",
      "Epoch 83: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3058 - acc: 0.8950 - val_loss: 0.3360 - val_acc: 0.8973\n",
      "Epoch 84/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.8957\n",
      "Epoch 84: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2975 - acc: 0.8959 - val_loss: 0.3960 - val_acc: 0.8949\n",
      "Epoch 85/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.8955\n",
      "Epoch 85: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2987 - acc: 0.8957 - val_loss: 0.3722 - val_acc: 0.8989\n",
      "Epoch 86/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3237 - acc: 0.8940\n",
      "Epoch 86: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3237 - acc: 0.8940 - val_loss: 0.3397 - val_acc: 0.8945\n",
      "Epoch 87/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.8958\n",
      "Epoch 87: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3100 - acc: 0.8959 - val_loss: 0.3391 - val_acc: 0.8969\n",
      "Epoch 88/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3010 - acc: 0.8932\n",
      "Epoch 88: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3010 - acc: 0.8932 - val_loss: 0.3692 - val_acc: 0.8969\n",
      "Epoch 89/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8984\n",
      "Epoch 89: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3098 - acc: 0.8979 - val_loss: 0.3506 - val_acc: 0.8994\n",
      "Epoch 90/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2956 - acc: 0.8967\n",
      "Epoch 90: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2956 - acc: 0.8967 - val_loss: 0.3425 - val_acc: 0.8998\n",
      "Epoch 91/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.8996\n",
      "Epoch 91: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2922 - acc: 0.8995 - val_loss: 0.3632 - val_acc: 0.9002\n",
      "Epoch 92/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8916\n",
      "Epoch 92: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3110 - acc: 0.8916 - val_loss: 0.3165 - val_acc: 0.9026\n",
      "Epoch 93/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.8953\n",
      "Epoch 93: val_acc did not improve from 0.90300\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2972 - acc: 0.8956 - val_loss: 0.3447 - val_acc: 0.8969\n",
      "Epoch 94/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.8954\n",
      "Epoch 94: val_acc improved from 0.90300 to 0.90341, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2918 - acc: 0.8956 - val_loss: 0.3767 - val_acc: 0.9034\n",
      "Epoch 95/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.8984\n",
      "Epoch 95: val_acc did not improve from 0.90341\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3025 - acc: 0.8983 - val_loss: 0.3522 - val_acc: 0.8989\n",
      "Epoch 96/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.8986\n",
      "Epoch 96: val_acc did not improve from 0.90341\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3055 - acc: 0.8984 - val_loss: 0.3451 - val_acc: 0.9002\n",
      "Epoch 97/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.8961\n",
      "Epoch 97: val_acc did not improve from 0.90341\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3002 - acc: 0.8963 - val_loss: 0.3770 - val_acc: 0.8981\n",
      "Epoch 98/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.8956\n",
      "Epoch 98: val_acc improved from 0.90341 to 0.90666, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3114 - acc: 0.8959 - val_loss: 0.3937 - val_acc: 0.9067\n",
      "Epoch 99/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.8995\n",
      "Epoch 99: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2922 - acc: 0.8997 - val_loss: 0.3378 - val_acc: 0.9022\n",
      "Epoch 100/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.8975\n",
      "Epoch 100: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3006 - acc: 0.8978 - val_loss: 0.3457 - val_acc: 0.9058\n",
      "Epoch 101/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9003\n",
      "Epoch 101: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2867 - acc: 0.9005 - val_loss: 0.3509 - val_acc: 0.9042\n",
      "Epoch 102/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9017\n",
      "Epoch 102: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2913 - acc: 0.9015 - val_loss: 0.3529 - val_acc: 0.8933\n",
      "Epoch 103/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.8972\n",
      "Epoch 103: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3120 - acc: 0.8972 - val_loss: 0.3572 - val_acc: 0.8998\n",
      "Epoch 104/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3003 - acc: 0.8967\n",
      "Epoch 104: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2996 - acc: 0.8968 - val_loss: 0.3965 - val_acc: 0.8965\n",
      "Epoch 105/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9031\n",
      "Epoch 105: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2844 - acc: 0.9032 - val_loss: 0.3406 - val_acc: 0.9022\n",
      "Epoch 106/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8980\n",
      "Epoch 106: val_acc did not improve from 0.90666\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2900 - acc: 0.8983 - val_loss: 0.3608 - val_acc: 0.9046\n",
      "Epoch 107/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9036\n",
      "Epoch 107: val_acc improved from 0.90666 to 0.90747, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2837 - acc: 0.9036 - val_loss: 0.3720 - val_acc: 0.9075\n",
      "Epoch 108/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9063\n",
      "Epoch 108: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2938 - acc: 0.9062 - val_loss: 0.3772 - val_acc: 0.9030\n",
      "Epoch 109/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9029\n",
      "Epoch 109: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3043 - acc: 0.9031 - val_loss: 0.3539 - val_acc: 0.9038\n",
      "Epoch 110/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.8998\n",
      "Epoch 110: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2851 - acc: 0.8999 - val_loss: 0.3377 - val_acc: 0.9034\n",
      "Epoch 111/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.8998\n",
      "Epoch 111: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2793 - acc: 0.8996 - val_loss: 0.3881 - val_acc: 0.9038\n",
      "Epoch 112/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9020\n",
      "Epoch 112: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2917 - acc: 0.9020 - val_loss: 0.3314 - val_acc: 0.8981\n",
      "Epoch 113/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9025\n",
      "Epoch 113: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2853 - acc: 0.9029 - val_loss: 0.3747 - val_acc: 0.9067\n",
      "Epoch 114/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9028\n",
      "Epoch 114: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2981 - acc: 0.9029 - val_loss: 0.3727 - val_acc: 0.9062\n",
      "Epoch 115/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9031\n",
      "Epoch 115: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2947 - acc: 0.9030 - val_loss: 0.3449 - val_acc: 0.9054\n",
      "Epoch 116/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9038\n",
      "Epoch 116: val_acc did not improve from 0.90747\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2907 - acc: 0.9041 - val_loss: 0.3990 - val_acc: 0.9034\n",
      "Epoch 117/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9059\n",
      "Epoch 117: val_acc improved from 0.90747 to 0.90990, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2929 - acc: 0.9059 - val_loss: 0.3423 - val_acc: 0.9099\n",
      "Epoch 118/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2870 - acc: 0.9059\n",
      "Epoch 118: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2870 - acc: 0.9059 - val_loss: 0.3656 - val_acc: 0.9026\n",
      "Epoch 119/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9055\n",
      "Epoch 119: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2879 - acc: 0.9055 - val_loss: 0.3626 - val_acc: 0.9099\n",
      "Epoch 120/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9030\n",
      "Epoch 120: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2820 - acc: 0.9031 - val_loss: 0.3427 - val_acc: 0.9034\n",
      "Epoch 121/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9055\n",
      "Epoch 121: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2917 - acc: 0.9056 - val_loss: 0.3489 - val_acc: 0.9050\n",
      "Epoch 122/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9034\n",
      "Epoch 122: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3068 - acc: 0.9039 - val_loss: 0.3747 - val_acc: 0.9018\n",
      "Epoch 123/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9044\n",
      "Epoch 123: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2920 - acc: 0.9045 - val_loss: 0.3585 - val_acc: 0.9038\n",
      "Epoch 124/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9041\n",
      "Epoch 124: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3072 - acc: 0.9042 - val_loss: 0.3580 - val_acc: 0.9079\n",
      "Epoch 125/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9045\n",
      "Epoch 125: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3001 - acc: 0.9045 - val_loss: 0.3526 - val_acc: 0.9046\n",
      "Epoch 126/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9047\n",
      "Epoch 126: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2843 - acc: 0.9048 - val_loss: 0.3645 - val_acc: 0.9018\n",
      "Epoch 127/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9065\n",
      "Epoch 127: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2917 - acc: 0.9063 - val_loss: 0.3389 - val_acc: 0.9046\n",
      "Epoch 128/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9052\n",
      "Epoch 128: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2861 - acc: 0.9052 - val_loss: 0.3634 - val_acc: 0.9050\n",
      "Epoch 129/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9053\n",
      "Epoch 129: val_acc did not improve from 0.90990\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2709 - acc: 0.9052 - val_loss: 0.3599 - val_acc: 0.9083\n",
      "Epoch 130/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9050\n",
      "Epoch 130: val_acc improved from 0.90990 to 0.91071, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2940 - acc: 0.9051 - val_loss: 0.3519 - val_acc: 0.9107\n",
      "Epoch 131/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9068\n",
      "Epoch 131: val_acc did not improve from 0.91071\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2811 - acc: 0.9070 - val_loss: 0.3485 - val_acc: 0.9107\n",
      "Epoch 132/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2879 - acc: 0.9026\n",
      "Epoch 132: val_acc did not improve from 0.91071\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2879 - acc: 0.9026 - val_loss: 0.3701 - val_acc: 0.9058\n",
      "Epoch 133/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9054\n",
      "Epoch 133: val_acc improved from 0.91071 to 0.91153, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2969 - acc: 0.9054 - val_loss: 0.4156 - val_acc: 0.9115\n",
      "Epoch 134/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9040\n",
      "Epoch 134: val_acc did not improve from 0.91153\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3007 - acc: 0.9043 - val_loss: 0.3750 - val_acc: 0.8981\n",
      "Epoch 135/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9055\n",
      "Epoch 135: val_acc did not improve from 0.91153\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2914 - acc: 0.9054 - val_loss: 0.3151 - val_acc: 0.9067\n",
      "Epoch 136/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9062\n",
      "Epoch 136: val_acc did not improve from 0.91153\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2795 - acc: 0.9064 - val_loss: 0.4036 - val_acc: 0.9062\n",
      "Epoch 137/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9037\n",
      "Epoch 137: val_acc did not improve from 0.91153\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2833 - acc: 0.9037 - val_loss: 0.4245 - val_acc: 0.9091\n",
      "Epoch 138/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9058\n",
      "Epoch 138: val_acc did not improve from 0.91153\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2834 - acc: 0.9062 - val_loss: 0.3651 - val_acc: 0.9034\n",
      "Epoch 139/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9038\n",
      "Epoch 139: val_acc improved from 0.91153 to 0.91193, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2756 - acc: 0.9040 - val_loss: 0.3456 - val_acc: 0.9119\n",
      "Epoch 140/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9070\n",
      "Epoch 140: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2972 - acc: 0.9067 - val_loss: 0.3327 - val_acc: 0.9087\n",
      "Epoch 141/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9043\n",
      "Epoch 141: val_acc did not improve from 0.91193\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2767 - acc: 0.9043 - val_loss: 0.3914 - val_acc: 0.9050\n",
      "Epoch 142/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2933 - acc: 0.9068\n",
      "Epoch 142: val_acc improved from 0.91193 to 0.91356, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2933 - acc: 0.9068 - val_loss: 0.3808 - val_acc: 0.9136\n",
      "Epoch 143/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9047\n",
      "Epoch 143: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2882 - acc: 0.9048 - val_loss: 0.3930 - val_acc: 0.9095\n",
      "Epoch 144/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9094\n",
      "Epoch 144: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2940 - acc: 0.9097 - val_loss: 0.3504 - val_acc: 0.9079\n",
      "Epoch 145/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9114\n",
      "Epoch 145: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2827 - acc: 0.9113 - val_loss: 0.3506 - val_acc: 0.9075\n",
      "Epoch 146/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9051\n",
      "Epoch 146: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3070 - acc: 0.9050 - val_loss: 0.3601 - val_acc: 0.9107\n",
      "Epoch 147/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9104\n",
      "Epoch 147: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2822 - acc: 0.9104 - val_loss: 0.3641 - val_acc: 0.9079\n",
      "Epoch 148/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9072\n",
      "Epoch 148: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3004 - acc: 0.9071 - val_loss: 0.3398 - val_acc: 0.9062\n",
      "Epoch 149/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9070\n",
      "Epoch 149: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2880 - acc: 0.9072 - val_loss: 0.3576 - val_acc: 0.9071\n",
      "Epoch 150/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9052\n",
      "Epoch 150: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2869 - acc: 0.9053 - val_loss: 0.3853 - val_acc: 0.9095\n",
      "Epoch 151/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9077\n",
      "Epoch 151: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2805 - acc: 0.9079 - val_loss: 0.3947 - val_acc: 0.9107\n",
      "Epoch 152/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9108\n",
      "Epoch 152: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2911 - acc: 0.9108 - val_loss: 0.3576 - val_acc: 0.9107\n",
      "Epoch 153/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9088\n",
      "Epoch 153: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2844 - acc: 0.9090 - val_loss: 0.3871 - val_acc: 0.9136\n",
      "Epoch 154/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9073\n",
      "Epoch 154: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2890 - acc: 0.9073 - val_loss: 0.3254 - val_acc: 0.9119\n",
      "Epoch 155/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9061\n",
      "Epoch 155: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2905 - acc: 0.9064 - val_loss: 0.3573 - val_acc: 0.9091\n",
      "Epoch 156/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9073\n",
      "Epoch 156: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2895 - acc: 0.9073 - val_loss: 0.3561 - val_acc: 0.9123\n",
      "Epoch 157/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9094\n",
      "Epoch 157: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2797 - acc: 0.9095 - val_loss: 0.3958 - val_acc: 0.9071\n",
      "Epoch 158/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9084\n",
      "Epoch 158: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2935 - acc: 0.9086 - val_loss: 0.3402 - val_acc: 0.9136\n",
      "Epoch 159/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2797 - acc: 0.9095\n",
      "Epoch 159: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2797 - acc: 0.9095 - val_loss: 0.3712 - val_acc: 0.9131\n",
      "Epoch 160/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9113\n",
      "Epoch 160: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2855 - acc: 0.9112 - val_loss: 0.3696 - val_acc: 0.9095\n",
      "Epoch 161/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9080\n",
      "Epoch 161: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2918 - acc: 0.9080 - val_loss: 0.3499 - val_acc: 0.9079\n",
      "Epoch 162/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9096\n",
      "Epoch 162: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2860 - acc: 0.9095 - val_loss: 0.3922 - val_acc: 0.9103\n",
      "Epoch 163/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9140\n",
      "Epoch 163: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2767 - acc: 0.9139 - val_loss: 0.3484 - val_acc: 0.9136\n",
      "Epoch 164/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9104\n",
      "Epoch 164: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2952 - acc: 0.9104 - val_loss: 0.3590 - val_acc: 0.9107\n",
      "Epoch 165/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9139\n",
      "Epoch 165: val_acc did not improve from 0.91356\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2629 - acc: 0.9141 - val_loss: 0.3909 - val_acc: 0.9079\n",
      "Epoch 166/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9087\n",
      "Epoch 166: val_acc improved from 0.91356 to 0.91558, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2923 - acc: 0.9089 - val_loss: 0.3882 - val_acc: 0.9156\n",
      "Epoch 167/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2985 - acc: 0.9121\n",
      "Epoch 167: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2985 - acc: 0.9121 - val_loss: 0.3676 - val_acc: 0.9079\n",
      "Epoch 168/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9117\n",
      "Epoch 168: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2700 - acc: 0.9116 - val_loss: 0.3370 - val_acc: 0.9091\n",
      "Epoch 169/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9118\n",
      "Epoch 169: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2782 - acc: 0.9118 - val_loss: 0.3739 - val_acc: 0.9103\n",
      "Epoch 170/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9088\n",
      "Epoch 170: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2872 - acc: 0.9089 - val_loss: 0.3566 - val_acc: 0.9099\n",
      "Epoch 171/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9138\n",
      "Epoch 171: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2951 - acc: 0.9139 - val_loss: 0.3738 - val_acc: 0.9144\n",
      "Epoch 172/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9115\n",
      "Epoch 172: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2829 - acc: 0.9118 - val_loss: 0.3695 - val_acc: 0.9095\n",
      "Epoch 173/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9143\n",
      "Epoch 173: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2737 - acc: 0.9144 - val_loss: 0.3628 - val_acc: 0.9131\n",
      "Epoch 174/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9149\n",
      "Epoch 174: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2601 - acc: 0.9151 - val_loss: 0.3787 - val_acc: 0.9111\n",
      "Epoch 175/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2741 - acc: 0.9143\n",
      "Epoch 175: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2741 - acc: 0.9143 - val_loss: 0.3612 - val_acc: 0.9067\n",
      "Epoch 176/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9106\n",
      "Epoch 176: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2760 - acc: 0.9107 - val_loss: 0.4009 - val_acc: 0.9087\n",
      "Epoch 177/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2795 - acc: 0.9103\n",
      "Epoch 177: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2795 - acc: 0.9103 - val_loss: 0.4087 - val_acc: 0.9083\n",
      "Epoch 178/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2712 - acc: 0.9111\n",
      "Epoch 178: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2712 - acc: 0.9111 - val_loss: 0.3805 - val_acc: 0.8965\n",
      "Epoch 179/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9140\n",
      "Epoch 179: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2938 - acc: 0.9140 - val_loss: 0.3677 - val_acc: 0.9099\n",
      "Epoch 180/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9089\n",
      "Epoch 180: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2945 - acc: 0.9091 - val_loss: 0.3406 - val_acc: 0.9140\n",
      "Epoch 181/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2732 - acc: 0.9155\n",
      "Epoch 181: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2732 - acc: 0.9155 - val_loss: 0.3388 - val_acc: 0.9131\n",
      "Epoch 182/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9111\n",
      "Epoch 182: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2884 - acc: 0.9112 - val_loss: 0.3428 - val_acc: 0.9127\n",
      "Epoch 183/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9131\n",
      "Epoch 183: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2797 - acc: 0.9132 - val_loss: 0.4447 - val_acc: 0.9140\n",
      "Epoch 184/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9113\n",
      "Epoch 184: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2689 - acc: 0.9111 - val_loss: 0.3774 - val_acc: 0.9140\n",
      "Epoch 185/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.9118\n",
      "Epoch 185: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2723 - acc: 0.9118 - val_loss: 0.3876 - val_acc: 0.9091\n",
      "Epoch 186/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2778 - acc: 0.9145\n",
      "Epoch 186: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2778 - acc: 0.9145 - val_loss: 0.3947 - val_acc: 0.9111\n",
      "Epoch 187/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9117\n",
      "Epoch 187: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2776 - acc: 0.9122 - val_loss: 0.4412 - val_acc: 0.9156\n",
      "Epoch 188/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9138\n",
      "Epoch 188: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2750 - acc: 0.9132 - val_loss: 0.3897 - val_acc: 0.9075\n",
      "Epoch 189/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9146\n",
      "Epoch 189: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2793 - acc: 0.9148 - val_loss: 0.3868 - val_acc: 0.9152\n",
      "Epoch 190/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9139\n",
      "Epoch 190: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2817 - acc: 0.9140 - val_loss: 0.3971 - val_acc: 0.9103\n",
      "Epoch 191/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9130\n",
      "Epoch 191: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2843 - acc: 0.9131 - val_loss: 0.3934 - val_acc: 0.9140\n",
      "Epoch 192/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9151\n",
      "Epoch 192: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2690 - acc: 0.9153 - val_loss: 0.3918 - val_acc: 0.9123\n",
      "Epoch 193/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2705 - acc: 0.9132\n",
      "Epoch 193: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2705 - acc: 0.9132 - val_loss: 0.4104 - val_acc: 0.9127\n",
      "Epoch 194/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9120\n",
      "Epoch 194: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2652 - acc: 0.9120 - val_loss: 0.3958 - val_acc: 0.9115\n",
      "Epoch 195/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9107\n",
      "Epoch 195: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2863 - acc: 0.9106 - val_loss: 0.3939 - val_acc: 0.9079\n",
      "Epoch 196/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2645 - acc: 0.9151\n",
      "Epoch 196: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2645 - acc: 0.9151 - val_loss: 0.3806 - val_acc: 0.9136\n",
      "Epoch 197/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9122\n",
      "Epoch 197: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2658 - acc: 0.9122 - val_loss: 0.3622 - val_acc: 0.9127\n",
      "Epoch 198/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9136\n",
      "Epoch 198: val_acc did not improve from 0.91558\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2768 - acc: 0.9136 - val_loss: 0.4208 - val_acc: 0.9099\n",
      "Epoch 199/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2684 - acc: 0.9128\n",
      "Epoch 199: val_acc improved from 0.91558 to 0.91680, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2684 - acc: 0.9128 - val_loss: 0.3927 - val_acc: 0.9168\n",
      "Epoch 200/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9148\n",
      "Epoch 200: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2644 - acc: 0.9150 - val_loss: 0.4256 - val_acc: 0.9131\n",
      "Epoch 201/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9213\n",
      "Epoch 201: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2606 - acc: 0.9214 - val_loss: 0.3724 - val_acc: 0.9067\n",
      "Epoch 202/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9155\n",
      "Epoch 202: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2778 - acc: 0.9156 - val_loss: 0.4628 - val_acc: 0.9123\n",
      "Epoch 203/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9126\n",
      "Epoch 203: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2704 - acc: 0.9125 - val_loss: 0.3730 - val_acc: 0.9030\n",
      "Epoch 204/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9159\n",
      "Epoch 204: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2602 - acc: 0.9156 - val_loss: 0.3616 - val_acc: 0.9107\n",
      "Epoch 205/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2765 - acc: 0.9154\n",
      "Epoch 205: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2765 - acc: 0.9154 - val_loss: 0.4220 - val_acc: 0.9046\n",
      "Epoch 206/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9141\n",
      "Epoch 206: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2649 - acc: 0.9144 - val_loss: 0.3881 - val_acc: 0.9168\n",
      "Epoch 207/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9169\n",
      "Epoch 207: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2839 - acc: 0.9170 - val_loss: 0.4252 - val_acc: 0.9152\n",
      "Epoch 208/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9163\n",
      "Epoch 208: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2711 - acc: 0.9164 - val_loss: 0.3622 - val_acc: 0.9136\n",
      "Epoch 209/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9161\n",
      "Epoch 209: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2662 - acc: 0.9163 - val_loss: 0.3652 - val_acc: 0.9160\n",
      "Epoch 210/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9170\n",
      "Epoch 210: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2842 - acc: 0.9171 - val_loss: 0.3605 - val_acc: 0.9107\n",
      "Epoch 211/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9191\n",
      "Epoch 211: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2681 - acc: 0.9190 - val_loss: 0.3546 - val_acc: 0.9103\n",
      "Epoch 212/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2708 - acc: 0.9165\n",
      "Epoch 212: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2708 - acc: 0.9165 - val_loss: 0.4132 - val_acc: 0.9115\n",
      "Epoch 213/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9146\n",
      "Epoch 213: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2794 - acc: 0.9144 - val_loss: 0.4029 - val_acc: 0.9156\n",
      "Epoch 214/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9153\n",
      "Epoch 214: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2655 - acc: 0.9155 - val_loss: 0.3737 - val_acc: 0.9148\n",
      "Epoch 215/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2786 - acc: 0.9158\n",
      "Epoch 215: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2786 - acc: 0.9158 - val_loss: 0.4025 - val_acc: 0.9119\n",
      "Epoch 216/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9143\n",
      "Epoch 216: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2706 - acc: 0.9144 - val_loss: 0.4178 - val_acc: 0.9119\n",
      "Epoch 217/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9152\n",
      "Epoch 217: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2885 - acc: 0.9148 - val_loss: 0.4353 - val_acc: 0.9144\n",
      "Epoch 218/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9130\n",
      "Epoch 218: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2834 - acc: 0.9131 - val_loss: 0.4081 - val_acc: 0.9152\n",
      "Epoch 219/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9152\n",
      "Epoch 219: val_acc did not improve from 0.91680\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2775 - acc: 0.9151 - val_loss: 0.3758 - val_acc: 0.9127\n",
      "Epoch 220/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2838 - acc: 0.9145\n",
      "Epoch 220: val_acc improved from 0.91680 to 0.92045, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/4/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2838 - acc: 0.9145 - val_loss: 0.4580 - val_acc: 0.9205\n",
      "Epoch 221/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9158\n",
      "Epoch 221: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2784 - acc: 0.9157 - val_loss: 0.3769 - val_acc: 0.9172\n",
      "Epoch 222/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2678 - acc: 0.9153\n",
      "Epoch 222: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2668 - acc: 0.9158 - val_loss: 0.4042 - val_acc: 0.9083\n",
      "Epoch 223/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9150\n",
      "Epoch 223: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2824 - acc: 0.9153 - val_loss: 0.4256 - val_acc: 0.9164\n",
      "Epoch 224/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9137\n",
      "Epoch 224: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2723 - acc: 0.9139 - val_loss: 0.4125 - val_acc: 0.9115\n",
      "Epoch 225/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9194\n",
      "Epoch 225: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2823 - acc: 0.9191 - val_loss: 0.3550 - val_acc: 0.9119\n",
      "Epoch 226/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9155\n",
      "Epoch 226: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2605 - acc: 0.9154 - val_loss: 0.3978 - val_acc: 0.9099\n",
      "Epoch 227/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9174\n",
      "Epoch 227: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2533 - acc: 0.9173 - val_loss: 0.4343 - val_acc: 0.9136\n",
      "Epoch 228/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9180\n",
      "Epoch 228: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2697 - acc: 0.9181 - val_loss: 0.3846 - val_acc: 0.9164\n",
      "Epoch 229/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9149\n",
      "Epoch 229: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2788 - acc: 0.9146 - val_loss: 0.4519 - val_acc: 0.9103\n",
      "Epoch 230/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9184\n",
      "Epoch 230: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2614 - acc: 0.9183 - val_loss: 0.4421 - val_acc: 0.9099\n",
      "Epoch 231/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9143\n",
      "Epoch 231: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2954 - acc: 0.9146 - val_loss: 0.3947 - val_acc: 0.9111\n",
      "Epoch 232/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9188\n",
      "Epoch 232: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2690 - acc: 0.9186 - val_loss: 0.4341 - val_acc: 0.9176\n",
      "Epoch 233/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9149\n",
      "Epoch 233: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2750 - acc: 0.9149 - val_loss: 0.4472 - val_acc: 0.9152\n",
      "Epoch 234/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.9180\n",
      "Epoch 234: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2744 - acc: 0.9180 - val_loss: 0.4388 - val_acc: 0.9123\n",
      "Epoch 235/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9182\n",
      "Epoch 235: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2976 - acc: 0.9185 - val_loss: 0.4043 - val_acc: 0.9071\n",
      "Epoch 236/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9173\n",
      "Epoch 236: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2736 - acc: 0.9169 - val_loss: 0.3836 - val_acc: 0.9075\n",
      "Epoch 237/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9153\n",
      "Epoch 237: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2609 - acc: 0.9154 - val_loss: 0.4271 - val_acc: 0.9160\n",
      "Epoch 238/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2611 - acc: 0.9167\n",
      "Epoch 238: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2611 - acc: 0.9167 - val_loss: 0.4366 - val_acc: 0.9144\n",
      "Epoch 239/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9147\n",
      "Epoch 239: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2754 - acc: 0.9147 - val_loss: 0.4388 - val_acc: 0.9168\n",
      "Epoch 240/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9182\n",
      "Epoch 240: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2805 - acc: 0.9182 - val_loss: 0.4115 - val_acc: 0.9164\n",
      "Epoch 241/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9179\n",
      "Epoch 241: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2572 - acc: 0.9176 - val_loss: 0.3892 - val_acc: 0.9115\n",
      "Epoch 242/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2754 - acc: 0.9194\n",
      "Epoch 242: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2754 - acc: 0.9194 - val_loss: 0.4052 - val_acc: 0.9136\n",
      "Epoch 243/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9149\n",
      "Epoch 243: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2751 - acc: 0.9148 - val_loss: 0.4083 - val_acc: 0.9127\n",
      "Epoch 244/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9141\n",
      "Epoch 244: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2887 - acc: 0.9143 - val_loss: 0.3911 - val_acc: 0.9152\n",
      "Epoch 245/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9184\n",
      "Epoch 245: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2741 - acc: 0.9181 - val_loss: 0.3957 - val_acc: 0.9140\n",
      "Epoch 246/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9162\n",
      "Epoch 246: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2664 - acc: 0.9163 - val_loss: 0.4376 - val_acc: 0.9115\n",
      "Epoch 247/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9197\n",
      "Epoch 247: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2691 - acc: 0.9198 - val_loss: 0.4035 - val_acc: 0.9115\n",
      "Epoch 248/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9166\n",
      "Epoch 248: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2779 - acc: 0.9167 - val_loss: 0.3640 - val_acc: 0.9152\n",
      "Epoch 249/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9171\n",
      "Epoch 249: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2799 - acc: 0.9171 - val_loss: 0.4119 - val_acc: 0.9160\n",
      "Epoch 250/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9149\n",
      "Epoch 250: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2796 - acc: 0.9152 - val_loss: 0.4731 - val_acc: 0.9099\n",
      "Epoch 251/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2802 - acc: 0.9170\n",
      "Epoch 251: val_acc did not improve from 0.92045\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2802 - acc: 0.9170 - val_loss: 0.4618 - val_acc: 0.9127\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_5 (Reshape)         (None, 80, 1)             0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 512)               41472     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,057\n",
      "Trainable params: 173,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.5994 - acc: 0.7280\n",
      "Epoch 1: val_acc improved from -inf to 0.78197, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.5990 - acc: 0.7282 - val_loss: 0.4899 - val_acc: 0.7820\n",
      "Epoch 2/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.7742\n",
      "Epoch 2: val_acc improved from 0.78197 to 0.78563, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.5008 - acc: 0.7738 - val_loss: 0.4571 - val_acc: 0.7856\n",
      "Epoch 3/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.7905\n",
      "Epoch 3: val_acc improved from 0.78563 to 0.80024, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4704 - acc: 0.7904 - val_loss: 0.4249 - val_acc: 0.8002\n",
      "Epoch 4/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.7938\n",
      "Epoch 4: val_acc did not improve from 0.80024\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4508 - acc: 0.7938 - val_loss: 0.4174 - val_acc: 0.7986\n",
      "Epoch 5/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8097\n",
      "Epoch 5: val_acc improved from 0.80024 to 0.81039, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4338 - acc: 0.8100 - val_loss: 0.4047 - val_acc: 0.8104\n",
      "Epoch 6/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8145\n",
      "Epoch 6: val_acc improved from 0.81039 to 0.81933, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.4255 - acc: 0.8142 - val_loss: 0.4033 - val_acc: 0.8193\n",
      "Epoch 7/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8198\n",
      "Epoch 7: val_acc improved from 0.81933 to 0.82095, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4173 - acc: 0.8204 - val_loss: 0.3940 - val_acc: 0.8210\n",
      "Epoch 8/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8174\n",
      "Epoch 8: val_acc improved from 0.82095 to 0.82785, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4152 - acc: 0.8177 - val_loss: 0.3836 - val_acc: 0.8279\n",
      "Epoch 9/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8270\n",
      "Epoch 9: val_acc improved from 0.82785 to 0.82866, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4108 - acc: 0.8273 - val_loss: 0.3805 - val_acc: 0.8287\n",
      "Epoch 10/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8332\n",
      "Epoch 10: val_acc improved from 0.82866 to 0.83232, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4023 - acc: 0.8333 - val_loss: 0.3811 - val_acc: 0.8323\n",
      "Epoch 11/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8318\n",
      "Epoch 11: val_acc did not improve from 0.83232\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3972 - acc: 0.8319 - val_loss: 0.3838 - val_acc: 0.8283\n",
      "Epoch 12/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8340\n",
      "Epoch 12: val_acc improved from 0.83232 to 0.83841, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3946 - acc: 0.8343 - val_loss: 0.3793 - val_acc: 0.8384\n",
      "Epoch 13/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8393\n",
      "Epoch 13: val_acc improved from 0.83841 to 0.84328, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3869 - acc: 0.8400 - val_loss: 0.3809 - val_acc: 0.8433\n",
      "Epoch 14/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3851 - acc: 0.8448\n",
      "Epoch 14: val_acc did not improve from 0.84328\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3851 - acc: 0.8448 - val_loss: 0.3779 - val_acc: 0.8413\n",
      "Epoch 15/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3794 - acc: 0.8457\n",
      "Epoch 15: val_acc did not improve from 0.84328\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3794 - acc: 0.8457 - val_loss: 0.3742 - val_acc: 0.8400\n",
      "Epoch 16/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8472\n",
      "Epoch 16: val_acc improved from 0.84328 to 0.84775, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3808 - acc: 0.8471 - val_loss: 0.3654 - val_acc: 0.8477\n",
      "Epoch 17/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3748 - acc: 0.8515\n",
      "Epoch 17: val_acc did not improve from 0.84775\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3748 - acc: 0.8515 - val_loss: 0.3723 - val_acc: 0.8469\n",
      "Epoch 18/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8478\n",
      "Epoch 18: val_acc did not improve from 0.84775\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3717 - acc: 0.8480 - val_loss: 0.3744 - val_acc: 0.8457\n",
      "Epoch 19/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8528\n",
      "Epoch 19: val_acc improved from 0.84775 to 0.84937, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3662 - acc: 0.8529 - val_loss: 0.3651 - val_acc: 0.8494\n",
      "Epoch 20/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8565\n",
      "Epoch 20: val_acc improved from 0.84937 to 0.85830, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3658 - acc: 0.8568 - val_loss: 0.3896 - val_acc: 0.8583\n",
      "Epoch 21/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3622 - acc: 0.8577\n",
      "Epoch 21: val_acc did not improve from 0.85830\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3622 - acc: 0.8577 - val_loss: 0.3697 - val_acc: 0.8522\n",
      "Epoch 22/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3631 - acc: 0.8548\n",
      "Epoch 22: val_acc did not improve from 0.85830\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3631 - acc: 0.8548 - val_loss: 0.3848 - val_acc: 0.8559\n",
      "Epoch 23/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8561\n",
      "Epoch 23: val_acc did not improve from 0.85830\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3606 - acc: 0.8561 - val_loss: 0.3770 - val_acc: 0.8551\n",
      "Epoch 24/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8616\n",
      "Epoch 24: val_acc did not improve from 0.85830\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3532 - acc: 0.8621 - val_loss: 0.3811 - val_acc: 0.8555\n",
      "Epoch 25/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8612\n",
      "Epoch 25: val_acc did not improve from 0.85830\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3587 - acc: 0.8614 - val_loss: 0.3652 - val_acc: 0.8563\n",
      "Epoch 26/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8620\n",
      "Epoch 26: val_acc improved from 0.85830 to 0.86439, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3563 - acc: 0.8623 - val_loss: 0.3550 - val_acc: 0.8644\n",
      "Epoch 27/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8669\n",
      "Epoch 27: val_acc did not improve from 0.86439\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3573 - acc: 0.8670 - val_loss: 0.3535 - val_acc: 0.8632\n",
      "Epoch 28/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8664\n",
      "Epoch 28: val_acc did not improve from 0.86439\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3505 - acc: 0.8664 - val_loss: 0.3603 - val_acc: 0.8644\n",
      "Epoch 29/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8668\n",
      "Epoch 29: val_acc improved from 0.86439 to 0.87170, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3532 - acc: 0.8670 - val_loss: 0.3465 - val_acc: 0.8717\n",
      "Epoch 30/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8671\n",
      "Epoch 30: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3405 - acc: 0.8677 - val_loss: 0.3771 - val_acc: 0.8652\n",
      "Epoch 31/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8659\n",
      "Epoch 31: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3479 - acc: 0.8658 - val_loss: 0.3661 - val_acc: 0.8685\n",
      "Epoch 32/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8673\n",
      "Epoch 32: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3448 - acc: 0.8671 - val_loss: 0.3669 - val_acc: 0.8689\n",
      "Epoch 33/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8701\n",
      "Epoch 33: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3495 - acc: 0.8705 - val_loss: 0.3570 - val_acc: 0.8701\n",
      "Epoch 34/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8716\n",
      "Epoch 34: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3471 - acc: 0.8717 - val_loss: 0.3796 - val_acc: 0.8664\n",
      "Epoch 35/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8708\n",
      "Epoch 35: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3526 - acc: 0.8710 - val_loss: 0.3591 - val_acc: 0.8685\n",
      "Epoch 36/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3375 - acc: 0.8727\n",
      "Epoch 36: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3370 - acc: 0.8727 - val_loss: 0.3691 - val_acc: 0.8676\n",
      "Epoch 37/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8753\n",
      "Epoch 37: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3445 - acc: 0.8754 - val_loss: 0.3480 - val_acc: 0.8701\n",
      "Epoch 38/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8789\n",
      "Epoch 38: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3302 - acc: 0.8790 - val_loss: 0.3632 - val_acc: 0.8709\n",
      "Epoch 39/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8792\n",
      "Epoch 39: val_acc did not improve from 0.87170\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3347 - acc: 0.8793 - val_loss: 0.3496 - val_acc: 0.8705\n",
      "Epoch 40/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8773\n",
      "Epoch 40: val_acc improved from 0.87170 to 0.87942, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3364 - acc: 0.8770 - val_loss: 0.3408 - val_acc: 0.8794\n",
      "Epoch 41/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8790\n",
      "Epoch 41: val_acc did not improve from 0.87942\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3365 - acc: 0.8789 - val_loss: 0.3558 - val_acc: 0.8778\n",
      "Epoch 42/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8772\n",
      "Epoch 42: val_acc did not improve from 0.87942\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3356 - acc: 0.8774 - val_loss: 0.3586 - val_acc: 0.8758\n",
      "Epoch 43/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8770\n",
      "Epoch 43: val_acc did not improve from 0.87942\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3317 - acc: 0.8773 - val_loss: 0.3704 - val_acc: 0.8745\n",
      "Epoch 44/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8801\n",
      "Epoch 44: val_acc did not improve from 0.87942\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3256 - acc: 0.8799 - val_loss: 0.3595 - val_acc: 0.8721\n",
      "Epoch 45/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8817\n",
      "Epoch 45: val_acc did not improve from 0.87942\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3291 - acc: 0.8818 - val_loss: 0.3646 - val_acc: 0.8754\n",
      "Epoch 46/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8805\n",
      "Epoch 46: val_acc did not improve from 0.87942\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3452 - acc: 0.8806 - val_loss: 0.3576 - val_acc: 0.8754\n",
      "Epoch 47/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3286 - acc: 0.8829\n",
      "Epoch 47: val_acc did not improve from 0.87942\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3286 - acc: 0.8829 - val_loss: 0.3762 - val_acc: 0.8782\n",
      "Epoch 48/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8845\n",
      "Epoch 48: val_acc improved from 0.87942 to 0.88145, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3274 - acc: 0.8844 - val_loss: 0.3525 - val_acc: 0.8814\n",
      "Epoch 49/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8834\n",
      "Epoch 49: val_acc did not improve from 0.88145\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3286 - acc: 0.8836 - val_loss: 0.3455 - val_acc: 0.8782\n",
      "Epoch 50/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8851\n",
      "Epoch 50: val_acc did not improve from 0.88145\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3231 - acc: 0.8851 - val_loss: 0.3668 - val_acc: 0.8806\n",
      "Epoch 51/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8841\n",
      "Epoch 51: val_acc improved from 0.88145 to 0.88429, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3251 - acc: 0.8843 - val_loss: 0.3385 - val_acc: 0.8843\n",
      "Epoch 52/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8881\n",
      "Epoch 52: val_acc did not improve from 0.88429\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3278 - acc: 0.8880 - val_loss: 0.3698 - val_acc: 0.8814\n",
      "Epoch 53/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8857\n",
      "Epoch 53: val_acc did not improve from 0.88429\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3251 - acc: 0.8859 - val_loss: 0.3762 - val_acc: 0.8794\n",
      "Epoch 54/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8843\n",
      "Epoch 54: val_acc did not improve from 0.88429\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3339 - acc: 0.8845 - val_loss: 0.3850 - val_acc: 0.8737\n",
      "Epoch 55/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8868\n",
      "Epoch 55: val_acc did not improve from 0.88429\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3204 - acc: 0.8870 - val_loss: 0.3555 - val_acc: 0.8827\n",
      "Epoch 56/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8874\n",
      "Epoch 56: val_acc improved from 0.88429 to 0.88672, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3174 - acc: 0.8877 - val_loss: 0.3611 - val_acc: 0.8867\n",
      "Epoch 57/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.8876\n",
      "Epoch 57: val_acc did not improve from 0.88672\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3132 - acc: 0.8878 - val_loss: 0.3530 - val_acc: 0.8827\n",
      "Epoch 58/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8874\n",
      "Epoch 58: val_acc did not improve from 0.88672\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3307 - acc: 0.8877 - val_loss: 0.3541 - val_acc: 0.8827\n",
      "Epoch 59/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3302 - acc: 0.8893\n",
      "Epoch 59: val_acc did not improve from 0.88672\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3302 - acc: 0.8893 - val_loss: 0.3354 - val_acc: 0.8827\n",
      "Epoch 60/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8857\n",
      "Epoch 60: val_acc did not improve from 0.88672\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3262 - acc: 0.8866 - val_loss: 0.3712 - val_acc: 0.8798\n",
      "Epoch 61/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.8859\n",
      "Epoch 61: val_acc did not improve from 0.88672\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3185 - acc: 0.8861 - val_loss: 0.3547 - val_acc: 0.8810\n",
      "Epoch 62/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8882\n",
      "Epoch 62: val_acc did not improve from 0.88672\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3311 - acc: 0.8885 - val_loss: 0.3577 - val_acc: 0.8810\n",
      "Epoch 63/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8899\n",
      "Epoch 63: val_acc improved from 0.88672 to 0.88713, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3358 - acc: 0.8900 - val_loss: 0.3538 - val_acc: 0.8871\n",
      "Epoch 64/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8873\n",
      "Epoch 64: val_acc did not improve from 0.88713\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3240 - acc: 0.8875 - val_loss: 0.3599 - val_acc: 0.8871\n",
      "Epoch 65/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8907\n",
      "Epoch 65: val_acc improved from 0.88713 to 0.88794, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3257 - acc: 0.8913 - val_loss: 0.3560 - val_acc: 0.8879\n",
      "Epoch 66/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8886\n",
      "Epoch 66: val_acc did not improve from 0.88794\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3303 - acc: 0.8891 - val_loss: 0.3671 - val_acc: 0.8851\n",
      "Epoch 67/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.8932\n",
      "Epoch 67: val_acc did not improve from 0.88794\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3315 - acc: 0.8932 - val_loss: 0.3354 - val_acc: 0.8855\n",
      "Epoch 68/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8878\n",
      "Epoch 68: val_acc did not improve from 0.88794\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3209 - acc: 0.8883 - val_loss: 0.3447 - val_acc: 0.8794\n",
      "Epoch 69/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8905\n",
      "Epoch 69: val_acc did not improve from 0.88794\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3223 - acc: 0.8906 - val_loss: 0.3425 - val_acc: 0.8859\n",
      "Epoch 70/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8904\n",
      "Epoch 70: val_acc did not improve from 0.88794\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3123 - acc: 0.8904 - val_loss: 0.3535 - val_acc: 0.8875\n",
      "Epoch 71/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3198 - acc: 0.8919\n",
      "Epoch 71: val_acc did not improve from 0.88794\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3198 - acc: 0.8919 - val_loss: 0.3552 - val_acc: 0.8814\n",
      "Epoch 72/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3200 - acc: 0.8923\n",
      "Epoch 72: val_acc improved from 0.88794 to 0.88835, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3200 - acc: 0.8923 - val_loss: 0.3579 - val_acc: 0.8883\n",
      "Epoch 73/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8929\n",
      "Epoch 73: val_acc did not improve from 0.88835\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3158 - acc: 0.8931 - val_loss: 0.3560 - val_acc: 0.8875\n",
      "Epoch 74/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8903\n",
      "Epoch 74: val_acc did not improve from 0.88835\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3261 - acc: 0.8904 - val_loss: 0.3602 - val_acc: 0.8831\n",
      "Epoch 75/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.8923\n",
      "Epoch 75: val_acc improved from 0.88835 to 0.89444, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3172 - acc: 0.8929 - val_loss: 0.3331 - val_acc: 0.8944\n",
      "Epoch 76/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8956\n",
      "Epoch 76: val_acc did not improve from 0.89444\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3146 - acc: 0.8957 - val_loss: 0.3267 - val_acc: 0.8944\n",
      "Epoch 77/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.8919\n",
      "Epoch 77: val_acc improved from 0.89444 to 0.89525, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3216 - acc: 0.8923 - val_loss: 0.3327 - val_acc: 0.8952\n",
      "Epoch 78/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.8936\n",
      "Epoch 78: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3075 - acc: 0.8934 - val_loss: 0.3202 - val_acc: 0.8920\n",
      "Epoch 79/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8957\n",
      "Epoch 79: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3097 - acc: 0.8959 - val_loss: 0.3613 - val_acc: 0.8888\n",
      "Epoch 80/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8948\n",
      "Epoch 80: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3103 - acc: 0.8951 - val_loss: 0.3507 - val_acc: 0.8952\n",
      "Epoch 81/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8949\n",
      "Epoch 81: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3164 - acc: 0.8953 - val_loss: 0.3501 - val_acc: 0.8940\n",
      "Epoch 82/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8958\n",
      "Epoch 82: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3204 - acc: 0.8960 - val_loss: 0.3283 - val_acc: 0.8940\n",
      "Epoch 83/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.8953\n",
      "Epoch 83: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3005 - acc: 0.8955 - val_loss: 0.3525 - val_acc: 0.8936\n",
      "Epoch 84/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.8942\n",
      "Epoch 84: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3111 - acc: 0.8944 - val_loss: 0.3316 - val_acc: 0.8920\n",
      "Epoch 85/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.8978\n",
      "Epoch 85: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3143 - acc: 0.8980 - val_loss: 0.3396 - val_acc: 0.8940\n",
      "Epoch 86/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8977\n",
      "Epoch 86: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3186 - acc: 0.8977 - val_loss: 0.3905 - val_acc: 0.8871\n",
      "Epoch 87/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8960\n",
      "Epoch 87: val_acc did not improve from 0.89525\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3151 - acc: 0.8958 - val_loss: 0.3416 - val_acc: 0.8928\n",
      "Epoch 88/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.8977\n",
      "Epoch 88: val_acc improved from 0.89525 to 0.89769, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3021 - acc: 0.8978 - val_loss: 0.3271 - val_acc: 0.8977\n",
      "Epoch 89/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8947\n",
      "Epoch 89: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3233 - acc: 0.8947 - val_loss: 0.3597 - val_acc: 0.8928\n",
      "Epoch 90/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8962\n",
      "Epoch 90: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3197 - acc: 0.8966 - val_loss: 0.3539 - val_acc: 0.8924\n",
      "Epoch 91/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3126 - acc: 0.8988\n",
      "Epoch 91: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3126 - acc: 0.8988 - val_loss: 0.3642 - val_acc: 0.8932\n",
      "Epoch 92/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.8973\n",
      "Epoch 92: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3035 - acc: 0.8978 - val_loss: 0.3386 - val_acc: 0.8977\n",
      "Epoch 93/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.9002\n",
      "Epoch 93: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3170 - acc: 0.9007 - val_loss: 0.3677 - val_acc: 0.8928\n",
      "Epoch 94/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8955\n",
      "Epoch 94: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3171 - acc: 0.8957 - val_loss: 0.4052 - val_acc: 0.8859\n",
      "Epoch 95/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8961\n",
      "Epoch 95: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3511 - acc: 0.8957 - val_loss: 0.3146 - val_acc: 0.8944\n",
      "Epoch 96/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.8993\n",
      "Epoch 96: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3068 - acc: 0.8997 - val_loss: 0.3280 - val_acc: 0.8965\n",
      "Epoch 97/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8970\n",
      "Epoch 97: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3216 - acc: 0.8972 - val_loss: 0.3494 - val_acc: 0.8965\n",
      "Epoch 98/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.8983\n",
      "Epoch 98: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2951 - acc: 0.8989 - val_loss: 0.3514 - val_acc: 0.8932\n",
      "Epoch 99/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.8955\n",
      "Epoch 99: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3175 - acc: 0.8953 - val_loss: 0.3414 - val_acc: 0.8944\n",
      "Epoch 100/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9000\n",
      "Epoch 100: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3122 - acc: 0.9001 - val_loss: 0.3261 - val_acc: 0.8957\n",
      "Epoch 101/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.9018\n",
      "Epoch 101: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3102 - acc: 0.9023 - val_loss: 0.3294 - val_acc: 0.8896\n",
      "Epoch 102/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.8997\n",
      "Epoch 102: val_acc did not improve from 0.89769\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2948 - acc: 0.8997 - val_loss: 0.3121 - val_acc: 0.8961\n",
      "Epoch 103/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9039\n",
      "Epoch 103: val_acc improved from 0.89769 to 0.90134, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2992 - acc: 0.9038 - val_loss: 0.3417 - val_acc: 0.9013\n",
      "Epoch 104/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8985\n",
      "Epoch 104: val_acc did not improve from 0.90134\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3261 - acc: 0.8985 - val_loss: 0.3323 - val_acc: 0.8973\n",
      "Epoch 105/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9012\n",
      "Epoch 105: val_acc did not improve from 0.90134\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3005 - acc: 0.9014 - val_loss: 0.3825 - val_acc: 0.8989\n",
      "Epoch 106/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8999\n",
      "Epoch 106: val_acc improved from 0.90134 to 0.90418, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3285 - acc: 0.9001 - val_loss: 0.3203 - val_acc: 0.9042\n",
      "Epoch 107/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9000\n",
      "Epoch 107: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3037 - acc: 0.9003 - val_loss: 0.3123 - val_acc: 0.8961\n",
      "Epoch 108/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.8996\n",
      "Epoch 108: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3033 - acc: 0.8996 - val_loss: 0.3361 - val_acc: 0.8993\n",
      "Epoch 109/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9004\n",
      "Epoch 109: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2908 - acc: 0.9003 - val_loss: 0.3291 - val_acc: 0.8969\n",
      "Epoch 110/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.8983\n",
      "Epoch 110: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3005 - acc: 0.8988 - val_loss: 0.3713 - val_acc: 0.8981\n",
      "Epoch 111/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.9008\n",
      "Epoch 111: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3188 - acc: 0.9013 - val_loss: 0.3146 - val_acc: 0.9009\n",
      "Epoch 112/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9022\n",
      "Epoch 112: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3050 - acc: 0.9023 - val_loss: 0.3301 - val_acc: 0.8940\n",
      "Epoch 113/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9002\n",
      "Epoch 113: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2991 - acc: 0.9004 - val_loss: 0.3042 - val_acc: 0.9001\n",
      "Epoch 114/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2954 - acc: 0.9033\n",
      "Epoch 114: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2954 - acc: 0.9033 - val_loss: 0.3494 - val_acc: 0.8997\n",
      "Epoch 115/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9032\n",
      "Epoch 115: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2799 - acc: 0.9036 - val_loss: 0.3872 - val_acc: 0.8940\n",
      "Epoch 116/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9026\n",
      "Epoch 116: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2966 - acc: 0.9030 - val_loss: 0.3313 - val_acc: 0.9009\n",
      "Epoch 117/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9028\n",
      "Epoch 117: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2896 - acc: 0.9033 - val_loss: 0.3559 - val_acc: 0.8965\n",
      "Epoch 118/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9042\n",
      "Epoch 118: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2960 - acc: 0.9042 - val_loss: 0.3890 - val_acc: 0.8957\n",
      "Epoch 119/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3052 - acc: 0.9014\n",
      "Epoch 119: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3052 - acc: 0.9014 - val_loss: 0.3533 - val_acc: 0.8981\n",
      "Epoch 120/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.9046\n",
      "Epoch 120: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2962 - acc: 0.9046 - val_loss: 0.3072 - val_acc: 0.9005\n",
      "Epoch 121/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9055\n",
      "Epoch 121: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3086 - acc: 0.9060 - val_loss: 0.3182 - val_acc: 0.8940\n",
      "Epoch 122/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9053\n",
      "Epoch 122: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2900 - acc: 0.9055 - val_loss: 0.3279 - val_acc: 0.8989\n",
      "Epoch 123/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9043\n",
      "Epoch 123: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3038 - acc: 0.9045 - val_loss: 0.3359 - val_acc: 0.8977\n",
      "Epoch 124/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2964 - acc: 0.9046\n",
      "Epoch 124: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2964 - acc: 0.9046 - val_loss: 0.3132 - val_acc: 0.8997\n",
      "Epoch 125/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9053\n",
      "Epoch 125: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3015 - acc: 0.9052 - val_loss: 0.3384 - val_acc: 0.8924\n",
      "Epoch 126/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9036\n",
      "Epoch 126: val_acc did not improve from 0.90418\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2983 - acc: 0.9036 - val_loss: 0.3076 - val_acc: 0.9009\n",
      "Epoch 127/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9059\n",
      "Epoch 127: val_acc improved from 0.90418 to 0.90499, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3061 - acc: 0.9060 - val_loss: 0.3185 - val_acc: 0.9050\n",
      "Epoch 128/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9062\n",
      "Epoch 128: val_acc did not improve from 0.90499\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2980 - acc: 0.9066 - val_loss: 0.3254 - val_acc: 0.9022\n",
      "Epoch 129/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9076\n",
      "Epoch 129: val_acc did not improve from 0.90499\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2885 - acc: 0.9077 - val_loss: 0.3385 - val_acc: 0.8989\n",
      "Epoch 130/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9061\n",
      "Epoch 130: val_acc improved from 0.90499 to 0.90662, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3060 - acc: 0.9064 - val_loss: 0.3141 - val_acc: 0.9066\n",
      "Epoch 131/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9059\n",
      "Epoch 131: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3036 - acc: 0.9061 - val_loss: 0.3438 - val_acc: 0.8989\n",
      "Epoch 132/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3014 - acc: 0.9058\n",
      "Epoch 132: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3014 - acc: 0.9058 - val_loss: 0.3323 - val_acc: 0.8997\n",
      "Epoch 133/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9018\n",
      "Epoch 133: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3026 - acc: 0.9021 - val_loss: 0.3351 - val_acc: 0.8969\n",
      "Epoch 134/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3211 - acc: 0.9087\n",
      "Epoch 134: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3211 - acc: 0.9087 - val_loss: 0.3322 - val_acc: 0.9022\n",
      "Epoch 135/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9039\n",
      "Epoch 135: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2882 - acc: 0.9042 - val_loss: 0.3274 - val_acc: 0.9022\n",
      "Epoch 136/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.9067\n",
      "Epoch 136: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3159 - acc: 0.9073 - val_loss: 0.3302 - val_acc: 0.8973\n",
      "Epoch 137/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9036\n",
      "Epoch 137: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2987 - acc: 0.9038 - val_loss: 0.3302 - val_acc: 0.9038\n",
      "Epoch 138/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9060\n",
      "Epoch 138: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3014 - acc: 0.9068 - val_loss: 0.3535 - val_acc: 0.8977\n",
      "Epoch 139/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9060\n",
      "Epoch 139: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3046 - acc: 0.9058 - val_loss: 0.3197 - val_acc: 0.8981\n",
      "Epoch 140/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9065\n",
      "Epoch 140: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2941 - acc: 0.9068 - val_loss: 0.3196 - val_acc: 0.9005\n",
      "Epoch 141/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9092\n",
      "Epoch 141: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3011 - acc: 0.9095 - val_loss: 0.3141 - val_acc: 0.9034\n",
      "Epoch 142/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9072\n",
      "Epoch 142: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2901 - acc: 0.9073 - val_loss: 0.3361 - val_acc: 0.9026\n",
      "Epoch 143/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.9034\n",
      "Epoch 143: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3275 - acc: 0.9033 - val_loss: 0.3198 - val_acc: 0.9022\n",
      "Epoch 144/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9117\n",
      "Epoch 144: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2810 - acc: 0.9119 - val_loss: 0.3278 - val_acc: 0.9013\n",
      "Epoch 145/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9074\n",
      "Epoch 145: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3012 - acc: 0.9078 - val_loss: 0.3569 - val_acc: 0.9030\n",
      "Epoch 146/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9089\n",
      "Epoch 146: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 6s 10ms/step - loss: 0.2714 - acc: 0.9092 - val_loss: 0.3427 - val_acc: 0.9022\n",
      "Epoch 147/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9105\n",
      "Epoch 147: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2812 - acc: 0.9106 - val_loss: 0.3616 - val_acc: 0.8993\n",
      "Epoch 148/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9095\n",
      "Epoch 148: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3204 - acc: 0.9094 - val_loss: 0.3015 - val_acc: 0.9022\n",
      "Epoch 149/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.9064\n",
      "Epoch 149: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3107 - acc: 0.9068 - val_loss: 0.3468 - val_acc: 0.9038\n",
      "Epoch 150/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9089\n",
      "Epoch 150: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3075 - acc: 0.9090 - val_loss: 0.3133 - val_acc: 0.9066\n",
      "Epoch 151/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9117\n",
      "Epoch 151: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2841 - acc: 0.9117 - val_loss: 0.3728 - val_acc: 0.8896\n",
      "Epoch 152/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9098\n",
      "Epoch 152: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3051 - acc: 0.9099 - val_loss: 0.3507 - val_acc: 0.9030\n",
      "Epoch 153/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9113\n",
      "Epoch 153: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2913 - acc: 0.9113 - val_loss: 0.3156 - val_acc: 0.8993\n",
      "Epoch 154/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9073\n",
      "Epoch 154: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3004 - acc: 0.9074 - val_loss: 0.3495 - val_acc: 0.9050\n",
      "Epoch 155/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9116\n",
      "Epoch 155: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2905 - acc: 0.9122 - val_loss: 0.3561 - val_acc: 0.9042\n",
      "Epoch 156/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9104\n",
      "Epoch 156: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2995 - acc: 0.9109 - val_loss: 0.3335 - val_acc: 0.8961\n",
      "Epoch 157/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2928 - acc: 0.9068\n",
      "Epoch 157: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2928 - acc: 0.9068 - val_loss: 0.3031 - val_acc: 0.8993\n",
      "Epoch 158/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9100\n",
      "Epoch 158: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2969 - acc: 0.9102 - val_loss: 0.3323 - val_acc: 0.9054\n",
      "Epoch 159/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9082\n",
      "Epoch 159: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2995 - acc: 0.9083 - val_loss: 0.3629 - val_acc: 0.9030\n",
      "Epoch 160/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9091\n",
      "Epoch 160: val_acc did not improve from 0.90662\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2950 - acc: 0.9095 - val_loss: 0.3276 - val_acc: 0.9042\n",
      "Epoch 161/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9087\n",
      "Epoch 161: val_acc improved from 0.90662 to 0.90784, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2848 - acc: 0.9091 - val_loss: 0.3191 - val_acc: 0.9078\n",
      "Epoch 162/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9088\n",
      "Epoch 162: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2844 - acc: 0.9089 - val_loss: 0.3222 - val_acc: 0.9034\n",
      "Epoch 163/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9070\n",
      "Epoch 163: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3039 - acc: 0.9073 - val_loss: 0.3370 - val_acc: 0.9058\n",
      "Epoch 164/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2778 - acc: 0.9097\n",
      "Epoch 164: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2778 - acc: 0.9097 - val_loss: 0.3690 - val_acc: 0.9022\n",
      "Epoch 165/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9116\n",
      "Epoch 165: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2852 - acc: 0.9122 - val_loss: 0.3579 - val_acc: 0.9026\n",
      "Epoch 166/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9114\n",
      "Epoch 166: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2963 - acc: 0.9115 - val_loss: 0.3441 - val_acc: 0.9050\n",
      "Epoch 167/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9128\n",
      "Epoch 167: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2921 - acc: 0.9127 - val_loss: 0.3690 - val_acc: 0.9022\n",
      "Epoch 168/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9121\n",
      "Epoch 168: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3196 - acc: 0.9120 - val_loss: 0.3406 - val_acc: 0.9042\n",
      "Epoch 169/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9108\n",
      "Epoch 169: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2753 - acc: 0.9108 - val_loss: 0.3494 - val_acc: 0.9066\n",
      "Epoch 170/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9150\n",
      "Epoch 170: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3111 - acc: 0.9156 - val_loss: 0.3321 - val_acc: 0.9038\n",
      "Epoch 171/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9142\n",
      "Epoch 171: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2828 - acc: 0.9142 - val_loss: 0.3194 - val_acc: 0.9062\n",
      "Epoch 172/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9111\n",
      "Epoch 172: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3057 - acc: 0.9114 - val_loss: 0.3350 - val_acc: 0.8969\n",
      "Epoch 173/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3168 - acc: 0.9108\n",
      "Epoch 173: val_acc did not improve from 0.90784\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3168 - acc: 0.9108 - val_loss: 0.3416 - val_acc: 0.9046\n",
      "Epoch 174/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9130\n",
      "Epoch 174: val_acc improved from 0.90784 to 0.91190, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2853 - acc: 0.9131 - val_loss: 0.3557 - val_acc: 0.9119\n",
      "Epoch 175/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9138\n",
      "Epoch 175: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3063 - acc: 0.9139 - val_loss: 0.3350 - val_acc: 0.9099\n",
      "Epoch 176/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9127\n",
      "Epoch 176: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2863 - acc: 0.9129 - val_loss: 0.3465 - val_acc: 0.9030\n",
      "Epoch 177/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9107\n",
      "Epoch 177: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2841 - acc: 0.9109 - val_loss: 0.3521 - val_acc: 0.8997\n",
      "Epoch 178/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.9123\n",
      "Epoch 178: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2737 - acc: 0.9124 - val_loss: 0.3419 - val_acc: 0.9062\n",
      "Epoch 179/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9110\n",
      "Epoch 179: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3050 - acc: 0.9116 - val_loss: 0.3476 - val_acc: 0.9066\n",
      "Epoch 180/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9126\n",
      "Epoch 180: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2978 - acc: 0.9125 - val_loss: 0.3551 - val_acc: 0.9070\n",
      "Epoch 181/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.9151\n",
      "Epoch 181: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2979 - acc: 0.9151 - val_loss: 0.3309 - val_acc: 0.9013\n",
      "Epoch 182/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9122\n",
      "Epoch 182: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2931 - acc: 0.9123 - val_loss: 0.3370 - val_acc: 0.9050\n",
      "Epoch 183/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9153\n",
      "Epoch 183: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2872 - acc: 0.9154 - val_loss: 0.3140 - val_acc: 0.9103\n",
      "Epoch 184/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9143\n",
      "Epoch 184: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2705 - acc: 0.9146 - val_loss: 0.3577 - val_acc: 0.9046\n",
      "Epoch 185/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9141\n",
      "Epoch 185: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2944 - acc: 0.9143 - val_loss: 0.3463 - val_acc: 0.9058\n",
      "Epoch 186/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9104\n",
      "Epoch 186: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2804 - acc: 0.9105 - val_loss: 0.3186 - val_acc: 0.9070\n",
      "Epoch 187/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9138\n",
      "Epoch 187: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2871 - acc: 0.9139 - val_loss: 0.3181 - val_acc: 0.9091\n",
      "Epoch 188/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9153\n",
      "Epoch 188: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2976 - acc: 0.9155 - val_loss: 0.3336 - val_acc: 0.9070\n",
      "Epoch 189/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2933 - acc: 0.9140\n",
      "Epoch 189: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2933 - acc: 0.9140 - val_loss: 0.3857 - val_acc: 0.8961\n",
      "Epoch 190/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3007 - acc: 0.9179\n",
      "Epoch 190: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3007 - acc: 0.9179 - val_loss: 0.3380 - val_acc: 0.9058\n",
      "Epoch 191/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9086\n",
      "Epoch 191: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3103 - acc: 0.9088 - val_loss: 0.3269 - val_acc: 0.9038\n",
      "Epoch 192/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9123\n",
      "Epoch 192: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2938 - acc: 0.9126 - val_loss: 0.3338 - val_acc: 0.9026\n",
      "Epoch 193/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9130\n",
      "Epoch 193: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2770 - acc: 0.9130 - val_loss: 0.3298 - val_acc: 0.9066\n",
      "Epoch 194/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9139\n",
      "Epoch 194: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2823 - acc: 0.9140 - val_loss: 0.3412 - val_acc: 0.9062\n",
      "Epoch 195/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9134\n",
      "Epoch 195: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3022 - acc: 0.9140 - val_loss: 0.3374 - val_acc: 0.9026\n",
      "Epoch 196/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2992 - acc: 0.9080\n",
      "Epoch 196: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2992 - acc: 0.9080 - val_loss: 0.3292 - val_acc: 0.9054\n",
      "Epoch 197/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9120\n",
      "Epoch 197: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2884 - acc: 0.9119 - val_loss: 0.3468 - val_acc: 0.9058\n",
      "Epoch 198/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9150\n",
      "Epoch 198: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2884 - acc: 0.9152 - val_loss: 0.3267 - val_acc: 0.9054\n",
      "Epoch 199/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9165\n",
      "Epoch 199: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2932 - acc: 0.9164 - val_loss: 0.3396 - val_acc: 0.9030\n",
      "Epoch 200/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9149\n",
      "Epoch 200: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2833 - acc: 0.9148 - val_loss: 0.3461 - val_acc: 0.9058\n",
      "Epoch 201/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9166\n",
      "Epoch 201: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2930 - acc: 0.9166 - val_loss: 0.3252 - val_acc: 0.9103\n",
      "Epoch 202/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9141\n",
      "Epoch 202: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3003 - acc: 0.9144 - val_loss: 0.3318 - val_acc: 0.9062\n",
      "Epoch 203/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9141\n",
      "Epoch 203: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3117 - acc: 0.9142 - val_loss: 0.3471 - val_acc: 0.9058\n",
      "Epoch 204/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9140\n",
      "Epoch 204: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2757 - acc: 0.9141 - val_loss: 0.3248 - val_acc: 0.9070\n",
      "Epoch 205/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9174\n",
      "Epoch 205: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2981 - acc: 0.9177 - val_loss: 0.3588 - val_acc: 0.9074\n",
      "Epoch 206/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9126\n",
      "Epoch 206: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2848 - acc: 0.9126 - val_loss: 0.3337 - val_acc: 0.9054\n",
      "Epoch 207/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9117\n",
      "Epoch 207: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2883 - acc: 0.9120 - val_loss: 0.3592 - val_acc: 0.9107\n",
      "Epoch 208/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9155\n",
      "Epoch 208: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3153 - acc: 0.9156 - val_loss: 0.3286 - val_acc: 0.9046\n",
      "Epoch 209/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2937 - acc: 0.9131\n",
      "Epoch 209: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2937 - acc: 0.9131 - val_loss: 0.3324 - val_acc: 0.9082\n",
      "Epoch 210/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3053 - acc: 0.9138\n",
      "Epoch 210: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3053 - acc: 0.9138 - val_loss: 0.3356 - val_acc: 0.9054\n",
      "Epoch 211/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9150\n",
      "Epoch 211: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2807 - acc: 0.9151 - val_loss: 0.3715 - val_acc: 0.9086\n",
      "Epoch 212/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9154\n",
      "Epoch 212: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3001 - acc: 0.9155 - val_loss: 0.3329 - val_acc: 0.9086\n",
      "Epoch 213/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9172\n",
      "Epoch 213: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3045 - acc: 0.9174 - val_loss: 0.3826 - val_acc: 0.9050\n",
      "Epoch 214/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9122\n",
      "Epoch 214: val_acc did not improve from 0.91190\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3054 - acc: 0.9122 - val_loss: 0.3466 - val_acc: 0.9095\n",
      "Epoch 215/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9143\n",
      "Epoch 215: val_acc improved from 0.91190 to 0.91352, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3170 - acc: 0.9145 - val_loss: 0.3386 - val_acc: 0.9135\n",
      "Epoch 216/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9159\n",
      "Epoch 216: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3136 - acc: 0.9159 - val_loss: 0.3310 - val_acc: 0.9111\n",
      "Epoch 217/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9185\n",
      "Epoch 217: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3025 - acc: 0.9185 - val_loss: 0.3905 - val_acc: 0.8932\n",
      "Epoch 218/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.9137\n",
      "Epoch 218: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3273 - acc: 0.9140 - val_loss: 0.3272 - val_acc: 0.9095\n",
      "Epoch 219/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9164\n",
      "Epoch 219: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3134 - acc: 0.9167 - val_loss: 0.3412 - val_acc: 0.9127\n",
      "Epoch 220/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9136\n",
      "Epoch 220: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3085 - acc: 0.9137 - val_loss: 0.3450 - val_acc: 0.9123\n",
      "Epoch 221/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2795 - acc: 0.9174\n",
      "Epoch 221: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2795 - acc: 0.9174 - val_loss: 0.3782 - val_acc: 0.9086\n",
      "Epoch 222/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9145\n",
      "Epoch 222: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2922 - acc: 0.9148 - val_loss: 0.3307 - val_acc: 0.9070\n",
      "Epoch 223/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9176\n",
      "Epoch 223: val_acc did not improve from 0.91352\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2914 - acc: 0.9178 - val_loss: 0.3514 - val_acc: 0.9038\n",
      "Epoch 224/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9157\n",
      "Epoch 224: val_acc improved from 0.91352 to 0.91433, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2811 - acc: 0.9157 - val_loss: 0.3241 - val_acc: 0.9143\n",
      "Epoch 225/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9171\n",
      "Epoch 225: val_acc did not improve from 0.91433\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2871 - acc: 0.9175 - val_loss: 0.3648 - val_acc: 0.9119\n",
      "Epoch 226/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9172\n",
      "Epoch 226: val_acc did not improve from 0.91433\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2904 - acc: 0.9173 - val_loss: 0.3594 - val_acc: 0.9111\n",
      "Epoch 227/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9177\n",
      "Epoch 227: val_acc did not improve from 0.91433\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2887 - acc: 0.9176 - val_loss: 0.3161 - val_acc: 0.9050\n",
      "Epoch 228/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9151\n",
      "Epoch 228: val_acc did not improve from 0.91433\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2786 - acc: 0.9153 - val_loss: 0.4110 - val_acc: 0.9091\n",
      "Epoch 229/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.9170\n",
      "Epoch 229: val_acc did not improve from 0.91433\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3268 - acc: 0.9170 - val_loss: 0.3640 - val_acc: 0.9095\n",
      "Epoch 230/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9184\n",
      "Epoch 230: val_acc did not improve from 0.91433\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2941 - acc: 0.9184 - val_loss: 0.3408 - val_acc: 0.9111\n",
      "Epoch 231/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.9172\n",
      "Epoch 231: val_acc improved from 0.91433 to 0.91474, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3253 - acc: 0.9171 - val_loss: 0.3224 - val_acc: 0.9147\n",
      "Epoch 232/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9143\n",
      "Epoch 232: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2901 - acc: 0.9144 - val_loss: 0.3564 - val_acc: 0.9078\n",
      "Epoch 233/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9140\n",
      "Epoch 233: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3013 - acc: 0.9144 - val_loss: 0.3284 - val_acc: 0.9099\n",
      "Epoch 234/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9142\n",
      "Epoch 234: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3053 - acc: 0.9144 - val_loss: 0.3244 - val_acc: 0.9086\n",
      "Epoch 235/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.9171\n",
      "Epoch 235: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3109 - acc: 0.9176 - val_loss: 0.3590 - val_acc: 0.9091\n",
      "Epoch 236/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9154\n",
      "Epoch 236: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2948 - acc: 0.9158 - val_loss: 0.3431 - val_acc: 0.9135\n",
      "Epoch 237/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2929 - acc: 0.9172\n",
      "Epoch 237: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2929 - acc: 0.9172 - val_loss: 0.3471 - val_acc: 0.9127\n",
      "Epoch 238/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9207\n",
      "Epoch 238: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3017 - acc: 0.9208 - val_loss: 0.3216 - val_acc: 0.9139\n",
      "Epoch 239/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9211\n",
      "Epoch 239: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3023 - acc: 0.9214 - val_loss: 0.3863 - val_acc: 0.9082\n",
      "Epoch 240/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.9165\n",
      "Epoch 240: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3156 - acc: 0.9167 - val_loss: 0.3809 - val_acc: 0.9091\n",
      "Epoch 241/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2854 - acc: 0.9176\n",
      "Epoch 241: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2842 - acc: 0.9180 - val_loss: 0.3845 - val_acc: 0.9099\n",
      "Epoch 242/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.9171\n",
      "Epoch 242: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3276 - acc: 0.9170 - val_loss: 0.3203 - val_acc: 0.9135\n",
      "Epoch 243/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9166\n",
      "Epoch 243: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3059 - acc: 0.9171 - val_loss: 0.3533 - val_acc: 0.9127\n",
      "Epoch 244/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9181\n",
      "Epoch 244: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3163 - acc: 0.9179 - val_loss: 0.3685 - val_acc: 0.9091\n",
      "Epoch 245/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9176\n",
      "Epoch 245: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2924 - acc: 0.9176 - val_loss: 0.4000 - val_acc: 0.9143\n",
      "Epoch 246/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.9193\n",
      "Epoch 246: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3241 - acc: 0.9192 - val_loss: 0.3437 - val_acc: 0.9082\n",
      "Epoch 247/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9189\n",
      "Epoch 247: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3041 - acc: 0.9189 - val_loss: 0.3591 - val_acc: 0.9111\n",
      "Epoch 248/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9171\n",
      "Epoch 248: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2815 - acc: 0.9172 - val_loss: 0.3836 - val_acc: 0.9147\n",
      "Epoch 249/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9190\n",
      "Epoch 249: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2961 - acc: 0.9190 - val_loss: 0.3195 - val_acc: 0.9095\n",
      "Epoch 250/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9170\n",
      "Epoch 250: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 5s 9ms/step - loss: 0.2986 - acc: 0.9175 - val_loss: 0.3399 - val_acc: 0.9143\n",
      "Epoch 251/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3154 - acc: 0.9171\n",
      "Epoch 251: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3154 - acc: 0.9171 - val_loss: 0.3452 - val_acc: 0.9123\n",
      "Epoch 252/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9201\n",
      "Epoch 252: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3139 - acc: 0.9200 - val_loss: 0.3717 - val_acc: 0.9123\n",
      "Epoch 253/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9224\n",
      "Epoch 253: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3005 - acc: 0.9226 - val_loss: 0.3468 - val_acc: 0.9086\n",
      "Epoch 254/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9222\n",
      "Epoch 254: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2770 - acc: 0.9224 - val_loss: 0.4408 - val_acc: 0.9135\n",
      "Epoch 255/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.9196\n",
      "Epoch 255: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3367 - acc: 0.9197 - val_loss: 0.3553 - val_acc: 0.9103\n",
      "Epoch 256/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9209\n",
      "Epoch 256: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2891 - acc: 0.9211 - val_loss: 0.3249 - val_acc: 0.9123\n",
      "Epoch 257/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9198\n",
      "Epoch 257: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3011 - acc: 0.9200 - val_loss: 0.3463 - val_acc: 0.9131\n",
      "Epoch 258/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2810 - acc: 0.9211\n",
      "Epoch 258: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2810 - acc: 0.9211 - val_loss: 0.3737 - val_acc: 0.9091\n",
      "Epoch 259/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9196\n",
      "Epoch 259: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 5s 9ms/step - loss: 0.2792 - acc: 0.9202 - val_loss: 0.3539 - val_acc: 0.9111\n",
      "Epoch 260/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9182\n",
      "Epoch 260: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.2965 - acc: 0.9185 - val_loss: 0.3624 - val_acc: 0.9091\n",
      "Epoch 261/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9239\n",
      "Epoch 261: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2610 - acc: 0.9242 - val_loss: 0.3875 - val_acc: 0.9127\n",
      "Epoch 262/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9225\n",
      "Epoch 262: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2782 - acc: 0.9223 - val_loss: 0.3309 - val_acc: 0.9111\n",
      "Epoch 263/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9201\n",
      "Epoch 263: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3076 - acc: 0.9203 - val_loss: 0.3279 - val_acc: 0.9131\n",
      "Epoch 264/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9178\n",
      "Epoch 264: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2972 - acc: 0.9178 - val_loss: 0.3496 - val_acc: 0.9119\n",
      "Epoch 265/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9160\n",
      "Epoch 265: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3097 - acc: 0.9162 - val_loss: 0.3425 - val_acc: 0.9115\n",
      "Epoch 266/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9185\n",
      "Epoch 266: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3036 - acc: 0.9188 - val_loss: 0.3670 - val_acc: 0.9119\n",
      "Epoch 267/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9163\n",
      "Epoch 267: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2983 - acc: 0.9167 - val_loss: 0.3681 - val_acc: 0.9127\n",
      "Epoch 268/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2909 - acc: 0.9198\n",
      "Epoch 268: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2909 - acc: 0.9198 - val_loss: 0.3555 - val_acc: 0.9074\n",
      "Epoch 269/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3035 - acc: 0.9189\n",
      "Epoch 269: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3035 - acc: 0.9189 - val_loss: 0.3623 - val_acc: 0.9123\n",
      "Epoch 270/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9220\n",
      "Epoch 270: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2735 - acc: 0.9223 - val_loss: 0.3424 - val_acc: 0.9115\n",
      "Epoch 271/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9209\n",
      "Epoch 271: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3021 - acc: 0.9212 - val_loss: 0.3418 - val_acc: 0.9099\n",
      "Epoch 272/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9197\n",
      "Epoch 272: val_acc did not improve from 0.91474\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3097 - acc: 0.9197 - val_loss: 0.3579 - val_acc: 0.9078\n",
      "Epoch 273/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9204\n",
      "Epoch 273: val_acc improved from 0.91474 to 0.91555, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.2799 - acc: 0.9207 - val_loss: 0.3520 - val_acc: 0.9156\n",
      "Epoch 274/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.9197\n",
      "Epoch 274: val_acc did not improve from 0.91555\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3172 - acc: 0.9197 - val_loss: 0.3337 - val_acc: 0.9123\n",
      "Epoch 275/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3012 - acc: 0.9170\n",
      "Epoch 275: val_acc improved from 0.91555 to 0.91880, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3012 - acc: 0.9170 - val_loss: 0.3298 - val_acc: 0.9188\n",
      "Epoch 276/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9219\n",
      "Epoch 276: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3209 - acc: 0.9217 - val_loss: 0.3485 - val_acc: 0.9164\n",
      "Epoch 277/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9201\n",
      "Epoch 277: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2973 - acc: 0.9205 - val_loss: 0.3436 - val_acc: 0.9160\n",
      "Epoch 278/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9196\n",
      "Epoch 278: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2829 - acc: 0.9196 - val_loss: 0.3412 - val_acc: 0.9091\n",
      "Epoch 279/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.9168\n",
      "Epoch 279: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3322 - acc: 0.9168 - val_loss: 0.3296 - val_acc: 0.9115\n",
      "Epoch 280/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.9234\n",
      "Epoch 280: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3272 - acc: 0.9240 - val_loss: 0.3690 - val_acc: 0.9127\n",
      "Epoch 281/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9223\n",
      "Epoch 281: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2817 - acc: 0.9225 - val_loss: 0.3814 - val_acc: 0.9030\n",
      "Epoch 282/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2871 - acc: 0.9194\n",
      "Epoch 282: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2871 - acc: 0.9194 - val_loss: 0.3382 - val_acc: 0.9078\n",
      "Epoch 283/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9225\n",
      "Epoch 283: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 5s 8ms/step - loss: 0.3015 - acc: 0.9226 - val_loss: 0.3500 - val_acc: 0.9111\n",
      "Epoch 284/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9223\n",
      "Epoch 284: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2731 - acc: 0.9224 - val_loss: 0.3560 - val_acc: 0.9086\n",
      "Epoch 285/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3104 - acc: 0.9209\n",
      "Epoch 285: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3104 - acc: 0.9209 - val_loss: 0.3813 - val_acc: 0.9095\n",
      "Epoch 286/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3219 - acc: 0.9255\n",
      "Epoch 286: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3219 - acc: 0.9255 - val_loss: 0.3420 - val_acc: 0.9111\n",
      "Epoch 287/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9203\n",
      "Epoch 287: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2959 - acc: 0.9204 - val_loss: 0.3483 - val_acc: 0.9119\n",
      "Epoch 288/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2932 - acc: 0.9171\n",
      "Epoch 288: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2932 - acc: 0.9171 - val_loss: 0.3531 - val_acc: 0.9115\n",
      "Epoch 289/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9203\n",
      "Epoch 289: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3015 - acc: 0.9205 - val_loss: 0.3724 - val_acc: 0.9095\n",
      "Epoch 290/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9208\n",
      "Epoch 290: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2708 - acc: 0.9208 - val_loss: 0.4381 - val_acc: 0.9127\n",
      "Epoch 291/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9205\n",
      "Epoch 291: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3199 - acc: 0.9210 - val_loss: 0.3526 - val_acc: 0.9135\n",
      "Epoch 292/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9184\n",
      "Epoch 292: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3181 - acc: 0.9182 - val_loss: 0.3465 - val_acc: 0.9135\n",
      "Epoch 293/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9214\n",
      "Epoch 293: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3098 - acc: 0.9217 - val_loss: 0.3280 - val_acc: 0.9107\n",
      "Epoch 294/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3076 - acc: 0.9212\n",
      "Epoch 294: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3076 - acc: 0.9212 - val_loss: 0.3589 - val_acc: 0.9127\n",
      "Epoch 295/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3003 - acc: 0.9207\n",
      "Epoch 295: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3003 - acc: 0.9207 - val_loss: 0.3596 - val_acc: 0.9151\n",
      "Epoch 296/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9215\n",
      "Epoch 296: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2895 - acc: 0.9221 - val_loss: 0.4027 - val_acc: 0.9147\n",
      "Epoch 297/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3270 - acc: 0.9210\n",
      "Epoch 297: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3270 - acc: 0.9210 - val_loss: 0.3618 - val_acc: 0.9176\n",
      "Epoch 298/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9194\n",
      "Epoch 298: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3153 - acc: 0.9194 - val_loss: 0.3290 - val_acc: 0.9127\n",
      "Epoch 299/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9192\n",
      "Epoch 299: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3171 - acc: 0.9194 - val_loss: 0.3568 - val_acc: 0.9123\n",
      "Epoch 300/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2913 - acc: 0.9213\n",
      "Epoch 300: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2913 - acc: 0.9213 - val_loss: 0.3275 - val_acc: 0.9119\n",
      "Epoch 301/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9232\n",
      "Epoch 301: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3032 - acc: 0.9232 - val_loss: 0.3318 - val_acc: 0.9111\n",
      "Epoch 302/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9205\n",
      "Epoch 302: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3004 - acc: 0.9207 - val_loss: 0.3565 - val_acc: 0.9095\n",
      "Epoch 303/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.9251\n",
      "Epoch 303: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3038 - acc: 0.9249 - val_loss: 0.3653 - val_acc: 0.9095\n",
      "Epoch 304/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3195 - acc: 0.9251\n",
      "Epoch 304: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3195 - acc: 0.9251 - val_loss: 0.3830 - val_acc: 0.9119\n",
      "Epoch 305/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3404 - acc: 0.9267\n",
      "Epoch 305: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3404 - acc: 0.9267 - val_loss: 0.3423 - val_acc: 0.9147\n",
      "Epoch 306/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9236\n",
      "Epoch 306: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3026 - acc: 0.9238 - val_loss: 0.3604 - val_acc: 0.9086\n",
      "Epoch 307/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9231\n",
      "Epoch 307: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3062 - acc: 0.9233 - val_loss: 0.3625 - val_acc: 0.9135\n",
      "Epoch 308/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9220\n",
      "Epoch 308: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3063 - acc: 0.9220 - val_loss: 0.3794 - val_acc: 0.9111\n",
      "Epoch 309/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9203\n",
      "Epoch 309: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2811 - acc: 0.9204 - val_loss: 0.3712 - val_acc: 0.9066\n",
      "Epoch 310/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9224\n",
      "Epoch 310: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2898 - acc: 0.9227 - val_loss: 0.3675 - val_acc: 0.9095\n",
      "Epoch 311/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.9211\n",
      "Epoch 311: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3122 - acc: 0.9211 - val_loss: 0.3668 - val_acc: 0.9107\n",
      "Epoch 312/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.9237\n",
      "Epoch 312: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3164 - acc: 0.9239 - val_loss: 0.4120 - val_acc: 0.9115\n",
      "Epoch 313/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.9210\n",
      "Epoch 313: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3364 - acc: 0.9213 - val_loss: 0.3715 - val_acc: 0.9127\n",
      "Epoch 314/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2951 - acc: 0.9203\n",
      "Epoch 314: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2951 - acc: 0.9203 - val_loss: 0.3946 - val_acc: 0.9070\n",
      "Epoch 315/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9202\n",
      "Epoch 315: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2955 - acc: 0.9203 - val_loss: 0.4307 - val_acc: 0.9115\n",
      "Epoch 316/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9224\n",
      "Epoch 316: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2982 - acc: 0.9227 - val_loss: 0.3700 - val_acc: 0.9103\n",
      "Epoch 317/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3106 - acc: 0.9202\n",
      "Epoch 317: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3106 - acc: 0.9202 - val_loss: 0.3354 - val_acc: 0.9103\n",
      "Epoch 318/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9201\n",
      "Epoch 318: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2961 - acc: 0.9206 - val_loss: 0.3632 - val_acc: 0.9095\n",
      "Epoch 319/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.9224\n",
      "Epoch 319: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3161 - acc: 0.9224 - val_loss: 0.4025 - val_acc: 0.9147\n",
      "Epoch 320/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9183\n",
      "Epoch 320: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3068 - acc: 0.9183 - val_loss: 0.3764 - val_acc: 0.9103\n",
      "Epoch 321/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.9237\n",
      "Epoch 321: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3145 - acc: 0.9239 - val_loss: 0.3723 - val_acc: 0.9107\n",
      "Epoch 322/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.9251\n",
      "Epoch 322: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3348 - acc: 0.9252 - val_loss: 0.4018 - val_acc: 0.9119\n",
      "Epoch 323/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3211 - acc: 0.9223\n",
      "Epoch 323: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3211 - acc: 0.9223 - val_loss: 0.4601 - val_acc: 0.9078\n",
      "Epoch 324/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.9211\n",
      "Epoch 324: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3364 - acc: 0.9212 - val_loss: 0.3553 - val_acc: 0.9123\n",
      "Epoch 325/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9233\n",
      "Epoch 325: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3105 - acc: 0.9232 - val_loss: 0.3876 - val_acc: 0.9099\n",
      "Epoch 326/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.9193\n",
      "Epoch 326: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3230 - acc: 0.9196 - val_loss: 0.3379 - val_acc: 0.9095\n",
      "Epoch 327/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9192\n",
      "Epoch 327: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2923 - acc: 0.9192 - val_loss: 0.3489 - val_acc: 0.9119\n",
      "Epoch 328/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9203\n",
      "Epoch 328: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3052 - acc: 0.9206 - val_loss: 0.3692 - val_acc: 0.9172\n",
      "Epoch 329/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9293\n",
      "Epoch 329: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3055 - acc: 0.9293 - val_loss: 0.3537 - val_acc: 0.9176\n",
      "Epoch 330/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.9216\n",
      "Epoch 330: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3269 - acc: 0.9222 - val_loss: 0.3501 - val_acc: 0.9168\n",
      "Epoch 331/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9233\n",
      "Epoch 331: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3028 - acc: 0.9236 - val_loss: 0.3813 - val_acc: 0.9131\n",
      "Epoch 332/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9230\n",
      "Epoch 332: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2996 - acc: 0.9231 - val_loss: 0.3711 - val_acc: 0.9164\n",
      "Epoch 333/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3213 - acc: 0.9250\n",
      "Epoch 333: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3213 - acc: 0.9250 - val_loss: 0.3625 - val_acc: 0.9082\n",
      "Epoch 334/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9201\n",
      "Epoch 334: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3307 - acc: 0.9199 - val_loss: 0.3201 - val_acc: 0.9103\n",
      "Epoch 335/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9237\n",
      "Epoch 335: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2996 - acc: 0.9236 - val_loss: 0.3166 - val_acc: 0.9156\n",
      "Epoch 336/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.9218\n",
      "Epoch 336: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3212 - acc: 0.9219 - val_loss: 0.3164 - val_acc: 0.9139\n",
      "Epoch 337/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9237\n",
      "Epoch 337: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2961 - acc: 0.9239 - val_loss: 0.3823 - val_acc: 0.9147\n",
      "Epoch 338/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9249\n",
      "Epoch 338: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3191 - acc: 0.9251 - val_loss: 0.3810 - val_acc: 0.9135\n",
      "Epoch 339/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9215\n",
      "Epoch 339: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2840 - acc: 0.9219 - val_loss: 0.3747 - val_acc: 0.9151\n",
      "Epoch 340/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.9243\n",
      "Epoch 340: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3524 - acc: 0.9246 - val_loss: 0.3704 - val_acc: 0.9127\n",
      "Epoch 341/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.9219\n",
      "Epoch 341: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3289 - acc: 0.9221 - val_loss: 0.3425 - val_acc: 0.9086\n",
      "Epoch 342/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9192\n",
      "Epoch 342: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3224 - acc: 0.9191 - val_loss: 0.3955 - val_acc: 0.9135\n",
      "Epoch 343/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3006 - acc: 0.9272\n",
      "Epoch 343: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3006 - acc: 0.9272 - val_loss: 0.3648 - val_acc: 0.9111\n",
      "Epoch 344/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.9251\n",
      "Epoch 344: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3419 - acc: 0.9249 - val_loss: 0.3817 - val_acc: 0.9184\n",
      "Epoch 345/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3469 - acc: 0.9274\n",
      "Epoch 345: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3469 - acc: 0.9274 - val_loss: 0.3355 - val_acc: 0.9156\n",
      "Epoch 346/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.9233\n",
      "Epoch 346: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3508 - acc: 0.9231 - val_loss: 0.3915 - val_acc: 0.9123\n",
      "Epoch 347/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.9261\n",
      "Epoch 347: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3094 - acc: 0.9261 - val_loss: 0.3638 - val_acc: 0.9123\n",
      "Epoch 348/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.9222\n",
      "Epoch 348: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3227 - acc: 0.9223 - val_loss: 0.3940 - val_acc: 0.9131\n",
      "Epoch 349/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9249\n",
      "Epoch 349: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3352 - acc: 0.9251 - val_loss: 0.3965 - val_acc: 0.9123\n",
      "Epoch 350/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.9239\n",
      "Epoch 350: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3272 - acc: 0.9243 - val_loss: 0.4090 - val_acc: 0.9143\n",
      "Epoch 351/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9233\n",
      "Epoch 351: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3019 - acc: 0.9233 - val_loss: 0.3496 - val_acc: 0.9168\n",
      "Epoch 352/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.9260\n",
      "Epoch 352: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3161 - acc: 0.9263 - val_loss: 0.3785 - val_acc: 0.9160\n",
      "Epoch 353/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.9241\n",
      "Epoch 353: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3375 - acc: 0.9244 - val_loss: 0.3773 - val_acc: 0.9180\n",
      "Epoch 354/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9251\n",
      "Epoch 354: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2971 - acc: 0.9252 - val_loss: 0.4051 - val_acc: 0.9184\n",
      "Epoch 355/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.9230\n",
      "Epoch 355: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3329 - acc: 0.9231 - val_loss: 0.3627 - val_acc: 0.9160\n",
      "Epoch 356/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.9269\n",
      "Epoch 356: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3253 - acc: 0.9269 - val_loss: 0.3666 - val_acc: 0.9131\n",
      "Epoch 357/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2965 - acc: 0.9235\n",
      "Epoch 357: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2965 - acc: 0.9235 - val_loss: 0.3805 - val_acc: 0.9143\n",
      "Epoch 358/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9294\n",
      "Epoch 358: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3074 - acc: 0.9297 - val_loss: 0.4297 - val_acc: 0.9123\n",
      "Epoch 359/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3163 - acc: 0.9239\n",
      "Epoch 359: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3163 - acc: 0.9239 - val_loss: 0.3893 - val_acc: 0.9111\n",
      "Epoch 360/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.9224\n",
      "Epoch 360: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3607 - acc: 0.9227 - val_loss: 0.3837 - val_acc: 0.9168\n",
      "Epoch 361/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9239\n",
      "Epoch 361: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3061 - acc: 0.9241 - val_loss: 0.3635 - val_acc: 0.9184\n",
      "Epoch 362/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.9218\n",
      "Epoch 362: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3650 - acc: 0.9221 - val_loss: 0.3810 - val_acc: 0.9127\n",
      "Epoch 363/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.9234\n",
      "Epoch 363: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3309 - acc: 0.9235 - val_loss: 0.3709 - val_acc: 0.9127\n",
      "Epoch 364/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.9235\n",
      "Epoch 364: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3333 - acc: 0.9235 - val_loss: 0.3668 - val_acc: 0.9160\n",
      "Epoch 365/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.9257\n",
      "Epoch 365: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3280 - acc: 0.9255 - val_loss: 0.4183 - val_acc: 0.9143\n",
      "Epoch 366/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.9205\n",
      "Epoch 366: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3494 - acc: 0.9210 - val_loss: 0.4027 - val_acc: 0.9156\n",
      "Epoch 367/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3393 - acc: 0.9241\n",
      "Epoch 367: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3393 - acc: 0.9241 - val_loss: 0.3734 - val_acc: 0.9143\n",
      "Epoch 368/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3028 - acc: 0.9286\n",
      "Epoch 368: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3028 - acc: 0.9286 - val_loss: 0.4008 - val_acc: 0.9086\n",
      "Epoch 369/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.9251\n",
      "Epoch 369: val_acc did not improve from 0.91880\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3550 - acc: 0.9252 - val_loss: 0.3962 - val_acc: 0.9147\n",
      "Epoch 370/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.9218\n",
      "Epoch 370: val_acc improved from 0.91880 to 0.92083, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3310 - acc: 0.9220 - val_loss: 0.3969 - val_acc: 0.9208\n",
      "Epoch 371/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.2956 - acc: 0.9258\n",
      "Epoch 371: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2956 - acc: 0.9258 - val_loss: 0.3697 - val_acc: 0.9176\n",
      "Epoch 372/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.9218\n",
      "Epoch 372: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3212 - acc: 0.9220 - val_loss: 0.3618 - val_acc: 0.9139\n",
      "Epoch 373/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.9240\n",
      "Epoch 373: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3670 - acc: 0.9238 - val_loss: 0.3285 - val_acc: 0.9115\n",
      "Epoch 374/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.9205\n",
      "Epoch 374: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3510 - acc: 0.9207 - val_loss: 0.3717 - val_acc: 0.9172\n",
      "Epoch 375/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.9232\n",
      "Epoch 375: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3480 - acc: 0.9232 - val_loss: 0.3947 - val_acc: 0.9208\n",
      "Epoch 376/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.9223\n",
      "Epoch 376: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3334 - acc: 0.9224 - val_loss: 0.3948 - val_acc: 0.9147\n",
      "Epoch 377/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.9250\n",
      "Epoch 377: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3355 - acc: 0.9252 - val_loss: 0.4371 - val_acc: 0.9139\n",
      "Epoch 378/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.9245\n",
      "Epoch 378: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3788 - acc: 0.9242 - val_loss: 0.3699 - val_acc: 0.9135\n",
      "Epoch 379/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3421 - acc: 0.9301\n",
      "Epoch 379: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3421 - acc: 0.9301 - val_loss: 0.3952 - val_acc: 0.9160\n",
      "Epoch 380/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.9235\n",
      "Epoch 380: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3774 - acc: 0.9235 - val_loss: 0.3923 - val_acc: 0.9139\n",
      "Epoch 381/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.9241\n",
      "Epoch 381: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3209 - acc: 0.9244 - val_loss: 0.4246 - val_acc: 0.9204\n",
      "Epoch 382/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.9226\n",
      "Epoch 382: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3140 - acc: 0.9227 - val_loss: 0.3804 - val_acc: 0.9176\n",
      "Epoch 383/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.9258\n",
      "Epoch 383: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3079 - acc: 0.9260 - val_loss: 0.4040 - val_acc: 0.9151\n",
      "Epoch 384/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3493 - acc: 0.9250\n",
      "Epoch 384: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3493 - acc: 0.9250 - val_loss: 0.3729 - val_acc: 0.9192\n",
      "Epoch 385/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9264\n",
      "Epoch 385: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2958 - acc: 0.9266 - val_loss: 0.3732 - val_acc: 0.9091\n",
      "Epoch 386/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9230\n",
      "Epoch 386: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3047 - acc: 0.9233 - val_loss: 0.4305 - val_acc: 0.9196\n",
      "Epoch 387/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.9215\n",
      "Epoch 387: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3257 - acc: 0.9219 - val_loss: 0.4133 - val_acc: 0.9160\n",
      "Epoch 388/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.9259\n",
      "Epoch 388: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3398 - acc: 0.9259 - val_loss: 0.3575 - val_acc: 0.9156\n",
      "Epoch 389/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3346 - acc: 0.9244\n",
      "Epoch 389: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3346 - acc: 0.9244 - val_loss: 0.3407 - val_acc: 0.9176\n",
      "Epoch 390/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.9194\n",
      "Epoch 390: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3289 - acc: 0.9194 - val_loss: 0.3724 - val_acc: 0.9147\n",
      "Epoch 391/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3855 - acc: 0.9265\n",
      "Epoch 391: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3855 - acc: 0.9265 - val_loss: 0.3794 - val_acc: 0.9139\n",
      "Epoch 392/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.9305\n",
      "Epoch 392: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3231 - acc: 0.9306 - val_loss: 0.3953 - val_acc: 0.9176\n",
      "Epoch 393/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.9244\n",
      "Epoch 393: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3142 - acc: 0.9247 - val_loss: 0.3508 - val_acc: 0.9172\n",
      "Epoch 394/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9240\n",
      "Epoch 394: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3060 - acc: 0.9240 - val_loss: 0.3653 - val_acc: 0.9151\n",
      "Epoch 395/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9255\n",
      "Epoch 395: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3002 - acc: 0.9254 - val_loss: 0.3842 - val_acc: 0.9147\n",
      "Epoch 396/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.9268\n",
      "Epoch 396: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3109 - acc: 0.9268 - val_loss: 0.4317 - val_acc: 0.9066\n",
      "Epoch 397/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.9263\n",
      "Epoch 397: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3281 - acc: 0.9264 - val_loss: 0.3472 - val_acc: 0.9172\n",
      "Epoch 398/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.9239\n",
      "Epoch 398: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3222 - acc: 0.9240 - val_loss: 0.3704 - val_acc: 0.9168\n",
      "Epoch 399/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9256\n",
      "Epoch 399: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2987 - acc: 0.9258 - val_loss: 0.3794 - val_acc: 0.9131\n",
      "Epoch 400/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.9243\n",
      "Epoch 400: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3273 - acc: 0.9246 - val_loss: 0.3400 - val_acc: 0.9123\n",
      "Epoch 401/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.9221\n",
      "Epoch 401: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3403 - acc: 0.9222 - val_loss: 0.3672 - val_acc: 0.9196\n",
      "Epoch 402/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9282\n",
      "Epoch 402: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3113 - acc: 0.9285 - val_loss: 0.4268 - val_acc: 0.9143\n",
      "Epoch 403/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.9295\n",
      "Epoch 403: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3257 - acc: 0.9296 - val_loss: 0.4057 - val_acc: 0.9143\n",
      "Epoch 404/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.9282\n",
      "Epoch 404: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3433 - acc: 0.9283 - val_loss: 0.3743 - val_acc: 0.9164\n",
      "Epoch 405/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3319 - acc: 0.9270\n",
      "Epoch 405: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3319 - acc: 0.9270 - val_loss: 0.3741 - val_acc: 0.9131\n",
      "Epoch 406/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.9265\n",
      "Epoch 406: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3281 - acc: 0.9269 - val_loss: 0.4381 - val_acc: 0.9103\n",
      "Epoch 407/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.9228\n",
      "Epoch 407: val_acc did not improve from 0.92083\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3323 - acc: 0.9228 - val_loss: 0.3497 - val_acc: 0.9139\n",
      "Epoch 408/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.9276\n",
      "Epoch 408: val_acc improved from 0.92083 to 0.92245, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3139 - acc: 0.9278 - val_loss: 0.4297 - val_acc: 0.9225\n",
      "Epoch 409/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.9291\n",
      "Epoch 409: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3428 - acc: 0.9291 - val_loss: 0.3815 - val_acc: 0.9184\n",
      "Epoch 410/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3542 - acc: 0.9265\n",
      "Epoch 410: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3542 - acc: 0.9265 - val_loss: 0.3766 - val_acc: 0.9168\n",
      "Epoch 411/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3070 - acc: 0.9267\n",
      "Epoch 411: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3070 - acc: 0.9267 - val_loss: 0.3506 - val_acc: 0.9151\n",
      "Epoch 412/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.9286\n",
      "Epoch 412: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3527 - acc: 0.9287 - val_loss: 0.4011 - val_acc: 0.9164\n",
      "Epoch 413/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9258\n",
      "Epoch 413: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3123 - acc: 0.9257 - val_loss: 0.3976 - val_acc: 0.9176\n",
      "Epoch 414/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.9246\n",
      "Epoch 414: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3508 - acc: 0.9249 - val_loss: 0.3837 - val_acc: 0.9119\n",
      "Epoch 415/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.9292\n",
      "Epoch 415: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4054 - acc: 0.9292 - val_loss: 0.3781 - val_acc: 0.9164\n",
      "Epoch 416/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.9252\n",
      "Epoch 416: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3310 - acc: 0.9251 - val_loss: 0.3983 - val_acc: 0.9143\n",
      "Epoch 417/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.9276\n",
      "Epoch 417: val_acc did not improve from 0.92245\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3151 - acc: 0.9277 - val_loss: 0.3750 - val_acc: 0.9184\n",
      "Epoch 418/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.9265\n",
      "Epoch 418: val_acc improved from 0.92245 to 0.92286, saving model to train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/5/best_model.h5\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3661 - acc: 0.9266 - val_loss: 0.3849 - val_acc: 0.9229\n",
      "Epoch 419/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9251\n",
      "Epoch 419: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3225 - acc: 0.9251 - val_loss: 0.4145 - val_acc: 0.9180\n",
      "Epoch 420/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.9260\n",
      "Epoch 420: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3332 - acc: 0.9261 - val_loss: 0.3806 - val_acc: 0.9172\n",
      "Epoch 421/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.9233\n",
      "Epoch 421: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3590 - acc: 0.9233 - val_loss: 0.3762 - val_acc: 0.9164\n",
      "Epoch 422/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9256\n",
      "Epoch 422: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3102 - acc: 0.9255 - val_loss: 0.3910 - val_acc: 0.9131\n",
      "Epoch 423/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.9264\n",
      "Epoch 423: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3327 - acc: 0.9264 - val_loss: 0.3913 - val_acc: 0.9176\n",
      "Epoch 424/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.9284\n",
      "Epoch 424: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3267 - acc: 0.9286 - val_loss: 0.4007 - val_acc: 0.9111\n",
      "Epoch 425/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9274\n",
      "Epoch 425: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3076 - acc: 0.9278 - val_loss: 0.4337 - val_acc: 0.9184\n",
      "Epoch 426/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.9274\n",
      "Epoch 426: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3962 - acc: 0.9266 - val_loss: 0.3818 - val_acc: 0.9147\n",
      "Epoch 427/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.9264\n",
      "Epoch 427: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3782 - acc: 0.9264 - val_loss: 0.4658 - val_acc: 0.9135\n",
      "Epoch 428/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.9241\n",
      "Epoch 428: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4304 - acc: 0.9242 - val_loss: 0.3729 - val_acc: 0.9147\n",
      "Epoch 429/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.9283\n",
      "Epoch 429: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3950 - acc: 0.9284 - val_loss: 0.4152 - val_acc: 0.9156\n",
      "Epoch 430/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9317\n",
      "Epoch 430: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3057 - acc: 0.9316 - val_loss: 0.4047 - val_acc: 0.9099\n",
      "Epoch 431/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.9230\n",
      "Epoch 431: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3887 - acc: 0.9230 - val_loss: 0.4063 - val_acc: 0.9127\n",
      "Epoch 432/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.9250\n",
      "Epoch 432: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3753 - acc: 0.9251 - val_loss: 0.3787 - val_acc: 0.9164\n",
      "Epoch 433/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.9282\n",
      "Epoch 433: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3442 - acc: 0.9282 - val_loss: 0.3915 - val_acc: 0.9216\n",
      "Epoch 434/2000\n",
      "607/616 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.9237\n",
      "Epoch 434: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3469 - acc: 0.9240 - val_loss: 0.3617 - val_acc: 0.9164\n",
      "Epoch 435/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9246\n",
      "Epoch 435: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3025 - acc: 0.9247 - val_loss: 0.3688 - val_acc: 0.9184\n",
      "Epoch 436/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9303\n",
      "Epoch 436: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3215 - acc: 0.9304 - val_loss: 0.3609 - val_acc: 0.9111\n",
      "Epoch 437/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9243\n",
      "Epoch 437: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3093 - acc: 0.9244 - val_loss: 0.3930 - val_acc: 0.9172\n",
      "Epoch 438/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.9263\n",
      "Epoch 438: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3274 - acc: 0.9266 - val_loss: 0.3833 - val_acc: 0.9143\n",
      "Epoch 439/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.9236\n",
      "Epoch 439: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3337 - acc: 0.9237 - val_loss: 0.4043 - val_acc: 0.9176\n",
      "Epoch 440/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9267\n",
      "Epoch 440: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3359 - acc: 0.9268 - val_loss: 0.4141 - val_acc: 0.9196\n",
      "Epoch 441/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3688 - acc: 0.9275\n",
      "Epoch 441: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3688 - acc: 0.9275 - val_loss: 0.4117 - val_acc: 0.9111\n",
      "Epoch 442/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.9287\n",
      "Epoch 442: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3811 - acc: 0.9288 - val_loss: 0.3722 - val_acc: 0.9200\n",
      "Epoch 443/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9282\n",
      "Epoch 443: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2928 - acc: 0.9283 - val_loss: 0.3998 - val_acc: 0.9143\n",
      "Epoch 444/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9272\n",
      "Epoch 444: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3086 - acc: 0.9274 - val_loss: 0.3583 - val_acc: 0.9147\n",
      "Epoch 445/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9275\n",
      "Epoch 445: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3130 - acc: 0.9276 - val_loss: 0.3944 - val_acc: 0.9196\n",
      "Epoch 446/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.9271\n",
      "Epoch 446: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3509 - acc: 0.9274 - val_loss: 0.3882 - val_acc: 0.9180\n",
      "Epoch 447/2000\n",
      "608/616 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.9277\n",
      "Epoch 447: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3883 - acc: 0.9280 - val_loss: 0.4547 - val_acc: 0.9176\n",
      "Epoch 448/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.9254\n",
      "Epoch 448: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3582 - acc: 0.9255 - val_loss: 0.3635 - val_acc: 0.9107\n",
      "Epoch 449/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.9265\n",
      "Epoch 449: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3492 - acc: 0.9265 - val_loss: 0.3734 - val_acc: 0.9156\n",
      "Epoch 450/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.9287\n",
      "Epoch 450: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3384 - acc: 0.9289 - val_loss: 0.3988 - val_acc: 0.9147\n",
      "Epoch 451/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.9265\n",
      "Epoch 451: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3644 - acc: 0.9266 - val_loss: 0.3746 - val_acc: 0.9115\n",
      "Epoch 452/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.9296\n",
      "Epoch 452: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3545 - acc: 0.9296 - val_loss: 0.3897 - val_acc: 0.9143\n",
      "Epoch 453/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.9231\n",
      "Epoch 453: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3692 - acc: 0.9233 - val_loss: 0.4203 - val_acc: 0.9164\n",
      "Epoch 454/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3572 - acc: 0.9242\n",
      "Epoch 454: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3563 - acc: 0.9243 - val_loss: 0.3806 - val_acc: 0.9164\n",
      "Epoch 455/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.9289\n",
      "Epoch 455: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3568 - acc: 0.9290 - val_loss: 0.3835 - val_acc: 0.9172\n",
      "Epoch 456/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.9264\n",
      "Epoch 456: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3206 - acc: 0.9264 - val_loss: 0.4063 - val_acc: 0.9127\n",
      "Epoch 457/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.9270\n",
      "Epoch 457: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3509 - acc: 0.9269 - val_loss: 0.3969 - val_acc: 0.9168\n",
      "Epoch 458/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.9275\n",
      "Epoch 458: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3492 - acc: 0.9280 - val_loss: 0.3561 - val_acc: 0.9192\n",
      "Epoch 459/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.9232\n",
      "Epoch 459: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3364 - acc: 0.9233 - val_loss: 0.3883 - val_acc: 0.9216\n",
      "Epoch 460/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.9288\n",
      "Epoch 460: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3784 - acc: 0.9289 - val_loss: 0.3959 - val_acc: 0.9180\n",
      "Epoch 461/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.9240\n",
      "Epoch 461: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3178 - acc: 0.9239 - val_loss: 0.3778 - val_acc: 0.9212\n",
      "Epoch 462/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.9285\n",
      "Epoch 462: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3249 - acc: 0.9286 - val_loss: 0.4155 - val_acc: 0.9143\n",
      "Epoch 463/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.9268\n",
      "Epoch 463: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3429 - acc: 0.9268 - val_loss: 0.3450 - val_acc: 0.9164\n",
      "Epoch 464/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.9274\n",
      "Epoch 464: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3917 - acc: 0.9276 - val_loss: 0.3687 - val_acc: 0.9168\n",
      "Epoch 465/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3539 - acc: 0.9284\n",
      "Epoch 465: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3539 - acc: 0.9284 - val_loss: 0.3706 - val_acc: 0.9164\n",
      "Epoch 466/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.9251\n",
      "Epoch 466: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3582 - acc: 0.9252 - val_loss: 0.4270 - val_acc: 0.9107\n",
      "Epoch 467/2000\n",
      "613/616 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9282\n",
      "Epoch 467: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.2941 - acc: 0.9282 - val_loss: 0.3963 - val_acc: 0.9172\n",
      "Epoch 468/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9294\n",
      "Epoch 468: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3164 - acc: 0.9294 - val_loss: 0.3742 - val_acc: 0.9151\n",
      "Epoch 469/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.9243\n",
      "Epoch 469: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3619 - acc: 0.9243 - val_loss: 0.3977 - val_acc: 0.9123\n",
      "Epoch 470/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.9288\n",
      "Epoch 470: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3242 - acc: 0.9290 - val_loss: 0.4171 - val_acc: 0.9200\n",
      "Epoch 471/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3389 - acc: 0.9301\n",
      "Epoch 471: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3389 - acc: 0.9301 - val_loss: 0.4049 - val_acc: 0.9172\n",
      "Epoch 472/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.9218\n",
      "Epoch 472: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3535 - acc: 0.9219 - val_loss: 0.4125 - val_acc: 0.9139\n",
      "Epoch 473/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3336 - acc: 0.9266\n",
      "Epoch 473: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3336 - acc: 0.9266 - val_loss: 0.4225 - val_acc: 0.9196\n",
      "Epoch 474/2000\n",
      "612/616 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.9231\n",
      "Epoch 474: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 5s 7ms/step - loss: 0.3474 - acc: 0.9234 - val_loss: 0.3955 - val_acc: 0.9200\n",
      "Epoch 475/2000\n",
      "611/616 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.9251\n",
      "Epoch 475: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.4051 - acc: 0.9254 - val_loss: 0.4689 - val_acc: 0.9103\n",
      "Epoch 476/2000\n",
      "609/616 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.9292\n",
      "Epoch 476: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3916 - acc: 0.9295 - val_loss: 0.4448 - val_acc: 0.9147\n",
      "Epoch 477/2000\n",
      "616/616 [==============================] - ETA: 0s - loss: 0.3441 - acc: 0.9267\n",
      "Epoch 477: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3441 - acc: 0.9267 - val_loss: 0.4574 - val_acc: 0.9139\n",
      "Epoch 478/2000\n",
      "614/616 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.9261\n",
      "Epoch 478: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3474 - acc: 0.9262 - val_loss: 0.3849 - val_acc: 0.9135\n",
      "Epoch 479/2000\n",
      "615/616 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.9256\n",
      "Epoch 479: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3644 - acc: 0.9256 - val_loss: 0.4728 - val_acc: 0.9188\n",
      "Epoch 480/2000\n",
      "610/616 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.9309\n",
      "Epoch 480: val_acc did not improve from 0.92286\n",
      "616/616 [==============================] - 4s 7ms/step - loss: 0.3562 - acc: 0.9312 - val_loss: 0.4180 - val_acc: 0.9196\n",
      "97/97 [==============================] - 1s 4ms/step - loss: 0.4735 - acc: 0.9123\n"
     ]
    }
   ],
   "source": [
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(columns=range(1, 6))\n",
    "    training_time = pd.DataFrame(columns=[f'CPU_Time_{i}' for i in range(1, 6)] + [f'Wall_Time_{i}' for i in range(1, 6)])\n",
    "    \n",
    "\n",
    "    train_temp_dir = train_dir\n",
    "    train = tf.data.Dataset.load(train_temp_dir)\n",
    "    flattened_train = train.unbatch()\n",
    "    \n",
    "    train_data = list(flattened_train.as_numpy_iterator())\n",
    "    train_size = len(train_data)\n",
    "    # print(train_data[0].shape)\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(train_data), 1):\n",
    "        train_fold_data = ([train_data[i][0] for i in train_index], [train_data[i][1] for i in train_index])\n",
    "        val_fold_data = ([train_data[i][0] for i in val_index], [train_data[i][1] for i in val_index])\n",
    "        \n",
    "        train_fold = tf.data.Dataset.from_tensor_slices(train_fold_data).batch(16)\n",
    "        val_fold = tf.data.Dataset.from_tensor_slices(val_fold_data).batch(16)\n",
    "        \n",
    "        log_path = os.path.join(log_dir, str(fold))\n",
    "        \n",
    "        model = create_model()\n",
    "        model.summary()\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(momentum=0.01), metrics=['acc'])\n",
    "        \n",
    "        cpu_start = time.process_time()\n",
    "        wt_start = time.time()\n",
    "        \n",
    "        history = model.fit(train_fold, epochs=epochs, validation_data=val_fold, callbacks=myCallbacks(log_path))\n",
    "        \n",
    "        wt_end = time.time()\n",
    "        cpu_end = time.process_time()\n",
    "        wall_time = wt_end - wt_start\n",
    "        cpu_time = cpu_end - cpu_start\n",
    "        \n",
    "        training_time.loc[f'CPU_Time_{fold}'] = cpu_time\n",
    "        training_time.loc[f'Wall_Time_{fold}'] = wall_time\n",
    "        \n",
    "        recap.loc[fold] = history.history['acc'][-1]\n",
    "    \n",
    "    # Evaluate on the test dataset after cross-validation\n",
    "    test_temp_dir = test_dir\n",
    "    test_ds = tf.data.Dataset.load(test_temp_dir)\n",
    "    results = model.evaluate(test_ds, callbacks=myCallbacks(log_path))\n",
    "    \n",
    "    recap[f'test'] = results[1]\n",
    "    \n",
    "    log_recap_dir = os.path.join(log_dir, 'Recap')\n",
    "    if not os.path.exists(log_recap_dir):\n",
    "        os.makedirs(log_recap_dir)\n",
    "    \n",
    "    recap.to_csv(os.path.join(log_recap_dir, 'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_recap_dir, 'Training_time.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "test_dir = \"datasets/tf_batch/fft_relative/segment_1 seconds/test\"\n",
    "test_ds = tf.data.Dataset.load(test_dir)\n",
    "model_dir = [f\"train_logs/logs7/FFT_Relative_ANN_512_256_RMSprop/{i}/best_model.h5\" for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_test = test_ds.unbatch()\n",
    "test_data = list(flattened_test.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_value = np.array([test_data[i][0] for i in range(len(test_data))])\n",
    "test_data_label = np.array([test_data[i][1] for i in range(len(test_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3080, 80)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_value.reshape(test_data_value.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 1s 5ms/step - loss: 0.3335 - acc: 0.9114\n",
      "0.3334595263004303 0.9113636612892151\n",
      "97/97 [==============================] - 1s 3ms/step\n",
      "97/97 [==============================] - 1s 4ms/step - loss: 0.3429 - acc: 0.9130\n",
      "0.34287992119789124 0.9129869937896729\n",
      "97/97 [==============================] - 0s 3ms/step\n",
      "97/97 [==============================] - 1s 3ms/step - loss: 0.3625 - acc: 0.9120\n",
      "0.36248549818992615 0.9120129942893982\n",
      "97/97 [==============================] - 0s 3ms/step\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 0.3436 - acc: 0.9068\n",
      "0.34364187717437744 0.9068182110786438\n",
      "97/97 [==============================] - 0s 4ms/step\n",
      "97/97 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.9146\n",
      "0.4044612646102905 0.9146103858947754\n",
      "97/97 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, model_path in enumerate(model_dir):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(loss, acc)\n",
    "    pred = model.predict(test_data_value.reshape(test_data_value.shape[0], -1 ))\n",
    "    results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62991303, 0.65858334, 0.64691746, ..., 0.64113957, 0.64485747,\n",
       "       0.62906283], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].reshape(results[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_data_label, pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'autism'])\n",
    "    cm_display.plot()\n",
    "    plt.title(f\"Result: FFT ANN for model in fold {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: FFT ANN for model in fold 1\n",
      "Precision: 0.9105545617173524\n",
      "Recall: 0.9653864390706496\n",
      "Accuracy: 0.9113636363636364\n",
      "F1-score: 0.9371691599539701\n",
      "\n",
      "\n",
      "Result: FFT ANN for model in fold 2\n",
      "Precision: 0.910022271714922\n",
      "Recall: 0.968705547652916\n",
      "Accuracy: 0.912987012987013\n",
      "F1-score: 0.9384474046853468\n",
      "\n",
      "\n",
      "Result: FFT ANN for model in fold 3\n",
      "Precision: 0.9147111913357401\n",
      "Recall: 0.9611190137505927\n",
      "Accuracy: 0.912012987012987\n",
      "F1-score: 0.9373410404624276\n",
      "\n",
      "\n",
      "Result: FFT ANN for model in fold 4\n",
      "Precision: 0.9038120567375887\n",
      "Recall: 0.9668089141773353\n",
      "Accuracy: 0.9068181818181819\n",
      "F1-score: 0.9342497136311569\n",
      "\n",
      "\n",
      "Result: FFT ANN for model in fold 5\n",
      "Precision: 0.9191643960036331\n",
      "Recall: 0.959696538643907\n",
      "Accuracy: 0.9146103896103897\n",
      "F1-score: 0.9389932730225005\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "    pred = np.round(res.reshape(res.shape[0])).astype(bool)\n",
    "    test_data_label = test_data_label.astype(bool)  \n",
    "\n",
    "    print(f\"Result: FFT ANN for model in fold {i}\")\n",
    "\n",
    "    TP = np.sum((test_data_label == True) & (pred == True))\n",
    "    FP = np.sum((test_data_label == False) & (pred == True))\n",
    "    TN = np.sum((test_data_label == False) & (pred == False))\n",
    "    FN = np.sum((test_data_label == True) & (pred == False))  \n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
